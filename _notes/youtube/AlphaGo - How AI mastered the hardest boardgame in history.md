---
note_type: metamedia
mm_source: youtube
mm_url: https://www.youtube.com/watch?v=MgowR4pq3e8
---

# Video

AlphaGo - How AI mastered the hardest boardgame in history - YouTube
![](https://www.youtube.com/watch?v=MgowR4pq3e8)

Transcript:
(00:00) so about two weeks ago the alphago team by google deepmind they published their latest paper in the alphago series this time it's called alphago zero and I want to dive into some of the technical details that make this version of alphago so much better than the previous version that beat lee sedol you ready to dive in deep my name is Andrew and welcome to archive insights so I want to go over five of the main changes that were implemented in the latest alphago P so the first thing is that in comparison with the previous version
(00:41) alphago zero trains entirely from self-blame that means that it does not use any datasets from human professional goal players it learns the game of go entirely from scratch using only sethbling the second thing is that the previous version used a lot of predefined features that were handcrafted for the game of gold the new network alphago 0 uses none of those features and learns entirely by observing the board state another interesting thing for me is that they change from a normal convolutional architecture based on inception to a
(01:12) completely residual architecture I guess ResNet is here to stay another major tweak to the network is that instead of having a separate policy and evaluation network the two are now combined into a single large network that does both of these things and then finally they replaced the Montecarlo rollouts with a simpler two research approach that uses this single network to both do value prediction and to come up with very strong moves so let's go over all of those tweaks one by one so we start out with a board position in
(01:43) the game of gold the game of go consists of a 19 by 19 grid so you have 19 by 19 squares every single one of them containing either a white stone a black stone or nothing at all what they decided to do is they create a separate feature map for the white stones and for the black stones so that means that for all of the white pieces on the board you have a 19 by nineteen binary matrix which just says a1 whenever there is a white piece on that position and is zero when there is not a white piece there in the black
(02:13) feature map you do the same thing a 1 whenever there's a black stone any zero and there's nothing anyway now instead of simply having those two planes that actually represents the current position of the board they also include seven other feature planes which represent the seven past board States so they include a little bit of a history of the game inside in these board representation I was kind of surprised as to why they did this so I asked the question on their Q&A on reddit and the reply I've had is
(02:40) that this history actually acts as some kind of an attention mechanism so if you also include the past moves of your opponent then your network can actually focus on what the reporting was playing and this works as some kind of an attention mechanisms are kind of interesting additionally there are also some ghosts specific rules that require you to take into account the recent history in order to play moves that are allowed by the game so we end up with this 19 by 19 by 16 board representation eight feature maps with the white stones
(03:11) and eight feature map for the black stones all of which are binary numbers either 1 or 0 and then in order to play the game you need one final thing you need to know whose move it is is it White's move or is it black speed and so for this in theory you only need a single bit but the google deepmind team they actually decided to duplicate this bit and to actually duplicated over the entire 19 by 19 grid the reason for this is mainly an implementation detail because if you want to feed this whole representation 19 by 19 by 17 to a
(03:42) convolutional network it's easier if you can just duplicate that last bit over the entire future frame so we take this input board representation and then we're going to feed it to the network so the previous version of alphago actually used a standard convolutional architecture whereas this new version of alpha by 0 they replace the standard convolutions with the residual network so that means that in every single layer there is actually a pathway from the input directly to the output without any convolutions being applied to it and the
(04:12) major reason why residual connections not to work so well is because they allow the gradient signal that is used for training this network to actually pass straight through the layers so if you are early on in your network training process and your convolutional network is not really doing anything useful yet you can still allow your useful learning signals to pass through those layers in order to fine tune other layers so we take our board's representation run it through the residual network and then we end up with
(04:39) this feature vector and now from this feature vector we need to create two things the first thing they create is the value representation this is simply a number between 0 & 1 that represents how certain the alphago network is that it is going to win the current game the second part of the network output is the policy vector and this is actually a probability distribution over all the possible moves that alphago can play given the current position and obviously we want to train our system to play very good moves so the whole board
(05:09) representation actually needs to lead to high probabilities of good moves and low probabilities of very bad moves that is the whole goal of training the system ok so now that we've looked at the network architecture going from a board representation through the residual connections onto a value representation and in policy vector how do we train this thing if we look at the version of alphago the beat lee sedol this version was actually trained in two stages the first stage used supervised learning on a data set of professional
(05:39) go moves whereas the second stage kind of fine-tune that pre trained network using self play the new version of alphago zero uses no data set whatsoever it does not use any professional gameplay from humans it learns entirely by self play and for people who have a little bit of a background in reinforcement learning this might sound very surprising because many of you will know that if you train a network using only self play your network becomes very very unstable this was actually the top question on the reddit Q&A and the
(06:12) answer the alphago deep mine team gave is that it is actually the Monte Carlo tree search that stabilizes the self weight training process so well so the training stage goes as follows you get a board representation you run it through your net which will initially contain random weights and you get this output let's look at the policy vector in the beginning this policy vector will be random so what you do is you select a bunch of these possible moves and you select the ones that have the highest probability so you're gonna assume that
(06:43) the ones that have high probabilities are also potentially the strongest moves you select those moves and then based on those moves you get a bunch of different game states because you're going to play each and every single one of those moves from your top policy vector results and you're going to actually simulate playing those moves you end up with a bunch of new board states and then for every single one of those board states you're going to repeat the process and so in the end what ends up happening is
(07:10) that your current board position that you're evaluating is actually going to explode into this gigantic tree of possible board states that's going to expand and expand as you run through more and more simulations and the idea is that if you explode this search tree you can go on to a certain depth because obviously the explosion is going to be exponential and you are limited by the computational power of your infrastructure so the alphago team they decided to play about 1,600 simulations for every single
(07:41) board evaluation so that means that for every single board state you're gonna run the Monte Carlo tree search until you have exactly 1,600 simulations and at that point we're going to use your value network so the value network is going to decide okay which of these board positions are actually good ones which ones am i potentially going to win and then you can back up all those values all the way up to the top of your network and then you have this very solid estimate of which moves are strong and which moves are not so strong and to
(08:09) give you an example of this they included a very interesting graph in the paper which shows you the performance of the alphago Network in different versions so you can see the light blue color is the alphago version that won against Lisa doll but then the interesting thing is the the lower bar on the left side the gray bar because that one actually represents the e low strength of the network without using Monte Carlo tree search so if you were to take the network as it is trained fully trained but you're only using it once so you
(08:40) took a board position you run it through the network and then you select the single best move from that policy vector without doing any one to call it research you end up with the strength that you see there in the graph so even though we deep learning enthusiasts might think that this is a great victory for deep learning I think we should keep in the back of our minds that raw computational force by actually using a lot of those simulated board states is still a very big part of the solution an additional note to that is that this
(09:10) procedure is very specific to a game like go where you have perfect information games and you also have a perfect simulator because in this case in the game of Go you can select a potential move and then you can simulate everything that would happen down that path in the search tree but in the real world that is usually not possible so having a perfect simulator in this case is a very very big advantage that you normally wouldn't have in the real world and let's take a look at some graphs in the paper here you can see the training
(09:41) procedure of the alphago 0 network when compared to a purely supervised learning approach and what is very interesting to me is for example the middle one it shows you the prediction accuracy that the network has on a data set of professional moves so it turns out that you can actually see in this graph that alphago plays the game of gold differently than a human professional work but also noting Li if you look at the right graph you can definitely see that the Alpha by 0 network is much better at predicting the outcome of the
(10:12) game you know whose side is going to win by the black based on a current board position and this graph is also very interesting because it shows you what the effects are of transitioning from a normal convolutional architecture to residual network so the if you look at the left graph the blue the blue bottom bar on the right side is the original network that we in the previous version and then the red one is actually where they combine both the policy vector and the value representation in a single network using
(10:41) only the same convolutional stack so you can see a major improvement in evil rating there the same improvement or almost the same improvement is made when you go from the original paper to a residual architecture so by just switching out the normal convolutions with residual connections you actually get a very strong improvement as well and then if you combine the two in a single network both using residual connections and using the combination of value representation and policy vector then you get the purple bar which is the
(11:12) final system and then to finalize here this is very interesting graph as well it shows you a couple of the very known moves in the game of Go so the game of Go has been you know it's been around for thousands of years and there are some very strong potential moves that are known and things like you know the Knights move or one space jump or a 3/3 invasion these things they have gotten names because they are very known and strong tactics and you can see for some of them that alphago actually discovers them using only self play but then after
(11:41) a while it figures out stronger tactics and it doesn't use these as much anymore so kind of interesting to see that right so that was my take on the alphago zero paper amazing results from the google deepmind team I hope you all learned something if there are questions that I didn't address please post them in the comments down below and I hope to see you next time in the next episode of archive insights

## Keywords