---
note_type: metamedia
mm_source: youtube
mm_url: https://www.youtube.com/watch?v=N1WwNnamXhM
---

# Video
T8.5: RSA-Svl: A Rational Speech-Act Model for the Pragmatic Use of Vague Terms in Natural Language
![](https://www.youtube.com/watch?v=N1WwNnamXhM)

## Transcript:
all right so without further Ado so
today I'm going to present uh RSA svi
which is a model for the use of pragmac
of the pragmatic use of big terms uh in
natural language
let me start with the puzzle if someone
tells you that n is not very tall you're
likely to infer that she's actually
rather short not tall but if someone
tells you that the table is not very
dirty you likely conclude that the table
is somewhat dirty and so the puzzle here
is we have two sentences with very
similar structure not very pleasant
adjective and in the first case we
derive a negative inference not tall in
the second case a positive inference
rather somewhat dirty
and of those two cases two is actually
expected we can explain it as a
structural implicature not very dirty
competes with not dirty which is shorter
and more informative and so by gracian
reasoning or whatever mechanism you like
for implicature you negate the
alternative not not dirty and you get
the inference somewhat dirty
so the person is why doesn't this happen
with one and what we argued at this time
is that the relevant difference is that
tall is vague where has 30 is not and
indeed if you replace tall with taller
than Jon for instance which is not vague
then the implicature comes back and is
not much taller than Jon you get the
inference that n is somewhat taller than
John
so the explanation we gave at the time
was that deodorants not very tall and
it's alternative not tall are both fake
and what happens is that you can't
really satisfy both at the same time you
can't have a height that would be
clearly not very tall but also clearly
tall
and so we generalized the notion of
borderline contradiction that had been
introduced by Ripley for sentences like
tall and not tall and we argued that
basically any sentence with vague terms
that cannot be clearly true for any
height for instance would be a
borderline contradiction and so tall and
not very tall would be such a sentence
and then we generalized the notion of
innocent exclusion which is which comes
from the grammatical theory of
implications uh which is designed to
avoid classical contradiction and we
argue that we can just uh postulate that
it also avoids borderline contradiction
and not just classical contradictions
the problem with that explanation there
are a few
the first one is it's hard to quantify
what counts as a borderline
contradiction because the problem with
weakness is that not only you can't
clearly distinguished tall people from
not tall people but you can't even
distinguish clearly tall people from
borderline tall people and so on and so
forth that's what we call the problem of
higher order vagueness
so it would be very hard to quantify
when exactly you have a sentence that is
a borderline contradiction
the second issue that it will also
require a very specific interface
between semantics and pragmatics because
you would need the X operator which is a
semantic operator to be sensitive to
pragmatic factors such as the
distribution of the threshold that's not
impossible but it would have to be
spelled out explicitly and the last
issue is empirical that doesn't explain
some effects of focus so if you stress
vary if you go back to the first
sentence but you now say n is not very
tall then you get the implication again
that she's somewhat tall
whereas you don't get that for other
cases where innocent exclusion was meant
to block an implication so if you say
answer bill or Chris you don't get the
implicator at the and didn't see Bill
which is what you would get if Focus was
enough to overcome innocent exclusion
so the RSA svi is a an attempt to is a
model which will recast basically the
same kind of explanation informally but
within a quantitative pragmatic model
without having to encode the sensitivity
to vagueness in the semantic operator
it builds on the rational speech
framework which Irena thanks thankfully
introduced for me so I won't have I can
skip a bit of that but unlike previous
work in this framework our goal will not
be to explain why there is vagueness in
language we'll take that for granted and
we'll try to explain how it affects
communication and in particular how it
interacts with implications
and there are three key ingredients to
the model the first one is second order
vagueness which I mentioned a second ago
the idea is that when you have a vague
adjective you don't have just
uncertainty on this threshold of the
adjective but also higher level
uncertainty on the Distribution on that
threshold of that threshold
the second ingredient is super
evaluationism which I'll come back to in
a second and the third one is
grammatical existification so thankfully
again I don't have to introduce that the
idea is that you have an operator which
is a bit like a silent only that can
derive the implications directly in the
grammar in line with some formal a lot
of formal semantic pragmatic literature
but also recent RAC work and crucially
we won't encode in this operator the
sensitivity to vagueness will stick to
the classical definition of the X
operator
now back to Super evolutionism this is
an old idea from the philosophical logic
literature
um and the idea is that some sentences
can be underdetermined because they have
vagueness or some other properties and
those sentences can be precisified in
multiple different ways and we say that
the sentence is true if and only if it's
true under any Precision to take an
example a plural definite like the books
would have would be under determined
between an existential and a universal
interpretation and so when you say Sue
read the books this is true if and only
if it's true on the both interpretation
which means that Sue has to have read
all of the books whereas the negations
who didn't read the books would be true
if and only if not only did she not read
all of the books but she didn't read any
of the books and so if she read some but
not all of the books neither sentence is
true
Specter 2017 proposed to implement this
in the RSA model within the RSC
framework and the idea is to introduce
multiple interpretation functions which
correspond to the different
personification of super evaluationism
and you encode the you parameterize the
literal listener l0 with this so the
posterior
um to have a pointer so the posterior on
the world given a message and an
interpretation function function would
be proportional to the usual pry on the
world and the truth of that sentence in
this world given this interpretation
at the next step the pragmatic speaker
will now consider the average utility
across all possible interpretation so
instead of just looking at the log l0
the informativity we have in the RSA
framework we take the average across all
possible interpretation functions with a
certain prior on them
so and then the next step we take the
usual softmax function to to transform
this utility function into a probability
distribution of a messages you may ask
what is the link with super evolutionism
the idea is that if you assume that if
you imagine that one of the
interpretation makes a message U false
in World W then the posterior for w for
the l0 literal listener parametrized by
this elf given this message will be zero
and so the log of that will be minus
infinity and when you fit that into the
average utility this means that the
whole thing will turn return minus
infinity so the utility of this message
in this world is minus infinity and the
property of use is zero by
contraposition if they want the message
to be usable if you wanted to have a
priority of use Superior to zero you
need to make sure that all of the
interpretations makes the message true
otherwise the priority of use will be
zero in practice in my model I would be
dealing with vague terms so the
interpolation functions will be graded
so they won't really reach zero but
because of the way the log works as soon
as you have a few interpretations that
have a priority of truths close to zero
this will drag down the utility a lot
and interpretations which are true will
basically just add some zeros in the
utilities so they won't compensate for
that
um for the semantics I assume the the
usual degree semantics so Anna is tall
would mean something like with a silent
pause operator which turns the adjective
into a predicate and this means that the
measurement of and by the adjective
typically her height would be above a
certain threshold Theta which at this
point is a free variable
for intensifiers words like vary I
assume that they do the exact same thing
except they maybe add a little variable
Delta which means that they increase a
little bit the threshold and we can
discuss how we derive the value of this
Delta but this is not the point of this
model today
the X operator so the exhaustification
operator in particular with negate the
not pass adjective so the not tall
alternative to something like not very
tall and so the exhaustive interpolation
of Ana is not very tall would be that
Anna's height Falls someone between
betweens Theta and theta plus Delta
on top of that I'll turn the model into
a Grady through semantics I am as I
mentioned earlier by looking at the
probability distribution on those three
parameters Zeta Delta and so on
parametrized by your big Theta
um I'll write that like this so for
instance the interpretation under a
certain big seat of the not very edge
sentence when we want to convey a degree
D and we have an exhaustive parse would
just be the probability that defaults
between Theta and step plus Delta and if
you really don't like Grady through
semantics you can keep the semantics
binary and move that step to the RSA
model by introducing an s0 it's formally
equivalent so it's really just a matter
of taste the next step is to define the
elliptical listener here I will
parameterize by this parameter big Zeta
and a parse so in particular whether the
sentence is exhaustified or not and I
assume that happens at the grammatical
level
this looks like the usual formula for
the RSA so the probability uh the prior
on the world information here it's a
degree and the interpretation of the
sentence given to the other three
parameters note that here this is not
just zero one it can be also anything in
between because it's actually a
probability already
the next step is to take the utility of
a message parse pair so if we have
message U Enterprise I we just average
over the big Theta and big Theta is a
continuous parameter so this average is
actually an integral or even a multiple
integral but the idea is exactly the
same as the sum inspectors model
uh note that here we introduce a
priority Distribution on Theta just like
Specter had a priority Distribution on
the interpretations interpretation
functions and this corresponds to second
order weakness because Theta quantifies
uh parameterizes the distribution of
literacy that's the the threshold for
the adjective so we have uncertainty on
little Sira and on top of that we now
have uncertainty on big Theta the second
order uh so that's why we call it second
order and we won't go any further so
that will be the highest we go crucially
here we're averaging over big Theta and
not little Theta because if we did this
by averaging over a little see that we
would end up with a basically a
trivalent model the old-fashioned uh
supervolutionist model where you can say
tall if and only if you're 100 sure that
under any interpretation you're clearly
Above This threshold so here we want to
keep the gradedness the fuzziness of
weakness so we we do that at the next
level on at the level of second order
weakness and the other thing to notice
is that we are not averaging over parses
so the I variable is still a free
variable at this point because we apply
the final and Bergen Global's intention
model Global intentions model
which assumes that the the pragmatic
speaker S1 picks a pair of message and
parts that maximizes utility and they
assume that it's a literal the sorry
it's a listener's job to retrieve the
parts they just pick the parts that they
like best
at the final step we have the pragmatic
listener who will try to retrieve both
information about the world the degree
and the bars from a message and they
have they do that in a variation way as
usual and note that the parses of course
depend on the message because different
messages have different parses so this
probability distribution will be I'll
just assume it's uniform but it depends
on the message
okay and just so you know the isvi
corresponds to Super versions The
intentions
the next step is to evaluate the model
of course so we test the model on the
data from left volatile experiment one
and the tested adjectives tall and late
tall uh well totally stole late as in
late for work which is a minimum
standard adjective
sorry
yeah they tested a number of
constructions what's important for us is
that had the bare adjective very
adjective not adjective and not very
adjective they tested at different
points so assuming that works start at
9am they had sentences like Donna showed
up to work at 8 40 or 8 48 am and Mary
said tuna was not very late then
participants answer with a slider
whether they disagree or they agree and
the idea is that if they don't derive
any implicature this is actually true
because she arrived on time or even
earlier so it's not the case that she
was very late but if you derive the
implicature late but not very late then
this sentence become unacceptable
because the implication is false
all right so um I'll just go through the
linking hypothesis we assume following
Leicester and Goodman that the
acceptability of a sentence in such an
experimental design just track the
probability that the message is true but
because we have different parses in
particular exhaustive parses we assume
that we we take each pass we take the
average over the different parses and we
have a pragmatic component here because
we take the posterior at the L1 level
for parsis given messages
um and the last thing we need is the
distribution P of theta because the
model assumes that there is weakness but
it doesn't actually derive it so we need
to fit this information to the model and
we do that by just measuring it on the
affirmative sentences Edge and very edge
so the process to evaluate the model is
a bit complicated because we start with
the affirmative sentences like X is tall
X was very late Etc we fit that to a
first model which extracts hyper
parameter for the vagueness so the
parameter that determine the
distribution of big Theta
um and that gives us the basically the
vague the vague denotation with first
order and second order vagueness
given that we can compute the expected
informativity of every message given
every degree
um and we fit that to a second model
which tries to fit the negative
sentences X is not tall not late Etc and
not very tall not very late by adjusting
the parameters of the RSA svi models so
the rationality parameter and the cost
parameters with the hierarchical model
over participants
the model gives a very good fit of the
data thankfully so on the top of the
screen you have the not very tall cases
where you kind of see those downward
sigmoids which is what you expect for a
gradable expression negative expression
and for late we see a different number
of patterns so some participants for
instance clearly derive the implications
so they reject not very late when the
person is early some of them fully
accepted and some are in between so
there's a pattern an array of different
behaviors but the model can capture all
of them just by assuming different uh
cost and rationality parameters
importantly the posterior distribution
for the X bars for not very tall is much
lower than that for late and we can
explain why it's not even zero
um and the next thing we want to do is
that we want to make sure that the model
is actually responsible for the good fit
because don't forget that I fed it all
the information about the vague
denotations and each individual
participants interpretation of the
adjective so it has a lot of information
to begin with so to make sure that this
is there's a real contribution from the
model I compared it to a literal
Baseline
um and I had also some concerns about
overfitting and biases so I compared uh
I adjusted the number of cost parameters
that go into the message and the number
of parses it considers because we can I
skipped some details about each parses
but when we do that what happens that
every implementation of the RSA SBI
fares much much better than the literal
Baseline model so to conclude uh not
sure I'm having during this time but
yeah I should finish maybe so the model
incorporates uh recent advances from the
RSC framework in particular the global
intention and supervolution is the
racing model together with some old
insights from formal semantics and
Fields of ecologic in particular higher
order vagueness and together this gives
us a very good account of the
interaction between vagueness and
implications both quantitatively and
qualitatively and I focused on this like
not very adjective construction which is
quite specific but the model makes very
general predictions about any sentence
with vague terms so empirical now I'm
looking at cases like all of the tool
students left which have also some
interested okay
um properties and the main limitation of
the model is it needs quantitative input
for vagueness so we need to either
measure that or derive that from another
model there are some candidate
quantitative models that have been
proposed the RSA elasticity and Goodman
model but also the Som
but those only make a prediction about
first order weakness so we need to
expand them a bit but if we do that that
would immediately make the RSS VI more
practical because we could just feed it
some data and it would make immediately
predictions okay thank you and the code
is available online yeah


## Keywords:
