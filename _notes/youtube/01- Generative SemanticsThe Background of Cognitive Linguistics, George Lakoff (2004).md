---
note_type: metamedia
mm_source: youtube
mm_url: https://www.youtube.com/watch?v=umF-jv3AQtY
---

# Video
01- Generative Semantics:The Background of Cognitive Linguistics, George Lakoff (2004)
![](https://www.youtube.com/watch?v=umF-jv3AQtY)


Transcript:
(00:00) What I am going to do in the next few lectures   is go over a lot of work that I’ve done. I can’t believe my work has spanned over   the last 41 years. I can hardly believe I have  been doing linguistics for 41 years. But let me   try to give you some sense of the difference  between linguistics today and linguistics 41   years ago.
(00:39) What has happened since then and why?  There is an enormous change that has come about   in linguistics and the reason is that people  simply have be-come linguists as they study   things and discover more things about language,  and the other reason is that with the development   of cognitive science, that is, the study  of the mind and the study of the brain,   we’ve learned a lot about what is it to be human  being and language makes up a great deal of what   it is to be human.
(01:14) So what we will see as we  go along is that we are discovering not merely   technical things about language but rather a  great deal about what it means to be a person.   That’s really what these lectures are about. Why is it that you study linguistics? My first   linguistics teacher was a great linguist  named Roman Jakobson. My second linguistics   teacher was another great linguist named  Noam Chomsky.
(01:42) They had two opposite views:   Roman Jakobson viewed language as central to all  human activities. What he said was this: that when   you study language you study everything, that  to study language is to study thought, to study   language is to study interpersonal communication,  to study anthropology, to study the way people   interact with one another and requires fitting  language into the web of science, fitting it into   how science works, how physics works in, let’s  say, acoustics, how the brain works, and so on.  
(02:23) It is a lesson I’ve never forgotten. And I think  it is extremely important one that we will see.   The view from my other teacher professor Chomsky  is that language is the opposite, completely   autonomous, independent of all communication,  independent of thought, independent of the body,   independent of interpersonal relationships, and  so on. I’ve never believed that.
(02:55) I’ve been more   a follower of Dr. Jakobson. And you will see  as we go along, that I think the evidence shows   that Dr. Jakobson had the better idea, but we  will see as we go along what the evidence is. Let me begin with a view of linguistics from 1963  when I first started in the field. In 1963, the   study of meaning was the study of formal logic.
(03:30)   My own training and background was at MIT, the   Massachusetts Institute of Technology, a technical  university where I studied mathematics and English   literature. And one of the things that I learned  there was that I learned to do both: the study   of meaning from the perspective of literature,  but also the study of logic, mathematical logic.   And mathematical logic has certain properties that  we inherited from Aristotle.
(04:04) That is mathematical   logic as it developed, and was a philosophical  endeavor and it assumed certain very interesting   things, things which are not necessarily true, but  interesting. These came from 2,500 years ago in   Greece, as Aristotle constructed logic. Aristotle  assumed that meaning was out there in the world,   that the meanings of words were the essences of  things out there in the world, that the mind could   directly grasp the world. Logic was logic not  of the human mind necessarily.
(04:52) It could be of   the human mind but it was the logic of the world,  and if you are smart, you could grasp the logic of   the world with your mind. That was the assumption.  And that assumption carried through up until 1963.   It was assumed that the study of logic was the  study of all rational structures in the universe,   and mathematical logic was supposed to be  the mathematics of that structure and that   rational thought limited the structure of the  universe.
(05:32) That was an assumption, just taken   for granted about the nature of reasoning and  logic. Now, logic of course, could also be a   purely mathematical endeavor. There are all sorts  of logics that are made up by mathematicians that   have nothing to do with the world. So if you are  a mathematician and you look at pure mathematical   logic, it doesn’t mat-ter.
(05:57) You can make up all  sorts of logics and there are tens of thousands   of logics that logicians have made up. But the  general philosophical assumption when I came   into linguistics was that the study of meaning  was the study of the way words fit the world.   That was the assumption how language fit onto the  structure of the world and that the structure of   the world will be characterized in terms of logic.
(06:26) Now there was another assumption that came from   the modern theory of logic and that was that  logic was formal. There was a formal structure   to logic and that formal structure has certain  properties. I will go through them in a minute   and I’ll check and see how they work. At  the same time, Noam Chomsky came along,   and in 1957 he wrote a book called Syntactic  Structures.
(07:05) And in that book he introduced a   metaphor into the study of linguistics, a very  powerful metaphor. He said that a sentence is   a string of symbols. Now when he said that he  was referring to formal mathematics, which was   the theory of simple manipulation. It is called  recursive function theory. Sometimes it is called   the theory of formal languages.
(07:34) It is a theory  of how symbols get moved around without regard   to their meaning and what regularities you  could find by studying systems of symbols.   This theory is the basis for computer science. In addition, Chomsky said that a language was   a set of such sequences of symbols and that  grammar was a system for generating that set.   When he said that, he was referring to again the  mathematical recursive function theory.
(08:04) In short,   he was claiming that a language was a purely  mechanical system for manipulating symbols in   some regular fashion by rules and that they had  nothing to do with meaning, nothing to do with   communication. As a mathematical system, it didn’t  even have anything necessarily to do with the   fact that you have a brain, a body, or anything  else.
(08:33) It was a disembodied mathematical system,   so that generative grammar, as was characterized  back in 1963, was a symbol system had nothing to   do with meaning and that you could then hook it up  to meaning by relating it to the structure of the   world and to logic. As a student of Chomsky’s,  I did a lot of work on this theory. I did many   investigations of phenomena from the point of  view of this theory, and got to know it very   well. And I soon began to suspect that it didn’t  work very well.
(09:18) The part that didn’t work very   well has to do with the claim that the structure  of language has nothing to do with meaning, that   the structure of language has nothing to do with  social interaction or contact or communication,   because it seems to me as I started looking at  examples, it has everything to do with meaning,   communication, social interaction,  relationships between people, and so on. 
(09:45) So let me try to give you some examples  of what I began to find. As I did this,   I tried to take the theory of meaning in logic  and apply it to meaning in linguistics. During   the first 12 years of my career as a linguist,  I did what was called generative semantics;   that is, I was trying to apply logic to the study  of meaning in language and account for syntax in   terms of properties of meaning. So that is what  I am going to talk about.
(10:26) Let me give you a few   examples. Many of you likely know the game  of baseball. In baseball, you hit the ball,   and there are certain verbs you use to describe  “hit,” like single, double, triple, so you can   say he doubled to left field or he doubled off  the wall in left field. Now the expressions   “off the wall” or “to left field.” Those are two  prepositional phrases very familiar to you.
(11:04) And   they are directional prepositional phrases. They  describe the direction in which something moves.  Now in Chomskyan theory, with pure symbols, if  you have a sentence like “John doubled off the   wall,” the only way you could ask how “off the  wall” fits with “double” is to have “off the   wall” just fit with this verb, the symbol.
(11:33) But  if you ask what is the general principle, where,   by which you can account for where the phrase  “off the wall” occurs, is there any generalization   about it? It appears that it has to do with  meaning. When you say “he doubled off the wall,”   the “off the wall” refers to the ball moving off  the wall. If you say “he doubled to left field,”   it means the ball went to left field.
(12:01) That is  “to left field” describes the direction of the   movement of the ball. And if you had a verb with  no movement of an object, you could not get “off   the wall” or “to left field.” You can’t say “he  slept to left field” or “he slept off the wall.”   There is no movement of an object. If I say “he  fell,” I mean “he fell” is his movement. But if   you say something like “he sat,” you can’t say he  sat to left field.
(12:36) The generalization is in the   meaning. It’s not in the grammar. But for Chomsky  everything that puts phrases together has to do   with the symbols of the grammar, not with the  meaning. And when you find a generalization about   how things fit together it is about meaning,  then that violates the theory. It violates   the theory that grammar has only to do with the  symbols that fit together, and not their meaning. 
(13:06) Let me give you many other examples of this. I  want to give you a sense of how broad this is.   Suppose that you look at certain expressions in  English that occur only with negatives, but not   with positive sentences. So there are sentences  like, take a word “ever,” e-v-e-r, “ever.” You   can say “I didn’t ever see him” but you can’t say  “I ever saw him.
(13:50) ” “Ever” only goes with negatives,   “not ever” but not simply “I ever saw him,” that  is impossible in normal sentences. Now there are   some cases, some kinds of sentences where you  have a negative meaning and a positive grammar   and a negative grammar with a positive meaning. It  is special constructions in English. For example,   if I say to you “why paint your house purple?”  There is no subject, there is no auxiliary, and   what I’m saying is “I’m suggesting that perhaps  you should not paint your house purple unless   you have a good reason.” Right? And if I say “why  not paint your house purple?” I’m suggesting you  
(14:44) should paint your house purple unless you have a  good reason not to. OK? So “why not” is a positive   suggestion and “why” is a negative suggestion in  this construction only, that is, when there is no   subject and no auxiliary.
(15:05) So “why not paint your  house purple?” and “why paint your house purple?”   Now the question is this if you have a negative  grammar with a positive meaning, there’s “ever.”   Do words like this, these negative polarity items,  do they go with the grammar or with the meaning?   The theory that says grammar is about simple  manipulations says it should go with the grammar.   The theory that says it is about meaning says it  should go with the meaning. So we test it.
(15:37) You   can say, “why not ever help him?” It doesn’t  work. But “Why ever give him any money?” is   OK. If you say there are expressions like “give a  dime.” Why can’t you say “I don’t give a dime” not   “I give a dime.” Furthermore, you can say “why  give a dime” but not “why not give a dime.” You   can say things like “he doesn’t have a red cent.
(16:16) ”  It’s an idiom that can only occur with negatives,   not “he has a red cent.” You can say “why give  him a red cent” but not “why not give him a red   cent.” That is impossible. These negative polarity  items go with the negative meaning, not with the   negative grammar. And that is profound. Because  it says the combination of words that is what   syntax is about is dependent upon the meaning of  the sentence.
(16:49) It is not merely dependent upon the   other words and the surface forms in a sentence.  It is a very deep and an important property.  There are many cases like this and let me give you  some more. Take the English distinction between   the words “who” and “which.” Now, “who” usually  refers to a person and “which” usually refers   to something that’s not a person.
(17:34) So I can say  something like “the man who I saw” or “the bicycle   which I saw,” but not “the bicycle who I saw” and  not “the man which I saw.” OK? So “who” is with   people and “which” is with non people. But what  do you say about a cat? If you have a pet cat,   what do you say about your pet cat? It depends on  how you think about your pet cat. So you can say   “my cat who always tries to fool me,” but “my cat  which weighs ten pounds.
(18:26) ” Now if you say “my cat   who weighs ten pounds,” you’re talking about how  much he eats, that he may be over eating a bit,   may have a personality like that. That is, when  you use “who” you are metaphorically attributing   personhood to the cat. If you just look at a  cat as a thing, then you use “which.” In short,   the distribution between “which” and “who” is just  part of the grammar of English, and it depends   upon the meaning, not just grammar. It depends  upon how you understand what the cat is.
(19:13) And it   doesn’t just depend upon the fixed meaning of the  word. It depends on how you understand the meaning   of the word in context. That is the crucial thing,  how you understand it in context. Now that is an   extremely important part of language: the way  context works. This is the study of what is   called pragmatics. Context has to do with how you  fit things into your understanding of the world.  
(19:52) One of the arguments that we have found, one of  the results of generative semantics is that the   study of context is not separate from the study  of meaning. Context is the semantics of the world,   of your understanding of situations.
(20:13) The semantics  of situations is the semantics of communication,   of interpersonal communication. There  are generalizations about just that.  Now let me give you some examples of such  generalizations. Suppose I have a sentence   like “can you give me a cup of tea?” Now  in English there are expressions like “can   you give me a cup of tea?” which is literally a  question about your ability, but it’s actually a   request to give you tea. Right? These are indirect  speech acts.
(21:07) I wrote a paper about this back in   1971 called Conversational Postulates. The idea  here is that there is a systematic relationship   between what you say and what you mean. There is  a system behind it. When you say “can you,” there   is a reason why that is a request rather than a  statement. There is a reason why it’s a request   and not a promise. Let me explain why. If you  look at requests such as John L.
(21:43) Austin’s or John   Searle’s, who have both writ-ten books on speech  acts, they observed that requests have certain   conditions. When you make a directive or a request  or give directions to someone to do something,   an imperative sentence, what you are doing  is this: you are assuming that you have the   right to make the request first, that is, you  are in a social position to do that.
(22:12) Second,   you are assuming that the other person is willing  to undertake this. That is at least possible,   and the other person is able to do it. You  generally don’t make requests of the things that   you know are impossible in a normal situations.  Now you might for certain reasons want to do this,   for certain rhetorical reasons you might want  to request something impossible of your parents,   for example.
(22:48) But those are special cases, and with  normal re-quests, you assume someone can do it,   that they would be willing to do it, that they  would want to do it, and moreover, if you are   making your request, it is assumed between you and  the other person that you really want the request   to be carried out. So look at the ways that you  can use other sentences to make re-quests. You   can say “I would like you to give me a cup of  tea” or “I want a cup of tea.
(23:20) ” That is, you can   express a desire. This is one of the conditions of  a request. You can request by giving the condition   of the request. You can make a request by asking  if someone is able to carry it out. That is,   being able to carry it out is a condition for  the request. And so they say “yes,” they are   able to carry it out, it is like saying “yes,  I will fulfill the request.
(23:58) ” So I say “can you   pour me a cup of tea?” You say “yes.” You don’t  just sit there. “Can you pour me a cup of tea?”   You say “yes!” and do nothing. It’s socially not  right, because when you said “yes,” you actually   said yes to “I will give you a cup of tea.” I  can say “will you give me a cup of tea?” That is,   this will happen in the future. That is a future  question about what will happen.
(24:23) It’s one of the   questions about one of the conditions. So if  you say “yes, will you?” then you’re saying in   the future you will, and that means you’ll do  it. So the conditions on requests can be stated   or questioned in such a way that it can be used  to evoke the rest of the request. In cognitive   linguistics this is called metonymy, where a  part stands for the whole.
(24:54) And the name for   this phenomenon is “indirect speech acts.” Now indirect speech acts are particularly   interesting because of the phenomena I just  mentioned about “why paint your house purple?”   If I ask you something like “why aren’t you doing  your homework?” I come and ask you this negative   question “why aren’t you doing your homework?”  That is a suggestion that you should be doing your   homework unless you have a good reason. That is  an indirect speech act.
(25:32) And “why paint your house   purple” is an indirect speech act construction.  It is a combination of the indirect speech act   with “why” and the imperative “paint your house  purple.” It’s thinking two constructions and parts   of them and putting them together. And this is  called an amalgam. It is fitting two parts of a   sentence together.
(26:05) And you fit them together under  pragmatic conditions, conditions of communication,   of successful communication. What that  means is that you cannot just by looking   at the symbols themselves, fit them together.  You have to know what they mean in a context.  And this is the same as with “can you.” Let me  give you some examples of the differences of   this.
(26:38) With “can you,” “can you open the door?” the  relationship between “can you” and the request is   part of the conventional part of the language.  It’s a fixed construction of English. It is   something you learn, you learn to do it by saying  “can you.” Now there are other indirect ways of   speaking, that are not just part of language,  but that are still understood in the context.   Let me give you an example.
(27:08) Suppose I would say  to someone sitting next to the window “it’s very,   very hot in here,” and you jump up and open the  window under these social situations. You would   feel impelled to open the window if you could.  But that’s not part of the grammar of English.   We may say “it’s hot in here” does not mean  open the window, but in this social situation,   it would mean just that. Now notice that there is  a grammatical difference between these.
(27:44) Suppose   you take the word “please” in English. The  word “please,” is used in a polite request,   but you can put “please” in different parts of the  sentence. You can say “Please, can you open the   window?” or “Can you please open the window?”  and you might think that they are the same,   but they are not.
(28:16) Right? So here is a subtle  difference in English that native speakers of   English know. It’s the difference between internal  “please” and external “please.” And I will give   you the examples. I can say “please can you open  the window” or I can say “please it’s hot in   here,” and you open the window. I can say “can you  please open the window?” but I cannot say “it’s   please hot in here,” because “internal please”  goes exactly before what I am telling you to do,   before the directive, so when it’s inside it  goes before what I’m telling you to do.
(29:01) When   it is outside, it just says I’m making a polite  request. But it’s not necessarily right before.   So that’s a special constraint on “please.” What does that mean for a theory of linguistics,   for a theory of grammar? The combination of  “please,” which is part of the grammar of English   with other words, is part of syntax, it’s part  of grammar, how you put words together.
(29:31) But you   cannot put “please” in the middle of a declarative  sentence like “it’s hot in here.” You cannot say   “it’s please hot in here.” What is the principle?  The principle is that when you put “please” in the   middle of a sentence, it must go right before  a directive that the sentence is grammatically   giving. And that is conditioned on speech acts.  It’s a condition on meaning in context.
(30:00) It is not   just how you put the words together. So here  you have another example of something in the   grammar of English that does not just depend  upon a formal system of simple manipulation.   It depends upon what you mean, how you are  communicating with something else and what are   the principles of communication.
(30:28) So this says that  pragmatics, the understanding of communication,   is entering into the grammar of English. Now these are very important results. Let   me pause for a minute to just go over them. What  they show is that the fundamental assumption made   by Chomsky in his basic metaphor. Remember the  basic metaphor? A sentence is a string of symbols,   where a symbol is independent of the meaning, it’s  a meaningless symbol, just a bunch of symbols.
(31:05) A   grammar is a set, in the sense of set theory,  a mathematically precise notion, a set of such   strings, and notice what it is a set and what  is not a set, it’s independent of context. Sets   don’t change with context. Sets are defined by  their members, strictly in mathematics. So you   can’t say here is a set, but over there it is not  a set.
(31:38) You can’t say in this auditorium something   is a set, but in the other auditorium it is not.  So you said this is the set and rules of English   generate that set without looking at meaning.  It’s false. It does not apply to real English.  All these examples we’ve just given you are cases  where meaning matters, where context matters,   and where communication matters, for when  things can fit together.
(32:02) So this is crucial   to the understanding of what is going on. Now, what was the response to these examples?   Why didn’t Chomsky just give up? The response was  to say grammaticality is not something that you   just know. A sentence like “it’s please cold  in here” could be grammatical. We can call it   grammatical and just call it semantically  anomalous.
(32:40) What you do is just redefine   what you mean by grammar or with something like  constructions like “why paint your house purple?”   They can say that’s not a core construction of  English, that it is a peripheral construction   of English, and that we’re only interested in  the core constructions. And what he did over   the years was to shrink those core constructions  until there are only thirty or forty of them out   of hundreds of constructions in English so that  the result is a theory of gram-mar that does   not apply very much. It doesn’t apply to all of  language. It just applies to just a small amount  
(33:21) of language. But you only know this if you are  inside the theory, if you know it very, very well.  Most people who read Chomsky can’t understand  him very well. If you try to read Chomsky,   it’s difficult sometimes to understand what he’s  saying. But if you know the inside of the theory,   then you know that this is what he’s doing.
(33:49) He is  shrinking the notional grammar, or he is changing   it in some way so that the theory will stay the  same, and your understanding of what grammar   is will not fit your normal understanding. So one of the commitments that I undertook   was to say I’m going to study all of language,  not just the core constructions, and I’m going   to study language as it is used in context  among people, not just language independent   of context and people. Now that is a commitment  as a scientist.
(34:27) I could have done something else,   I could have said “I’m only going to study those  things that work according to the theory. I’m only   going to study these thirty constructions, and  that’s all I will do.” People could do that, but I   refused to do it. That’s a commitment that I made  personally.
(34:48) So, as a result, what I had to do was   give a theory of how meaning could affect grammar,  and that was what generative semantics was about.  Now, in transformational grammar, generative  grammar just has to do with having some means   of characterizing. “Generating” just means  characterizing a set by some principles.   There are two very different ways in which you can  characterize an infinite set.
(35:43) You can try to have   principles that go step by step and generate  them by a sequence of steps. These are called   transformations, or you could have your principles  simply be static and give constraints so that you   can have what is called a constraint satisfaction  system. And generative semantics was a constraint   satisfaction system.
(36:15) It says there are certain  constraints that fit in the language, and so   it’s cognitive linguistics. It’s a constraint  satisfaction system. It gives certain principles,   and then in a given sentence, ten of these  principles may work at once. In transformational   grammar, the principles work one at a time  in sequence, and they require a derivation,   and that requires a first step of derivation  that is called the underlying structure.
(36:41) Now   in Chomsky’s early work, he used the term deep  structure, but he gave that up by 1967 because   I gave proofs that such things couldn’t exist.  It turned out that a deep structure in Chomsky’s   sense had to also fit meaning, and when he saw  that meaning was entering the paradigm, he gave up   that condition that deep structure determined the  meaning, and he called these initial structures   D structures. D, but that does not mean deep.  It’s just D, so a continuation.
(37:25) He dropped out   one of the conditions. Now so he has that initial  structure from which the others can be derived.  I tried working with this system for many years,  and I tried adapting it to generative semantics.   I tried to say the underlying structure was its  logical form, and that from its logical form you   could derive in various steps the surface form.
(38:03)   And in 1974, I came upon a set of sentences that   could not be done this way. These are sentences  that cannot have any deep structure or D structure   or underlying structure with transformations. Let me give you an example of such a sentence.   Let’s take a simple case, say, “I invited you’ll  never guess who to the party.” (1) Now compare   that to “you’ll never guess who I invited to the  party.” (2) So “you’ll never guess who”…. this  
(39:32) part occurs. This (sentence 2) is the normal way  to say the sentence in simple constructions of   English. “You’ll never guess who I invited to the  party” is a simple indirect question, here. This   occurs here (after “I invited”). Compare that with  “I invited John to the party.” Instead of John,   I can say “you’ll never guess who.
(40:11) ” Now this  (sentence 2) is the normal form of the sentence,   and this sentence (sentence 1) basically  means that (sentence 2). And in this sentence   (sentence 2), this piece (“I invited to the  party”) is embedded after the verb “guess,”   so this is an embedded clause. But here (sentence  1), “you’ll never guess,” which is the top clause,   is embedded.
(40:48) It is sort of embedded here  and then you have the “who” which forms   this noun phrase, and this is the top clause. This sentence was first noticed by a linguist   at MIT named Avery Andrews, who had been an  undergraduate student of mine at Harvard and   went to study with Chomsky. I got a call from Haj  Ross one day telling me this sentence. And he said   “There must be a very strange transformation that  takes this top clause, drops off the ‘who’ and   sticks this part in there.” That’s an impossible  transformation.
(41:36) In the theory of transformational   grammar, that could not happen. It was very odd. I said, “Look, the first thing you do if you are a   grammarian and you find a phenomenon that iterates  it, see if you can do two of them, three of them,   four of them.
(41:58) Check if you can embed one in terms  of the other, see how they iterate, see if there   are not two but three, four, five, etc.” So I  constructed some more examples. You can say “John   invited you’ll never guess who to you can imagine  what kind of a party for God knows what reason on   wasn’t it last Tuesday.” Now, “John invited you’ll  never guess who to you can’t imagine what kind  
(43:33) of a party for God knows what reason on wasn’t it  last Tuesday.” What is the deep structure of this   sentence, or the D structure, or the under-lying  structure? It has none. Notice that here this is   an underlying structure “you’ll never guess who  John invited”; but so is this, “you can’t imagine   what kind of a party John invited someone to.
(44:11) ”  That is also an underlying semantic structure or   deep structure, but so is this, “God knows for  what reason John invited someone to a party”;   and so is this “wasn’t it last Tuesday that John  invited someone to a party for some reason.”  That is, if you try to give a single underlying  structure for that sentence within a theory of   transformational grammar, you will fail. Try  it.
(44:47) Go home for homework if you are taking a   course in transformational grammar and just try  it. Each of these pieces underlined is part of   the highest sentence from a deep structure point  of view, an underlying structure point of view,   but they don’t fit together in any way. It’s  like you have four sentences coming together,   lots of different clauses fitting together in  a way that makes sense.
(45:20) That is, grammar has to   do with it, and this construction has to do with  fitting pieces together under certain conditions.   Now what are the conditions? Notice what you  cannot say. You cannot say “John invited Harry   guessed who to the party.” You can only say  “you’ll never guess who.” Right? So take this,   “John invited Harry guessed who to the party,” no  good.
(45:52) You say “Harry guessed who John invited to   the party,” but you can’t put it in here (after  “invite”). You can say “John invited God knows   who to the party,” but you can’t say “John  invited God doesn’t know who to the party.”   The constraint on what can go here is that it must  be an exclamation. You must be exclaiming on how   extreme this is, and only certain constructions  express exclamations.
(46:29) So you can say “God knows   when the sun will shine again,” but not “God  doesn’t know when the sun will shine again.”   You can say “you’ll never guess when spring will  come.” You know, that’s an exclamation, but if you   say “Bill guessed when spring would come,” that’s  not an exclamation. Only expressions that express   exclamations fit here, regardless of the grammar  used to express them.
(47:02) The condition is a pragmatic   condition on the grammar. It’s the condition on  what you are expressing. That should never happen   in a generative grammar where meaning is ignored,  where exclamation should not be part of how things   fit together, but it happens in English. It  does not fit any Chomskyan account of grammar.  Now let me give you another example like this.
(48:08)   There are certain kinds of constructions that   are supposed to only occur as main clauses.  So, for example, you all know […] who the   basketball player Yao Ming is. What team does  he play for? (Answer: Rockets) So consider the   following sentence “Who could stop Yao?” It is  an exclamation, and it is a question that isn’t   really a question. It is expressing a negative  statement that no one can stop Yao Ming.
(49:12) I say   “Who could stop Yao Ming?” it means no one  could stop him. As opposed to “Who stopped   Yao Ming yesterday?” … “Shaqille O’Neill!” “Who  stopped Yao Ming yesterday” is not an exclamation,   does not express a negative, but “who  could stop Yao Ming” expresses a negative.   You can say “John believes that no one can stop  Yao Ming,” but not “John believes who could stop   Yao.” You cannot make it into a relative clause.
(49:50)   You cannot say “Yao Ming is the person that John   believes who could stop.” Impossible. It  can’t be a relative clause. It can’t be a   complement of something like “believe” or “say.” Now there is, however, one place in English where   it can be embedded, and it’s in a sentence like  this. “The Rockets […] are going to win because   who could stop Yao Ming!” It occurs in this  “because” clause.
(50:50) Notice it does not occur   in every embedded clause. You cannot say “The  Rockets are going to win if who could stop Yao   Ming.” You can say “The Rockets are going to  win if no one can stop Yao,” but not “if who   could stop Yao.” That is, English has a very  interesting construction with the “because”   clause at the end of a sentence where you can  have an exclamation that expresses a meaning   that can fit in the “because” clause. The meaning  is a negative meaning.
(51:34) So under those conditions,   you can have that sentence, but you  can’t do it with “if,” only “because.”  Now think about what this means for a theory of  grammar. It means that this exclamation means   something else—it means a negative statement. That  is, if you understand the meaning of “because” and   the meaning of that negative statement, you can  express it with this.
(52:13) In a theory of grammar,   could you tell exactly where “who could stop Yao  Ming!” can occur without looking at what it meant?   The answer is no. That is, this construction  depends upon knowing that only exclamations   expressing negative statements can occur here,  and that means you have to use the concept   “exclamation” which is a pragmatic concept and you  have to use the concept “express” which has to do   with meaning. That’s the only way you can account  for this phenomenon in the grammar of English.  
(53:01) So now think about what we have. We have cases  where you have to look at indirect speech acts,   that in the grammar of English, like “who could  stop Yao!” to understand the grammar. You have   cases like “John invited you’ll never guess who  to you can’t imagine what kind of a party for God   knows what reason on wasn’t it last Tuesday” that  we just saw where there can be no deep structure.  
(53:27) You have cases where the semantics is determining  the choice of grammatical expressions. That is,   in short, the basic claim behind generative  linguistics is false, massively false. These   are not rare examples. There are hundreds  of examples like this. They are not rare,   and they were published all by 1974, so they are  in the literature, and anyone who believes in   generative gram-mar ignored them.
(54:11) They decided  they’re going to do it anyway and just ignore   these sentences and only look at core grammar. Now let’s look at another part of Chomsky’s more   recent theories. By more recent I mean after 1966.  Having started studying with Chomsky in 1962,   that seems more recent to me. In 1966, there was  a proposal made by one of Chomsky’s students Ray   Jackendoff to claim that the sentence structure  was based on heads and modifiers.
(54:51) This is the   so-called X-bar theory. And in the X-bar theory,  the assumption was that nouns were N zeros (Nº)   and verbs were V zeros (Vº), and then there  were N bars (N) and V bars (V) and so on,   and N double bars (N”), and there were certain  kind of rules that allowed one to look like   this. Look at it.
(55:24) The sentence looked like a V  double bar, and that in particular, if you look   at this part of the system, it was claimed that  the lowest N bar could take a complement. Sorry,   this is Nº and takes a complement and becomes  N bar. And then there comes the N bar, then you   could have a modifier like an adjective modifying  this N bar over here, so this is a complement and   this is an adjective (see Figure 1).
(56:08) Jackendoff’s  original argument for this had to do with the   word “one” in English. “One” can be either a  number or it can be a pronoun, and as a pronoun,   that’s indefinite. So you could say “I bought a  car and Bill bought one, too,” “I bought a large   car and Bill bought a small one,” or “Bill bought  one, too.” You say “one” means a large car. And   the idea here is you can say “I bought a car  with a V-6 engine,” here’s the complement “car   with a V -6 engine,” “I bought a large car with a  V-6 engine and Bill bought a small one, a car with   a V-6 engine” […] or you can say “I bought a large  car with a V-6 engine and Bill bought one, too.”  
(57:02) So Jackendoff’s hypothesis was that “one” could  refer back to any N bar. That was the hypothesis.  Now within a few days after that, Haj Ross and I  constructed counter examples to this, which have   been ignored up to today. Actually, it’s still  ignored. And the counter examples go as follows:   John bought a large expensive portrait of the  queen by Smith and Bill bought a small cheap   portrait of the princess by Jones. That’s one  sentence. Now where can you put “one” and what  
(59:03) does “one” refer to? You can say “portrait.” No  problem. And “Bill bought a small cheap one of   the princess by Jones,” so “one” could be an N  bar in this account; but also you can have “and   Bill bought a small one of the princess by Jones.”  And if you say “small one,” here it doesn’t mean   cheap portrait, it means expensive portrait;  or you can say “and Bill bought a small cheap   one by Jones.
(1:00:04) ” A small cheap one by Jones,  and that means a cheap one of the queen;   or you could say “and Bill bought a cheap, …”  that’s a small cheap one of the queen, “and Bill   bought a cheap one by Smith,” and that means  a small one of the princess. So basically any   combination can be used. And if you put a “one”  there and you drop out one of these modifiers,   then it takes the meaning from the previous  sentence.
(1:00:51) Is that all clear? Now the question is:   can “one” refer back to any particular node  in the tree? And the answer is no, because,   for example, you have something like “cheap  portrait of the queen” (Figure 1) versus   “cheap portrait” (Figure 2), you can use one  for either of those with the single node,   but in fact you could have it refer to either one  of these, and there is no tree that will have a   single node in it.
(1:01:51) So even if you had “cheap  portrait of the queen” (Figure 3), there would   be no node that this “one” would refer back to.  So in short, there can be no rule for the meaning   of “one” that refers back to just a single node  with the meaning of that node. That is, it is not   a possible rule of transformational grammar. Now that does not mean there is no rule for   it.
(1:02:30) And in fact, in the Neural theory of  grammar, there is a perfectly good rule for   it and it is a very simple rule. In the Neural  theory of grammar, what you are doing here,   if you have “one,” is the following: notice first  that in these sentences, you have opposites,   “large” and “small,” and “expensive” and “cheap,”  as well as things that are mutually exclusive that   can not both hold—that is “of the queen” and  “of the princess,” “by Smith” and “by Jones.
(1:02:59) ”   These are mutually exclusive meanings. In a Neural  theory of language, how do you express something   that is mutually exclusive? The answer is mutual  neural inhibition. Neural inhibition is when a   neuron fires and it stops another one from firing  and there are certain circuits where you have two   neurons and they are connected, and so any one  or if either one fires, it stops the other.
(1:03:30) So   mutual inhibition is the way you characterize, you  know, things that are semantically incompatible   in a Neural theory. So if you have a Neural  theory and you have a sentence like “John   bought a large expensive portrait of the queen by  Smith and Bill bought a cheap one by Jones,” this   “one,” will refer back and activate the meaning  of everything in this previous noun phrase except   what is inhibited by what’s in the noun phrase.
(1:04:24)   So “cheap” will inhibit “expensive” if you have   “cheap,” and “by Jones” will inhibit “Smith,”  and then the rest will just work. That is,   this will refer back to “large” and “of the  queen,” and they will be fitted together in a   neural theory. This should not be obvious. It’s  something we haven’t discussed in Neural theory   yet and we are going to discuss it a bit, but  this is how that will be done in the contemporary   cognitive theory of construction grammar based  on a Neural system.
(1:04:57) That is, these sentences   work this way because we are human beings that  have brains where mutual inhibition is possible.   They cannot work in any transformational theory  where you have to look at the syntax of “one.”  Let’s look more generally at X-bar theory. Now,  we’ll show that there are sentences where X-bar   theory is not possible. There are sentences  where you cannot have X-bar theory.
(1:05:31) So let   me give you an example like that. Here “The  Rockets are going to win because who could stop   Yao Ming.” First you have to understand the claims  of X-bar theory. If you have an X-bar theory,   it says that a verb agrees―in English you have  verb agreement―with the head of the noun phrase.   The verbs agree with the heads.
(1:06:20) In terms of  selection, the verb selects the meaning of   the head, it selects the head that will go  with it. So for example, if you have a verb   like “believe.” “Believe” requires a human being. You could say “the tall man who I saw yesterday   believes so and so,” and “man” is what it  selected. If I say “the large car that I saw   yesterday believes so and so,” that is no good.  So it’s the head that determines the selection.  
(1:06:54) And that’s not just true of subjects, it’s also  true of objects. Secondly the modifier, like the   adjective, modifies the head, if I have something  like “a large car,” “large” is the modifier,   “car” is the head, “large” modifies “car.” All  X-bar theories follow these principles. If they   don’t follow these principles, they are not an  X-bar theory. So those are the basic principles. 
(1:07:24) Now consider a sentence like “I picked up and  drank a large delicious cup of tea,” and the   question is: what is the head of “cup of tea”?  Is “cup” the head or is “tea” the head? There   are two possible X-bar structures here. In one of  them, you have “cup of tea,” where this phrase is   the head. So here you have an N zero and  then you have some other structures here,  
(1:08:41) whatever it is (See Figure 2). In the other  theory, in the other analysis, “cup” is the head,   an N zero, and then a prepositional phrase  “of tea,” and this is a complement and this   is an N bar (see Figure 3). So those are two  possible analyses, and under X-bar theory, one   of these must be right.
(1:09:27) And since there’s only one  of these in the sentence, and one must be right,   the other must be wrong because you can’t have  two analyses at the same time. But you do. In   this sentence, neither of these could work. Why?  Notice “pick up” is a verb, the verb selects the   head. What does “pick up” select? I picked up the  cup. So that says “cup” should be the head. But I   also drank the tea, so that says “tea” should be  the head. You can’t have both at once, but you do.  
(1:10:20) And look at “large.” “Large” is a modifier.  What does it modify? It modifies the head,   “cup.” “Delicious” is a modifier. What does it  modify, the head “tea.” X-bar theory can’t work   for that sentence or any sentence like it. It  is a true counter example to X-bar theory. Now   this is not a problem in construction grammar at  all, because in construction grammar, you don’t   have constraints as there is no X-bar theory.
(1:11:20)   You can say what things mean and you can have   a construction like “a cup of tea.” “A cup of tea”  is a measure construction, and there’re two parts,   a physical object that measures something and  the entity that it measures. Period. A modifier   can modify the object or what’s measured,  no problem, either one, depending on the   semantics. So in the construction grammar, this is  no problem but in X-bar grammar it’s impossible. 
(1:11:53) These are some of the reasons why I gave  up being a transformational grammarian,   and there are hundreds of more like these.  But these are some of the reasons why by 1974,   I gave it up completely. All of these examples  were known by 1974, that is, thirty years ago.   I knew all of these and I said “this cannot  work,” and I ought to do something else, and that   something else is called cognitive linguistics for  reasons that we will get to in the next lecture.


## Keywords:
