---
source: en.wikipedia.org
url: https://en.wikipedia.org/wiki/Bayesian_inference
---

**Bayesian inference** ( [_BAY\-zee-ən_](https://en.wikipedia.org/wiki/Help:Pronunciation_respelling_key "Help:Pronunciation respelling key") or [_BAY\-zhən_](https://en.wikipedia.org/wiki/Help:Pronunciation_respelling_key "Help:Pronunciation respelling key"))<sup id="cite_ref-1"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-1">[1]</a></sup> is a method of [statistical inference](https://en.wikipedia.org/wiki/Statistical_inference "Statistical inference") in which [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem "Bayes' theorem") is used to update the probability for a hypothesis as more [evidence](https://en.wikipedia.org/wiki/Evidence "Evidence") or [information](https://en.wikipedia.org/wiki/Information "Information") becomes available. Fundamentally, Bayesian inference uses prior knowledge, in the form of a [prior distribution](https://en.wikipedia.org/wiki/Prior_probability "Prior probability") in order to estimate [posterior probabilities.](https://en.wikipedia.org/wiki/Posterior_probability "Posterior probability") Bayesian inference is an important technique in [statistics](https://en.wikipedia.org/wiki/Statistics "Statistics"), and especially in [mathematical statistics](https://en.wikipedia.org/wiki/Mathematical_statistics "Mathematical statistics"). Bayesian updating is particularly important in the [dynamic analysis of a sequence of data](https://en.wikipedia.org/wiki/Sequential_analysis "Sequential analysis"). Bayesian inference has found application in a wide range of activities, including [science](https://en.wikipedia.org/wiki/Science "Science"), [engineering](https://en.wikipedia.org/wiki/Engineering "Engineering"), [philosophy](https://en.wikipedia.org/wiki/Philosophy "Philosophy"), [medicine](https://en.wikipedia.org/wiki/Medicine "Medicine"), [sport](https://en.wikipedia.org/wiki/Sport "Sport"), and [law](https://en.wikipedia.org/wiki/Law "Law"). In the philosophy of [decision theory](https://en.wikipedia.org/wiki/Decision_theory "Decision theory"), Bayesian inference is closely related to subjective probability, often called "[Bayesian probability](https://en.wikipedia.org/wiki/Bayesian_probability "Bayesian probability")".

## Introduction to Bayes' rule\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=1 "Edit section: Introduction to Bayes' rule")\]

[![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Bayes_theorem_visualisation.svg/260px-Bayes_theorem_visualisation.svg.png)](https://en.wikipedia.org/wiki/File:Bayes_theorem_visualisation.svg)

A geometric visualisation of Bayes' theorem. In the table, the values 2, 3, 6 and 9 give the relative weights of each corresponding condition and case. The figures denote the cells of the table involved in each metric, the probability being the fraction of each figure that is shaded. This shows that P(A|B) P(B) = P(B|A) P(A) i.e. P(A|B) = P(B|A) P(A)/P(B) . Similar reasoning can be used to show that P(¬A|B) = P(B|¬A) P(¬A)/P(B) etc.

### Formal explanation\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=2 "Edit section: Formal explanation")\]

<table><caption><a href="https://en.wikipedia.org/wiki/Contingency_table" title="Contingency table">Contingency table</a></caption><tbody><tr><th><p>Hypothesis</p><p>Evidence</p></th><th>Satisfies<br>hypothesis<br>H</th><th>Violates<br>hypothesis<br>¬H</th><th rowspan="5"></th><th><br>Total</th></tr><tr><th>Has evidence<br>E</th><td><b>P(H|E)·P(E)</b><br>= P(E|H)·P(H)</td><td><b>P(¬H|E)·P(E)</b><br>= P(E|¬H)·P(¬H)</td><td><b>P(E)</b></td></tr><tr><th>No evidence<br>¬E</th><td nowrap=""><b>P(H|¬E)·P(¬E)</b><br>= P(¬E|H)·P(H)</td><td nowrap=""><b>P(¬H|¬E)·P(¬E)</b><br>= P(¬E|¬H)·P(¬H)</td><td nowrap=""><b>P(¬E) =<br>1−P(E)</b></td></tr><tr><td colspan="5"></td></tr><tr><th>Total</th><td>&nbsp;&nbsp; P(H)</td><td nowrap="">P(¬H) = 1−P(H)</td><td>1</td></tr></tbody></table>

Bayesian inference derives the [posterior probability](https://en.wikipedia.org/wiki/Posterior_probability "Posterior probability") as a [consequence](https://en.wikipedia.org/wiki/Consequence_relation "Consequence relation") of two [antecedents](https://en.wikipedia.org/wiki/Antecedent_(logic) "Antecedent (logic)"): a [prior probability](https://en.wikipedia.org/wiki/Prior_probability "Prior probability") and a "[likelihood function](https://en.wikipedia.org/wiki/Likelihood_function "Likelihood function")" derived from a [statistical model](https://en.wikipedia.org/wiki/Statistical_model "Statistical model") for the observed data. Bayesian inference computes the posterior probability according to [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem "Bayes' theorem"):

![{\displaystyle P(H\mid E)={\frac {P(E\mid H)\cdot P(H)}{P(E)}},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2d6fd27873a3124e53f11f54eccc0f12200139ba)

where

-   ![{\displaystyle H}](https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b) stands for any _hypothesis_ whose probability may be affected by [data](https://en.wikipedia.org/wiki/Experimental_data "Experimental data") (called _evidence_ below). Often there are competing hypotheses, and the task is to determine which is the most probable.
-   ![{\displaystyle P(H)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/bd7c4e1deccceac8c85d862b60dc64545d67b82e), the _[prior probability](https://en.wikipedia.org/wiki/Prior_probability "Prior probability")_, is the estimate of the probability of the hypothesis ![{\displaystyle H}](https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b) _before_ the data ![{\displaystyle E}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b), the current evidence, is observed.
-   ![{\displaystyle E}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b), the _evidence_, corresponds to new data that were not used in computing the prior probability.
-   ![{\displaystyle P(H\mid E)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c79731ffa8cc8640d198cf4b474972a2e732c2ae), the _[posterior probability](https://en.wikipedia.org/wiki/Posterior_probability "Posterior probability")_, is the probability of ![{\displaystyle H}](https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b) _given_ ![{\displaystyle E}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b), i.e., _after_ ![{\displaystyle E}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b) is observed. This is what we want to know: the probability of a hypothesis _given_ the observed evidence.
-   ![{\displaystyle P(E\mid H)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8a5204605db5c0396b36603bbd3acc267586d0e6) is the probability of observing ![{\displaystyle E}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b) _given_ ![{\displaystyle H}](https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b) and is called the _[likelihood](https://en.wikipedia.org/wiki/Likelihood_function "Likelihood function")_. As a function of ![{\displaystyle E}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b) with ![{\displaystyle H}](https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b) fixed, it indicates the compatibility of the evidence with the given hypothesis. The likelihood function is a function of the evidence, ![{\displaystyle E}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b), while the posterior probability is a function of the hypothesis, ![{\displaystyle H}](https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b).
-   ![{\displaystyle P(E)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/687fe7f4688af755503fd00e7538f285e2a9954b) is sometimes termed the [marginal likelihood](https://en.wikipedia.org/wiki/Marginal_likelihood "Marginal likelihood") or "model evidence". This factor is the same for all possible hypotheses being considered (as is evident from the fact that the hypothesis ![{\displaystyle H}](https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b) does not appear anywhere in the symbol, unlike for all the other factors) and hence does not factor into determining the relative probabilities of different hypotheses.
-   ![{\displaystyle P(E)>0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2c58247aa8849d270aa1313a621c8c7a22d88143) (Else one has ![{\displaystyle 0/0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f1260a93f6fb76e30f25d5633b42e39e2c2fda79).)

For different values of ![{\displaystyle H}](https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b), only the factors ![{\displaystyle P(H)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/bd7c4e1deccceac8c85d862b60dc64545d67b82e) and ![{\displaystyle P(E\mid H)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8a5204605db5c0396b36603bbd3acc267586d0e6), both in the numerator, affect the value of ![{\displaystyle P(H\mid E)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c79731ffa8cc8640d198cf4b474972a2e732c2ae) – the posterior probability of a hypothesis is proportional to its prior probability (its inherent likeliness) and the newly acquired likelihood (its compatibility with the new observed evidence).

In cases where ![{\displaystyle \neg H}](https://wikimedia.org/api/rest_v1/media/math/render/svg/960718987a946fc39e915198096cecd2eb42b14c) ("not ![{\displaystyle H}](https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b)"), the [logical negation](https://en.wikipedia.org/wiki/Logical_negation "Logical negation") of ![{\displaystyle H}](https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b), is a valid likelihood, Bayes' rule can be rewritten as follows:

![{\displaystyle {\begin{aligned}P(H\mid E)&={\frac {P(E\mid H)P(H)}{P(E)}}\\&={\frac {P(E\mid H)P(H)}{P(E\mid H)P(H)+P(E\mid \neg H)P(\neg H)}}\\&={\frac {1}{1+\left({\frac {1}{P(H)}}-1\right){\frac {P(E\mid \neg H)}{P(E\mid H)}}}}\end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b0ceb88f064479b3d17d2d49fbe4b5ab00a435cc)

because

![{\displaystyle P(E)=P(E\mid H)P(H)+P(E\mid \neg H)P(\neg H)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/41b9117b8594db48e0dbcf80a4e1f898e541b5bf)

and

![{\displaystyle P(H)+P(\neg H)=1.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a333930a0da9c1c28670fa80234950020328b23f)

One quick and easy way to remember the equation would be to use [rule of multiplication](https://en.wikipedia.org/wiki/Conditional_probability#As_an_axiom_of_probability "Conditional probability"):

![{\displaystyle P(E\cap H)=P(E\mid H)P(H)=P(H\mid E)P(E).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3270c4aea9a5883052c813699fd4466fdceda5be)

### Alternatives to Bayesian updating\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=3 "Edit section: Alternatives to Bayesian updating")\]

Bayesian updating is widely used and computationally convenient. However, it is not the only updating rule that might be considered rational.

[Ian Hacking](https://en.wikipedia.org/wiki/Ian_Hacking "Ian Hacking") noted that traditional "[Dutch book](https://en.wikipedia.org/wiki/Dutch_book "Dutch book")" arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. Hacking wrote:<sup id="cite_ref-2"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-2">[2]</a></sup> "And neither the Dutch book argument nor any other in the personalist arsenal of proofs of the probability axioms entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour."

Indeed, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on "[probability kinematics](https://en.wikipedia.org/wiki/Probability_kinematics "Probability kinematics")") following the publication of [Richard C. Jeffrey](https://en.wikipedia.org/wiki/Richard_C._Jeffrey "Richard C. Jeffrey")'s rule, which applies Bayes' rule to the case where the evidence itself is assigned a probability.<sup id="cite_ref-3"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-3">[3]</a></sup> The additional hypotheses needed to uniquely require Bayesian updating have been deemed to be substantial, complicated, and unsatisfactory.<sup id="cite_ref-4"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-4">[4]</a></sup>

## Inference over exclusive and exhaustive possibilities\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=4 "Edit section: Inference over exclusive and exhaustive possibilities")\]

If evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions, Bayesian inference may be thought of as acting on this belief distribution as a whole.

### General formulation\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=5 "Edit section: General formulation")\]

[![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Bayesian_inference_event_space.svg/220px-Bayesian_inference_event_space.svg.png)](https://en.wikipedia.org/wiki/File:Bayesian_inference_event_space.svg)

Diagram illustrating event space ![{\displaystyle \Omega }](https://wikimedia.org/api/rest_v1/media/math/render/svg/24b0d5ca6f381068d756f6337c08e0af9d1eeb6f) in general formulation of Bayesian inference. Although this diagram shows discrete models and events, the continuous case may be visualized similarly using probability densities.

Suppose a process is generating independent and identically distributed events ![{\displaystyle E_{n},\ n=1,2,3,\ldots }](https://wikimedia.org/api/rest_v1/media/math/render/svg/9d2205791e152b46c2db60cb458ee4dfb7128164), but the [probability distribution](https://en.wikipedia.org/wiki/Probability_distribution "Probability distribution") is unknown. Let the event space ![{\displaystyle \Omega }](https://wikimedia.org/api/rest_v1/media/math/render/svg/24b0d5ca6f381068d756f6337c08e0af9d1eeb6f) represent the current state of belief for this process. Each model is represented by event ![{\displaystyle M_{m}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3a6da794af407f26d1eb865a918c28453b32e9b5). The conditional probabilities ![{\displaystyle P(E_{n}\mid M_{m})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4a97bce15bade3aa6581a1d713652f9296f4fbc8) are specified to define the models. ![{\displaystyle P(M_{m})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/80070c98c4c6d7f32c893462ecf986c61341695b) is the [degree of belief](https://en.wikipedia.org/wiki/Credence_(statistics) "Credence (statistics)") in ![{\displaystyle M_{m}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3a6da794af407f26d1eb865a918c28453b32e9b5). Before the first inference step, ![{\displaystyle \{P(M_{m})\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/46455ef34ab6727ae171f57b697be773ce545867) is a set of _initial prior probabilities_. These must sum to 1, but are otherwise arbitrary.

Suppose that the process is observed to generate ![{\displaystyle E\in \{E_{n}\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c33fc0a120006f9f052b22946b004664204c1a3f). For each ![{\displaystyle M\in \{M_{m}\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2356659b3e2726fe5df3c0b545d942054bd05355), the prior ![{\displaystyle P(M)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/afa81e21b1d085edd94f267faa1546609999affd) is updated to the posterior ![{\displaystyle P(M\mid E)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a9188f536504fc7f4d13f1abed8aeba476817df7). From [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem "Bayes' theorem"):<sup id="cite_ref-5"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-5">[5]</a></sup>

![{\displaystyle P(M\mid E)={\frac {P(E\mid M)}{\sum _{m}{P(E\mid M_{m})P(M_{m})}}}\cdot P(M).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/087ca47f90d0f08474ab86c7dfb9f665545e5885)

Upon observation of further evidence, this procedure may be repeated.

### Multiple observations\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=6 "Edit section: Multiple observations")\]

For a sequence of [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed "Independent and identically distributed") observations ![{\displaystyle \mathbf {E} =(e_{1},\dots ,e_{n})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/96c7bfcb6017fa5c6dd82f1804384e503127e519), it can be shown by induction that repeated application of the above is equivalent to

![{\displaystyle P(M\mid \mathbf {E} )={\frac {P(\mathbf {E} \mid M)}{\sum _{m}{P(\mathbf {E} \mid M_{m})P(M_{m})}}}\cdot P(M),}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3f89ec1128f86081ddc32b0d3343f26ec84a8235)

where

![{\displaystyle P(\mathbf {E} \mid M)=\prod _{k}{P(e_{k}\mid M)}.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/9a514b1c8f99bd5128fab5e7969cf6d29ac6a47b)

### Parametric formulation: motivating the formal description\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=7 "Edit section: Parametric formulation: motivating the formal description")\]

By parameterizing the space of models, the belief in all models may be updated in a single step. The distribution of belief over the model space may then be thought of as a distribution of belief over the parameter space. The distributions in this section are expressed as continuous, represented by probability densities, as this is the usual situation. The technique is, however, equally applicable to discrete distributions.

Let the vector ![{\displaystyle {\boldsymbol {\theta }}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/33b025a6bf54ec02e65c871dc3e5897c921419cf) span the parameter space. Let the initial prior distribution over ![{\displaystyle {\boldsymbol {\theta }}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/33b025a6bf54ec02e65c871dc3e5897c921419cf) be ![{\displaystyle p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/268feca73e96ae4149a42153d22da4095356d224), where ![{\displaystyle {\boldsymbol {\alpha }}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a585d2bb19071162720ea56a7b087dab3ec17156) is a set of parameters to the prior itself, or _[hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter "Hyperparameter")_. Let ![{\displaystyle \mathbf {E} =(e_{1},\dots ,e_{n})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/96c7bfcb6017fa5c6dd82f1804384e503127e519) be a sequence of [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables "Independent and identically distributed random variables") event observations, where all ![{\displaystyle e_{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ebdc3a9cb1583d3204eff8918b558c293e0d2cf3) are distributed as ![{\displaystyle p(e\mid {\boldsymbol {\theta }})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c228140a2647e23f85a6eea46f94d0da8936eadc) for some ![{\displaystyle {\boldsymbol {\theta }}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/33b025a6bf54ec02e65c871dc3e5897c921419cf). [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem "Bayes' theorem") is applied to find the [posterior distribution](https://en.wikipedia.org/wiki/Posterior_distribution "Posterior distribution") over ![{\displaystyle {\boldsymbol {\theta }}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/33b025a6bf54ec02e65c871dc3e5897c921419cf):

![{\displaystyle {\begin{aligned}p({\boldsymbol {\theta }}\mid \mathbf {E} ,{\boldsymbol {\alpha }})&={\frac {p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})}{p(\mathbf {E} \mid {\boldsymbol {\alpha }})}}\cdot p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }})\\&={\frac {p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})}{\int p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }})\,d{\boldsymbol {\theta }}}}\cdot p({\boldsymbol {\theta }}\mid {\boldsymbol {\alpha }}),\end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e8ee1696ffa3bad5959ab8cb2a917a799d650cb9)

where

![{\displaystyle p(\mathbf {E} \mid {\boldsymbol {\theta }},{\boldsymbol {\alpha }})=\prod _{k}p(e_{k}\mid {\boldsymbol {\theta }}).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e63abd2f704c345536b8ac9605dcdc08665a1800)

## Formal description of Bayesian inference\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=8 "Edit section: Formal description of Bayesian inference")\]

### Definitions\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=9 "Edit section: Definitions")\]

-   ![{\displaystyle x}](https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4), a data point in general. This may in fact be a [vector](https://en.wikipedia.org/wiki/Random_vector "Random vector") of values.
-   ![{\displaystyle \theta }](https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af), the [parameter](https://en.wikipedia.org/wiki/Parameter "Parameter") of the data point's distribution, i.e., ![{\displaystyle x\sim p(x\mid \theta )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e229220de9e183d8e2814fd2b68f495ec64caf56). This may be a [vector](https://en.wikipedia.org/wiki/Random_vector "Random vector") of parameters.
-   ![{\displaystyle \alpha }](https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3), the [hyperparameter](https://en.wikipedia.org/wiki/Hyperparameter "Hyperparameter") of the parameter distribution, i.e., ![{\displaystyle \theta \sim p(\theta \mid \alpha )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c217fc8ac69afedb83eab8b2b1dd940b2b8eb911). This may be a [vector](https://en.wikipedia.org/wiki/Random_vector "Random vector") of hyperparameters.
-   ![{\displaystyle \mathbf {X} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/9f75966a2f9d5672136fa9401ee1e75008f95ffd) is the sample, a set of ![{\displaystyle n}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b) observed data points, i.e., ![{\displaystyle x_{1},\ldots ,x_{n}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/737e02a5fbf8bc31d443c91025339f9fd1de1065).
-   ![{\displaystyle {\tilde {x}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4f5c5435030c952a58a756e691ea64f60c1bd240), a new data point whose distribution is to be predicted.

### Bayesian inference\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=10 "Edit section: Bayesian inference")\]

-   The [prior distribution](https://en.wikipedia.org/wiki/Prior_distribution "Prior distribution") is the distribution of the parameter(s) before any data is observed, i.e. ![{\displaystyle p(\theta \mid \alpha )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e1ba39272e00ae83061a2b1f4c4f3e04345db806) . The prior distribution might not be easily determined; in such a case, one possibility may be to use the [Jeffreys prior](https://en.wikipedia.org/wiki/Jeffreys_prior "Jeffreys prior") to obtain a prior distribution before updating it with newer observations.
-   The [sampling distribution](https://en.wikipedia.org/wiki/Sampling_distribution "Sampling distribution") is the distribution of the observed data conditional on its parameters, i.e. ![{\displaystyle p(\mathbf {X} \mid \theta )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/42011d81f158fafd5cea73226376a1fd9b9435bf). This is also termed the [likelihood](https://en.wikipedia.org/wiki/Likelihood_function "Likelihood function"), especially when viewed as a function of the parameter(s), sometimes written ![{\displaystyle \operatorname {L} (\theta \mid \mathbf {X} )=p(\mathbf {X} \mid \theta )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6710afe7dee646a3511abe4bb052a2d8bccda007).
-   The [marginal likelihood](https://en.wikipedia.org/wiki/Marginal_likelihood "Marginal likelihood") (sometimes also termed the _evidence_) is the distribution of the observed data [marginalized](https://en.wikipedia.org/wiki/Marginal_distribution "Marginal distribution") over the parameter(s), i.e.
    
    ![{\displaystyle p(\mathbf {X} \mid \alpha )=\int p(\mathbf {X} \mid \theta )p(\theta \mid \alpha )d\theta .}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3f4749799bde8b3dcbe3f5119ad0948441f82013)
    
    It quantifies the agreement between data and expert opinion, in a geometric sense that can be made precise.<sup id="cite_ref-deCarvalho-Geometry_6-0"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-deCarvalho-Geometry-6">[6]</a></sup> If the marginal likelihood is 0 then there is no agreement between the data and expert opinion and Bayes' rule cannot be applied.
-   The [posterior distribution](https://en.wikipedia.org/wiki/Posterior_distribution "Posterior distribution") is the distribution of the parameter(s) after taking into account the observed data. This is determined by [Bayes' rule](https://en.wikipedia.org/wiki/Bayes%27_rule "Bayes' rule"), which forms the heart of Bayesian inference:
    
    ![{\displaystyle p(\theta \mid \mathbf {X} ,\alpha )={\frac {p(\theta ,\mathbf {X} ,\alpha )}{p(\mathbf {X} ,\alpha )}}={\frac {p(\mathbf {X} \mid \theta ,\alpha )p(\theta ,\alpha )}{p(\mathbf {X} \mid \alpha )p(\alpha )}}={\frac {p(\mathbf {X} \mid \theta ,\alpha )p(\theta \mid \alpha )}{p(\mathbf {X} \mid \alpha )}}\propto p(\mathbf {X} \mid \theta ,\alpha )p(\theta \mid \alpha ).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/9ebe91a88c52adc3914fe80cdb16e948dd1d288d)
    
    This is expressed in words as "posterior is proportional to likelihood times prior", or sometimes as "posterior = likelihood times prior, over evidence".
-   In practice, for almost all complex Bayesian models used in machine learning, the posterior distribution ![{\displaystyle p(\theta \mid \mathbf {X} ,\alpha )}](https://wikimedia.org/api/rest_v1/media/math/render/svg/800a7ac7cb03d1e58763462ea55d1c6a475ec6ea) is not obtained in a closed form distribution, mainly because the parameter space for ![{\displaystyle \theta }](https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af) can be very high, or the Bayesian model retains certain hierarchical structure formulated from the observations ![{\displaystyle \mathbf {X} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/9f75966a2f9d5672136fa9401ee1e75008f95ffd) and parameter ![{\displaystyle \theta }](https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af). In such situations, we need to resort to approximation techniques.<sup id="cite_ref-Lee-GibbsSampler_7-0"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-Lee-GibbsSampler-7">[7]</a></sup>
-   General case: Let ![{\displaystyle P_{Y}^{x}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d92eb885c1778b36fe0ea7654f5fa17aacfa7d9d) be the conditional distribution of ![{\displaystyle Y}](https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f) given ![{\displaystyle X=x}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0661396d873679039ffe8e908a39f02402d4912d) and let ![{\displaystyle P_{X}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8348dd8ce7e6f7f4778ee01fa5bdc7b828afd98c) be the distribution of ![{\displaystyle X}](https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab). The joint distribution is then ![{\displaystyle P_{X,Y}(dx,dy)=P_{Y}^{x}(dy)P_{X}(dx)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ec7fe44cedd3314ca245dfd53a2c11e3ed1c9aca). The conditional distribution ![{\displaystyle P_{X}^{y}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/40ad7fcb2df4aacda6af4df410c7e2259292cc7c) of ![{\displaystyle X}](https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab) given ![{\displaystyle Y=y}](https://wikimedia.org/api/rest_v1/media/math/render/svg/678864c5e9a7ce08acfc22d0d7f726d2cade5b45) is then determined by

![{\displaystyle P_{X}^{y}(A)=E(1_{A}(X)|Y=y)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ac77ab2aa324d8ef6653f6e5ff984e1fc0304fbb)

Existence and uniqueness of the needed [conditional expectation](https://en.wikipedia.org/wiki/Conditional_expectation "Conditional expectation") is a consequence of the [Radon–Nikodym theorem](https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem "Radon–Nikodym theorem"). This was formulated by [Kolmogorov](https://en.wikipedia.org/wiki/Andrey_Kolmogorov "Andrey Kolmogorov") in his famous book from 1933. Kolmogorov underlines the importance of conditional probability by writing "I wish to call attention to ... and especially the theory of conditional probabilities and conditional expectations ..." in the Preface.<sup id="cite_ref-8"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-8">[8]</a></sup> The Bayes theorem determines the posterior distribution from the prior distribution. Uniqueness requires continuity assumptions.<sup id="cite_ref-9"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-9">[9]</a></sup><sup id="cite_ref-10"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-10">[10]</a></sup> Bayes' theorem can be generalized to include improper prior distributions such as the uniform distribution on the real line.<sup id="cite_ref-11"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-11">[11]</a></sup> Modern [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo "Markov chain Monte Carlo") methods have boosted the importance of Bayes' theorem including cases with improper priors.<sup id="cite_ref-12"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-12">[12]</a></sup>

### Bayesian prediction\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=11 "Edit section: Bayesian prediction")\]

-   The [posterior predictive distribution](https://en.wikipedia.org/wiki/Posterior_predictive_distribution "Posterior predictive distribution") is the distribution of a new data point, marginalized over the posterior:
    
    ![{\displaystyle p({\tilde {x}}\mid \mathbf {X} ,\alpha )=\int p({\tilde {x}}\mid \theta )p(\theta \mid \mathbf {X} ,\alpha )d\theta }](https://wikimedia.org/api/rest_v1/media/math/render/svg/a4e975e60dad61d2a2ddf648846d6af3bae950fc)
    
-   The [prior predictive distribution](https://en.wikipedia.org/wiki/Prior_predictive_distribution "Prior predictive distribution") is the distribution of a new data point, marginalized over the prior:
    
    ![{\displaystyle p({\tilde {x}}\mid \alpha )=\int p({\tilde {x}}\mid \theta )p(\theta \mid \alpha )d\theta }](https://wikimedia.org/api/rest_v1/media/math/render/svg/18ec77db9d4f54daacee41fe6f5cc4a8e3b82fa0)
    

Bayesian theory calls for the use of the posterior predictive distribution to do [predictive inference](https://en.wikipedia.org/wiki/Predictive_inference "Predictive inference"), i.e., to [predict](https://en.wikipedia.org/wiki/Prediction "Prediction") the distribution of a new, unobserved data point. That is, instead of a fixed point as a prediction, a distribution over possible points is returned. Only this way is the entire posterior distribution of the parameter(s) used. By comparison, prediction in [frequentist statistics](https://en.wikipedia.org/wiki/Frequentist_statistics "Frequentist statistics") often involves finding an optimum point estimate of the parameter(s)—e.g., by [maximum likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood "Maximum likelihood") or [maximum a posteriori estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation "Maximum a posteriori estimation") (MAP)—and then plugging this estimate into the formula for the distribution of a data point. This has the disadvantage that it does not account for any uncertainty in the value of the parameter, and hence will underestimate the [variance](https://en.wikipedia.org/wiki/Variance "Variance") of the predictive distribution.

In some instances, frequentist statistics can work around this problem. For example, [confidence intervals](https://en.wikipedia.org/wiki/Confidence_interval "Confidence interval") and [prediction intervals](https://en.wikipedia.org/wiki/Prediction_interval "Prediction interval") in frequentist statistics when constructed from a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution "Normal distribution") with unknown [mean](https://en.wikipedia.org/wiki/Mean "Mean") and [variance](https://en.wikipedia.org/wiki/Variance "Variance") are constructed using a [Student's t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution "Student's t-distribution"). This correctly estimates the variance, due to the facts that (1) the average of normally distributed random variables is also normally distributed, and (2) the predictive distribution of a normally distributed data point with unknown mean and variance, using conjugate or uninformative priors, has a Student's t-distribution. In Bayesian statistics, however, the posterior predictive distribution can always be determined exactly—or at least to an arbitrary level of precision when numerical methods are used.

Both types of predictive distributions have the form of a [compound probability distribution](https://en.wikipedia.org/wiki/Compound_probability_distribution "Compound probability distribution") (as does the [marginal likelihood](https://en.wikipedia.org/wiki/Marginal_likelihood "Marginal likelihood")). In fact, if the prior distribution is a [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior "Conjugate prior"), such that the prior and posterior distributions come from the same family, it can be seen that both prior and posterior predictive distributions also come from the same family of compound distributions. The only difference is that the posterior predictive distribution uses the updated values of the hyperparameters (applying the Bayesian update rules given in the [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior "Conjugate prior") article), while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution.

## Mathematical properties\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=12 "Edit section: Mathematical properties")\]

### Interpretation of factor\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=13 "Edit section: Interpretation of factor")\]

![{\textstyle {\frac {P(E\mid M)}{P(E)}}>1\Rightarrow P(E\mid M)>P(E)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/fefc5adf9f1830e82ea72c4e983defe7ea173aed). That is, if the model were true, the evidence would be more likely than is predicted by the current state of belief. The reverse applies for a decrease in belief. If the belief does not change, ![{\textstyle {\frac {P(E\mid M)}{P(E)}}=1\Rightarrow P(E\mid M)=P(E)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ca89d04a8e2728a1c0f7346a51ba09254453dbea). That is, the evidence is independent of the model. If the model were true, the evidence would be exactly as likely as predicted by the current state of belief.

### Cromwell's rule\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=14 "Edit section: Cromwell's rule")\]

If ![{\displaystyle P(M)=0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/86c78ea77f5fd2eb0d9f6f515b8692c6d546e452) then ![{\displaystyle P(M\mid E)=0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/fe4936caa9d6721395d01420f44e8dc4457591e9). If ![{\displaystyle P(M)=1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2c9f6299fece5f2b7af3c1efd63a757b590083a3) and ![{\displaystyle P(E)>0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2c58247aa8849d270aa1313a621c8c7a22d88143), then ![{\displaystyle P(M|E)=1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e4b5eb6223880b7986cb4cc79a8be1ac8be6de8b). This can be interpreted to mean that hard convictions are insensitive to counter-evidence.

The former follows directly from Bayes' theorem. The latter can be derived by applying the first rule to the event "not ![{\displaystyle M}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f82cade9898ced02fdd08712e5f0c0151758a0dd)" in place of "![{\displaystyle M}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f82cade9898ced02fdd08712e5f0c0151758a0dd)", yielding "if ![{\displaystyle 1-P(M)=0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/acb748c826e88ea4aec1ac4410b0419bcfc043f1), then ![{\displaystyle 1-P(M\mid E)=0}](https://wikimedia.org/api/rest_v1/media/math/render/svg/5ab0d269079eaada48461cb962765c2b0c638f15)", from which the result immediately follows.

### Asymptotic behaviour of posterior\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=15 "Edit section: Asymptotic behaviour of posterior")\]

Consider the behaviour of a belief distribution as it is updated a large number of times with [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed "Independent and identically distributed") trials. For sufficiently nice prior probabilities, the [Bernstein-von Mises theorem](https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem "Bernstein–von Mises theorem") gives that in the limit of infinite trials, the posterior converges to a [Gaussian distribution](https://en.wikipedia.org/wiki/Gaussian_distribution "Gaussian distribution") independent of the initial prior under some conditions firstly outlined and rigorously proven by [Joseph L. Doob](https://en.wikipedia.org/wiki/Joseph_L._Doob "Joseph L. Doob") in 1948, namely if the random variable in consideration has a finite [probability space](https://en.wikipedia.org/wiki/Probability_space "Probability space"). The more general results were obtained later by the statistician [David A. Freedman](https://en.wikipedia.org/wiki/David_A._Freedman_(statistician) "David A. Freedman (statistician)") who published in two seminal research papers in 1963 <sup id="cite_ref-13"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-13">[13]</a></sup> and 1965 <sup id="cite_ref-14"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-14">[14]</a></sup> when and under what circumstances the asymptotic behaviour of posterior is guaranteed. His 1963 paper treats, like Doob (1949), the finite case and comes to a satisfactory conclusion. However, if the random variable has an infinite but countable [probability space](https://en.wikipedia.org/wiki/Probability_space "Probability space") (i.e., corresponding to a die with infinite many faces) the 1965 paper demonstrates that for a dense subset of priors the [Bernstein-von Mises theorem](https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem "Bernstein–von Mises theorem") is not applicable. In this case there is [almost surely](https://en.wikipedia.org/wiki/Almost_surely "Almost surely") no asymptotic convergence. Later in the 1980s and 1990s [Freedman](https://en.wikipedia.org/wiki/David_A._Freedman_(statistician) "David A. Freedman (statistician)") and [Persi Diaconis](https://en.wikipedia.org/wiki/Persi_Diaconis "Persi Diaconis") continued to work on the case of infinite countable probability spaces.<sup id="cite_ref-15"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-15">[15]</a></sup> To summarise, there may be insufficient trials to suppress the effects of the initial choice, and especially for large (but finite) systems the convergence might be very slow.

### Conjugate priors\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=16 "Edit section: Conjugate priors")\]

In parameterized form, the prior distribution is often assumed to come from a family of distributions called [conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior "Conjugate prior"). The usefulness of a conjugate prior is that the corresponding posterior distribution will be in the same family, and the calculation may be expressed in [closed form](https://en.wikipedia.org/wiki/Closed-form_expression "Closed-form expression").

### Estimates of parameters and predictions\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=17 "Edit section: Estimates of parameters and predictions")\]

It is often desired to use a posterior distribution to estimate a parameter or variable. Several methods of Bayesian estimation select [measurements of central tendency](https://en.wikipedia.org/wiki/Central_tendency "Central tendency") from the posterior distribution.

For one-dimensional problems, a unique median exists for practical continuous problems. The posterior median is attractive as a [robust estimator](https://en.wikipedia.org/wiki/Robust_statistics "Robust statistics").<sup id="cite_ref-16"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-16">[16]</a></sup>

If there exists a finite mean for the posterior distribution, then the posterior mean is a method of estimation.<sup id="cite_ref-17"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-17">[17]</a></sup>

![{\displaystyle {\tilde {\theta }}=\operatorname {E} [\theta ]=\int \theta \,p(\theta \mid \mathbf {X} ,\alpha )\,d\theta }](https://wikimedia.org/api/rest_v1/media/math/render/svg/fe77d50024b7504dd853e6cee501d293653c546b)

Taking a value with the greatest probability defines [maximum _a posteriori_ (MAP)](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation "Maximum a posteriori estimation") estimates:<sup id="cite_ref-18"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-18">[18]</a></sup>

![{\displaystyle \{\theta _{\text{MAP}}\}\subset \arg \max _{\theta }p(\theta \mid \mathbf {X} ,\alpha ).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4237a6abe3a53ee81860ffd11cf694e06d51e4ff)

There are examples where no maximum is attained, in which case the set of MAP estimates is [empty](https://en.wikipedia.org/wiki/Empty_set "Empty set").

There are other methods of estimation that minimize the posterior _[risk](https://en.wikipedia.org/wiki/Risk "Risk")_ (expected-posterior loss) with respect to a [loss function](https://en.wikipedia.org/wiki/Loss_function "Loss function"), and these are of interest to [statistical decision theory](https://en.wikipedia.org/wiki/Statistical_decision_theory "Statistical decision theory") using the sampling distribution ("frequentist statistics").<sup id="cite_ref-19"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-19">[19]</a></sup>

The [posterior predictive distribution](https://en.wikipedia.org/wiki/Posterior_predictive_distribution "Posterior predictive distribution") of a new observation ![{\displaystyle {\tilde {x}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4f5c5435030c952a58a756e691ea64f60c1bd240) (that is independent of previous observations) is determined by<sup id="cite_ref-20"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-20">[20]</a></sup>

![{\displaystyle p({\tilde {x}}|\mathbf {X} ,\alpha )=\int p({\tilde {x}},\theta \mid \mathbf {X} ,\alpha )\,d\theta =\int p({\tilde {x}}\mid \theta )p(\theta \mid \mathbf {X} ,\alpha )\,d\theta .}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d675b4ed24259be3a5a84130f49992f40108a93e)

## Examples\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=18 "Edit section: Examples")\]

### Probability of a hypothesis\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=19 "Edit section: Probability of a hypothesis")\]

<table><caption><a href="https://en.wikipedia.org/wiki/Contingency_table" title="Contingency table">Contingency table</a></caption><tbody><tr><th><p>Bowl</p><div><p>Cookie</p></div></th><th>#1<br><i>H</i><sub>1</sub></th><th>#2<br><i>H</i><sub>2</sub></th><th rowspan="4"></th><th><br>Total</th></tr><tr><th>Plain, <i>E</i></th><td><b>30</b></td><td>20</td><td><b>50</b></td></tr><tr><th>Choc, ¬<i>E</i></th><td>10</td><td>20</td><td>30</td></tr><tr><th>Total</th><td>40</td><td>40</td><td>80</td></tr><tr><td colspan="5"><i>P</i>(<i>H</i><sub>1</sub>|<i>E</i>) = 30 / 50 = 0.6</td></tr></tbody></table>

Suppose there are two full bowls of cookies. Bowl #1 has 10 chocolate chip and 30 plain cookies, while bowl #2 has 20 of each. Our friend Fred picks a bowl at random, and then picks a cookie at random. We may assume there is no reason to believe Fred treats one bowl differently from another, likewise for the cookies. The cookie turns out to be a plain one. How probable is it that Fred picked it out of bowl #1?

Intuitively, it seems clear that the answer should be more than a half, since there are more plain cookies in bowl #1. The precise answer is given by Bayes' theorem. Let ![{\displaystyle H_{1}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4d4d9a872a55b209f2eb7cc23a71e5e1541bd1f4) correspond to bowl #1, and ![{\displaystyle H_{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7fa4324515cc7343ee952e3840a1bb1aa8c7f74c) to bowl #2. It is given that the bowls are identical from Fred's point of view, thus ![{\displaystyle P(H_{1})=P(H_{2})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/64b9732a91ab616a55e7edd2e7d3676c65899871), and the two must add up to 1, so both are equal to 0.5. The event ![{\displaystyle E}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b) is the observation of a plain cookie. From the contents of the bowls, we know that ![{\displaystyle P(E\mid H_{1})=30/40=0.75}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e105aed81081c021c764effa60d3e4b13bdf4816) and ![{\displaystyle P(E\mid H_{2})=20/40=0.5.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c07c1b90033034b84b7b072263194f3795a6c7f2) Bayes' formula then yields

![{\displaystyle {\begin{aligned}P(H_{1}\mid E)&={\frac {P(E\mid H_{1})\,P(H_{1})}{P(E\mid H_{1})\,P(H_{1})\;+\;P(E\mid H_{2})\,P(H_{2})}}\\\\\ &={\frac {0.75\times 0.5}{0.75\times 0.5+0.5\times 0.5}}\\\\\ &=0.6\end{aligned}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1fda4070065205dfeaef3c012baa7da22284edef)

Before we observed the cookie, the probability we assigned for Fred having chosen bowl #1 was the prior probability, ![{\displaystyle P(H_{1})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/85009a290ad8a188184da807fde697b99331da6d), which was 0.5. After observing the cookie, we must revise the probability to ![{\displaystyle P(H_{1}\mid E)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/2930de0b5ddf2a7e0cd2b0285cb0ea9289aae85a), which is 0.6.

### Making a prediction\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=20 "Edit section: Making a prediction")\]

[![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bayesian_inference_archaeology_example.jpg/220px-Bayesian_inference_archaeology_example.jpg)](https://en.wikipedia.org/wiki/File:Bayesian_inference_archaeology_example.jpg)

Example results for archaeology example. This simulation was generated using c=15.2.

An archaeologist is working at a site thought to be from the medieval period, between the 11th century to the 16th century. However, it is uncertain exactly when in this period the site was inhabited. Fragments of pottery are found, some of which are glazed and some of which are decorated. It is expected that if the site were inhabited during the early medieval period, then 1% of the pottery would be glazed and 50% of its area decorated, whereas if it had been inhabited in the late medieval period then 81% would be glazed and 5% of its area decorated. How confident can the archaeologist be in the date of inhabitation as fragments are unearthed?

The degree of belief in the continuous variable ![{\displaystyle C}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4fc55753007cd3c18576f7933f6f089196732029) (century) is to be calculated, with the discrete set of events ![{\displaystyle \{GD,G{\bar {D}},{\bar {G}}D,{\bar {G}}{\bar {D}}\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3c4b231136f82fdaf1ed4920a8bda80fb850fad6) as evidence. Assuming linear variation of glaze and decoration with time, and that these variables are independent,

![{\displaystyle P(E=GD\mid C=c)=(0.01+{\frac {0.81-0.01}{16-11}}(c-11))(0.5-{\frac {0.5-0.05}{16-11}}(c-11))}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7eb7761a3452796f74d836818893dfe6d9f9e89a)

![{\displaystyle P(E=G{\bar {D}}\mid C=c)=(0.01+{\frac {0.81-0.01}{16-11}}(c-11))(0.5+{\frac {0.5-0.05}{16-11}}(c-11))}](https://wikimedia.org/api/rest_v1/media/math/render/svg/fdd19acaaa9254b17c4e9bead7d580488ea3eef0)

![{\displaystyle P(E={\bar {G}}D\mid C=c)=((1-0.01)-{\frac {0.81-0.01}{16-11}}(c-11))(0.5-{\frac {0.5-0.05}{16-11}}(c-11))}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a2bf68e7cbe3d6d67d48eea6b515b0a997e87dd8)

![{\displaystyle P(E={\bar {G}}{\bar {D}}\mid C=c)=((1-0.01)-{\frac {0.81-0.01}{16-11}}(c-11))(0.5+{\frac {0.5-0.05}{16-11}}(c-11))}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d7bdc61c1f6b29d42bcce089147ca7d0fe93199b)

Assume a uniform prior of ![{\textstyle f_{C}(c)=0.2}](https://wikimedia.org/api/rest_v1/media/math/render/svg/91060d3a706c221a7f2ecfe83ae1058a253b453a), and that trials are [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed "Independent and identically distributed"). When a new fragment of type ![{\displaystyle e}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cd253103f0876afc68ebead27a5aa9867d927467) is discovered, Bayes' theorem is applied to update the degree of belief for each ![{\displaystyle c}](https://wikimedia.org/api/rest_v1/media/math/render/svg/86a67b81c2de995bd608d5b2df50cd8cd7d92455):

![{\displaystyle f_{C}(c\mid E=e)={\frac {P(E=e\mid C=c)}{P(E=e)}}f_{C}(c)={\frac {P(E=e\mid C=c)}{\int _{11}^{16}{P(E=e\mid C=c)f_{C}(c)dc}}}f_{C}(c)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6192dbe9bc0ce025d46d806e5061b03c97ce53f6)

A computer simulation of the changing belief as 50 fragments are unearthed is shown on the graph. In the simulation, the site was inhabited around 1420, or ![{\displaystyle c=15.2}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7166b08e33a05e32a75b663f13ea7c6a5e036c95). By calculating the area under the relevant portion of the graph for 50 trials, the archaeologist can say that there is practically no chance the site was inhabited in the 11th and 12th centuries, about 1% chance that it was inhabited during the 13th century, 63% chance during the 14th century and 36% during the 15th century. The [Bernstein-von Mises theorem](https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem "Bernstein–von Mises theorem") asserts here the asymptotic convergence to the "true" distribution because the [probability space](https://en.wikipedia.org/wiki/Probability_space "Probability space") corresponding to the discrete set of events ![{\displaystyle \{GD,G{\bar {D}},{\bar {G}}D,{\bar {G}}{\bar {D}}\}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/3c4b231136f82fdaf1ed4920a8bda80fb850fad6) is finite (see above section on asymptotic behaviour of the posterior).

## In frequentist statistics and decision theory\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=21 "Edit section: In frequentist statistics and decision theory")\]

A [decision-theoretic](https://en.wikipedia.org/wiki/Statistical_decision_theory "Statistical decision theory") justification of the use of Bayesian inference was given by [Abraham Wald](https://en.wikipedia.org/wiki/Abraham_Wald "Abraham Wald"), who proved that every unique Bayesian procedure is [admissible](https://en.wikipedia.org/wiki/Admissible_decision_rule "Admissible decision rule"). Conversely, every [admissible](https://en.wikipedia.org/wiki/Admissible_decision_rule "Admissible decision rule") statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures.<sup id="cite_ref-Bickel_&amp;_Doksum_2001,_page_32_21-0"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-Bickel_&amp;_Doksum_2001,_page_32-21">[21]</a></sup>

Wald characterized admissible procedures as Bayesian procedures (and limits of Bayesian procedures), making the Bayesian formalism a central technique in such areas of [frequentist inference](https://en.wikipedia.org/wiki/Frequentist_inference "Frequentist inference") as [parameter estimation](https://en.wikipedia.org/wiki/Parameter_estimation "Parameter estimation"), [hypothesis testing](https://en.wikipedia.org/wiki/Hypothesis_testing "Hypothesis testing"), and computing [confidence intervals](https://en.wikipedia.org/wiki/Confidence_intervals "Confidence intervals").<sup id="cite_ref-22"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-22">[22]</a></sup><sup id="cite_ref-23"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-23">[23]</a></sup><sup id="cite_ref-24"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-24">[24]</a></sup> For example:

-   "Under some conditions, all admissible procedures are either Bayes procedures or limits of Bayes procedures (in various senses). These remarkable results, at least in their original form, are due essentially to Wald. They are useful because the property of being Bayes is easier to analyze than admissibility."<sup id="cite_ref-Bickel_&amp;_Doksum_2001,_page_32_21-1"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-Bickel_&amp;_Doksum_2001,_page_32-21">[21]</a></sup>
-   "In decision theory, a quite general method for proving admissibility consists in exhibiting a procedure as a unique Bayes solution."<sup id="cite_ref-25"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-25">[25]</a></sup>
-   "In the first chapters of this work, prior distributions with finite support and the corresponding Bayes procedures were used to establish some of the main theorems relating to the comparison of experiments. Bayes procedures with respect to more general prior distributions have played a very important role in the development of statistics, including its asymptotic theory." "There are many problems where a glance at posterior distributions, for suitable priors, yields immediately interesting information. Also, this technique can hardly be avoided in sequential analysis."<sup id="cite_ref-26"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-26">[26]</a></sup>
-   "A useful fact is that any Bayes decision rule obtained by taking a proper prior over the whole parameter space must be admissible"<sup id="cite_ref-27"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-27">[27]</a></sup>
-   "An important area of investigation in the development of admissibility ideas has been that of conventional sampling-theory procedures, and many interesting results have been obtained."<sup id="cite_ref-28"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-28">[28]</a></sup>

### Model selection\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=22 "Edit section: Model selection")\]

Bayesian methodology also plays a role in [model selection](https://en.wikipedia.org/wiki/Model_selection "Model selection") where the aim is to select one model from a set of competing models that represents most closely the underlying process that generated the observed data. In Bayesian model comparison, the model with the highest [posterior probability](https://en.wikipedia.org/wiki/Posterior_probability "Posterior probability") given the data is selected. The posterior probability of a model depends on the evidence, or [marginal likelihood](https://en.wikipedia.org/wiki/Marginal_likelihood "Marginal likelihood"), which reflects the probability that the data is generated by the model, and on the [prior belief](https://en.wikipedia.org/wiki/Prior_probability "Prior probability") of the model. When two competing models are a priori considered to be equiprobable, the ratio of their posterior probabilities corresponds to the [Bayes factor](https://en.wikipedia.org/wiki/Bayes_factor "Bayes factor"). Since Bayesian model comparison is aimed on selecting the model with the highest posterior probability, this methodology is also referred to as the maximum a posteriori (MAP) selection rule <sup id="cite_ref-29"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-29">[29]</a></sup> or the MAP probability rule.<sup id="cite_ref-30"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-30">[30]</a></sup>

## Probabilistic programming\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=23 "Edit section: Probabilistic programming")\]

While conceptually simple, Bayesian methods can be mathematically and numerically challenging. Probabilistic programming languages (PPLs) implement functions to easily build Bayesian models together with efficient automatic inference methods. This helps separate the model building from the inference, allowing practitioners to focus on their specific problems and leaving PPLs to handle the computational details for them.<sup id="cite_ref-31"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-31">[31]</a></sup><sup id="cite_ref-32"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-32">[32]</a></sup><sup id="cite_ref-33"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-33">[33]</a></sup>

## Applications\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=24 "Edit section: Applications")\]

### Statistical data analysis\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=25 "Edit section: Statistical data analysis")\]

See the separate Wikipedia entry on [Bayesian statistics](https://en.wikipedia.org/wiki/Bayesian_statistics "Bayesian statistics"), specifically the [statistical modeling](https://en.wikipedia.org/wiki/Bayesian_statistics#Statistical_modeling "Bayesian statistics") section in that page.

### Computer applications\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=26 "Edit section: Computer applications")\]

Bayesian inference has applications in [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence "Artificial intelligence") and [expert systems](https://en.wikipedia.org/wiki/Expert_system "Expert system"). Bayesian inference techniques have been a fundamental part of computerized [pattern recognition](https://en.wikipedia.org/wiki/Pattern_recognition "Pattern recognition") techniques since the late 1950s.<sup id="cite_ref-34"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-34">[34]</a></sup> There is also an ever-growing connection between Bayesian methods and simulation-based [Monte Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method "Monte Carlo method") techniques since complex models cannot be processed in closed form by a Bayesian analysis, while a [graphical model](https://en.wikipedia.org/wiki/Graphical_model "Graphical model") structure _may_ allow for efficient simulation algorithms like the [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling "Gibbs sampling") and other [Metropolis–Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm "Metropolis–Hastings algorithm") schemes.<sup id="cite_ref-35"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-35">[35]</a></sup> Recently<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Dates_and_numbers#Chronological_items" title="Wikipedia:Manual of Style/Dates and numbers"><span title="The time period mentioned near this tag is ambiguous. (September 2018)">when?</span></a></i>]</sup> Bayesian inference has gained popularity among the [phylogenetics](https://en.wikipedia.org/wiki/Phylogenetics "Phylogenetics") community for these reasons; a number of applications allow many demographic and evolutionary parameters to be estimated simultaneously.

As applied to [statistical classification](https://en.wikipedia.org/wiki/Statistical_classification "Statistical classification"), Bayesian inference has been used to develop algorithms for identifying [e-mail spam](https://en.wikipedia.org/wiki/E-mail_spam "E-mail spam"). Applications which make use of Bayesian inference for spam filtering include [CRM114](https://en.wikipedia.org/wiki/CRM114_(program) "CRM114 (program)"), [DSPAM](https://en.wikipedia.org/w/index.php?title=DSPAM&action=edit&redlink=1 "DSPAM (page does not exist)"), [Bogofilter](https://en.wikipedia.org/wiki/Bogofilter "Bogofilter"), [SpamAssassin](https://en.wikipedia.org/wiki/SpamAssassin "SpamAssassin"), [SpamBayes](https://en.wikipedia.org/wiki/SpamBayes "SpamBayes"), [Mozilla](https://en.wikipedia.org/wiki/Mozilla "Mozilla"), XEAMS, and others. Spam classification is treated in more detail in the article on the [naïve Bayes classifier](https://en.wikipedia.org/wiki/Na%C3%AFve_Bayes_classifier "Naïve Bayes classifier").

[Solomonoff's Inductive inference](https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference "Solomonoff's theory of inductive inference") is the theory of prediction based on observations; for example, predicting the next symbol based upon a given series of symbols. The only assumption is that the environment follows some unknown but computable [probability distribution](https://en.wikipedia.org/wiki/Probability_distribution "Probability distribution"). It is a formal inductive framework that combines two well-studied principles of inductive inference: Bayesian statistics and [Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_Razor "Occam's Razor").<sup id="cite_ref-36"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-36">[36]</a></sup><sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources" title="Wikipedia:Reliable sources"><span title="The material near this tag may rely on an unreliable source. (September 2018)">unreliable source?</span></a></i>]</sup> Solomonoff's universal prior probability of any prefix _p_ of a computable sequence _x_ is the sum of the probabilities of all programs (for a universal computer) that compute something starting with _p_. Given some _p_ and any computable but unknown probability distribution from which _x_ is sampled, the universal prior and Bayes' theorem can be used to predict the yet unseen parts of _x_ in optimal fashion.<sup id="cite_ref-37"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-37">[37]</a></sup><sup id="cite_ref-38"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-38">[38]</a></sup>

### Bioinformatics and healthcare applications\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=27 "Edit section: Bioinformatics and healthcare applications")\]

Bayesian inference has been applied in different [Bioinformatics](https://en.wikipedia.org/wiki/Bioinformatics "Bioinformatics") applications, including differential gene expression analysis.<sup id="cite_ref-:edgr_39-0"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-:edgr-39">[39]</a></sup> Bayesian inference is also used in a general cancer risk model, called [CIRI](https://en.wikipedia.org/wiki/Continuous_Individualized_Risk_Index "Continuous Individualized Risk Index") (Continuous Individualized Risk Index), where serial measurements are incorporated to update a Bayesian model which is primarily built from prior knowledge.<sup id="cite_ref-40"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-40">[40]</a></sup><sup id="cite_ref-41"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-41">[41]</a></sup>

### In the courtroom\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=28 "Edit section: In the courtroom")\]

Bayesian inference can be used by jurors to coherently accumulate the evidence for and against a defendant, and to see whether, in totality, it meets their personal threshold for "[beyond a reasonable doubt](https://en.wikipedia.org/wiki/Beyond_a_reasonable_doubt "Beyond a reasonable doubt")".<sup id="cite_ref-42"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-42">[42]</a></sup><sup id="cite_ref-43"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-43">[43]</a></sup><sup id="cite_ref-44"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-44">[44]</a></sup> Bayes' theorem is applied successively to all evidence presented, with the posterior from one stage becoming the prior for the next. The benefit of a Bayesian approach is that it gives the juror an unbiased, rational mechanism for combining evidence. It may be appropriate to explain Bayes' theorem to jurors in [odds form](https://en.wikipedia.org/wiki/Bayes%27_rule "Bayes' rule"), as [betting odds](https://en.wikipedia.org/wiki/Betting_odds "Betting odds") are more widely understood than probabilities. Alternatively, a [logarithmic approach](https://en.wikipedia.org/wiki/Gambling_and_information_theory "Gambling and information theory"), replacing multiplication with addition, might be easier for a jury to handle.

[![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Ebits2c.png/220px-Ebits2c.png)](https://en.wikipedia.org/wiki/File:Ebits2c.png)

Adding up evidence

If the existence of the crime is not in doubt, only the identity of the culprit, it has been suggested that the prior should be uniform over the qualifying population.<sup id="cite_ref-45"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-45">[45]</a></sup> For example, if 1,000 people could have committed the crime, the prior probability of guilt would be 1/1000.

The use of Bayes' theorem by jurors is controversial. In the United Kingdom, a defence [expert witness](https://en.wikipedia.org/wiki/Expert_witness "Expert witness") explained Bayes' theorem to the jury in _[R v Adams](https://en.wikipedia.org/wiki/Regina_versus_Denis_John_Adams "Regina versus Denis John Adams")_. The jury convicted, but the case went to appeal on the basis that no means of accumulating evidence had been provided for jurors who did not wish to use Bayes' theorem. The Court of Appeal upheld the conviction, but it also gave the opinion that "To introduce Bayes' Theorem, or any similar method, into a criminal trial plunges the jury into inappropriate and unnecessary realms of theory and complexity, deflecting them from their proper task."

Gardner-Medwin<sup id="cite_ref-46"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-46">[46]</a></sup> argues that the criterion on which a verdict in a criminal trial should be based is _not_ the probability of guilt, but rather the _probability of the evidence, given that the defendant is innocent_ (akin to a [frequentist](https://en.wikipedia.org/wiki/Frequentist "Frequentist") [p-value](https://en.wikipedia.org/wiki/P-value "P-value")). He argues that if the posterior probability of guilt is to be computed by Bayes' theorem, the prior probability of guilt must be known. This will depend on the incidence of the crime, which is an unusual piece of evidence to consider in a criminal trial. Consider the following three propositions:

_A_ – the known facts and testimony could have arisen if the defendant is guilty.

_B_ – the known facts and testimony could have arisen if the defendant is innocent.

_C_ – the defendant is guilty.

Gardner-Medwin argues that the jury should believe both _A_ and not-_B_ in order to convict. _A_ and not-_B_ implies the truth of _C_, but the reverse is not true. It is possible that _B_ and _C_ are both true, but in this case he argues that a jury should acquit, even though they know that they will be letting some guilty people go free. See also [Lindley's paradox](https://en.wikipedia.org/wiki/Lindley%27s_paradox "Lindley's paradox").

### Bayesian epistemology\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=29 "Edit section: Bayesian epistemology")\]

[Bayesian epistemology](https://en.wikipedia.org/wiki/Bayesian_epistemology "Bayesian epistemology") is a movement that advocates for Bayesian inference as a means of justifying the rules of inductive logic.

[Karl Popper](https://en.wikipedia.org/wiki/Karl_Popper "Karl Popper") and [David Miller](https://en.wikipedia.org/wiki/David_Miller_(philosopher) "David Miller (philosopher)") have rejected the idea of Bayesian rationalism, i.e. using Bayes rule to make epistemological inferences:<sup id="cite_ref-47"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-47">[47]</a></sup> It is prone to the same [vicious circle](https://en.wikipedia.org/wiki/Vicious_circle "Vicious circle") as any other [justificationist](https://en.wikipedia.org/wiki/Justificationism "Justificationism") epistemology, because it presupposes what it attempts to justify. According to this view, a rational interpretation of Bayesian inference would see it merely as a probabilistic version of [falsification](https://en.wikipedia.org/wiki/Falsifiability "Falsifiability"), rejecting the belief, commonly held by Bayesians, that high likelihood achieved by a series of Bayesian updates would prove the hypothesis beyond any reasonable doubt, or even with likelihood greater than 0.

### Other\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=30 "Edit section: Other")\]

-   The [scientific method](https://en.wikipedia.org/wiki/Scientific_method "Scientific method") is sometimes interpreted as an application of Bayesian inference. In this view, Bayes' rule guides (or should guide) the updating of probabilities about [hypotheses](https://en.wikipedia.org/wiki/Hypothesis "Hypothesis") conditional on new observations or [experiments](https://en.wikipedia.org/wiki/Experiment "Experiment").<sup id="cite_ref-48"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-48">[48]</a></sup> The Bayesian inference has also been applied to treat [stochastic scheduling](https://en.wikipedia.org/wiki/Stochastic_scheduling "Stochastic scheduling") problems with incomplete information by Cai et al. (2009).<sup id="cite_ref-Cai_et_al._2009_49-0"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-Cai_et_al._2009-49">[49]</a></sup>
-   [Bayesian search theory](https://en.wikipedia.org/wiki/Bayesian_search_theory "Bayesian search theory") is used to search for lost objects.
-   [Bayesian inference in phylogeny](https://en.wikipedia.org/wiki/Bayesian_inference_in_phylogeny "Bayesian inference in phylogeny")
-   [Bayesian tool for methylation analysis](https://en.wikipedia.org/wiki/Bayesian_tool_for_methylation_analysis "Bayesian tool for methylation analysis")
-   [Bayesian approaches to brain function](https://en.wikipedia.org/wiki/Bayesian_approaches_to_brain_function "Bayesian approaches to brain function") investigate the brain as a Bayesian mechanism.
-   Bayesian inference in ecological studies<sup id="cite_ref-50"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-50">[50]</a></sup><sup id="cite_ref-51"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-51">[51]</a></sup>
-   Bayesian inference is used to estimate parameters in stochastic chemical kinetic models<sup id="cite_ref-52"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-52">[52]</a></sup>
-   Bayesian inference in [econophysics](https://en.wikipedia.org/wiki/Econophysics "Econophysics") for currency or [stock market prediction](https://en.wikipedia.org/wiki/Stock_market_prediction "Stock market prediction")<sup id="cite_ref-53"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-53">[53]</a></sup><sup id="cite_ref-54"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-54">[54]</a></sup>
-   [Bayesian inference in marketing](https://en.wikipedia.org/wiki/Bayesian_inference_in_marketing "Bayesian inference in marketing")
-   [Bayesian inference in motor learning](https://en.wikipedia.org/wiki/Bayesian_inference_in_motor_learning "Bayesian inference in motor learning")
-   Bayesian inference is used in [probabilistic numerics](https://en.wikipedia.org/wiki/Probabilistic_numerics "Probabilistic numerics") to solve numerical problems

## Bayes and Bayesian inference\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=31 "Edit section: Bayes and Bayesian inference")\]

The problem considered by Bayes in Proposition 9 of his essay, "[An Essay towards solving a Problem in the Doctrine of Chances](https://en.wikipedia.org/wiki/An_Essay_towards_solving_a_Problem_in_the_Doctrine_of_Chances "An Essay towards solving a Problem in the Doctrine of Chances")", is the posterior distribution for the parameter _a_ (the success rate) of the [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution "Binomial distribution").<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (August 2010)">citation needed</span></a></i>]</sup>

## History\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=32 "Edit section: History")\]

The term _Bayesian_ refers to [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes "Thomas Bayes") (1701–1761), who proved that probabilistic limits could be placed on an unknown event.<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (July 2022)">citation needed</span></a></i>]</sup> However, it was [Pierre-Simon Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace "Pierre-Simon Laplace") (1749–1827) who introduced (as Principle VI) what is now called [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem "Bayes' theorem") and used it to address problems in [celestial mechanics](https://en.wikipedia.org/wiki/Celestial_mechanics "Celestial mechanics"), medical statistics, [reliability](https://en.wikipedia.org/wiki/Reliability_(statistics) "Reliability (statistics)"), and [jurisprudence](https://en.wikipedia.org/wiki/Jurisprudence "Jurisprudence").<sup id="cite_ref-Stigler1986_55-0"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-Stigler1986-55">[55]</a></sup> Early Bayesian inference, which used uniform priors following Laplace's [principle of insufficient reason](https://en.wikipedia.org/wiki/Principle_of_insufficient_reason "Principle of insufficient reason"), was called "[inverse probability](https://en.wikipedia.org/wiki/Inverse_probability "Inverse probability")" (because it [infers](https://en.wikipedia.org/wiki/Inductive_reasoning "Inductive reasoning") backwards from observations to parameters, or from effects to causes<sup id="cite_ref-Fienberg2006_56-0"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-Fienberg2006-56">[56]</a></sup>). After the 1920s, "inverse probability" was largely supplanted by a collection of methods that came to be called [frequentist statistics](https://en.wikipedia.org/wiki/Frequentist_statistics "Frequentist statistics").<sup id="cite_ref-Fienberg2006_56-1"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-Fienberg2006-56">[56]</a></sup>

In the 20th century, the ideas of Laplace were further developed in two different directions, giving rise to _objective_ and _subjective_ currents in Bayesian practice. In the objective or "non-informative" current, the statistical analysis depends on only the model assumed, the data analyzed,<sup id="cite_ref-Bernardo2005_57-0"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-Bernardo2005-57">[57]</a></sup> and the method assigning the prior, which differs from one objective Bayesian practitioner to another. In the subjective or "informative" current, the specification of the prior depends on the belief (that is, propositions on which the analysis is prepared to act), which can summarize information from experts, previous studies, etc.

In the 1980s, there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo "Markov chain Monte Carlo") methods, which removed many of the computational problems, and an increasing interest in nonstandard, complex applications.<sup id="cite_ref-Wolpert2004_58-0"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-Wolpert2004-58">[58]</a></sup> Despite growth of Bayesian research, most undergraduate teaching is still based on frequentist statistics.<sup id="cite_ref-Bernardo2006_59-0"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-Bernardo2006-59">[59]</a></sup> Nonetheless, Bayesian methods are widely accepted and used, such as for example in the field of [machine learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning").<sup id="cite_ref-Bishop2007_60-0"><a href="https://en.wikipedia.org/wiki/Bayesian_inference#cite_note-Bishop2007-60">[60]</a></sup>

## See also\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=33 "Edit section: See also")\]

-   [Bayesian approaches to brain function](https://en.wikipedia.org/wiki/Bayesian_approaches_to_brain_function "Bayesian approaches to brain function")
-   [Credibility theory](https://en.wikipedia.org/wiki/Credibility_theory "Credibility theory")
-   [Epistemology](https://en.wikipedia.org/wiki/Epistemology "Epistemology")
-   [Free energy principle](https://en.wikipedia.org/wiki/Free_energy_principle "Free energy principle")
-   [Inductive probability](https://en.wikipedia.org/wiki/Inductive_probability "Inductive probability")
-   [Information field theory](https://en.wikipedia.org/wiki/Information_field_theory "Information field theory")
-   [Principle of maximum entropy](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy "Principle of maximum entropy")
-   [Probabilistic causation](https://en.wikipedia.org/wiki/Probabilistic_causation "Probabilistic causation")
-   [Probabilistic programming](https://en.wikipedia.org/wiki/Probabilistic_programming "Probabilistic programming")

## References\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=34 "Edit section: References")\]

### Citations\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=35 "Edit section: Citations")\]

1.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-1 "Jump up")** ["Bayesian"](https://www.merriam-webster.com/dictionary/Bayesian). _[Merriam-Webster.com Dictionary](https://en.wikipedia.org/wiki/Merriam-Webster "Merriam-Webster")_.
2.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-2 "Jump up")** Hacking, Ian (December 1967). "Slightly More Realistic Personal Probability". _Philosophy of Science_. **34** (4): 316. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1086/288169](https://doi.org/10.1086%2F288169). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [14344339](https://api.semanticscholar.org/CorpusID:14344339).
3.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-3 "Jump up")** ["Bayes' Theorem (Stanford Encyclopedia of Philosophy)"](http://plato.stanford.edu/entries/bayes-theorem/). Plato.stanford.edu. Retrieved 2014-01-05.
4.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-4 "Jump up")** [van Fraassen, B.](https://en.wikipedia.org/wiki/Bas_van_Fraassen "Bas van Fraassen") (1989) _Laws and Symmetry_, Oxford University Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [0-19-824860-1](https://en.wikipedia.org/wiki/Special:BookSources/0-19-824860-1 "Special:BookSources/0-19-824860-1").
5.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-5 "Jump up")** Gelman, Andrew; Carlin, John B.; Stern, Hal S.; Dunson, David B.; Vehtari, Aki; Rubin, Donald B. (2013). _Bayesian Data Analysis_, Third Edition. Chapman and Hall/CRC. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-1-4398-4095-5](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4398-4095-5 "Special:BookSources/978-1-4398-4095-5").
6.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-deCarvalho-Geometry_6-0 "Jump up")** de Carvalho, Miguel; Page, Garritt; Barney, Bradley (2019). ["On the geometry of Bayesian inference"](https://www.maths.ed.ac.uk/~mdecarv/papers/decarvalho2018.pdf) (PDF). _Bayesian Analysis_. **14** (4): 1013‒1036. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1214/18-BA1112](https://doi.org/10.1214%2F18-BA1112). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [88521802](https://api.semanticscholar.org/CorpusID:88521802).
7.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-Lee-GibbsSampler_7-0 "Jump up")** Lee, Se Yoon (2021). "Gibbs sampler and coordinate ascent variational inference: A set-theoretical review". _Communications in Statistics – Theory and Methods_. **51** (6): 1549–1568. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[2008.01006](https://arxiv.org/abs/2008.01006). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1080/03610926.2021.1921214](https://doi.org/10.1080%2F03610926.2021.1921214). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [220935477](https://api.semanticscholar.org/CorpusID:220935477).
8.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-8 "Jump up")** Kolmogorov, A.N. (1933) \[1956\]. _Foundations of the Theory of Probability_. Chelsea Publishing Company.
9.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-9 "Jump up")** Tjur, Tue (1980). [_Probability based on Radon measures_](http://archive.org/details/probabilitybased0000tjur). Internet Archive. Chichester \[Eng.\] ; New York : Wiley. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-471-27824-5](https://en.wikipedia.org/wiki/Special:BookSources/978-0-471-27824-5 "Special:BookSources/978-0-471-27824-5").
10.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-10 "Jump up")** Taraldsen, Gunnar (2024). ["Two annoying problems and their solution: Is Bayesian inference well defined? What about improper priors?"](https://rgdoi.net/10.13140/RG.2.2.26246.38726). _RG_. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.13140/RG.2.2.26246.38726](https://doi.org/10.13140%2FRG.2.2.26246.38726).
11.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-11 "Jump up")** Taraldsen, Gunnar; Tufto, Jarle; Lindqvist, Bo H. (2021-07-24). ["Improper priors and improper posteriors"](https://doi.org/10.1111%2Fsjos.12550). _Scandinavian Journal of Statistics_. **49** (3): 969–991. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1111/sjos.12550](https://doi.org/10.1111%2Fsjos.12550). [hdl](https://en.wikipedia.org/wiki/Hdl_(identifier) "Hdl (identifier)"):[11250/2984409](https://hdl.handle.net/11250%2F2984409). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)") [0303-6898](https://www.worldcat.org/issn/0303-6898). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [237736986](https://api.semanticscholar.org/CorpusID:237736986).
12.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-12 "Jump up")** Robert, Christian P.; Casella, George (2004). [_Monte Carlo Statistical Methods_](http://worldcat.org/oclc/1159112760). Springer. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-1475741452](https://en.wikipedia.org/wiki/Special:BookSources/978-1475741452 "Special:BookSources/978-1475741452"). [OCLC](https://en.wikipedia.org/wiki/OCLC_(identifier) "OCLC (identifier)") [1159112760](https://www.worldcat.org/oclc/1159112760).
13.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-13 "Jump up")** Freedman, DA (1963). ["On the asymptotic behavior of Bayes' estimates in the discrete case"](https://doi.org/10.1214%2Faoms%2F1177703871). _The Annals of Mathematical Statistics_. **34** (4): 1386–1403. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1214/aoms/1177703871](https://doi.org/10.1214%2Faoms%2F1177703871). [JSTOR](https://en.wikipedia.org/wiki/JSTOR_(identifier) "JSTOR (identifier)") [2238346](https://www.jstor.org/stable/2238346).
14.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-14 "Jump up")** Freedman, DA (1965). ["On the asymptotic behavior of Bayes estimates in the discrete case II"](https://doi.org/10.1214%2Faoms%2F1177700155). _The Annals of Mathematical Statistics_. **36** (2): 454–456. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1214/aoms/1177700155](https://doi.org/10.1214%2Faoms%2F1177700155). [JSTOR](https://en.wikipedia.org/wiki/JSTOR_(identifier) "JSTOR (identifier)") [2238150](https://www.jstor.org/stable/2238150).
15.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-15 "Jump up")** Robins, James; Wasserman, Larry (2000). "Conditioning, likelihood, and coherence: A review of some foundational concepts". _Journal of the American Statistical Association_. **95** (452): 1340–1346. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1080/01621459.2000.10474344](https://doi.org/10.1080%2F01621459.2000.10474344). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [120767108](https://api.semanticscholar.org/CorpusID:120767108).
16.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-16 "Jump up")** [Sen, Pranab K.](https://en.wikipedia.org/wiki/Pranab_K._Sen "Pranab K. Sen"); Keating, J. P.; Mason, R. L. (1993). _Pitman's measure of closeness: A comparison of statistical estimators_. Philadelphia: SIAM.
17.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-17 "Jump up")** Choudhuri, Nidhan; Ghosal, Subhashis; Roy, Anindya (2005-01-01). "Bayesian Methods for Function Estimation". _Handbook of Statistics_. Bayesian Thinking. Vol. 25. pp. 373–414. [CiteSeerX](https://en.wikipedia.org/wiki/CiteSeerX_(identifier) "CiteSeerX (identifier)") [10.1.1.324.3052](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.324.3052). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1016/s0169-7161(05)25013-7](https://doi.org/10.1016%2Fs0169-7161%2805%2925013-7). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [9780444515391](https://en.wikipedia.org/wiki/Special:BookSources/9780444515391 "Special:BookSources/9780444515391").
18.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-18 "Jump up")** ["Maximum A Posteriori (MAP) Estimation"](https://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php). _www.probabilitycourse.com_. Retrieved 2017-06-02.
19.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-19 "Jump up")** Yu, Angela. ["Introduction to Bayesian Decision Theory"](https://web.archive.org/web/20130228060536/http://www.cogsci.ucsd.edu/~ajyu/Teaching/Tutorials/bayes_dt.pdf) (PDF). _cogsci.ucsd.edu/_. Archived from [the original](http://www.cogsci.ucsd.edu/~ajyu/Teaching/Tutorials/bayes_dt.pdf) (PDF) on 2013-02-28.
20.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-20 "Jump up")** Hitchcock, David. ["Posterior Predictive Distribution Stat Slide"](http://people.stat.sc.edu/Hitchcock/stat535slidesday18.pdf) (PDF). _stat.sc.edu_.
21.  ^ [Jump up to: <sup><i><b>a</b></i></sup>](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-Bickel_&_Doksum_2001,_page_32_21-0) [<sup><i><b>b</b></i></sup>](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-Bickel_&_Doksum_2001,_page_32_21-1) Bickel & Doksum (2001, p. 32)
22.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-22 "Jump up")** [Kiefer, J.](https://en.wikipedia.org/wiki/Jack_Kiefer_(mathematician) "Jack Kiefer (mathematician)"); Schwartz R. (1965). ["Admissible Bayes Character of T<sup>2</sup>\-, R<sup>2</sup>\-, and Other Fully Invariant Tests for Multivariate Normal Problems"](https://doi.org/10.1214%2Faoms%2F1177700051). _Annals of Mathematical Statistics_. **36** (3): 747–770. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1214/aoms/1177700051](https://doi.org/10.1214%2Faoms%2F1177700051).
23.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-23 "Jump up")** Schwartz, R. (1969). ["Invariant Proper Bayes Tests for Exponential Families"](https://doi.org/10.1214%2Faoms%2F1177697822). _Annals of Mathematical Statistics_. **40**: 270–283. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1214/aoms/1177697822](https://doi.org/10.1214%2Faoms%2F1177697822).
24.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-24 "Jump up")** Hwang, J. T. & Casella, George (1982). ["Minimax Confidence Sets for the Mean of a Multivariate Normal Distribution"](http://ecommons.cornell.edu/bitstream/1813/32852/1/BU-750-M.pdf) (PDF). _Annals of Statistics_. **10** (3): 868–881. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1214/aos/1176345877](https://doi.org/10.1214%2Faos%2F1176345877).
25.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-25 "Jump up")** [Lehmann, Erich](https://en.wikipedia.org/wiki/Erich_Leo_Lehmann "Erich Leo Lehmann") (1986). _Testing Statistical Hypotheses_ (Second ed.). (see p. 309 of Chapter 6.7 "Admissibility", and pp. 17–18 of Chapter 1.8 "Complete Classes"
26.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-26 "Jump up")** [Le Cam, Lucien](https://en.wikipedia.org/wiki/Lucien_Le_Cam "Lucien Le Cam") (1986). _Asymptotic Methods in Statistical Decision Theory_. Springer-Verlag. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-387-96307-5](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-96307-5 "Special:BookSources/978-0-387-96307-5"). (From "Chapter 12 Posterior Distributions and Bayes Solutions", p. 324)
27.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-27 "Jump up")** [Cox, D. R.](https://en.wikipedia.org/wiki/David_R._Cox "David R. Cox"); Hinkley, D.V. (1974). _Theoretical Statistics_. Chapman and Hall. p. 432. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-04-121537-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-04-121537-3 "Special:BookSources/978-0-04-121537-3").
28.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-28 "Jump up")** [Cox, D. R.](https://en.wikipedia.org/wiki/David_R._Cox "David R. Cox"); Hinkley, D.V. (1974). _Theoretical Statistics_. Chapman and Hall. p. 433. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-04-121537-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-04-121537-3 "Special:BookSources/978-0-04-121537-3").)
29.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-29 "Jump up")** Stoica, P.; Selen, Y. (2004). "A review of information criterion rules". _IEEE Signal Processing Magazine_. **21** (4): 36–47. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1109/MSP.2004.1311138](https://doi.org/10.1109%2FMSP.2004.1311138). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [17338979](https://api.semanticscholar.org/CorpusID:17338979).
30.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-30 "Jump up")** Fatermans, J.; Van Aert, S.; den Dekker, A.J. (2019). "The maximum a posteriori probability rule for atom column detection from HAADF STEM images". _Ultramicroscopy_. **201**: 81–91. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[1902.05809](https://arxiv.org/abs/1902.05809). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1016/j.ultramic.2019.02.003](https://doi.org/10.1016%2Fj.ultramic.2019.02.003). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)") [30991277](https://pubmed.ncbi.nlm.nih.gov/30991277). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [104419861](https://api.semanticscholar.org/CorpusID:104419861).
31.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-31 "Jump up")** Bessiere, P., Mazer, E., Ahuactzin, J. M., & Mekhnacha, K. (2013). Bayesian Programming (1 edition) Chapman and Hall/CRC.
32.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-32 "Jump up")** Daniel Roy (2015). ["Probabilistic Programming"](https://web.archive.org/web/20160110035042/http://probabilistic-programming.org/wiki/Home). _probabilistic-programming.org_. Archived from [the original](http://probabilistic-programming.org/wiki/Home) on 2016-01-10. Retrieved 2020-01-02.
33.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-33 "Jump up")** Ghahramani, Z (2015). ["Probabilistic machine learning and artificial intelligence"](https://www.repository.cam.ac.uk/handle/1810/248538). _Nature_. **521** (7553): 452–459. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[2015Natur.521..452G](https://ui.adsabs.harvard.edu/abs/2015Natur.521..452G). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1038/nature14541](https://doi.org/10.1038%2Fnature14541). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)") [26017444](https://pubmed.ncbi.nlm.nih.gov/26017444). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [216356](https://api.semanticscholar.org/CorpusID:216356).
34.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-34 "Jump up")** Fienberg, Stephen E. (2006-03-01). ["When did Bayesian inference become "Bayesian"?"](https://doi.org/10.1214%2F06-BA101). _Bayesian Analysis_. **1** (1). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1214/06-BA101](https://doi.org/10.1214%2F06-BA101).
35.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-35 "Jump up")** Jim Albert (2009). _Bayesian Computation with R, Second edition_. New York, Dordrecht, etc.: Springer. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-387-92297-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-92297-3 "Special:BookSources/978-0-387-92297-3").
36.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-36 "Jump up")** Rathmanner, Samuel; Hutter, Marcus; Ormerod, Thomas C (2011). ["A Philosophical Treatise of Universal Induction"](https://doi.org/10.3390%2Fe13061076). _Entropy_. **13** (6): 1076–1136. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[1105.5721](https://arxiv.org/abs/1105.5721). [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[2011Entrp..13.1076R](https://ui.adsabs.harvard.edu/abs/2011Entrp..13.1076R). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.3390/e13061076](https://doi.org/10.3390%2Fe13061076). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [2499910](https://api.semanticscholar.org/CorpusID:2499910).
37.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-37 "Jump up")** Hutter, Marcus; He, Yang-Hui; Ormerod, Thomas C (2007). "On Universal Prediction and Bayesian Confirmation". _Theoretical Computer Science_. **384** (2007): 33–48. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[0709.1516](https://arxiv.org/abs/0709.1516). [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[2007arXiv0709.1516H](https://ui.adsabs.harvard.edu/abs/2007arXiv0709.1516H). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1016/j.tcs.2007.05.016](https://doi.org/10.1016%2Fj.tcs.2007.05.016). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [1500830](https://api.semanticscholar.org/CorpusID:1500830).
38.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-38 "Jump up")** Gács, Peter; Vitányi, Paul M. B. (2 December 2010). "Raymond J. Solomonoff 1926-2009". [CiteSeerX](https://en.wikipedia.org/wiki/CiteSeerX_(identifier) "CiteSeerX (identifier)") [10.1.1.186.8268](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.186.8268).
39.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-:edgr_39-0 "Jump up")** Robinson, Mark D & McCarthy, Davis J & Smyth, Gordon K edgeR: a Bioconductor package for differential expression analysis of digital gene expression data, Bioinformatics.
40.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-40 "Jump up")** ["CIRI"](https://ciri.stanford.edu/). _ciri.stanford.edu_. Retrieved 2019-08-11.
41.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-41 "Jump up")** Kurtz, David M.; Esfahani, Mohammad S.; Scherer, Florian; Soo, Joanne; Jin, Michael C.; Liu, Chih Long; Newman, Aaron M.; Dührsen, Ulrich; Hüttmann, Andreas (2019-07-25). ["Dynamic Risk Profiling Using Serial Tumor Biomarkers for Personalized Outcome Prediction"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7380118). _Cell_. **178** (3): 699–713.e19. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1016/j.cell.2019.06.011](https://doi.org/10.1016%2Fj.cell.2019.06.011). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)") [1097-4172](https://www.worldcat.org/issn/1097-4172). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) "PMC (identifier)") [7380118](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7380118). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)") [31280963](https://pubmed.ncbi.nlm.nih.gov/31280963).
42.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-42 "Jump up")** Dawid, A. P. and Mortera, J. (1996) "Coherent Analysis of Forensic Identification Evidence". _[Journal of the Royal Statistical Society](https://en.wikipedia.org/wiki/Journal_of_the_Royal_Statistical_Society "Journal of the Royal Statistical Society")_, Series B, 58, 425–443.
43.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-43 "Jump up")** Foreman, L. A.; Smith, A. F. M., and Evett, I. W. (1997). "Bayesian analysis of deoxyribonucleic acid profiling data in forensic identification applications (with discussion)". _Journal of the Royal Statistical Society_, Series A, 160, 429–469.
44.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-44 "Jump up")** Robertson, B. and Vignaux, G. A. (1995) _Interpreting Evidence: Evaluating Forensic Science in the Courtroom_. John Wiley and Sons. Chichester. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-471-96026-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-471-96026-3 "Special:BookSources/978-0-471-96026-3").
45.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-45 "Jump up")** Dawid, A. P. (2001) [Bayes' Theorem and Weighing Evidence by Juries](http://128.40.111.250/evidence/content/dawid-paper.pdf). [Archived](https://web.archive.org/web/20150701112146/http://128.40.111.250/evidence/content/dawid-paper.pdf) 2015-07-01 at the [Wayback Machine](https://en.wikipedia.org/wiki/Wayback_Machine "Wayback Machine")
46.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-46 "Jump up")** Gardner-Medwin, A. (2005) "What Probability Should the Jury Address?". _[Significance](https://en.wikipedia.org/wiki/Significance_(journal) "Significance (journal)")_, 2 (1), March 2005.
47.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-47 "Jump up")** Miller, David (1994). [_Critical Rationalism_](https://books.google.com/books?id=bh_yCgAAQBAJ). Chicago: Open Court. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-8126-9197-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8126-9197-9 "Special:BookSources/978-0-8126-9197-9").
48.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-48 "Jump up")** Howson & Urbach (2005), Jaynes (2003)
49.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-Cai_et_al._2009_49-0 "Jump up")** Cai, X.Q.; Wu, X.Y.; Zhou, X. (2009). "Stochastic scheduling subject to breakdown-repeat breakdowns with incomplete information". _Operations Research_. **57** (5): 1236–1249. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1287/opre.1080.0660](https://doi.org/10.1287%2Fopre.1080.0660).
50.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-50 "Jump up")** Ogle, Kiona; Tucker, Colin; Cable, Jessica M. (2014-01-01). "Beyond simple linear mixing models: process-based isotope partitioning of ecological processes". _Ecological Applications_. **24** (1): 181–195. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1890/1051-0761-24.1.181](https://doi.org/10.1890%2F1051-0761-24.1.181). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)") [1939-5582](https://www.worldcat.org/issn/1939-5582). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)") [24640543](https://pubmed.ncbi.nlm.nih.gov/24640543).
51.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-51 "Jump up")** Evaristo, Jaivime; McDonnell, Jeffrey J.; Scholl, Martha A.; Bruijnzeel, L. Adrian; Chun, Kwok P. (2016-01-01). "Insights into plant water uptake from xylem-water isotope measurements in two tropical catchments with contrasting moisture conditions". _Hydrological Processes_. **30** (18): 3210–3227. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[2016HyPr...30.3210E](https://ui.adsabs.harvard.edu/abs/2016HyPr...30.3210E). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1002/hyp.10841](https://doi.org/10.1002%2Fhyp.10841). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)") [1099-1085](https://www.worldcat.org/issn/1099-1085). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [131588159](https://api.semanticscholar.org/CorpusID:131588159).
52.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-52 "Jump up")** Gupta, Ankur; Rawlings, James B. (April 2014). ["Comparison of Parameter Estimation Methods in Stochastic Chemical Kinetic Models: Examples in Systems Biology"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4946376). _AIChE Journal_. **60** (4): 1253–1268. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[2014AIChE..60.1253G](https://ui.adsabs.harvard.edu/abs/2014AIChE..60.1253G). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1002/aic.14409](https://doi.org/10.1002%2Faic.14409). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) "ISSN (identifier)") [0001-1541](https://www.worldcat.org/issn/0001-1541). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) "PMC (identifier)") [4946376](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4946376). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)") [27429455](https://pubmed.ncbi.nlm.nih.gov/27429455).
53.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-53 "Jump up")** Fornalski, K.W. (2016). ["The Tadpole Bayesian Model for Detecting Trend Changes in Financial Quotations"](http://www.rroij.com/open-access/the-tadpole-bayesian-model-for-detecting-trend-changesin-financial-quotations-.pdf) (PDF). _R&R Journal of Statistics and Mathematical Sciences_. **2** (1): 117–122.
54.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-54 "Jump up")** Schütz, N.; Holschneider, M. (2011). "Detection of trend changes in time series using Bayesian inference". _Physical Review E_. **84** (2): 021120. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) "ArXiv (identifier)"):[1104.3448](https://arxiv.org/abs/1104.3448). [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[2011PhRvE..84b1120S](https://ui.adsabs.harvard.edu/abs/2011PhRvE..84b1120S). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1103/PhysRevE.84.021120](https://doi.org/10.1103%2FPhysRevE.84.021120). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)") [21928962](https://pubmed.ncbi.nlm.nih.gov/21928962). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [11460968](https://api.semanticscholar.org/CorpusID:11460968).
55.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-Stigler1986_55-0 "Jump up")** Stigler, Stephen M. (1986). ["Chapter 3"](https://archive.org/details/historyofstatist00stig). _The History of Statistics_. Harvard University Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [9780674403406](https://en.wikipedia.org/wiki/Special:BookSources/9780674403406 "Special:BookSources/9780674403406").
56.  ^ [Jump up to: <sup><i><b>a</b></i></sup>](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-Fienberg2006_56-0) [<sup><i><b>b</b></i></sup>](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-Fienberg2006_56-1) Fienberg, Stephen E. (2006). ["When did Bayesian Inference Become 'Bayesian'?"](https://doi.org/10.1214%2F06-ba101). _Bayesian Analysis_. **1** (1): 1–40 \[p. 5\]. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1214/06-ba101](https://doi.org/10.1214%2F06-ba101).
57.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-Bernardo2005_57-0 "Jump up")** [Bernardo, José-Miguel](https://en.wikipedia.org/wiki/Jos%C3%A9-Miguel_Bernardo "José-Miguel Bernardo") (2005). "Reference analysis". _Handbook of statistics_. Vol. 25. pp. 17–90.
58.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-Wolpert2004_58-0 "Jump up")** Wolpert, R. L. (2004). "A Conversation with James O. Berger". _Statistical Science_. **19** (1): 205–218. [CiteSeerX](https://en.wikipedia.org/wiki/CiteSeerX_(identifier) "CiteSeerX (identifier)") [10.1.1.71.6112](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.71.6112). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1214/088342304000000053](https://doi.org/10.1214%2F088342304000000053). [MR](https://en.wikipedia.org/wiki/MR_(identifier) "MR (identifier)") [2082155](https://mathscinet.ams.org/mathscinet-getitem?mr=2082155). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [120094454](https://api.semanticscholar.org/CorpusID:120094454).
59.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-Bernardo2006_59-0 "Jump up")** [Bernardo, José M.](https://en.wikipedia.org/wiki/Jos%C3%A9-Miguel_Bernardo "José-Miguel Bernardo") (2006). ["A Bayesian mathematical statistics primer"](http://www.ime.usp.br/~abe/ICOTS7/Proceedings/PDFs/InvitedPapers/3I2_BERN.pdf) (PDF). _Icots-7_.
60.  **[^](https://en.wikipedia.org/wiki/Bayesian_inference#cite_ref-Bishop2007_60-0 "Jump up")** Bishop, C. M. (2007). _Pattern Recognition and Machine Learning_. New York: Springer. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0387310732](https://en.wikipedia.org/wiki/Special:BookSources/978-0387310732 "Special:BookSources/978-0387310732").

### Sources\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=36 "Edit section: Sources")\]

-   Aster, Richard; Borchers, Brian, and Thurber, Clifford (2012). _Parameter Estimation and Inverse Problems_, Second Edition, Elsevier. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [0123850487](https://en.wikipedia.org/wiki/Special:BookSources/0123850487 "Special:BookSources/0123850487"), [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0123850485](https://en.wikipedia.org/wiki/Special:BookSources/978-0123850485 "Special:BookSources/978-0123850485")
-   Bickel, Peter J. & Doksum, Kjell A. (2001). _Mathematical Statistics, Volume 1: Basic and Selected Topics_ (Second (updated printing 2007) ed.). Pearson Prentice–Hall. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-13-850363-5](https://en.wikipedia.org/wiki/Special:BookSources/978-0-13-850363-5 "Special:BookSources/978-0-13-850363-5").
-   [Box, G. E. P.](https://en.wikipedia.org/wiki/George_E._P._Box "George E. P. Box") and [Tiao, G. C.](https://en.wikipedia.org/w/index.php?title=George_Tiao&action=edit&redlink=1 "George Tiao (page does not exist)") (1973). _Bayesian Inference in Statistical Analysis_, Wiley, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [0-471-57428-7](https://en.wikipedia.org/wiki/Special:BookSources/0-471-57428-7 "Special:BookSources/0-471-57428-7")
-   Edwards, Ward (1968). "Conservatism in Human Information Processing". In Kleinmuntz, B. (ed.). _Formal Representation of Human Judgment_. Wiley.
-   Edwards, Ward (1982). [Daniel Kahneman](https://en.wikipedia.org/wiki/Daniel_Kahneman "Daniel Kahneman"); [Paul Slovic](https://en.wikipedia.org/wiki/Paul_Slovic "Paul Slovic"); [Amos Tversky](https://en.wikipedia.org/wiki/Amos_Tversky "Amos Tversky") (eds.). "Judgment under uncertainty: Heuristics and biases". _Science_. **185** (4157): 1124–1131. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[1974Sci...185.1124T](https://ui.adsabs.harvard.edu/abs/1974Sci...185.1124T). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) "Doi (identifier)"):[10.1126/science.185.4157.1124](https://doi.org/10.1126%2Fscience.185.4157.1124). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) "PMID (identifier)") [17835457](https://pubmed.ncbi.nlm.nih.gov/17835457). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) "S2CID (identifier)") [143452957](https://api.semanticscholar.org/CorpusID:143452957). Chapter: Conservatism in Human Information Processing (excerpted)
-   [Jaynes E. T.](https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes "Edwin Thompson Jaynes") (2003) _Probability Theory: The Logic of Science_, CUP. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-521-59271-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-59271-0 "Special:BookSources/978-0-521-59271-0") ([Link to Fragmentary Edition of March 1996](http://www-biba.inrialpes.fr/Jaynes/prob.html)).
-   [Howson, C.](https://en.wikipedia.org/wiki/Colin_Howson "Colin Howson") & Urbach, P. (2005). _Scientific Reasoning: the Bayesian Approach_ (3rd ed.). [Open Court Publishing Company](https://en.wikipedia.org/wiki/Open_Court_Publishing_Company "Open Court Publishing Company"). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-8126-9578-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8126-9578-6 "Special:BookSources/978-0-8126-9578-6").
-   Phillips, L. D.; Edwards, Ward (October 2008). "Chapter 6: Conservatism in a Simple Probability Inference Task (_Journal of Experimental Psychology_ (1966) 72: 346-354)". In Jie W. Weiss; David J. Weiss (eds.). _A Science of Decision Making:The Legacy of Ward Edwards_. Oxford University Press. p. 536. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-19-532298-9](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-532298-9 "Special:BookSources/978-0-19-532298-9").

## Further reading\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=37 "Edit section: Further reading")\]

-   For a full report on the history of Bayesian statistics and the debates with frequentists approaches, read Vallverdu, Jordi (2016). _Bayesians Versus Frequentists A Philosophical Debate on Statistical Reasoning_. New York: Springer. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-3-662-48638-2](https://en.wikipedia.org/wiki/Special:BookSources/978-3-662-48638-2 "Special:BookSources/978-3-662-48638-2").
-   [Clayton, Aubrey](https://en.wikipedia.org/w/index.php?title=Aubrey_Clayton&action=edit&redlink=1 "Aubrey Clayton (page does not exist)") (August 2021). [_Bernoulli's Fallacy: Statistical Illogic and the Crisis of Modern Science_](https://cup.columbia.edu/book/bernoullis-fallacy/9780231199940). Columbia University Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-231-55335-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-231-55335-3 "Special:BookSources/978-0-231-55335-3").

### Elementary\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=38 "Edit section: Elementary")\]

The following books are listed in ascending order of probabilistic sophistication:

-   Stone, JV (2013), "Bayes' Rule: A Tutorial Introduction to Bayesian Analysis", [Download first chapter here](http://jim-stone.staff.shef.ac.uk/BookBayes2012/BayesRuleBookMain.html), Sebtel Press, England.
-   [Dennis V. Lindley](https://en.wikipedia.org/wiki/Dennis_V._Lindley "Dennis V. Lindley") (2013). _Understanding Uncertainty, Revised Edition_ (2nd ed.). John Wiley. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-1-118-65012-7](https://en.wikipedia.org/wiki/Special:BookSources/978-1-118-65012-7 "Special:BookSources/978-1-118-65012-7").
-   [Colin Howson](https://en.wikipedia.org/wiki/Colin_Howson "Colin Howson") & Peter Urbach (2005). _Scientific Reasoning: The Bayesian Approach_ (3rd ed.). [Open Court Publishing Company](https://en.wikipedia.org/wiki/Open_Court_Publishing_Company "Open Court Publishing Company"). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-8126-9578-6](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8126-9578-6 "Special:BookSources/978-0-8126-9578-6").
-   Berry, Donald A. (1996). _Statistics: A Bayesian Perspective_. Duxbury. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-534-23476-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-534-23476-8 "Special:BookSources/978-0-534-23476-8").
-   [Morris H. DeGroot](https://en.wikipedia.org/wiki/Morris_H._DeGroot "Morris H. DeGroot") & Mark J. Schervish (2002). [_Probability and Statistics_](https://archive.org/details/probabilitystati00degr_0) (third ed.). Addison-Wesley. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-201-52488-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-201-52488-8 "Special:BookSources/978-0-201-52488-8").
-   Bolstad, William M. (2007) _Introduction to Bayesian Statistics_: Second Edition, John Wiley [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [0-471-27020-2](https://en.wikipedia.org/wiki/Special:BookSources/0-471-27020-2 "Special:BookSources/0-471-27020-2")
-   Winkler, Robert L (2003). _Introduction to Bayesian Inference and Decision_ (2nd ed.). Probabilistic. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-9647938-4-2](https://en.wikipedia.org/wiki/Special:BookSources/978-0-9647938-4-2 "Special:BookSources/978-0-9647938-4-2"). Updated classic textbook. Bayesian theory clearly presented.
-   Lee, Peter M. _Bayesian Statistics: An Introduction_. Fourth Edition (2012), John Wiley [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-1-1183-3257-3](https://en.wikipedia.org/wiki/Special:BookSources/978-1-1183-3257-3 "Special:BookSources/978-1-1183-3257-3")
-   Carlin, Bradley P. & Louis, Thomas A. (2008). _Bayesian Methods for Data Analysis, Third Edition_. Boca Raton, FL: Chapman and Hall/CRC. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-1-58488-697-6](https://en.wikipedia.org/wiki/Special:BookSources/978-1-58488-697-6 "Special:BookSources/978-1-58488-697-6").
-   [Gelman, Andrew](https://en.wikipedia.org/wiki/Andrew_Gelman "Andrew Gelman"); Carlin, John B.; Stern, Hal S.; Dunson, David B.; Vehtari, Aki; [Rubin, Donald B.](https://en.wikipedia.org/wiki/Donald_Rubin "Donald Rubin") (2013). _Bayesian Data Analysis, Third Edition_. Chapman and Hall/CRC. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-1-4398-4095-5](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4398-4095-5 "Special:BookSources/978-1-4398-4095-5").

### Intermediate or advanced\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=39 "Edit section: Intermediate or advanced")\]

-   [Berger, James O](https://en.wikipedia.org/wiki/James_Berger_(statistician) "James Berger (statistician)") (1985). _Statistical Decision Theory and Bayesian Analysis_. Springer Series in Statistics (Second ed.). Springer-Verlag. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) "Bibcode (identifier)"):[1985sdtb.book.....B](https://ui.adsabs.harvard.edu/abs/1985sdtb.book.....B). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-387-96098-2](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-96098-2 "Special:BookSources/978-0-387-96098-2").
-   [Bernardo, José M.](https://en.wikipedia.org/wiki/Jos%C3%A9-Miguel_Bernardo "José-Miguel Bernardo"); [Smith, Adrian F. M.](https://en.wikipedia.org/wiki/Adrian_Smith_(statistician) "Adrian Smith (statistician)") (1994). _Bayesian Theory_. Wiley.
-   [DeGroot, Morris H.](https://en.wikipedia.org/wiki/Morris_H._DeGroot "Morris H. DeGroot"), _Optimal Statistical Decisions_. Wiley Classics Library. 2004. (Originally published (1970) by McGraw-Hill.) [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [0-471-68029-X](https://en.wikipedia.org/wiki/Special:BookSources/0-471-68029-X "Special:BookSources/0-471-68029-X").
-   Schervish, Mark J. (1995). _Theory of statistics_. Springer-Verlag. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-387-94546-0](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-94546-0 "Special:BookSources/978-0-387-94546-0").
-   Jaynes, E. T. (1998). [_Probability Theory: The Logic of Science_](http://www-biba.inrialpes.fr/Jaynes/prob.html).
-   O'Hagan, A. and Forster, J. (2003). _Kendall's Advanced Theory of Statistics_, Volume 2B: _Bayesian Inference_. Arnold, New York. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [0-340-52922-9](https://en.wikipedia.org/wiki/Special:BookSources/0-340-52922-9 "Special:BookSources/0-340-52922-9").
-   Robert, Christian P (2007). _The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation_ (paperback ed.). Springer. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-0-387-71598-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-387-71598-8 "Special:BookSources/978-0-387-71598-8").
-   [Pearl, Judea](https://en.wikipedia.org/wiki/Judea_Pearl "Judea Pearl"). (1988). _Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference_, San Mateo, CA: Morgan Kaufmann.
-   Pierre Bessière et al. (2013). "[Bayesian Programming](http://www.crcpress.com/product/isbn/9781439880326)". CRC Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [9781439880326](https://en.wikipedia.org/wiki/Special:BookSources/9781439880326 "Special:BookSources/9781439880326")
-   Francisco J. Samaniego (2010). "A Comparison of the Bayesian and Frequentist Approaches to Estimation". Springer. New York, [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) "ISBN (identifier)") [978-1-4419-5940-9](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4419-5940-9 "Special:BookSources/978-1-4419-5940-9")

## External links\[[edit](https://en.wikipedia.org/w/index.php?title=Bayesian_inference&action=edit&section=40 "Edit section: External links")\]

-   ["Bayesian approach to statistical problems"](https://www.encyclopediaofmath.org/index.php?title=Bayesian_approach_to_statistical_problems), _[Encyclopedia of Mathematics](https://en.wikipedia.org/wiki/Encyclopedia_of_Mathematics "Encyclopedia of Mathematics")_, [EMS Press](https://en.wikipedia.org/wiki/European_Mathematical_Society "European Mathematical Society"), 2001 \[1994\]
-   [Bayesian Statistics](http://www.scholarpedia.org/article/Bayesian_statistics) from Scholarpedia.
-   [Introduction to Bayesian probability](http://www.dcs.qmw.ac.uk/%7Enorman/BBNs/BBNs.htm) from Queen Mary University of London
-   [Mathematical Notes on Bayesian Statistics and Markov Chain Monte Carlo](http://webuser.bus.umich.edu/plenk/downloads.htm)
-   [Bayesian reading list](http://cocosci.berkeley.edu/tom/bayes.html), categorized and annotated by [Tom Griffiths](https://web.archive.org/web/20060711151352/http://psychology.berkeley.edu/faculty/profiles/tgriffiths.html)
-   A. Hajek and S. Hartmann: [Bayesian Epistemology](https://web.archive.org/web/20110728055439/http://stephanhartmann.org/HajekHartmann_BayesEpist.pdf), in: J. Dancy et al. (eds.), A Companion to Epistemology. Oxford: Blackwell 2010, 93–106.
-   S. Hartmann and J. Sprenger: [Bayesian Epistemology](https://web.archive.org/web/20110728055519/http://stephanhartmann.org/HartmannSprenger_BayesEpis.pdf), in: S. Bernecker and D. Pritchard (eds.), Routledge Companion to Epistemology. London: Routledge 2010, 609–620.
-   [_Stanford Encyclopedia of Philosophy_: "Inductive Logic"](http://plato.stanford.edu/entries/logic-inductive/)
-   [Bayesian Confirmation Theory](https://web.archive.org/web/20150905093734/http://faculty-staff.ou.edu/H/James.A.Hawthorne-1/Hawthorne--Bayesian_Confirmation_Theory.pdf) (PDF)
-   [What is Bayesian Learning?](http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-7.html)
-   [_Data, Uncertainty and Inference_](https://causascientia.org/math_stat/DataUnkInf.html) — Informal introduction with many examples, ebook (PDF) freely available at [causaScientia](https://causascientia.org/)
