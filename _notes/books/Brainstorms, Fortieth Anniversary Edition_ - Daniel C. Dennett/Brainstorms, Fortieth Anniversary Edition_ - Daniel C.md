---
title: Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett
author: Daniel C. Dennett
tags:
  - book
cover: _resources/books/Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett/cover.jpg
---




  

![[cover.jpg]] 

## Praise for Daniel C. Dennett’s _Brainstorms_

> “The problems that Daniel Dennett addresses in his essays are crucial ones for philosophy and contemporary science. With a sure touch and a great deal of insight, he has subjected to analysis questions that lie at, or perhaps just beyond, the frontiers of the scientific study of mind and brain. Dennett’s work should help guide progress in the understanding of the profound and troubling issues that have intrigued and perplexed critical minds for many centuries. His work is stimulating and impressive. …”
> 
> —Noam Chomsky

> “Starting with a profound knowledge of general philosophy, including the older work of the ‘ordinary language’ school, Dennett has developed a very sophisticated version of the ‘functionalist’ position in the philosophy of mind—a version which combines philosophical and psychological sophistication in an extraordinary way. The papers the Dennett has published since _Content and Consciousness_ show in particular that Dennett is aware of all of the subtleties of the problems involving such psychological phenomena as pain and mental imagery and that he is capable of integrating psychological information and philosophical discussion in an extremely exciting way. I think that his work in the philosophy of mind is of very great importance.”
> 
> —Hilary Putnam

> “There is a new coherent field in which cognitive psychologists, neurophysiologists, computer scientists in artificial intelligence, and epistemological philosophers collaborate as equals, carefully studying each other’s works. The names for the field are not as yet stabilized or equi-centered: naturalistic epistemology, descriptive epistemology, psychological epistemology, cognitive-neuropsychology, brain science, theory of intelligence (expanded from ‘artificial intelligence’), etc. Dennett is one of the leading philosophers forming this new area. He has really done his homework on computers, learning theory, central nervous system neurophysiology, and psycholinguistics. Readers in these fields will recognize Dennett as a fellow scientist, as well as a skilled philosopher, as he relates your findings and theories to the traditional epistemological puzzles of intentionality, the mind-body (mind-brain) problem, and free will. Cognitive psychologists interested in integrating their theories with neurophysiology and behavior will find this book exciting and relevant, even if they have not previously indulged an interest in epistemology issues. I enthusiastically recommend _Brainstorms_ to all of us verging on and into this exciting new field!”
> 
> —Donald Campbell

> “The essays in this volume comprise what is perhaps the most important single, sustained contribution by a philosopher to the current inter-disciplinary conversation about the nature and methodology of the ‘cognitive sciences.’ This book will surely provide a landmark in what the optimists among us think of as the post-behaviorist period in the philosophy of mind. Informed discussion of explanation in linguistics, cognitive psychology and AI starts here.”
> 
> —Jerry Fodor

> “These essays are delightful philosophical gems. I have assigned most of them in courses, both graduate and undergraduate, and have found them extraordinarily useful. They make the reader _think_. Dennett’s writing—like that of Putnam and Kripke—is somehow comprehensible and illuminating to students at all levels. The convergence of research in philosophy of mind with work in psychology and artificial intelligence has created a need for teaching material that is sophisticated about all these fields. Dennett’s book fills the bill. It is one of those rare books that pushes back the frontiers of knowledge, and at the same time is charming, entertaining, and accessible. For anyone interested in foundational issues in the sciences of the mind, Dennett’s book is a must.”
> 
> —Ned Block

> “Dennett’s studies in the philosophy of psychology are gems of conceptual analysis. His expert knowledge of the many disciplines which deal in one way or another with the nature and behavior of intentional systems gives what he has to say a substantive content which distinguishes his dialectic from the usual ballet of shadows. The topics he takes up are of strategic importance in the philosophy of mind. Each essay stands on its own feet, and that is how one would meet them if it were not for this publishing venture. Bound together, they constitute a whole which is truly more than the sum of its parts. They are deftly written and a joy to read.”
> 
> —Wilfrid Sellars

> “I am delighted to see that Daniel Dennett’s essays are being collected into a book. Dennett is one of the few writers who combines philosophical sophistication with a clear insight into the nature of computers and artificial intelligence. He raises issues which are of crucial importance to understanding what researchers in artificial intelligence are really doing, and he does so in a clear common-sensical way. He succeeds at clarifying the problems, rather than entering into the partisan debate which all too often ensues when scientists and philosophers tangle. I look forward to seeing the book and to participating in the discussion which it is bound to provoke.”
> 
> —Terry Winograd







Brainstorms


Philosophical Essays on Mind and Psychology
Fortieth Anniversary Edition



Daniel C. Dennett


 
 
 
 
 
 
A Bradford Book
The MIT Press
Cambridge, Massachusetts
London, England


 

First MIT Press edition, 1981

© 1978 Bradford Books, Publishers

Drawings in the front matter and on the cover are by Edward Gorey and are published by arrangement with The Edward Gorey Charitable Trust.

All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher.

This book was set in ITC Stone Sans Std and ITC Stone Serif Std by Toppan Best-set Premedia Limited. Printed and bound in the United States of America.

Library of Congress Cataloging-in-Publication Data

Names: Dennett, D. C. (Daniel Clement), author.

Title: Brainstorms : philosophical essays on mind and psychology / Daniel C. Dennett.

Description: 40th Anniversary Edition. | Cambridge, MA : MIT Press, 2017. | Includes bibliographical references and index.

Identifiers: LCCN 2017009922 | ISBN 9780262534383 (pbk. : alk. paper)

eISBN 9780262343718

Subjects: LCSH: Psychology--Philosophy. | Cognition. | Philosophy.

Classification: LCC BF38 .D46 2017 | DDC 153--dc23 LC record available at [https://lccn.loc.gov/2017009922](https://lccn.loc.gov/2017009922\)

ePub Version 1.0 

> for Andrea and Peter 

# Table of Contents

1. Epigraph]]
2. Title page]]
3. Copyright page]]
4. Dedication]]
5. Reflections on _Brainstorms_ after Forty Years]]
6. Preface]]
7. Introduction]]
8. I Intentional Explanation and Attributions of Mentality]]
    1. 1 Intentional Systems]]
    2. 2 Reply to Arbib and Gunderson]]
    3. 3 Brain Writing and Mind Reading]]
9. II The Nature of Theory in Psychology]]
    1. 4 Skinner Skinned]]
    2. 5 Why the Law of Effect Will Not Go Away]]
    3. 6 A Cure for the Common Code?]]
    4. 7 Artificial Intelligence as Philosophy and as Psychology]]
10. III Objects of Consciousness and the Nature of Experience]]
    1. 8 Are Dreams Experiences?]]
    2. 9 Toward a Cognitive Theory of Consciousness]]
    3. 10 Two Approaches to Mental Images]]
    4. 11 Why You Can’t Make a Computer That Feels Pain]]
11. IV Free Will and Personhood]]
    1. 12 Mechanism and Responsibility]]
    2. 13 The Abilities of Men and Machines]]
    3. 14 Conditions of Personhood]]
    4. 15 On Giving Libertarians What They Say They Want]]
    5. 16 How to Change Your Mind]]
    6. 17 Where Am I?]]
12. Acknowledgments]]
13. Index]]

## List of Illustrations

1. Figure 7.1]]
2. Figure 7.2]]
3. Figure 7.3]]
4. Figure 9.1]]
5. Figure 9.2]]
6. Figure 11.1]]
7. Figure 11.2]]
8. Figure 11.3]]
9. Figure 11.4]]
10. Figure 11.5]]

## Guide

1. Cover]]
2. Table of Contents]]Reflections on _Brainstorms_ after Forty Years

_Brainstorms_ was published in 1978, the same year that the Alfred P. Sloan Foundation inaugurated its generous program of support for cognitive science, setting off something of a gold rush among researchers in many fields who thought they could see their own favored projects as cognitive science of one kind or another. Sloan-sponsored conferences and workshops were held around the country, and soon there emerged a cadre of speakers drawn from the various component disciplines who could be counted on to present perspectives on their own discipline that were accessible to outsiders, provocative, and in many cases actually useful across disciplinary boundaries. The “Sloan Rangers,” we were dubbed, and I was one of the most junior members of the roving band, since few philosophers of the generation who had been my teachers were interested in getting out of their armchairs. (The work of a few of those philosophers was influential: Austin’s work on performatives and Grice’s work on implicatures in particular.) The philosophical corner of the interdisciplinary pentagon[^1] (along with psychology, linguistics, computer science, and neuroscience) was anchored by Jerry Fodor, Gil Harman, Bert Dreyfus, John Searle, and me, with the Churchlands soon to invade from the north. The other corners were occupied by psychologists such as George Miller, Roger Shepard, Anne Treisman, Ulric “Dick” Neisser, Zenon Pylyshyn, and Philip Johnson-Laird; linguists Noam Chomsky, Ray Jackendoff, Jim McCawley, Barbara Partee, and George Lakoff; computer scientists Allen Newell, Herbert Simon, Marvin Minsky, John McCarthy, Roger Schank, and Terry Winograd; and neuroscientists Michael Gazzaniga, Brenda Milner, Marcus Raichle, and Michael Posner.

I already knew most of these folks from my early forays into interdisciplinarity, and some had encouraged me to publish a collection of my essays. The appearance of _Brainstorms_ in near synchrony with the Sloan road show was one of the factors, no doubt, that led to its gratifyingly large interdisciplinary readership. That, and a special arrangement I had made with Harry and Betty Stanton, the original publishers of the book. I had come to deplore the practice of university presses publishing monographs that were too expensive for students and even professors to buy. College libraries were obliged to buy them, and until a paperback edition came out some years later (in the case of a book that demonstrated its market in spite of its price), students had to settle for waiting in line to borrow the library copy. So I decided to find a publisher with whom I could try an experiment in alternative publishing. As luck would have it, Harry and Betty showed up in my office at Tufts with perfect timing.

Harry had been a highly successful editor at Addison-Wesley in Boston, but he and Betty were also avid skiers with a ski lodge in Vermont, and he had decided to resign his position and set up their own imprint, Bradford Books, in the village of Montgomery, Vermont. His little address book of freelance copy editors and designers could take care of the editorial side handsomely, a nearby barn would be the warehouse for the newly printed books, and a neighborhood sheltered workshop employing people with disabilities would handle the shipping and mailing. High-quality production and low overhead. What should they publish? They did some prospecting at MIT, where Noam Chomsky encouraged them to publish the proceedings of a conference on the biology of language that he and Ed Walker had organized, which they readily agreed to do (Edward Walker, ed., _Explorations in the Biology of Language_, Montgomery, VT: Bradford Books, 1978—their first volume). Who else might have a promising manuscript? Chomsky suggested that I might have something interesting in the works. As it happened, I had just finished organizing a stack of my papers which I thought would make a good collection, so when the elegant couple appeared at my office hours, I was more than ready to talk with them.

I explained my disapproval of high-priced academic books, and proposed a deal: I wanted to publish this collection simultaneously in hardback and paperback, and while they could charge whatever they liked for the hardback, I had to have control over the price of the paperback. (I proposed to make a visit to a college bookstore with them to work out a reasonable price by comparison shopping of available paperbacks.) In exchange for this unusual arrangement I proposed to forgo all royalties on the first three thousand copies sold. Since few academic books, and almost none in academic philosophy, ever sell that many copies, I was clearly willing to risk a lot for my hunch that in fact this would open up the market dramatically. They got an imprimatur on the stack of papers from two young philosophers they sought at the University of Vermont (Philip and Patricia Kitcher—I didn’t know them at the time) and accepted my offer. Since the Stantons wanted to make a big splash with their first monograph, they went overboard on clever advertising, and enlisted their good friend Edward (Ted) Gorey to do the jacket and some more pieces for ads. The original Gorey art intended for the cover was lost in the mail in 1978, and at the last minute another already published piece of his work was substituted, drawn from his 1969 book _The Dong with the Luminous Nose._ I liked it because it looks for all the world like Harry and Betty Stanton perched in a lookout tower gazing solemnly into the stormy night. Few people have ever held a hardbound copy of _Brainstorms_ in their hands (I have a souvenir copy on my bookshelf), but the paperback has never been out of print. Bradford Books soon moved to MIT Press (one of the wisest acquisitions any press has ever made), and in 1985, Gorey redrew the original art—the enigmatic black-faced sheep and bearded chaps with scarves—for a second printing. The Gorey piece on the cover of this fortieth anniversary edition was originally drawn by Gorey just for use on a double-fold publicity mailer, hence its unusual placement of the book’s title. (A second, shorter mailer is reproduced in the front matter of the book.)

On rereading the chapters in 2016, I am beset with mixed emotions. First, I am relieved to find almost nothing to recant or regret, aside from a few embarrassing uses of “man” and “he” that passed for acceptable back then. Second, I have enough temporal distance from the author of these essays—forty years—that I can react to them the way I sometimes react on rereading another author’s work: I find myself thinking—“Wait, I thought _I_ had come up with this idea first, but here it is, clear as day, in this predecessor’s words, which I obviously read, and dimly remember reading, but apparently didn’t fully appreciate! I have unwittingly plagiarized this person!” There is no doubt, then, that I am a somewhat forgetful author who sometimes repeats himself without realizing it.

I guess forgetting one’s own work is better than forgetful plagiarism of somebody else’s, but it’s source amnesia in either case. Recognizing that I am not alone in this affliction, I can view the occasions when I encounter somebody else carefully spelling out some argument that I myself concocted and published back in the day as a source of contentment, not anger. My version went down so smoothly it left no trace of a struggle in the author’s memory. I may have succeeded to some degree in acquiring the skill I once discovered in my dissertation supervisor, Gilbert Ryle. Our weekly chats were so pleasant and unaggressive that I was unable to recognize anything I had learned from him until I compared an early draft of my dissertation to the final draft, and found the latter teeming with Rylean ideas and turns of phrase. Like many philosophy students, it took me a while to realize that philosophy that is not a chore to read can still be good, even deep, philosophy. Some professors never tumble to this fact.

On the spectrum of my mixed emotions, the other pole is frustration. It’s painful to realize that insights I had thought had achieved consensus are now ignored and in need of rediscovery, not just in my own work but that of many others. U. T. Place and J. J. C. Smart both wrote brilliant, pioneering articles on physicalism in the 1960s that have clear lessons yet to be digested by many in the field today. I sometimes think that in spite of his fame (among philosophers) W. V. O. Quine’s great contributions to naturalism are routinely ignored by the current generation, as are some of the best ideas of Wilfrid Sellars. Since I do not exempt myself from this criticism of our so-called discipline, I daresay there are fine anticipations or elaborations of points I pride myself in having authored to be found in the earlier philosophers whose work I have found it convenient to ignore or forget.

That’s the way philosophy goes, and I don’t think it should be embarrassing to acknowledge it. I am fitfully resigned to the fact that it is so hard to make _progress_ in philosophy! We philosophers deal with the tempting mistakes made by very smart thinkers, mistakes that will attract new victims perennially, and in spite of our best efforts, only a distilled and distorted residue of insights survive to become the (still shaky) foundation for the next generation. It’s a real quandary: we want to introduce our students to the best of the best, the highlights and classics, and this creates a canon of works everyone is presumed to have read, but only those who go on to become Kant _specialists_, or Aristotle _scholars_, ever replace the cartoon versions they learned as students. It’s an even more pressing problem when we philosophers try to explain to our colleagues in other fields what take-home message can be extracted from the philosophical debate over X or Y. There is no decision procedure for how to navigate between the Scylla of caricature and the Charybdis of jargon-laden, nitpicking, forced immersion in the world of philosophers P, Q, and R. Our colleagues are impatient and want an edited summary; the price they pay is very often the branding of a monolithic _ism_, to which they can then pledge their allegiance or dismiss as rubbish. Are we scientists going to be Dualists, or Physicalists, or Instrumentalists, or Illusionists, or Eliminative Materialists, or Functionalists, or Epiphenomenalists? Are we for Reductionism or against it?

I tried in _Brainstorms_ to write about the problems in language accessible to all serious thinkers, as jargon-free as possible, with lots of examples. There are philosophers who pride themselves in eschewing examples, much the way novelists pride themselves in not allowing illustrations to gild the lilies of their prose. The philosophers are tying both hands behind their backs. (Recall Richard Feynman’s wise advice for how to penetrate an obscure presentation by a theorist: “Can you please give me a simple _example_ of what you’re claiming?” The inability of a thinker to respond to that polite request is a danger sign.)

Some people just don’t _want_ to get it. Dimly foreseen moral implications, or imagined implications for the meaning of life, for instance, often provoke preemptive strikes against ideas that are felt to threaten one’s settled convictions, one’s home truths. Sometimes the blockade is subtle and unrecognized, and sometimes it is brandished. This is when the art of diplomacy comes in. Where a straightforward, in-your-face argument, with carefully articulated premises and fist-pounding emphases on the logical soundness of one’s train of argument may feel good, it often is utterly unpersuasive. One sometimes has to sneak around behind people to get them to so much as consider your message. That tactic can backfire, especially when it is addressed to philosophers who have a puritanical opinion of such _moves_. “Don’t let him duck your question with some seductive digression!” “Make him define his terms!”

> Remember, in these discussions, always insist on the first-person point of view. The first step in the operationalist sleight of hand occurs when we try to figure out how we would _know_ what it would be like for others.[^2]

Are my somewhat informal, even playful, expository tactics illicit or just hard for opponents to counter? Nothing I could say at this point should be taken as settling the issue; readers will just have to keep an eye out for subterfuge, and—I implore them—maintain an open mind as best they can. I don’t think I misrepresent anything deliberately or conceal serious alternatives, but I certainly leave large swathes of the literature undiscussed. It should come as no surprise to philosophy students that some of their professors refuse to regard me as a serious philosopher. In their opinion, I don’t behave the way I should: I don’t always cite their work, and I often refuse to discuss in detail the twists and turns of their carefully marshaled arguments. That is not just out of laziness but out of a conviction, right or wrong, that I have already seen and shown the futility of their enterprise and deem it not worth my time and effort to critique further. As Donald Hebb once put it, if it’s not worth doing, it’s not worth doing well. I can see that my unadvertised but unblushing willingness to ignore their efforts is plenty of reason for them to dismiss me. I don’t resent their opinion; it comes with the strategic choices I’ve decided to make. I’ll take my chances, especially since I have colleagues and graduate students who continue to do the dirty work of _demonstrating_ the flaws I see in these approaches, which I try to keep up on. If the armchair theorists are irritated by my cavalier attitude, I sympathize. I get similarly irritated when scientists think that philosophers have nothing to contribute beyond hot air and fancy footwork.

Some of the chapters cast longer shadows than others. Chapter 12, “Mechanism and Responsibility,” was actually my first articulation of the concept of the intentional stance. As an antidote to the widespread intuition that mechanistic explanations always drive out intentional explanations, the three stances set the stage for my later account of free will and moral responsibility (in _Elbow Room_, 1984, and _Freedom Evolves_, 2003) but also provided the more fundamental perspective for my account of intentionality or content, as presented in Chapter 1, “Intentional Systems.” That essay was primarily addressed to philosophers, but it spawned a number of influential interdisciplinary essays in _Behavioral and Brain Sciences_, including “Beliefs about Beliefs” (1978), a commentary that helped trigger the hundreds of false-belief experiments with children and animals, and a target article “Intentional Systems in Cognitive Ethology: The Panglossian Paradigm Defended”(1983), which inaugurated my multiyear challenge to Stephen Jay Gould’s distorted presentation of evolutionary biology. It also led to my book, _The Intentional Stance_ (1987), which was treated to a further target article treatment in _Behavioral and Brain Sciences_ in 1988. Recently (in _From Bacteria to Bach and Back: The Evolution of Minds_, 2017) I’ve revised and extended the concept of _free-floating rationales_, introduced in the 1983 essay, as a major element in my account of _competence without comprehension_, which clarifies the transition between the use of the intentional stance in explaining animal behavior and in explaining the design rationales exhibited by the products of natural selection.

Content first, and then comes consciousness. Chapter 8, “Are Dreams Experiences?,” foreshadows all the major ideas in my 1991 book, _Consciousness Explained_, as an astute student of mine, Paul Oppenheim (personal communication), noted a quarter century ago. Chapter 9, “Toward a Cognitive Theory of Consciousness,” contains early versions of arguments I have put forward in recent publications. Chapter 10, “Two Approaches to Mental Images,” outlines the justification for what I later dubbed “heterophenomenology” and sets out the crucial difference between the _causes_ of our beliefs and the _intentional objects_ of our beliefs, a difference that has in general been overlooked by many of the aficionados of David Chalmers’s Hard Problem, who think there are problematic “phenomenal properties.” (There aren’t, but it takes a lot of hard work to get people to see how this can be so.)

Chapter 13, “The Abilities of Men [sorry!] and Machines,” still has important work to do, challenging the recently resumed desire—apparently unquenchable—among mathematicians and logicians to apply Gödel’s Theorem to the issue of whether human beings (at least the mathematicians among them) are demonstrably not mere computers, not mere “meat machines,” because of some powers they exhibit. Chapter 15, “On Giving Libertarians What They Say They Want,” contains a model of free will that I viewed as an instructive throwaway when I concocted it, but it has recently found champions who credit me with finding the only defensible path (the “Valerian model”) to a libertarian position on free will! I have conceded this much to Bob Doyle (the “Information Philosopher”—see http://www.informationphilosopher.com]]): Thanks to his relentless questioning I can now conceive of a situation in which I, compatibilist that I am, would ardently wish for the truth of genuine indeterminism so that I could act with _absolute_ freedom: if I were playing rock-paper-scissors _with God_ for high stakes! An omniscient and antagonistic (I’m an atheist after all) God could foil me in this competition unless my choice was quantum indeterministic, not merely chaotic and _practically_ unpredictable. Some version of the Valerian model—minus the indeterminism—is all we need for free will worth wanting in this Godless world. Chapter 16, “How to Change Your Mind,” appears in retrospect to be heading toward my current thinking about Bayesian models of (animal) belief as the foundation for the more intellectual and reflective thinking we language-users can engage in (see my _From Bacteria to Bach and Back_).

Chapter 17, “Where Am I?,” has had a remarkable trajectory. In 1979, a novelist friend attempted to turn it into a coauthored story fit for a Hollywood film, but gave up for legal reasons: his agent found a story published a few years earlier in some sci-fi magazine with a _faintly_ similar story line, and decided Hollywood wouldn’t pay for a story that might unleash a copyright infringement battle. The main scene in the story was dramatically rendered in a 1981 BBC science documentary, “The Human Brain: The Self,” in which I appear, looking at my own brain in a fabulous fountain/vat on a pedestal in front of whirring computer tape drives (remember those?), and wondering why I am saying “Here I am staring at my own brain in a vat” instead of “Here I am, in a vat, being stared at by my own eyes.” In 1984, Lynn Jeffries, then a student at Harvard, produced a Javanese shadow puppet play at the Loeb Experimental Theater, with original music and a cast of hundreds (of cardboard Javanese shadow puppets). In 1987, a feature-length film made in the Netherlands by Piet Hoenderdos, _Victim of the Brain_, starring Piet, Doug Hofstadter, and me, contained a half-hour version of the whole story (I played the second Dennett body, Fortinbras, and the Dutch vat was larger and far more realistic than the BBC’s whimsical vat). Over the years I have received a variety of letters and emails asking me, apparently in all seriousness, if the story was true; once a student came up behind me after a public lecture and manually checked my skull for protruding antennas (I had enough hair back then so that a visual search was apparently not deemed adequate investigation). Forty years of research in cognitive science has not made the predicament I claimed to find myself in much closer to realization, but that was not the point, of course.

I didn’t invent the idea of the brain in the vat. There have been science fiction stories exploiting it for a century or more, and I wasn’t the first philosopher to entertain the idea. So far as I know, Gil Harman was first, in his 1973 book, _Thought_, which I taught in a graduate seminar at Harvard when it first came out. The passage is brief:

> Or perhaps you do not even have a body. Maybe you were in an accident and all that could be saved was your brain, which is kept alive in the laboratory. For your amusement you are being fed a tape from the twentieth century. Of course, that is to assume that you have to have a brain in order to have experiences; and that might be just part of the myth you are being given.[^3]

I see that I never underlined it, or wrote marginalia about it—my books are usually loaded with footprints from my rambles, and _Thought_ is no exception—but Harman may well have put the idea in my head. I was not particularly interested in the use he and many others have made of the predicament. In the eternal campaign of combatting radical skepticism, it comes out as a modern, high-tech version of Descartes’s evil demon hypothesis. I was interested in what light it could shed on subjectivity and point of view, and personal identity. I saw it as a naturalistic exploration of our concept of a soul or mind, the “owner” of a living body.

I concocted the first version of my tale in 1976 while driving on the Mass Pike from Tufts to Vassar, where I was to give a colloquium. That evening at a party with Oswaldo Chateaubriand and half a dozen students after my talk, I ad libbed the basic tale and we then spent several hilarious hours considering its implications and complications. When I left Vassar for home the next day, I knew what I was going to present as the after-dinner talk at the Chapel Hill Colloquium in October of that year. I would guess that “Where am I?” has been on the syllabus of more philosophy courses than anything else I have written, and some Vassar undergraduates deserve a bit of the credit.

I have tried in this book to bridge divides, to speak to everyone whose attention is worth attracting. No doubt I have failed in many cases to be, in this endeavor, a good advertisement for philosophy, but I continue to be proud to be a philosopher and I still think that this book is a good example of _one kind_ of philosophy worth doing.

Daniel C. Dennett

January 12, 2017

## Notes

1.]] At the outset, in a diagram included in the 1978 Sloan Foundation report, anthropology was a sixth discipline included, but while distinguished anthropologists have made significant contributions to cognitive science, few salient bridges were built in those early days, and little traffic has ensued, sad to say.

2.]] John Searle, “Minds, Brains, and Programs,” _Behavioral and Brain Sciences_ 3, no. 3 (1980): 451.

3.]] Gilbert Harman, _Thought_ (Princeton, NJ: Princeton University Press, 1973), 5.Preface

The essays collected here attempt to solve a wide variety of puzzles about the mind. Out of the solutions proposed emerges a relatively unified and comprehensive theory of the mind, a revised and extended version of the theory I presented in 1969, in _Content and Consciousness._ All the essays, with the exception of Chapter 6, “A Cure for the Common Code?,” were written to be presented individually at conferences and departmental colloquia around the country, and they have been shaped and reshaped by the interplay with many different audiences.

Several virtues and vices spring from this fact. Virtue first: since they were composed to be heard, and comprehended on first hearing, they make for relatively easy reading—for philosophy. Another virtue is that the essays are self-contained, which permits the reader to sample the theory at the point of most initial interest, with the reasonable hope of being able to understand the theory well enough in that domain to know whether or not to pursue it further. A collateral vice is that these essays can be self-contained only by dint of a certain amount of repetition, but this vice has its virtuous side, for it permits central concepts in my theory—such as the concept of an intentional system—and central arguments—such as the arguments about incorrigibility or introspective authority—to be presented from more than one perspective, with more than one emphasis. This provides the skeptical reader with both a demonstration of the broad applicability I claim for these ideas, and several different angles from which to launch attacks.

Ten of the essays have appeared before in print, and drafts of all seventeen have been read and discussed by philosophers, psychologists, and their students, in some cases for years. The interest they have occasioned has encouraged me to bring them together and seek a wider audience, not only of philosophers and psychologists, but of reflective readers generally, for many of the questions posed are not the private property of professionals, but tantalizers and bafflers familiar to the speculations of even the most untheoretical imaginations. If I close my eyes and imagine a purple cow, is something somewhere really purple and cow-shaped? Could a brain scientist someday read my thoughts in my brain? Could a robot be truly conscious? Is free will an illusion? My answers were developed one at a time over the years, but once I finally noticed the rise in my temptation to indulge in the unseemly habit of citing my own work, I decided to succumb totally and admit that I think these essays are truly interrelated and should be read together.

The unified theory I claim to provide here is presented in much the same order as its ancestor was in _Content and Consciousness,_ beginning, in Part I, with basic metaphysical and methodological concerns and then, in Part II, analyzing and defending the—careful—use of mentalistic or intentional formulations in psychological theories: the ascription of _content_ to events and states in the mind. The fruits of that analysis are then exploited in Part III to provide answers to the most persistent quandaries about consciousness and its relation to the rest of the world. Part IV pushes tentatively, gropingly into the area I consider most important: the analysis of the relationship between our vision of ourselves as responsible, free, rational agents, and our vision of ourselves as complex parts of the physical world of science. For almost ten years I have been trying to concentrate on this last area only to be repeatedly driven back by complexities (and their attendant fascinations) in the groundwork theory of mind on which my assault on the ethical domain was to rest. Nothing I have learned has changed my mind about the importance or feasibility of that assault, to which I am now returning.

There are many people to thank. I am grateful to hosts, commentators, and audiences around the country for their stimulation and their responses. I am indebted to my students, at Tufts, and during very happy visits at Harvard and Pittsburgh, for their relentless, intelligent, good-natured skepticism. And I have been especially helped by the advice and criticism of Annette Baier, Ronald Barnette, Ned Block, Bo Dahlbom, Jerry Fodor, Michael Hooker, Hilary Putnam, Zenon Pylyshyn, Georges Rey, Amelie Rorty, Joseph Weizenbaum, and my colleagues at Tufts over the years. Finally, I thank my wife Susan for her invariably clear-headed stylistic advice, encouragement and understanding.

D.C.D.

Tufts University

February, 1978Introduction

## What is a philosophical theory of the mind?

I claim that the essays in this book taken together express a theory of the mind, so I should begin by explaining what I take a theory of the mind to be. Several very different sorts of intellectual productions are called theories: particle theory in physics, set theory in mathematics, game theory, literary theory, the theory of evolution, the identity theory in the philosophy of mind. Some things are called theories that might better be called hypotheses. The theory of evolution by natural selection is surely a theory of sorts, but its rival, creationism, is a theory only by courtesy. It lacks the parts, the predictive power, the organization of a theory; it is merely a hypothesis, the hypothesis that the theory of evolution by natural selection is false and that God created the species. I suspect we call it a theory to acknowledge that it is a genuine alternative to something that clearly _is_ a theory. Creationism, after all, _might_ be true and Darwinism false—which goes to show that one needn’t always counter a theory with a _theory._ We won’t need a theory of clairvoyance, for instance, if we can confirm the hypothesis that all apparent clairvoyants are cheats. Hoaxism is a worthy opponent of the most elaborate theory of clairvoyance, and it consists of but a single statement, supported, of course, by a good deal of sleuthing.

Philosophical theories are often hypotheses of this sort: large generalizations that do not ramify into vast organized structures of details, or predict novel effects (like theories in chemistry or physics), but are still vulnerable to disconfirmation (like hoaxism), and require detailed and systematic support. Thus “minds are just brains” is one very informal way of expressing a version of _physicalism_ or the _identity theory_ of mind (so-called because it _identifies_ mental events with _physical_ events in the brain), and “minds are not just brains; they're something non-physical” is one very informal way of expressing a version of _dualism_ (so-called because it claims there are at least two fundamental sorts of events or things). Since philosophy often concerns itself with just such very general hypotheses, and the patterns of implication one lands oneself in when defending such hypotheses, philosophy often appears to the outsider to be a ludicrously overpopulated battlefield of “isms,” with each imaginable variation on each general assertion pompously called a theory and given a proprietary name.

This appearance is untrustworthy, however, and the proliferation of theories is not really an embarrassment. It is surely initially reasonable to suppose that such a general hypothesis about the mind _makes sense_, and then it is also reasonable both to suppose that either it or its denial is true, and to wonder which. A sensible way to try to answer this question is to explore the evidence for, and implications of, the possible alternatives, and defend the most plausible candidate until proven wrong. That process however soon gets complicated, and it becomes imperative to distinguish one’s hypothesis very precisely from closely resembling hypotheses whose hidden flaws one has uncovered. Technical terms—jargon—permit one to triangulate the possible positions in logical space and thus keep track of the implication chains one is avoiding or exploiting. Thus are born interactionism, anomolous monism, logical behaviorism, Turing machine functionalism and the other named locations in the logical space of possible general claims about the nature of the mind.

To a first approximation then a philosophical theory of the mind is supposed to be a consistent set of answers to the most general questions one can ask about minds, such as “are there any?,” “are they physical?,” “what happens in them?” and “how do we know anything about them?” Such a theory is not supposed to compete with or supplant neurophysiological or psychological theories, but rather both to ground such theories and to supplement them. It can ground such theories by providing the justification for the fundamental metaphysical assumptions such theories must unavoidably make. It can supplement them by providing answers to the simple, straightforward questions that those scientific theories are utterly unable to answer from their own resources. Every brain scientist knows that even in the Golden Age of neurophysiological knowledge, when the activity of every tract of fibers will be well understood, questions like “what is consciousness?” and “what is it about pains that makes them awful?” will find no answers in their textbooks—unless those textbooks include chapters of philosophy.

Many psychologists and brain scientists are embarrassed by the philosophical questions, and wish no one would ask them, but of course their students persist in asking them, because in the end these are the questions that motivate the enterprise. Synaptic junctures and response latencies have some intrinsic interest, to be sure, but if there were no hope that compounding enough facts about these would lead to discoveries about our _minds,_ enthusiasm for such research would not be as keen as it is. The distaste of many empirical scientists for the philosophical questions is no doubt due to the fact that until very recently philosophers’ attempts to answer them were conducted in blithe ignorance of and indifference to the discoveries, theories and problems of those sciences. That indifference was galling, I am sure—as galling as the counter-disdain of the scientists—but reasonable: until very recently there were few discoveries, theories or problems in the sciences that promised to illuminate the philosophical issues at all.

Times have changed. Psychology has become “cognitive” or “mentalistic” (in many quarters) and fascinating discoveries have been made about such familiar philosophical concerns as mental imagery, remembering and language comprehension. Even the brain scientists are beginning to tinker with models that founder on _conceptual_ puzzles. There is, for instance, the problem of avoiding the “grandmother neuron.” Many otherwise plausible theory sketches in brain science seem to lead ineluctably to the view that the “representation” of each particular “concept” or “idea” will be the responsibility of a particular neuron or other small part of the brain. Suppose your “grandmother” neuron died; not only could you not _say_ “grandmother,” you couldn’t _see_ her if she was standing right in front of you. You couldn’t even _think_ about grandmothers at all; you would have a complete cognitive blind spot. Nothing remotely like that pathology is observed, of course, and neurons malfunction or die with depressing regularity, so for these and other reasons, theories that require grandmother neurons are in trouble. The problem is to find a theory that avoids this difficulty in all its guises, and this is a problem so abstract as to be properly philosophical. Many other problems arising in these sciences—problems about concept learning, reasoning, memory, decision—also have an unmistakably philosophical cast.

Philosophy of mind has responded to these developments by becoming “naturalized”; it has become a branch of the philosophy of science concerning itself with the conceptual foundations and problems of the sciences of the mind.[^1]*]] This has changed the shape and texture of philosophical theories of the mind by introducing into the discussions of the traditional issues many of the data and conceptual tools of the new scientific approaches, and raising new issues arising from the puzzles and pitfalls of those approaches.

Philosophy of mind is unavoidable. As soon as one asserts anything substantive about anything mental, one _ipso facto_ answers at least by implication one or more of the traditional questions and thus places oneself in the camp of an _ism._ Perhaps some theorists arrive at their positions by methodically filling in the blanks on the branching checklist of possibilities, but this is not a strategy I recommend. The views already charted, named and cataloged have all been ably defended, but none has achieved consensus. One is not apt to find the magic words of support that will suddenly bring victory to an already articulated theory. A better strategy, or at least the strategy I have tried to follow, is to start not by looking hard at the possible answers to the traditional questions posed in traditional terms, but by looking hard at the empirical data, psychological theories, models of brain function and so forth, and letting the considerations and saliencies that appeared there suggest what would be important to keep distinct in a theory of the mind. The result is a theory that looks like an ungainly and inelegant hybrid, an unnameable hodge-podge of theory parts, when measured against the traditional pattern of categories. Since I think my theory carves nature at the joints, however, I am inclined to claim that it is the traditional pattern that is misshapen. For this reason I have until now refrained from giving my theory a name, and refrained from giving explicit answers to some of the most popular watershed questions, but the questions do remain to be answered, and now it is useful and perhaps even obligatory for me to give direct answers and take sides.

## What is my theory?

My theory can be distinguished easily from its rivals via a brief and oversimplified history of recent brands of _physicalism._ In the beginning was _type identity theory._ It attempted to answer two questions. To the question, “What are mental events?” it answered, “Every mental event is (identical with) a physical event in the brain,” and to the question, “What do two creatures have in common when they both believe that snow is white (both feel a twinge of pain, imagine an elephant, want a cracker)?” it answered, “In each case where creatures have something _mental_ in common, it is in virtue of having something _physical_ in common—e.g., their brains are in the same physical state or both exhibit the same physical feature.” The answer to the first question made the view an identity theory; the answer to the second established that _types_ of mental events were claimed to correspond to physically characterizable _types_ of brain events. In answering these two questions, type identity theory attempted to discharge two obligations, one “metaphysical” and the other “scientific.” The first answer amounts to the mere denial of dualism, the insistence that we don’t need a category of non-physical things in order to account for mentality. The second takes on the responsibility of explaining commonalities—the task isolated by Socrates’ incessant demands to know what is shared by things called by the same name.

Few today would quarrel with the first answer, but the second answer is hopelessly too strong. The claim it makes is that for _every_ mentalistic term, every “mental” predicate _“M,”_ there is some predicate _“P” expressible in the vocabulary of the physical sciences_ such that a creature is _M_ if and only if it is _P._ Symbolically,

(1) ![[11146_e000c_001.jpg]] 

For instance, for all _x, x_ is _thinking about baseball_ if and only if _x has F-neurons in electro-chemical state_ G; or, something _is in pain_ if and only if it _has a brain in such and such a physical condition._ This is all utterly unlikely.[^2] Consider some simpler cases to see why. Every clock and every can-opener is no doubt nothing but a physical thing, but is it remotely plausible to suppose or insist that one could compose a predicate in the restricted language of physics and chemistry that singled out all and only the can-openers or clocks? (What is the common physical feature in virtue of which this grandfather clock, this digital wristwatch, and this sundial can be ascribed the predicate “registers 10:00 A.M.”?) What can-openers have peculiarly in common is a purpose or function, regardless of their physical constitution or even their design, and the same is true of clocks.

This recognition led to the second wave of physicalism: _Turing machine functionalism._ The minimal denial of dualism was maintained—every mental event was a physical event—but the requirements for answering the second question were revised: for every “mental” predicate _“M”_ there is some predicate _“F”_ expressible in some language that is physically _neutral,_ but designed to specify abstract functions and functional relations. The obvious candidates for such a language were the systems used for describing computers or programs. The functional structure of a computer program can be described in an abstract way that is independent of any particular description of physical “hardware.” The most general functional language is the system for describing computers as “Turing machines.” (An elementary introduction to the concept of a Turing machine is provided in Chapter 13.) The states and activities of _any_ digital computer or program can be given a mathematical description as states and activities of a unique (numbered) Turing machine, and this description is its mathematical fingerprint that will distinguish it from all _functionally_ different computers or programs, but not from computers and programs that differ _only_ in “physical realization.” There are problems with this formulation, not germane to the issue at hand, but supposing them to be eliminable, the Turing machine functionalist proposed to say things like

(2) (_x_)(_x_ believes that snow is white ≡ _x_ “realizes” some Turing machine _k_ in logical state _A_)

In other words, for two things both to believe that snow is white, they need not be physically similar in any specifiable way, but they must both be in a “functional” condition or state specifiable in the most general functional language; they must share a Turing machine description according to which they are both in some particular logical state (which is roughly like two different computers having the same program and being in the same “place” in the program). The “reduction” of mental predicates to physical predicates attempted by type identity theory has been replaced in this view by a reduction of mental predicates to Turing machine predicates. While the resulting theory is only a _token_ identity theory—each _individual_ mental event is (identical with) some individual physical brain event or other—it is a _type_ functionalism—each mental type is identifiable as a functional type in the language of Turing machine description.

But alas, this second answer is still too strong (as I argue in Chapter 2).[^3] The supposition that there could be some principled way of describing all believers and pain-sufferers and dreamers as Turing machines so that they would be in the same logical state whenever they shared a mental epithet is at best a fond hope. There is really no more reason to believe you and I “have the same program” in _any_ relaxed and abstract sense, considering the differences in our nature and nurture, than that our brains have identical physico-chemical descriptions. What could be done to weaken the requirements for the second answer still further?

Consider what I will call _token functionalism,_ the view that while every mental event is indeed some physical event or other, and moreover some functional event or other (this is the minimal denial of epiphenomenalism—see footnote on p. 191), mental types are _not_ definable as Turing machine types. How will we answer the Socratic question? What do two people have in common when they both believe that snow is white? I propose this:

(3) (_x_)(_x_ believes that snow is white ≡ _x_ can be predictively attributed the belief that snow is white)

This appears to be blatantly circular and uninformative—“A horse is any animal to which the term 'horse' truly applies.” The language on the right seems simply to mimic the language on the left. What has happened to the goal of _reduction?_ It was, I submit, a mistaken goal.[^4]

All we need to make an informative answer of this formula is a systematic way of making the attributions alluded to on the right-hand side. Consider the parallel case of Turing machines. What do two different realizations or embodiments of a Turing machine have in common when they are in the same logical state? Just this: there is a system of description such that according to it both are described as being realizations of some particular Turing machine, and according to this description, which is predictive of the operation of both entities, both are in the same state of that Turing machine’s machine table. One doesn’t _reduce_ Turing machine talk to some more fundamental idiom; one _legitimizes_ Turing machine talk by providing it with rules of attribution and exhibiting its predictive powers. If we can similarly legitimize “mentalistic” talk, we will have no need of a reduction. That is the point of my concept of an _intentional system_ (see Chapter 1). Intentional systems are supposed to play a role in the legitimization of mentalistic predicates parallel to the role played by the abstract notion of a Turing machine in setting down rules for the interpretation of artifacts as computational automata. I fear my concept is woefully informal and unsystematic compared with Turing’s, but then the domain it attempts to systematize—our everyday attributions in mentalistic or intentional language—is itself something of a mess, at least compared with the clearly defined mathematical field of recursive function theory, the domain of Turing machines.

The analogy between the theoretical roles of Turing machines and intentional systems is more than superficial. Consider that warhorse in the philosophy of mind, _Brentano’s Thesis_ that intentionality is the mark of the mental: all mental phenomena exhibit intentionality and no physical phenomena exhibit intentionality. (The elusive concept of intentionality is introduced and explained in Chapters 1, 4 and 12.) This has been traditionally taken to be an _irreducibility_ thesis: the mental, in virtue of its intentionality, cannot be reduced to the physical.[^5] But given the concept of an intentional system, we can construe the first half of Brentano’s Thesis—all mental phenomena are intentional—as a _reductionist_ thesis of sorts, parallel to Church’s Thesis in the foundations of mathematics. According to Church’s Thesis, every “effective” procedure in mathematics is recursive, that is, Turing-computable. (The idea, metaphorically, is that any mathematical task for which there is a clear _recipe_ composed of simple steps can be performed by a very simple computer, a universal Turing machine, the universal recipe-follower.) Church’s Thesis is not provable, since it hinges on the intuitive and unformalizable notion of an effective procedure, but it is generally accepted, and it provides a very useful reduction of a fuzzy-but-useful mathematical notion to a crisply defined notion of apparently equivalent scope and greater power. Analogously, the claim that every mental phenomenon is _intentional-system-characterizable_ would, if true, provide a reduction of the mental—a domain whose boundaries are _at best_ fixed by mutual acknowledgment and shared intuition—to a clearly defined domain of entities, whose principles of organization are familiar, relatively formal and systematic.

In Chapter 1 the question is posed: are there mental treasures that cannot be purchased with intentional coin? The negative answer, like Church’s Thesis, cannot be proved, but only made plausible by the examination of a series of “tough” cases in which mental phenomena _are_ (I claim) captured in the net of intentional systems. That is the major burden of the book, and individual essays tackle particular phenomena: invention in Chapter 5, dreams in Chapter 8, mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett and some of their kin in Chapters 9 and 10, pain in Chapter 11, and free will in Chapters 12 through 15. This is hardly a complete list of mental treasures, but reasons are given along the way, in these chapters and in others, for thinking that parallel treatments can be devised for other phenomena. Complete success in this project would vindicate physicalism of a very modest and undoctrinaire sort: all mental events are in the end just physical events, and commonalities between mental events (or between people sharing a mentalistic attribute) are explicated via a description and prediction system that is neutral with regard to physicalism, but just for that reason entirely compatible with physicalism. We know that a merely physical object can be an intentional system, even if we can 't prove _either_ that every intentional system is physically realizable in principle, _or_ that every intuitively _mental_ item in the world can be adequately accounted for as a feature of a physically realized intentional system.

If one insisted on giving a name to this theory, it could be called _type intentionalism:_ every mental event is some functional, physical event or other, and the types are captured not by any reductionist language but by a regimentation of the very terms we _ordinarily_ use—we explain _what beliefs are_ by systematizing the notion of a believing-system, for instance. This theory has the virtues of fitting neatly into a niche left open by its rivals and being expressible in a few straight-forward general statements, but in that clean, uncomplicated form it is unacceptable to me. Sadly for the taxonomists, I cannot rest content with “type intentionalism” as it stands, for it appears to assume something I believe to be false: _viz_., that our _ordinary_ way of picking out putative mental features and entities succeeds in picking out real features and entities. Type intentionalism as so far described would assume this by assuming the _integrity_ of the ordinary mentalistic predicates used on the left-hand side of our definition schema (3). One might uncritically suppose that when we talk, as we ordinarily do, of peoples’ thoughts, desires, beliefs, pains, sensations, dreams, experiences, we are referring to members in good standing of usefully distinct classes of items in the world—“natural kinds.” Why else would one take on the burden of explaining how _these_ “types” are reducible to any others? But most if not all of our familiar mentalistic idioms fail to perform this task of perspicuous reference, because they embody conceptual infelicities and incoherencies of various sorts. I argue for this thesis in detail with regard to the ordinary concepts of pain in Chapter 11, belief in Chapters 6 and 16, and experience in Chapters 8, 9, and 10, but the strategic point of these criticisms is more graphically brought out by a fanciful example.

Suppose we find a society that lacks our knowledge of human physiology, and that speaks a language just like English except for one curious family of idioms. When they are tired they talk of being beset by _fatigues_, of having mental fatigues, muscular fatigues, fatigues in the eyes and fatigues of the spirit. Their sports lore contains such maxims as “too many fatigues spoils your aim” and “five fatigues in the legs are worth ten in the arms.” When we encounter them and tell them of our science, they want to know _what fatigues are._ They have been puzzling over such questions as whether numerically the same fatigue can come and go and return, whether fatigues have a definite location in matter or space and time, whether fatigues are identical with some particular physical states or processes or events in their bodies, or are made of some sort of stuff. We can see that they are off to a bad start with these questions, but what should we tell them? One thing we might tell them is that there simply are no such things as fatigues—they have a confused ontology. We can expect some of them to retort: “You don't think there are fatigues? Run around the block a few times and you'll know better! There are many things your science might teach us, but the non-existence of fatigues isn't one of them.”

We ought to be unmoved by this retort, but if we wanted to acknowledge this society’s “right” to go on talking about fatigues—it’s their language, after all—we might try to accommodate by agreeing to call at least some of the claims they make about fatigues true and false, depending on whether the relevant individuals are drowsy, exhausted or feigning, etc. We could then give as best we could the physiological conditions for the truth and falsity of those claims, but refuse to take the apparent ontology of those claims seriously; that is, we could refuse to attempt any _identification_ of fatigues. Depending on how much we choose to reform their usage before answering their questions at all, we will appear to be countenancing what is called the disappearance form of the identity theory, or eliminative materialism—for we legislate the putative items right out of existence. Fatigues are not good theoretical entities, however well entrenched the term “fatigues” is in the habits of thought of the imagined society. The same is true, I hold, of beliefs, desires, pains, mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, experiences—as all these are _ordinarily_ understood. Not only are _beliefs_ and _pains_ not good theoretical _things_ (like electrons or neurons), but the _state-of-believing-that-p_ is not a well-defined or definable theoretical _state_, and the _attribute_, _being-in-pain_, is not a well-behaved theoretical attribute. Some ordinary mental-entity terms (but not these) may perspicuously isolate features of people that deserve mention in a mature psychology; about such features I am a straightforward type-intentionalist or “homuncular functionalist,” as Lycan calls me,[^6] for reasons that will be clear from Chapters 5, 7, 9 and 11. About the theoretical entities in a mature psychology that eventually _supplant_ beliefs, desires, pains, mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, experiences … I am also a type-intentionalist or homuncular functionalist. About other putative mental entities I am an eliminative materialist. The details of my view must for this reason be built up piecemeal, by case studies and individual defenses that are not intended to generalize to all mental entities and all mental states. It is no easier to convince someone that there are no pains or beliefs than it would be to convince our imaginary people that there are no fatigues. If it can be done at all (supposing for the moment that one would want to, that it is true!), it can only be done by subjecting our intuitions and convictions about particular cases to skeptical scrutiny.

The foundation for that task is laid in Part I, where the concept of an intentional system is defined and subjected to a preliminary exploration in Chapter 1. Chapter 2 develops arguments against type functionalism and for type intentionalism, and in the second half provides a first look at some of the themes about consciousness explored in detail in Part III. Chapter 3 examines the prospects of a very tempting extension of intentionalism: the brain writing hypothesis. If we can predict someone’s behavior only by ascribing beliefs (and other intentions) to him, mustn’t we suppose those beliefs are somehow _stored_ in him and _used_ by him to govern his behavior, and isn’t a stored _sentence_ a good model—if not our only model—for a stored belief? I argue that while it might turn out that there is some such brain writing that “encodes” our thoughts, the reasons for believing so are far from overwhelming. Further caveats about brain writing are developed in other chapters, especially Chapter 6. It is important to protect type intentionalism, as a general theory of the nature of mentalistic attributions, from the compelling but problem-ridden “engineering” hypothesis that all sophisticated intentional systems must share at least one design feature: they must have an internal system or language of mental representation. In some very weak sense, no doubt, this must be true, and in a variety of strong senses it must be false. What intermediate sense can be made of the claim is a subject of current controversy to which I add fuel in several of the chapters.

Part II explores the foundations of psychology in more detail, and attempts to describe the conceptual environment in which psychology could survive its infancy and grow to maturity. Current wisdom has it that behaviorism is dead and that “cognitive science,” an alliance of cognitive psychology, linguistics and artificial intelligence, is the wave of the future. I share this optimism in part, but see some conceptual traps and false avenues worth pointing out. Chapters 4 and 5 attempt to diagnose both the weaknesses and underestimated strengths of behaviorism. They yield a vision of psychology more unified in both its methods and unsolved problems than the more impassioned spokesmen would have us believe. Chapter 6, a review of Fodor’s important book, _The Language of Thought_, promotes a cautious skepticism about some of the theoretical underpinnings of the cognitive science movement, and Chapter 7 is an introductory travel guide to the field of artificial intelligence, recommending points of interest while warning of alien customs and unreliable accommodations. Since some enemies of artificial intelligence have viewed the piece as an unseemly glorification of the field and some workers in the field have regarded it as an unsympathetic attack, it probably strikes the right balance.

Part III then tackles some of the traditional questions that have puzzled philosophers of mind concerned with consciousness: what are sensations, dreams, mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, pains? How can they be captured in the net of psychological theory? Together these chapters constitute a considerable revision of the account of consciousness given in the second half of _Content and Consciousness,_ though most of the strong claims about the relation of consciousness to language survive in one form or another.

Part IV considers a variety of related questions that might be grouped under one general question: can psychology support a vision of ourselves as moral agents, free to choose what we will do and responsible for our actions? Many have thought that materialism or mechanism or determinism—all _apparent_ assumptions of the reigning psychological theories—threaten this vision, but in Chapters 12 and 13 I consider the most persuasive of the arguments to this effect and reveal their flaws. Chapter 12 attempts to allay the worry that sheer mechanism—deterministic or indeterministic—would rule out free will and responsibility. By uncovering the missteps in the most compelling arguments for this thesis I claim not to refute it, but at least to strip it of its influence. Chapter 13 tackles the widespread conviction that Gödel’s Theorem proves we cannot be “machines,” and illustrates the fundamental confusions that give this idea whatever plausibility it has. Chapter 14 argues that persons can be defined as a particular subclass of intentional systems, “higher order” intentional systems with the capacity for natural language and (hence) consciousness in the fullest sense. In some regards then this is the unifying essay of the collection. Chapter 15 explores the relationship between free will and indeterminism, and argues that what is properly persuasive in the libertarians’ insistence that our wills be undetermined can be captured in a neutral model of rational decision-making. Chapter 16 develops this model of decision-making a bit further and proposes a _reform_ in our ordinary concept of belief, sharply distinguishing two phenomena I call belief and opinion. I view these chapters as developing fragments of a positive psychological theory of moral agents or persons. Chapter 17 is dessert.

## Notes

*]] Numbered footnotes contain only bibliographical information, and are placed at the end of the book.

1.]] See my “Current Issues in the Philosophy of Mind,” in _American Philosophical Quarterly_, October, 1978.

2.]] See Jerry Fodor, _The Language of Thought_ (New York: Crowell, 1975): Chapter 1, for a vigorous account of what is wrong with type identity theory.

3.]] See also Jerry Fodor and Ned Block, “What Psychological States are Not,” _Philosophical Review_, LXXXI (1972), and Ned Block, “Troubles with Functionalism” in C. W. Savage, ed., _Perception and Cognition: Issues in the Foundations of Psychology, Minnesota Studies in the Philosophy of Science_, vol. IX (1978).

4.]] Cf. Fodor, _op. cit_., “Introduction: Two Kinds of Reductionism,” and Hilary Putnam, “Reductionism and the Nature of Psychology,” _Cognition_, II (1973): 131–46.

5.]] See Richard Aquila, _Intentionality: A Study of Mental Acts_ (University Park: Pennsylvania State University, 1977), especially Chapter 1, for a sounder conception of Brentano’s sense of the thesis.

6.]] W. G. Lycan, “Form, Function, and Feel,” _Journal of Philosophy_ 78, no. 1 (Jan. 1981).Intentional Explanation and Attributions of Mentality

Intentional Systems

I wish to examine the concept of a system whose behavior can be—at least sometimes—explained and predicted by relying on ascriptions to the system of beliefs and desires (and hopes, fears, intentions, hunches, …). I will call such systems _intentional systems_, and such explanations and predictions intentional explanations and predictions, in virtue of the intentionality of the idioms of belief and desire (and hope, fear, intention, hunch, …).[^1]

I used to insist on capitalizing “intentional” wherever I meant to be using Brentano’s notion of _intentionality_, in order to distinguish this technical term from its cousin, e.g., “an intentional shove,” but the technical term is now in much greater currency, and since almost everyone else who uses the term seems content to risk this confusion, I have decided, with some trepidation, to abandon my typographical eccentricity. But let the uninitiated reader beware: “intentional” as it occurs here is _not_ the familiar term of layman’s English.[^2] For me, as for many recent authors, intentionality is primarily a feature of linguistic entities—idioms, contexts—and for my purposes here we can be satisfied that an idiom is intentional if substitution of codesignative terms do not preserve truth or if the “objects” of the idiom are not capturable in the usual way by quantifiers. I discuss this in more detail in _Content and Consciousness._[^3]

## I

The first point to make about intentional systems[^4] as I have just defined them is that a particular thing is an intentional system only in relation to the strategies of someone who is trying to explain and predict its behavior. What this amounts to can best be brought out by example. Consider the case of a chess-playing computer, and the different strategies or stances one might adopt as its opponent in trying to predict its moves. There are three different stances of interest to us. First there is the _design stance._ If one knows exactly how the computer is designed (including the impermanent part of its design: its program) one can predict its designed response to any move one makes by following the computation instructions of the program. One’s prediction will come true provided only that the computer performs as designed—that is, without breakdown. Different varieties of design-stance predictions can be discerned, but all of them are alike in relying on the notion of _function_, which is purpose-relative or teleological. That is, a design of a system breaks it up into larger or smaller functional parts, and design-stance predictions are generated by assuming that each functional part will function properly. For instance, the radio engineer’s schematic wiring diagrams have symbols for each resistor, capacitor, transistor, etc.—_each with its task to perform—_and he can give a design-stance prediction of the behavior of a circuit by assuming that each element performs its task. Thus one can make design-stance predictions of the computer’s response at several different levels of abstraction, depending on whether one’s design treats as smallest functional elements strategy-generators and consequence-testers, multipliers and dividers, or transistors and switches. (It should be noted that not all diagrams or pictures are designs in this sense, for a diagram may carry no information about the functions—intended or observed—of the elements it depicts.)

We generally adopt the design stance when making predictions about the behavior of mechanical objects, e.g., “As the typewriter carriage approaches the margin, a bell will ring (provided the machine is in working order),” and more simply, “Strike the match and it will light.” We also often adopt this stance in predictions involving natural objects: “Heavy pruning will stimulate denser foliage and stronger limbs.” The essential feature of the design stance is that we make predictions solely from knowledge or assumptions about the system’s functional design, irrespective of the physical constitution or condition of the innards of the particular object.

Second, there is what we may call the _physical stance._ From this stance our predictions are based on the actual physical state of the particular object, and are worked out by applying whatever knowledge we have of the laws of nature. It is from this stance alone that we can predict the malfunction of systems (unless, as sometimes happens these days, a system is _designed_ to malfunction after a certain time, in which case malfunctioning in one sense becomes a part of its proper functioning). Instances of predictions from the physical stance are common enough: “If you turn on the switch you’ll get a nasty shock,” and, “When the snows come that branch will break right off.” One seldom adopts the physical stance in dealing with a computer just because the number of critical variables in the physical constitution of a computer would overwhelm the most prodigious calculator. Significantly, the physical stance is generally reserved for instances of breakdown, where the condition preventing normal operation is generalized and easily locatable, e.g., “Nothing will happen when you type in your questions, because it isn’t plugged in,” or, “It won’t work with all that flood water in it.” Attempting to give a physical account or prediction of the chess-playing computer would be a pointless and herculean labor, but it would work in principle. One could predict the response it would make in a chess game by tracing out the effects of the input energies all the way through the computer until once more type was pressed against paper and a response was printed. (Because of the digital nature of computers, quantum-level indeterminacies, if such there be, will cancel out rather than accumulate, unless of course a radium “randomizer” or other amplifier of quantum effects is built into the computer).

The best chess-playing computers these days are practically inaccessible to prediction from either the design stance or the physical stance; they have become too complex for even their own designers to view from the design stance. A man’s best hope of defeating such a machine in a chess match is to predict its responses by figuring out as best he can what the best or most rational move would be, given the rules and goals of chess. That is, one assumes not only (1) that the machine will function as designed, but (2) that the design is optimal as well, that the computer will “choose” the most rational move. Predictions made on these assumptions may well fail if either assumption proves unwarranted in the particular case, but still this _means_ of prediction may impress us as the most fruitful one to adopt in dealing with a particular system. Put another way, when one can no longer hope to beat the machine by utilizing one’s knowledge of physics or programming to anticipate its responses, one may still be able to avoid defeat by treating the machine rather like an intelligent human opponent.

We must look more closely at this strategy. A prediction relying on the assumption of the system’s rationality is relative to a number of things. First, rationality here so far means nothing more than optimal design relative to a goal or optimally weighted hierarchy of goals (checkmate, winning pieces, defense, etc., in the case of chess) and a set of constraints (the rules and starting position). Prediction itself is, moreover, relative to the nature and extent of the information the system has at the time about the field of endeavor. The question one asks in framing a prediction of this sort is: What is the most rational thing for the computer to do, given goals _x_, _y_, _z_, …, constraints _a_, _b_, _c_, … and information (including misinformation, if any) about the present state of affairs _p_, _q_, _r_, … ? In predicting the computer’s response to my chess move, my assessment of the computer’s most rational move may depend, for instance, not only on my assumption that the computer has information about the present disposition of all the pieces, but also on whether I believe the computer has information about my inability to see four moves ahead, the relative powers of knights and bishops, and my weakness for knight–bishop exchanges. In the end I may not be able to frame a very good prediction, if I am unable to determine with any accuracy what information and goals the computer has, or if the information and goals I take to be given do not dictate any one best move, or if I simply am not so good as the computer is at generating an optimal move from this given. Such predictions then are very precarious; not only are they relative to a set of postulates about goals, constraints, and information, and not only do they hinge on determining an optimal response in situations where we may have no clear criteria for what is optimal, but also they are vulnerable to short-circuit falsifications that are in principle unpredictable from this stance. Just as design-stance predictions are vulnerable to malfunctions (by depending on the assumption of no malfunction), so these predictions are vulnerable to design weaknesses and lapses (by depending on the assumption of optimal design). It is a measure of the success of contemporary program designers that these precarious predictions turn out to be true with enough regularity to make the method useful.

The dénouement of this extended example should now be obvious: this third stance, with its assumption of rationality, is the _intentional stance;_ the predictions one makes from it are intentional predictions; one is viewing the computer as an intentional system. One predicts behavior in such a case by ascribing to the system _the possession of certain information_ and supposing it to be _directed by certain goals_, and then by working out the most reasonable or appropriate action on the basis of these ascriptions and suppositions. It is a small step to calling the information possessed the computer’s _beliefs_, its goals and subgoals its _desires._ What I mean by saying that this is a small step, is that the notion of possession of information or misinformation is just as intentional a notion as that of belief. The “possession” at issue is hardly the bland and innocent notion of storage one might suppose; it is, and must be, “epistemic possession”—an analogue of belief. Consider: the Frenchman who possesses the _Encyclopedia Britannica_ but knows no English might be said to “possess” the information in it, but if there is such a sense of possession, it is not strong enough to serve as the sort of possession the computer must be supposed to enjoy, relative to the information it _uses_ in “choosing” a chess move. In a similar way, the goals of a goal-directed computer must be specified intentionally, just like desires.

Lingering doubts about whether the chess-playing computer _really_ has beliefs and desires are misplaced; for the definition of intentional systems I have given does not say that intentional systems _really_ have beliefs and desires, but that one can explain and predict their behavior by _ascribing_ beliefs and desires to them, and whether one calls what one ascribes to the computer beliefs or belief-analogues or information complexes or intentional whatnots makes no difference to the nature of the calculation one makes on the basis of the ascriptions. One will arrive at the same predictions whether one forthrightly thinks in terms of the computer’s beliefs and desires, or in terms of the computer’s information-store and goal-specifications. The inescapable and interesting fact is that for the best chess-playing computers of today, intentional explanation and prediction of their behavior is not only common, but works when no other sort of prediction of their behavior is manageable. We do quite successfully treat these computers as intentional systems, and we do this independently of any considerations about what substance they are composed of, their origin, their position or lack of position in the community of moral agents, their consciousness or self-consciousness, or the determinacy or indeterminacy of their operations. The decision to adopt the strategy is pragmatic, and is not intrinsically right or wrong. One can always refuse to adopt the intentional stance toward the computer, and accept its checkmates. One can switch stances at will without involving oneself in any inconsistencies or inhumanities, adopting the intentional stance in one’s role as opponent, the design stance in one’s role as redesigner, and the physical stance in one’s role as repairman.

This celebration of our chess-playing computer is not intended to imply that it is a completely adequate model or simulation of Mind, or intelligent human or animal activity; nor am I saying that the attitude we adopt toward this computer is precisely the same that we adopt toward a creature we deem to be conscious and rational. All that has been claimed is that on occasion, a purely physical system can be so complex, and yet so organized, that we find it convenient, explanatory, pragmatically necessary for prediction, to treat it as if it had beliefs and desires and was rational. The chess-playing computer is just that, a machine for playing chess, which no man or animal is; and hence its “rationality” is pinched and artificial.

Perhaps we could straightforwardly expand the chess-playing computer into a more faithful model of human rationality, and perhaps not. I prefer to pursue a more fundamental line of inquiry first.

When should we expect the tactic of adopting the intentional stance to pay off? Whenever we have reason to suppose the assumption of optimal design is warranted, and doubt the practicality of prediction from the design or physical stance. Suppose we travel to a distant planet and find it inhabited by things moving about its surface, multiplying, decaying, apparently reacting to events in the environment, but otherwise as unlike human beings as you please. Can we make intentional predictions and explanations of their behavior? If we have reason to suppose that a process of natural selection has been in effect, then we can be assured that the populations we observe have been selected in virtue of their design: they will respond to at least some of the more common event-types in this environment in ways that are normally appropriate—that is, conducive to propagation of the species.*]] Once we have tentatively identified the perils and succors of the environment (relative to the constitution of the inhabitants, not ours), we shall be able to estimate which goals and which weighting of goals will be optimal relative to the creatures’ _needs_ (for survival and propagation), which sorts of information about the environment will be _useful_ in guiding goal-directed activity, and which activities will be appropriate given the environmental circumstances. Having doped out these conditions (which will always be subject to revision) we can proceed at once to ascribe beliefs and desires to the creatures. Their behavior will “manifest” their beliefs by being seen as the actions which, given the creatures’ desires, would be appropriate to such beliefs as would be appropriate to the environmental stimulation. Desires, in turn, will be “manifested” in behavior as those appropriate desires (given the needs of the creature) to which the actions of the creature would be appropriate, given the creature’s beliefs. The circularity of these interlocking specifications is no accident. Ascriptions of beliefs and desires must be interdependent, and the only points of anchorage are the demonstrable needs for survival, the regularities of behavior, and the assumption, grounded in faith in natural selection, of optimal design. Once one has ascribed beliefs and desires, however, one can at once set about predicting behavior on their basis, and if evolution has done its job—as it must over the long run—our predictions will be reliable enough to be useful.

It might at first seem that this tactic unjustifiably imposes human categories and attributes (belief, desire, and so forth) on these alien entities. It is a sort of anthropomorphizing, to be sure, but it is conceptually innocent anthropomorphizing. We do not have to suppose these creatures share with us any peculiarly human inclinations, attitudes, hopes, foibles, pleasures, or outlooks; their actions may not include running, jumping, hiding, eating, sleeping, listening, or copulating. All we transport from our world to theirs are the categories of rationality, perception (information input by some “sense” modality or modalities—perhaps radar or cosmic radiation), and action. The question of whether we can expect them to share any of our beliefs or desires is tricky, but there are a few points that can be made at this time; in virtue of their rationality they can be supposed to share our belief in logical truths,*]] and we cannot suppose that they normally desire their own destruction, for instance.

## II

When one deals with a system—be it man, machine, or alien creature—by explaining and predicting its behavior by citing its beliefs and desires, one has what might be called a “theory of behavior” for the system. Let us see how such intentional theories of behavior relate to other putative theories of behavior.

One fact so obvious that it is easily overlooked is that our “commonsense” explanations and predictions of the behavior of both men and animals are intentional. We start by assuming rationality. We do not _expect_ new acquaintances to react irrationally to particular topics or eventualities, but when they do we learn to adjust our strategies accordingly, just as, with a chess-playing computer, one sets out with a high regard for its rationality and adjusts one’s estimate downward wherever performance reveals flaws. The presumption of rationality is so strongly entrenched in our inference habits that when our predictions prove false, we at first cast about for adjustments in the information-possession conditions (he must not have heard, he must not know English, he must not have seen _x_, been aware that _y_, etc.) or goal weightings, before questioning the rationality of the system as a whole. In extreme cases personalities may prove to be so unpredictable from the intentional stance that we abandon it, and if we have accumulated a lot of evidence in the meanwhile about the nature of response patterns in the individual, we may find that a species of design stance can be effectively adopted. This is the fundamentally different attitude we occasionally adopt toward the insane. To watch an asylum attendant manipulate an obsessively countersuggestive patient, for instance, is to watch something radically unlike normal interpersonal relations.

Our prediction of animal behavior by “common sense” is also intentional. Whether or not sentimental folk go overboard when they talk to their dogs or fill their cats’ heads with schemes and worries, even the most hardboiled among us predict animals’ behavior intentionally. If we observe a mouse in a situation where it can see a cat waiting at one mousehole and cheese at another, we know which way the mouse will go, providing it is not deranged; our prediction is not based on our familiarity with maze-experiments or any assumptions about the sort of special training the mouse has been through. We suppose the mouse can see the cat and the cheese, and hence has beliefs (belief-analogues, intentional whatnots) to the effect that there is a cat to the left, cheese to the right, and we ascribe to the mouse also the desire to eat the cheese and the desire to avoid the cat (subsumed, appropriately enough, under the more general desires to eat and to avoid peril); so we predict that the mouse will do what is appropriate to such beliefs and desires, namely, go to the right in order to get the cheese and avoid the cat. Whatever academic allegiances or theoretical predilections we may have, we would be astonished if, in the general run, mice and other animals falsified such intentional predictions of their behavior. Indeed, experimental psychologists of every school would have a hard time devising experimental situations to support their various theories without the help of their intentional expectations of how the test animals will respond to circumstances.

Earlier I alleged that even creatures from another planet would share with us our beliefs in logical truths; light can be shed on this claim by asking whether mice and other animals, in virtue of being intentional systems, also believe the truths of logic. There is something bizarre in the picture of a dog or mouse cogitating a list of tautologies, but we can avoid that picture. The assumption that something is an intentional system is the assumption that it is rational; that is, one gets nowhere with the assumption that entity _x_ has beliefs _p_, _q_, _r_, … unless one also supposes that _x_ believes what follows from _p_, _q_, _r_, … ; otherwise there is no way of ruling out the prediction that _x_ will, in the face of its beliefs _p_, _q_, _r_, … do something utterly stupid, and, if we cannot rule out _that_ prediction, we will have acquired no predictive power at all. So whether or not the animal is said to _believe_ the _truths_ of logic, it must be supposed to _follow_ the _rules_ of logic. Surely our mouse follows or believes in _modus ponens_, for we ascribed to it the beliefs: (a) _there is a cat to the left_, and (b) _if there is a cat to the left_, _I had better not go left_, and our prediction relied on the mouse’s ability to get to the conclusion. In general there is a trade-off between rules and truths; we can suppose _x_ to have an inference rule taking _A_ to _B_ or we can give _x_ the belief in the “theorem”: _if A then B._ As far as our predictions are concerned, we are free to ascribe to the mouse either a few inference rules and belief in many logical propositions, or many inference rules and few if any logical beliefs.*]] We can even take a patently nonlogical belief like (b) and recast it as an inference rule taking (a) to the desired conclusion.

Will all logical truths appear among the beliefs of any intentional system? If the system were ideally or perfectly rational, all logical truths would appear, but any actual intentional system will be imperfect, and so not all logical truths must be ascribed as beliefs to any system. Moreover, not all the inference rules of an actual intentional system may be valid; not all its inference-licensing beliefs may be truths of logic. Experience may indicate where the shortcomings lie in any particular system. If we found an imperfectly rational creature whose allegiance to _modus ponens_, say, varied with the subject matter, we could characterize that by excluding _modus ponens_ as a rule and ascribing in its stead a set of nonlogical inference rules covering the _modus ponens_ step for each subject matter where the rule was followed. Not surprisingly, as we discover more and more imperfections (as we banish more and more logical truths from the creature’s beliefs), our efforts at intentional prediction become more and more cumbersome and undecidable, for we can no longer count on the beliefs, desires, and actions going together that _ought_ to go together. Eventually we end up, following this process, by predicting from the design stance; we end up, that is, dropping the assumption of rationality.*]]

This migration from common-sense intentional explanations and predictions to more reliable design-stance explanations and predictions that is forced on us when we discover that our subjects are imperfectly rational is, independently of any such discovery, the proper direction for theory builders to take whenever possible. In the end, we want to be able to explain the intelligence of man, or beast, in terms of his design, and this in turn in terms of the natural selection of this design; so whenever we stop in our explanations at the intentional level we have left over an unexplained instance of intelligence or rationality. This comes out vividly if we look at theory building from the vantage point of economics.

Any time a theory builder proposes to call any event, state, structure, etc., in any system (say the brain of an organism) a _signal_ or _message_ or _command_ or otherwise endows it with content, he _takes out a loan_ of intelligence. He implicitly posits along with his signals, messages, or commands, something that can serve as a signal-_reader_, message-_understander_, or _commander_, else his “signals” will be for naught, will decay unreceived, uncomprehended. This loan must be repaid eventually by finding and analyzing away these readers or comprehenders; for, failing this, the theory will have among its elements unanalyzed man-analogues endowed with enough intelligence to read the signals, etc., and thus the theory will _postpone_ answering the major question: what makes for intelligence? The intentionality of all such talk of signals and commands reminds us that rationality is being taken for granted, and in this way shows us where a theory is incomplete. It is this feature that, to my mind, puts a premium on the yet unfinished task of devising a rigorous definition of intentionality, for if we can lay claim to a purely formal criterion of intentional discourse, we will have what amounts to a medium of exchange for assessing theories of behavior. Intentionality _abstracts_ from the inessential details of the various forms intelligence-loans can take (e.g., signal-readers, volition-emitters, librarians in the corridors of memory, egos and superegos) and serves as a reliable means of detecting exactly where a theory is _in the red_ relative to the task of explaining intelligence; wherever a theory relies on a formulation bearing the logical marks of intentionality, there a little man is concealed.

This insufficiency of intentional explanation from the point of view of psychology has been widely felt and as widely misconceived. The most influential misgivings, expressed in the behaviorism of Skinner and Quine, can be succinctly characterized in terms of our economic metaphor. Skinner’s and Quine’s adamant prohibitions of intentional idioms at all levels of theory is the analogue of rock-ribbed New England conservatism: no deficit spending when building a theory! In Quine’s case, the abhorrence of loans is due mainly to his fear that they can never be repaid, whereas Skinner stresses rather that what is borrowed is worthless to begin with. Skinner’s suspicion is that intentionally couched claims are empirically vacuous, in the sense that they are altogether too easy to accommodate to the data, like the _virtus dormitiva_ Moliere’s doctor ascribes to the sleeping powder (see Chapter 4 for a more detailed discussion of these issues). Questions can be begged on a temporary basis, however, permitting a mode of prediction and explanation not totally vacuous. Consider the following intentional prediction: if I were to ask a thousand American mathematicians how much seven times five is, more than nine hundred would respond by saying that it was thirty-five. (I have allowed for a few to mis-hear my question, a few others to be obstreperous, a few to make slips of the tongue.) If you doubt the prediction, you can test it; I would bet good money on it. It seems to have empirical content because it can, in a fashion, be tested, and yet it is unsatisfactory as a prediction of an empirical theory of psychology. It works, of course, because of the contingent, empirical—but evolution-guaranteed—fact that men in general are well enough designed both to get the answer right and to want to get it right. It will hold with as few exceptions for any group of Martians with whom we are able to converse, for it is not a prediction just of _human_ psychology, but of the “psychology” of intentional systems generally.

Deciding on the basis of available empirical evidence that something is a piece of copper or a lichen permits one to make predictions based on the empirical theories dealing with copper and lichens, but deciding on the basis of available evidence that something is (may be treated as) an intentional system permits predictions having a normative or logical basis rather than an empirical one, and hence the success of an intentional prediction, based as it is on no particular picture of the system’s design, cannot be construed to confirm or disconfirm any particular pictures of the system’s design.

Skinner’s reaction to this has been to try to frame predictions purely in non-intentional language, by predicting bodily responses to physical stimuli, but to date this has not provided him with the alternative mode of prediction and explanation he has sought, as perhaps an extremely cursory review can indicate. To provide a setting for nonintentional prediction of behavior, he invented the Skinner box, in which the rewarded behavior of the occupant—say, a rat—is a highly restricted and stereotypic bodily motion—usually pressing a bar with the front paws.

The claim that is then made is that once the animal has been trained, a law-like relationship is discovered to hold between non-intentionally characterized events: controlling stimuli and bar-pressing responses. A regularity is discovered to hold, to be sure, but the fact that it is between non-intentionally defined events is due to a property of the Skinner box and not of the occupant. For let us turn our prediction about mathematicians into a Skinnerian prediction: strap a mathematician in a Skinner box so he can move only his head; display in front of him a card on which appear the marks: “How much is seven times five?”; move into the range of his head-motions two buttons, over one of which is the mark “35” and over the other “34”; place electrodes on the soles of his feet and give him a few quick shocks; the controlling stimulus is then to be the sound: “Answer now!” I predict that in a statistically significant number of cases, even _before_ training trials to condition the man to press button “35” with his forehead, he will do this when given the controlling stimulus. Is this a satisfactory scientific prediction just because it eschews the intentional vocabulary? No, it is an intentional prediction disguised by so restricting the environment that only one bodily motion is available to fulfill the intentional _action_ that anyone would prescribe as appropriate to the circumstances of perception, belief, desire. That it is action, not merely motion, that is predicted can also be seen in the case of subjects less intelligent than mathematicians. Suppose a mouse were trained, in a Skinner box with a food reward, to take exactly four steps forward and press a bar with its nose; if Skinner’s laws truly held between stimuli and responses defined in terms of bodily motion, were we to move the bar an inch farther away, so four steps did not reach it, Skinner would have to predict that the mouse would jab its nose into the empty air rather than take a fifth step.

A variation of Skinnerian theory designed to meet this objection acknowledges that the trained response one predicts is not truly captured in a description of skeletal motion alone, but rather in a description of an environmental effect achieved: the bar going down, the “35” button being depressed. This will also not do. Suppose we could in fact train a man or animal to achieve an environmental effect, as this theory proposes. Suppose, for instance, we train a man to push a button under the longer of two displays, such as drawings or simple designs, that is, we reward him when he pushes the button under the longer of two pictures of pencils, or cigars, etc. The miraculous consequence of this theory, were it correct, would be that if, after training him on simple views, we were to present him with the Müller-Lyer arrow-head illusion, he would be immune to it, for _ex hypothesi_ he has been trained to achieve an _actual_ environmental effect (choosing the display that _is_ longer), not a _perceived_ or _believed_ environmental effect (choosing the display that _seems_ longer). The reliable prediction, again, is the intentional one.*]]

Skinner’s experimental design is supposed to eliminate the intentional, but it merely masks it. Skinner’s non-intentional predictions work to the extent they do, not because Skinner has truly found non-intentional behavioral laws, but because the highly reliable intentional predictions underlying his experimental situations (the rat desires food and believes it will get food by pressing the bar—something for which it has been given good evidence—so it will press the bar) are disguised by leaving virtually no room in the environment for more than one bodily motion to be the appropriate action and by leaving virtually no room in the environment for discrepancy to arise between the subject’s beliefs and the reality.

Where, then, should we look for a satisfactory theory of behavior? Intentional theory is vacuous as psychology because it presupposes and does not explain rationality or intelligence. The apparent successes of Skinnerian behaviorism, however, rely on hidden intentional predictions. Skinner is right in recognizing that intentionality can be no _foundation_ for psychology, and right also to look for purely mechanistic regularities in the activities of his subjects, but there is little reason to suppose they will lie on the surface in gross behavior—except, as we have seen, when we put an artificial straitjacket on an intentional regularity. Rather, we will find whatever mechanistic regularities there are in the functioning of internal systems whose design approaches the optimal (relative to some ends). In seeking knowledge of internal design our most promising tactic is to take out intelligence-loans, endow peripheral and internal events with content, and then look for mechanisms that will function appropriately with such “messages” so that we can pay back the loans. This tactic is hardly untried. Research in artificial intelligence, which has produced, among other things, the chess-playing computer, proceeds by working from an intentionally characterized problem (how to get the computer to consider the right sorts of information, make the right decisions) to a design-stance solution—an approximation of optimal design. Psychophysicists and neurophysiologists who routinely describe events in terms of the transmission of information within the nervous system are similarly borrowing intentional capital—even if they are often inclined to ignore or disavow their debts.

Finally, it should not be supposed that, just because intentional theory is vacuous as psychology, in virtue of its assumption of rationality, it is vacuous from all points of view. Game theory, for example, is inescapably intentional,[^5] but as a formal normative theory and not a psychology this is nothing amiss. Game-theoretical predictions applied to human subjects achieve their accuracy in virtue of the evolutionary guarantee that man is well designed as a game player, a special case of rationality. Similarly, economics, the social science of greatest predictive power today, is not a psychological theory and presupposes what psychology must explain. Economic explanation and prediction is intentional (although some is disguised) and succeeds to the extent that it does because individual men are in general good approximations of the optimal operator in the marketplace.

## III

The concept of an intentional system is a relatively uncluttered and unmetaphysical notion, abstracted as it is from questions of the composition, constitution, consciousness, morality, or divinity of the entities falling under it. Thus, for example, it is much easier to decide whether a machine can be an intentional system than it is to decide whether a machine can _really_ think, or be conscious, or morally responsible. This simplicity makes it ideal as a source of order and organization in philosophical analyses of “mental” concepts. Whatever else a person might be—embodied mind or soul, self-conscious moral agent, “emergent” form of intelligence—he is an intentional system, and whatever follows just from being an intentional system is thus true of a person. It is interesting to see just how much of what we hold to be the case about persons or their minds follows directly from their being intentional systems. To revert for a moment to the economic metaphor, the guiding or challenging question that defines work in the philosophy of mind is this: are there mental treasures that cannot be purchased with intentional coin? If not, a considerable unification of science can be foreseen in outline. Of special importance for such an examination is the subclass of intentional systems that have language, that can communicate; for these provide a framework for a theory of consciousness. In _Content and Consciousness_, part II, and in parts III and IV of this volume I have attempted to elaborate such a theory; here I would like to consider its implications for the analysis of the concept of belief. What will be true of human believers just in virtue of their being intentional systems with the capacity to communicate?

Just as not all intentional systems currently known to us can fly or swim, so not all intentional systems can talk, but those which can do this raise special problems and opportunities when we come to ascribe beliefs and desires to them. That is a massive understatement; without the talking intentional systems, of course, there would be no ascribing beliefs, no theorizing, no assuming rationality, no predicting. The capacity for language is without doubt the crowning achievement of evolution, an achievement that feeds on itself to produce ever more versatile and subtle rational systems, but still it can be looked at as an adaptation which is subject to the same conditions of environmental utility as any other behavioral talent. When it is looked at in this way several striking facts emerge. One of the most pervasive features of evolutionary histories is the interdependence of distinct organs and capacities in a species. Advanced eyes and other distance receptors are of no utility to an organism unless it develops advanced means of locomotion; the talents of a predator will not accrue to a species that does not evolve a carnivore’s digestive system. The capacities of belief and communication have prerequisites of their own. We have already seen that there is no point in ascribing beliefs to a system unless the beliefs ascribed are in general appropriate to the environment, and the system responds appropriately to the beliefs. An eccentric expression of this would be: the capacity to believe would have no survival value unless it were a capacity to believe truths. What is eccentric and potentially misleading about this is that it hints at the picture of a species “trying on” a faculty giving rise to beliefs most of which were false, having its inutility demonstrated, and abandoning it. A species might “experiment” by mutation in any number of inefficacious systems, but none of these systems would deserve to be called belief systems precisely because of their defects, their nonrationality, and hence a false belief system is a conceptual impossibility. To borrow an example from a short story by MacDonald Harris, a soluble fish is an evolutionary impossibility, but a system for false beliefs cannot even be given a coherent description. The same evolutionary bias in favor of truth prunes the capacity to communicate as it develops; a capacity for false communication would not be a capacity for communication at all, but just an emission proclivity of no systematic value to the species. The faculty of communication would not gain ground in evolution unless it was by and large the faculty of transmitting true beliefs, which means only: the faculty of altering other members of the species in the direction of more optimal design.

This provides a foundation for explaining a feature of belief that philosophers have recently been at some pains to account for.[^6] The concept of belief seems to have a normative cast to it that is most difficult to capture. One way of putting it might be that an avowal like “I believe that _p_“ seems to imply in some fashion: “One ought to believe that _p._” This way of putting it has flaws, however, for we must then account for the fact that “I believe that _p_“ seems to have normative force that “He believes that _p_,” said of me, does not. Moreover, saying that one ought to believe this or that suggests that belief is voluntary, a view with notorious difficulties.[^7] So long as one tries to capture the normative element by expressing it in the form of moral or pragmatic injunctions to believers, such as “one ought to believe the truth” and “one ought to act in accordance with one’s beliefs,” dilemmas arise. How, for instance, is one to follow the advice to believe the truth? Could one abandon one’s sloppy habit of believing falsehoods? If the advice is taken to mean: believe only what you have convincing evidence for, it is the vacuous advice: believe only what you believe to be true. If alternatively it is taken to mean: believe only what is in fact the truth, it is an injunction we are powerless to obey.

The normative element of belief finds its home not in such injunctions but in the preconditions for the ascription of belief, what Phillips Griffiths calls “the general conditions for the possibility of application of the concept.” For the concept of belief to find application, two conditions, we have seen, must be met: (1) In general, normally, more often than not, if _x_ believes _p_, _p_ is true. (2) In general, normally, more often than not, if _x_ avows that _p_, he believes _p_ [and, by (1), _p_ is true]. Were these conditions not met, we would not have rational, communicating systems; we would not have believers or belief-avowers. The norm for belief is evidential well-foundedness (assuring truth in the long run), and the norm for avowal of belief is accuracy (which includes sincerity). These two norms determine pragmatic implications of our utterances. If I assert that _p_ (or that I believe that _p_—it makes no difference), I assume the burden of defending my assertion on two fronts: I can be asked for evidence for the truth of _p_, and I can be asked for behavioral evidence that I do in fact believe _p._[^8] I do not need to examine my own behavior in order to be in a position to avow my belief that _p_, but if my sincerity or self-knowledge is challenged, this is where I must turn to defend my assertion. But again, challenges on either point must be the exception rather than the rule if belief is to have a place among our concepts.

Another way of looking at the importance of this predominance of the normal is to consider the well-known circle of implications between beliefs and desires (or intentions) that prevent non-intentional behavioral definitions of intentional terms. A man’s standing under a tree is a behavioral indicator of his belief that it is raining, but only on the assumption that he desires to stay dry, and if we then look for evidence that he wants to stay dry, his standing under the tree will do, but only on the assumption that he believes the tree will shelter him; if we ask him if he believes the tree will shelter him, his positive response is confirming evidence only on the assumption that he desires to tell us the truth, and so forth _ad infinitum._ It is this apparently vicious circle that turned Quine against the intentional (and foiled Tolman’s efforts at operational definition of intentional terms), but if it is true that in any particular case a man’s saying that _p_ is evidence of his belief only conditionally, we can be assured that in the long run and in general the circle is broken; a man’s assertions are, unconditionally, indicative of his beliefs, as are his actions in general. We get around the “privacy” of beliefs and desires by recognizing that in general anyone’s beliefs and desires must be those he “ought to have” given the circumstances.

These two interdependent norms of belief, one favoring the truth and rationality of belief, the other favoring accuracy of avowal, normally complement each other, but on occasion can give rise to conflict. This is the “problem of incorrigibility.” If rationality is the mother of intention, we still must wean intentional systems from the criteria that give them life, and set them up on their own. Less figuratively, if we are to make use of the concept of an intentional system in particular instances, at some point we must cease _testing_ the assumption of the system’s rationality, adopt the intentional stance, and grant without further ado that the system is qualified for beliefs and desires. For mute animals—and chess-playing computers—this manifests itself in a tolerance for less than optimal performance. We continue to ascribe beliefs to the mouse, and explain its actions in terms of them, after we have tricked it into some stupid belief. This tolerance has its limits of course, and the less felicitous the behavior—especially the less adaptable the behavior—the more hedged are our ascriptions. For instance, we are inclined to say of the duckling that “imprints” on the first moving thing it sees upon emerging from its shell that it “believes” the thing is its mother, whom it follows around, but we emphasize the scare-quotes around “believes.” For intentional systems that can communicate—persons for instance—the tolerance takes the form of the convention that a man is incorrigible or a special authority about his own beliefs. This convention is “justified” by the fact that evolution does guarantee that our second norm is followed. What better source could there be of a system’s beliefs than its avowals? Conflict arises, however, whenever a person falls short of perfect rationality, and avows beliefs that either are strongly disconfirmed by the available empirical evidence or are self-contradictory or contradict other avowals he has made. If we lean on the myth that a man is perfectly rational, we must find his avowals less than authoritative: “You _can’t_ mean—understand—what you’re saying!”; if we lean on his “right” as a speaking intentional system to have his word accepted, we grant him an irrational set of beliefs. Neither position provides a stable resting place; for, as we saw earlier, intentional explanation and prediction cannot be accommodated either to breakdown or to less than optimal design, so there is no coherent intentional description of such an impasse.*]]

Can any other considerations be brought to bear in such an instance to provide us with justification for one ascription of beliefs rather than another? Where should one look for such considerations? The Phenomenologist will be inclined to suppose that individual introspection will provide us a sort of data not available to the outsider adopting the intentional stance; but how would such data get used? Let the introspector amass as much inside information as you please; he must then communicate it to us, and what are we to make of his communications? We can suppose that they are incorrigible (barring corrigible verbal errors, slips of the tongue, and so forth), but we do not need Phenomenology to give us that option, for it amounts to the decision to lean on the accuracy-of-avowal norm at the expense of the rationality norm. If, alternatively, we demand certain standards of consistency and rationality of his utterances before we accept them as authoritative, what standards will we adopt? If we demand perfect rationality, we have simply flown to the other norm at the expense of the norm of accuracy of avowal. If we try to fix minimum standards at something less than perfection, what will guide our choice? Not Phenomenological data, for the choice we make will determine what is to count as Phenomenological data. Not neurophysiological data either, for whether we interpret a bit of neural structure to be endowed with a particular belief content hinges on our having granted that the neural system under examination has met the standards of rationality for being an intentional system, an assumption jeopardized by the impasse we are trying to resolve. That is, one might have a theory about an individual’s neurology that permitted one to “read off “ or predict the propositions to which he would assent, but whether one’s theory had uncovered his _beliefs_, or merely a set of assent-inducers, would depend on how consistent, reasonable, true we found the set of propositions.

John Vickers has suggested to me a way of looking at this question. Consider a set _T_ of transformations that take beliefs into beliefs. The problem is to determine the set _TS_ for each intentional system _S_, so that if we know that _S_ believes _p_, we will be able to determine other things that _S_ believes by seeing what the transformations of _p_ are for _TS._ If _S_ were ideally rational, every valid transformation would be in _TS_; _S_ would believe every logical consequence of every belief (and, ideally, _S_ would have no false beliefs). Now we know that no actual intentional system will be ideally rational; so we must suppose any actual system will have a _T_ with less in it. But we also know that, to qualify as an intentional system at all, _S_ must have a _T_ with some integrity; _T_ cannot be empty. What rationale could we have, however, for fixing some set between the extremes and calling it _the_ set for belief (for _S_, for earthlings, or for ten-year-old girls)? This is another way of asking whether we could replace Hintikka’s normative theory of belief with an empirical theory of belief, and, if so, what evidence we would use. “Actually,” one is tempted to say, “people do believe contradictions on occasion, as their utterances demonstrate; so any adequate logic of belief or analysis of the concept of belief must accommodate this fact.” But any attempt to _legitimize_ human fallibility in a theory of belief by fixing a permissible level of error would be like adding one more rule to chess: an Official Tolerance Rule to the effect that any game of chess containing no more than _k_ moves that are illegal relative to the other rules of the game is a legal game of chess. Suppose we discovered that, in a particular large population of poor chess-players, each game on average contained three illegal moves undetected by either opponent. Would we claim that these people _actually_ play a different game from ours, a game with an Official Tolerance Rule with _k_ fixed at 3? This would be to confuse the norm they follow with what gets by in their world. We could claim in a similar vein that people _actually_ believe, say, all synonymous or intentionally isomorphic consequences of their beliefs, but not all their logical consequences, but of course the occasions when a man resists assenting to a logical consequence of some avowal of his are unstable cases; he comes in for criticism and cannot appeal in his own defense to any canon absolving him from believing nonsynonymous consequences. If one wants to get away from norms and predict and explain the “actual, empirical” behavior of the poor chess-players, one stops talking of their _chess moves_ and starts talking of their proclivities to move pieces of wood or ivory about on checkered boards; if one wants to predict and explain the “actual, empirical” behavior of believers, one must similarly cease talking of belief, and descend to the design stance or physical stance for one’s account.

The concept of an intentional system explicated in these pages is made to bear a heavy load. It has been used here to form a bridge connecting the intentional domain (which includes our “common-sense” world of persons and actions, game theory, and the “neural signals” of the biologist) to the non-intentional domain of the physical sciences. That is a lot to expect of one concept, but nothing less than Brentano himself expected when, in a day of less fragmented science, he proposed intentionality as the mark that sunders the universe in the most fundamental way: dividing the mental from the physical.

## Notes

*]] Note that what is _directly_ selected, the gene, is a diagram and not a design; it is selected, however, because it happens to ensure that its bearer has a certain (functional) design. This was pointed out to me by Woodruff.

*]] Cf. Quine’s argument about the necessity of “discovering” our logical connectives in any language we can translate in _Word and Object_ (Cambridge, Mass.: MIT, 1960), Section 13. More will be said in defense of this below.

*]] Accepting the argument of Lewis Carroll, in “What the Tortoise Said to Achilles,” _Mind_ (1895), reprinted in I. M. Copi and J. A. Gould, _Readings on Logic_ (New York: MacMillan, 1964), we cannot allow all the rules for a system to be replaced by beliefs, for this would generate an infinite and unproductive nesting of distinct beliefs about what can be inferred from what.

*]] This paragraph owes much to discussion with John Vickers, whose paper “Judgment and Belief,” in K. Lambert, _The Logical Way of Doing Things_ (New Haven, Conn.: Yale, 1969), goes beyond the remarks here by considering the problems of the relative strength or weighting of beliefs and desires.

*]] R. L. Gregory, _Eye and Brain_ (London: World University Library, 1966): p. 137, reports that pigeons and fish given just this training are, not surprisingly, susceptible to visual illusions of length.

*]] Hintikka takes this bull by the horns. His epistemic logic is acknowledged to hold only for the ideally rational believer; were we to apply this logic to persons in the actual world in other than a normative way, thus making its implications _authoritative_ about actual belief, the authority of persons would have to go by the board. Thus his rule A.CBB* (_Knowledge and Belief_, pp. 24–26), roughly that if one believes _p_ one believes that one believes _p_, cannot be understood, as it is tempting to suppose, as a version of the incorrigibility thesis.

1.]] I am indebted to Peter Woodruff for making extensive improvements in this chapter prior to its initial publication. Since it appeared, I have found anticipations and developments of similar or supporting themes in a variety of writers, most notably Carl Hempel, “Rational Action,” _Proceedings and Addresses of the American Philosophical Association_, XXXV (1962), reprinted in N. S. Care and C. Landesman, eds., _Readings in the Theory of Action_ (Bloomington, Indiana: University Press, 1968); L. Jonathan Cohen, “Teleological Explanation,” _Proceedings of the Aristotelian Society_ (1950–51), and “Can There Be Artificial Minds?,” _Analysis_ (1954–55); B. A. 0. Williams, “Deciding to Believe,” in H. E. Kiefer and M. K. Munitz, eds., _Language, Belief, and Metaphysics_ (Albany: SUNY Press, 1970), and David Lewis, “Radical Interpretation,” _Synthese_, III, IV (1974): 331–44.

2.]] For a lucid introduction to the concept and its history, see the entry on “intentionality” by Roderick Chisholm in P. Edwards, ed., _The Encyclopedia of Philosophy_ (New York: MacMillan, 1967).

3.]] _Content and Consciousness_ (London: Routledge & Kegan Paul, 1969).

4.]] The term “intentional system” occurs in Charles Taylor’s _The Explanation of Behaviour_ (London: Routledge & Kegan Paul, 1964), p. 62, where its use suggests it is co-extensive with the term as I use it, but Taylor does not develop the notion in depth. See, however, his p. 58ff. For an introduction to the concept of an intentional system with fewer philosophical presuppositions, see the first sections of Chapters 12 and 14 of this volume.

5.]] Hintikka notes in passing that game theory is like his epistemic logic in assuming rationality, in _Knowledge and Belief_ (Ithaca, N.Y.: Cornell, 1962): p. 38.

6.]] I have in mind especially A. Phillips Griffiths’ penetrating discussion “On Belief,” _Proceedings of the Aristotelian Society_, LXIII (1962/3): 167–86; and Bernard Mayo’s “Belief and Constraint,” ibid., LXIV (1964): 139–56, both reprinted in Phillips Griffiths, ed., _Knowledge and Belief_ (New York: Oxford, 1967).

7.]] See, e.g., H. H. Price, “Belief and Will,” _Proceedings of the Aristotelian Society_, suppl. vol. XXVIII (1954), reprinted in S. Hampshire, ed., _Philosophy of Mind_ (New York: Harper & Row, 1966).

8.]] Cf. A. W. Collins, “Unconscious Belief,” _Journal of Philosophy_, LXVI, 20 (Oct. 16, 1969): 667–80.

Reply to Arbib and Gunderson

In December, 1972, Michael Arbib and Keith Gunderson presented papers to an American Philosophical Association symposium on my earlier book, _Content and Consciousness_, to which this essay was a reply.[^1] While one might read it as a defense of the theory in my first book, I would rather have it considered an introduction to the offspring theory. In spite of a few references to Arbib’s and Gunderson’s papers and my book, this essay is designed to be comprehensible on its own, though I would not at all wish to discourage readers from exploring its antecedents. In the first section the ground rules for ascribing mental predicates to things are developed beyond the account given in Chapter 1. There I claimed that since intentional stance predictions can be made in ignorance of a thing’s design—solely on an assumption of the design’s excellence—verifying such predictions does not help to confirm any particular psychological theory about the actual design of the thing. This implies that what two things have in common when both are correctly attributed some mental feature need be no independently describable design feature, a result that threatens several familiar and compelling ideas about mental events and states. The second section threatens another familiar and compelling idea, _viz._, that we mean one _special_ thing when we talk of consciousness, rather than a variety of different and improperly united things.

## I

Suppose two artificial intelligence teams set out to build _face-recognizers._ We will be able to judge the contraptions they come up with, for we know in advance what a face-recognizer ought to be able to do. Our expectations of face-recognizers do not spring from induction over the observed behavior of large numbers of actual face-recognizers, but from a relatively a priori source: what might be called our intuitive epistemic logic, more particularly, “the logic of our concept” of recognition. The logic of the concept of recognition dictates an open-ended and shifting class of appropriate further tasks, abilities, reactions and distinctions that _ideally_ would manifest themselves in any face-recognizer under various conditions. Not only will we want a face-recognizer to answer questions correctly about the faces before it, but also to “use” its recognition capacities in a variety of other ways, depending on what else it does, what other tasks it performs, what other goals it has. These conditions and criteria are characterized intentionally; they are a part of what I call the theory of intentional systems, the theory of entities that are not just face-recognizers, but theorem-provers, grocery-choosers, danger-avoiders, music appreciators.

Since the Ideal Face-Recognizer, like a Platonic Form, can only be approximated by any hardware (or brainware) copy, and since the marks of successful approximation are characterized intentionally, the face-recognizers designed by the two teams may differ radically in material or design. At the physical level one might be electronic, the other hydraulic. Or one might rely on a digital computer, the other on an analogue computer. Or, at a higher level of design, one might use a system that analyzed exhibited faces via key features with indexed verbal labels—“balding,” “snub-nosed,” “lantern-jawed”—and then compared label-scores against master lists of label scores for previously encountered faces, while the other might use a system that reduced all face presentations to a standard size and orientation, and checked them quasi-optically against stored “templates” or “stencils.” The contraptions could differ this much in design and material while being equally good—and quite good—approximations of the ideal face-recognizer. This much is implicit in the fact that the concept of recognition, unlike the concepts of, say, protein or solubility, is an intentional concept, not a physical or mechanistic concept.

But obviously there must be _some_ similarity between the two face-recognizers, because they are, after all, both face-recognizers. For one thing, if they are roughly equally good approximations of the ideal, the intentional characterizations of their behaviors will have a good deal in common. They will often both be said to believe the same propositions about the faces presented to them, for instance. But what implications about further similarity can be drawn from the fact that their intentional characterizations are similar? Could they be similar only in their intentional characterizations?

Consider how we can criticize and judge the models from different points of view. From the biological point of view, one model may be applauded for utilizing elements bearing a closer resemblance in function or even chemistry to known elements in the brain. From the point of view of engineering, one model may be more efficient, failsafe, economical and sturdy. From an “introspective” point of view, one model may appear to reflect better the actual organization of processes and routines we human beings may claim to engage in when confronted with a face. Finally, one model may simply recognize faces better than the other, and even better than human beings can. The relevance of these various grounds waxes and wanes with our purposes. If we are attempting to model “the neural bases” of recognition, sturdiness and engineering economy are beside the point—except to the extent (no doubt large) that the neural bases are sturdy and economical. If we are engaged in “artificial intelligence” research as contrasted with “computer simulation of cognitive processes,”[^2] we will not care if our machine’s ways are not those of the man in the street, and we will not mind at all if our machine has an _inhuman_ capacity for recognizing faces.

Now as “philosophers of mind,” which criterion of success should we invoke? As guardians of the stock of common mentalistic concepts, we will not be concerned with rival biological theories, nor should we have any predilections about the soundness of “engineering” in our fellow face-recognizers. Nor, finally, should we grant the last word to introspective data, to the presumed phenomenology of face-recognition, for however uniform we might _discover_ the phenomenological reports of human face-recognizers to be, we can easily imagine discovering that people report a wide variety of feelings, hunches, _gestalts_, strategies, intuitions while sorting out faces, and we would not want to say this variation cast any doubt on the claim of each of them to be a _bona fide_ face-recognizer. Since it seems we must grant that two face-recognizers, whether natural or artificial, may accomplish this task in different ways, this suggests that even when we ascribe the same belief to two systems (e.g., the belief that one has seen face _n_ more than once before), there need be no elements of design, and _a fortiori_ of material, in common between them.

Let us see how this could work in more detail. The design of a face-recognizer would typically break down at the highest level into subsystems tagged with intentional labels: “the feature _detector_ sends a _report_ to the _decision unit_, which _searches_ the _memory_ for records of similar features, and if the result is positive, the system _commands_ the printer to write ‘I have seen this face before’”—or something like that. These intentionally labeled subsystems themselves have parts, or elements, or states, and _some_ of these may well be intentionally labeled in turn: the decision unit goes into the _conviction-that-I_’_ve-seen-this-face-before_ state, if you like. Other states or parts may not suggest any intentional characterization—e.g., the _open_ state of a particular switch may not be aptly associated with any particular belief, intention, perception, directive, or decision. When we are in a position to ascribe the single belief that _p_ to a system, we must, in virtue of our open-ended expectations of the ideal believer-that-_p_, be in a position to ascribe to the system an indefinite number of further beliefs, desires, etc. While no doubt _some_ of these ascriptions will line up well with salient features of the system’s design, other ascriptions will not, even though the system’s behavior is so regulated overall as to justify those ascriptions. There need not, and cannot, be a separately specifiable state of the mechanical elements for each of the myriad intentional ascriptions, and thus it will not in many cases be possible to isolate any feature of the system at any level of abstraction and say, “_This_ and _just this_ is the feature in the design of this system responsible for those aspects of its behavior in virtue of which we ascribe to it the belief that _p._” And so, from the fact that both system _S_ and system _T_ are well characterized as believing that _p_, it does not follow that they are both in some state uniquely characterizable in any other way than just as the state of believing that _p._ (Therefore, _S_ and _T_’s being in the same belief state need not amount to their being in the same logical state, if we interpret the latter motion as some Turing-machine state for some shared Turing-machine interpretation, for they need not share any relevant Turing-machine interpretation.)

This brings me to Arbib’s first major criticism. I had said that in explaining the behavior of a dog, for instance, precision in the intentional story was not an important scientific goal, since from any particular intentional ascription, no precise or completely reliable inferences about other intentional ascriptions or subsequent behavior could be drawn in any case, since we cannot know or specify how close the actual dog comes to the ideal. Arbib finds this “somewhat defeatist,” and urges that “there is nothing which precludes description at the intentional level from expressing causal sequences providing our intentional language is extended to allow us to provide descriptions with the flexibility of a program, rather than a statement of general tendencies.” Now we can see that what Arbib suggests is right. If we put _intentional labels_ on parts of a computer program, or on states the computer will pass through in executing a program, we gain access to the considerable predictive power and precision of the program.*]] When we put an intentional label on a program state, and want a prediction of what _precisely_ will happen when the system is in that intentional state, we get our prediction by taking a close look not at the terms used in the label—we can label as casually as you like—but at the specification of the program so labeled. But if Arbib is right, I am not thereby wrong, for Arbib and I are thinking of rather different strategies. The sort of precision I was saying was impossible was a precision _prior_ to labeling, a purely lexical refining which would permit the intentional calculus to operate more determinately in making its idealized predictions. Arbib, on the other hand, is talking about the access to predictive power and precision one gets when one sullies the ideal by using intentional ascriptions as more or less justifiable labels for program features that have precisely specified functional interrelations.

One might want to object: the word “label” suggests that Arbib gets his predictive power and precision out of intentional description by mere arbitrary fiat. If one assigns the intentional label “the _belief-that-p_ state” to a logical state of a computer, C, and then predicts from C’s program what it will do in that state, one is predicting what it will do when it believes that _p_ only in virtue of that assignment, obviously. Assignments of intentional labels, however, are not arbitrary: it can become apt so to label a state when one has designed a program of power and versatility. Similarly, one’s right to call a subsystem in his system the memory, or the nose-shape-detector, or the jawline analyzer hinges on the _success_ of the subsystem’s design rather than any other feature of it. The inescapably idealizing or normative cast to intentional discourse about an artificial system can be made honest by excellence of design, and by nothing else.

This idealizing of intentional discourse gives play to my tactic of ontological neutrality, which Gunderson finds so dubious. I wish to maintain physicalism—a motive that Gunderson finds congenial—but think identity theory is to be shunned. Here is one reason why. Our imagined face-recognizers were presumably purely physical entities, and we ascribed psychological predicates to them (albeit a very restricted set of psychological predicates, as we shall see). If we then restrict ourselves for the moment to the “mental features” putatively referred to in these ascriptions, I think we should be able to see that identity theory with regard to them is simply without appeal. The usual seductions of identification are two, I think: ontological economy, or access to generalization (since _this_ cloud is identical with a collection of water droplets, _that_ cloud is apt to be as well). The latter motive has been all but abandoned by identity theorists in response to Putnam’s objections (and others), and in this instance it is clearly unfounded; there is no reason to suppose that the physical state one identified with a particular belief in one system would have a physical twin in the other system with the same intentional characterization. So if we are to have identity, it will have to be something like Davidson’s “anomalous monism.”[^3] But what ontic house-cleaning would be accomplished by identifying each and every intentionally characterized “state” or “event” in a system with some particular physical state or event of its parts? In the first place there is no telling how many _different_ intentional states to ascribe to the system; there will be indefinitely many candidates. Is the state of believing that 100 < 101 distinct from the state of believing that 100 < 102, and if so, should we then expect to find distinct physical states of the system to ally with each? For _some_ ascriptions of belief there will be, as we have seen, an isolable state of the program well suited to the label, but for each group of belief-states thus anchored to saliencies in our system, our intuitive epistemic logic will tell us that anyone who believed _p_, _q_, _r_, … would have to believe _s_, _t_, _u_, _v_, … as well, and while the behavior of the system would harmonize well with the further ascription to it of belief in _s_, _t_, _u_, _v_, … (this being the sort of test that establishes a thing as an intentional system), we would find nothing in particular to point to in the system as the state of belief in _s_, or _t_ or _u_ or _v_. … This should not worry us, for the intentional story we tell about an entity is not a history of actual events, processes, states, objects, but a sort of abstraction.*]] The desire to identify each and every part of it with some node or charge or region just because _some_ parts can be so identified, is as misguided as trying to identify each line of longitude and latitude with a trail of molecules—changing, of course, with every wave and eddy—just because we have seen a bronze plaque at Greenwich or a row of posts along the Equator.

It is tempting to deny this, just because the intentional story we tell about each other is so apparently full of _activity_ and _objects:_ we are convicted of _ignoring something_ in our _memory_, _jumping_ to a _conclusion_, _confusing_ two different _ideas._ Grammar can be misleading. In baseball, _catching a fly ball_ is an exemplary physical event-type, tokens of which turn out on analysis to involve a _fly ball_ (a physical object) which is _caught_ (acted upon in a certain physical way). In crew, _catching a crab_ is just as bruisingly physical an event-type, but there is no crab that is caught. Not only is it not the case that oarsmen catch real live (or dead) crabs with their oars; and not only is it not the case that for each token of catching a crab, a physically similar thing—each token’s crab—is caught, it is not even the case that for each token there is a thing, its crab, however dissimilar from all other such crabs, that is caught. The parallel is not strong enough, however, for while there are no isolable crabs that are caught in crew races, there are isolable catchings-of-crabs, events that actually happen in the course of crew races, while in the case of many intentional ascriptions, there need be no such events at all. Suppose a programmer informs us that his face-recognizer “is designed to ignore blemishes” or “normally assumes that faces are symmetrical aside from hair styles.” We should not suppose he is alluding to recurrent activities of blemish-ignoring, or assuming, that his machine engages in, but rather that he is alluding to aspects of his machine’s design that determine its behavior along such lines as would be apt in one who ignored blemishes or assumed faces to be symmetrical. The pursuit of identities, in such instances, seems not only superfluous but positively harmful, since it presumes that a story that is, at least in large part, a calculator’s fiction is in fact a history of actual events, which if they are not physical will have to be non-physical.

At this point Gunderson, and Thomas Nagel,[^4] can be expected to comment that these observations of mine may solve the mind–body problem for certain machines—a dubious achievement if there ever was one—but have left untouched the traditional mind–body problem. To see what they are getting at, consider Gunderson’s useful distinction between “program-receptive and program-resistant features of mentality.”[^5] Some relatively colorless mental events, such as those involved in recognition and theorem-proving, can be well-simulated by computer programs, while others, such as pains and sensations, seem utterly unapproachable by the programmer’s artifices. In this instance the distinction would seem to yield the observation that so far only some program-receptive features of mentality have been spirited away unidentified, leaving such program-resistant features as pains, itches, _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, yearnings, thrills of lust and other raw feels unaccounted for. Doesn’t my very Rylean attempt fall down just where Ryle’s own seems to: on the undeniable _episodes_ of conscious experience? It is certainly clear that the intentional features so far considered have a less robust presence in our consciousness than the program-resistant variety, and as Gunderson insists, the latter are the sort to which we are supposed to have incorrigible or infallible or privileged access. The former, on the other hand, are notoriously elusive; we are often deceived about our own beliefs; we often do not know what train of “subconscious” choices or decisions or inferences led to our recognition of a face or solution to a problem. So there is some plausibility in relegating these putative events, states, achievements, processes to the role of idealized fictions in an action-predicting, action-explaining calculus, but this plausibility is notably absent when we try the same trick with pains or after-_resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett.

That is one reason why Gunderson is unsatisfied. He sees me handling the easy cases and thinks that I think they are the hard cases. It is embarrassing to me that I have given Gunderson and others that impression, for far from thinking that intentional ascriptions such as belief, desire, and decision are the stumbling blocks of physicalism, I think they are the building blocks. I agree with Gunderson that it is a long way from ascribing belief to a system to ascribing pain to a person (especially to _myself_), but I think that describing a system that exhibits the program-receptive features is the first step in accounting for the program-resistant features. As Gunderson says, the big problem resides in the investigational asymmetries he describes, and more particularly, in the ineliminable sense of _intimacy_ we feel with the program-resistant side of our mentality. To build a self, a first-person, with a privileged relation to some set of mental features, out of the third-person stuff of intentional systems is the hard part, and that is where _awareness1_, the notion Arbib finds of dubious utility, is supposed to play its role. Content is only half the battle; consciousness is the other.

## II

In _Content and Consciousness_ I proposed to replace the ordinary word “aware” with a pair of technical terms, defined as follows:

- (1) _A_ is aware1 that _p_ at time _t_ if and only if _p_ is the content of the input state of _A_’s “speech center” at time _t._- (2) _A_ is aware2 that _p_ at time _t_ if and only if _p_ is the content of an internal event in _A_ at time _t_ that is effective in directing current behavior.[^6]

The point of my _aware1-aware2_ distinction*]] was to drive a wedge between two sorts of allusions found in our everyday ascriptions of awareness or consciousness: allusions to _privileged access_ and to _control._ What I want to establish is that these two notions _wrongly coalesce_ in our intuitive grasp of what it is to be conscious of something. Many disagree with me, and Arbib is, I think, one of them, for he offers a new definition of his own, of an “awareness1.5,” that is supposed to split the difference and capture what is important in _both_ my terms, and perhaps capture some other important features of consciousness as well. But what I will argue is that Arbib has gravitated to the emphasis on control _at the expense of_ the emphasis on privileged access, and that the result is that his new notion offers some refinements to my crude definition of “aware2” but does not capture at all what I hoped to capture with “aware1.” First, to the refinements of “aware2.” Arbib points out that since a behavioral control system can tap sources of information or subprograms that find no actual exploitation in current behavior control but are only “potentially effective,” and since from such a multiplicity of sources, or “redundancy of potential command,” a higher-order choosing or decision element must pick or focus on one of these, it would be fruitful to highlight such target items as the objects of awareness for such a control system. So Arbib offers the following definition (which in its tolerance for hand-waving is a match for my definitions—he and I are playing the same game): “_A_ is aware1.5 that _p_ at time _t_ if and only if _p is_ a projection of the content of the mental state of _A_ which expresses the concentration of _A_’_s_ attention at time _t_.” I think this captures the connotations of control in our concept of awareness quite satisfactorily—better than my definition of “aware2.” If we want to _attract the attention_ of a dog so he will be _aware_ of our commands, or if we hope to _distract the attention_ of a chess-playing computer from a trap we hope to spring (before it becomes _aware_ of what we are doing), this definition does at least rough justice to those features of the situation we are trying to manipulate.

Let us suppose, as Arbib claims, that this notion of awareness1.5 can be interesting and useful in the analysis of complex natural and artificial behavioral control systems. Nevertheless, no matter how fancy such a control system becomes, if this is the only sort of awareness it has, it will never succeed in acquiring a soul. As Nagel would put it, _there will not be something it is like to be_ that control system.[^7] This is surprising, perhaps, for complex control systems seem in the first blush of their intentionality to exhibit all the traditional marks of consciousness. They exhibit a form of _subjectivity_, for we distinguish the objective environment of the system from how the environment _seems_ or _appears_ to the system. Moreover, their sensory input divides into the _objects of attention_ on the one hand, and the part temporarily ignored or relegated to the background on the other. They even may be seen to exhibit signs of _self-consciousness_ in having some subsystems that are the objects of scrutiny and criticism of other, overriding subsystems. Yet while they can be honored with some mental epithets, they are not yet persons or selves. Somehow these systems are all outside and no inside, or, as Gunderson says, “always at most a he or she or an it and never an I or a me to me.”

The reason is, I think, that _for purposes of control_, the program-receptive features of mentality suffice: belief, desire, recognition, analysis, decision and their associates can combine to control (nonverbal) activity of any sophistication. And since even for creatures who are genuine selves, _there is nothing it is like_ to believe that _p_, desire that _q_, and so forth, you can’t build a self, a something it is like something to be, out of the program-receptive features by themselves.

What I am saying is that belief does not have a phenomenology. Coming to believe that _p_ may be an event often or even typically accompanied by a rich phenomenology (of feelings of relief at the termination of doubt, glows of smugness, _frissons_ of sheer awe) but it has no phenomenology of its own, and the same holds for the other program-receptive features of mentality. It is just this, I suspect, that makes them program-receptive.

If we are to capture the program-resistant features in an artificial system, we must somehow give the system a phenomenology, an inner life. This will require giving the system something about which it is in a privileged position, something about which it is incorrigible, for whatever else one must be to have a phenomenology, one must be the ultimate authority with regard to its contents. On that point there is widespread agreement. Now I want to claim first that this incorrigibility, properly captured, is not just a necessary but a sufficient condition for having a phenomenology, and second, that my notion of awareness1 properly captures incorrigibility (see Chapter 9). This brings me to Arbib’s criticisms of the notion of awareness1, for they call for some clarifications and restatements on my part.

First Arbib points out, “the inadequacy of our verbal reports of our mental states to do justice to the richness that states must exhibit to play the role prescribed for them in system theory” (p. 583). At best, the utterances for which I claim a sort of infallibility express only a _partial sampling_ of one’s inner state at the time. The content of one’s reports does not exhaust the content of one’s inner states. I agree. Second, he points out that such a sample may well be unrepresentative. Again, I agree. Finally, he suggests that it is “a contingent fact that some reports are sufficiently reliable to delude some philosophers into believing that reports of mental states are infallible” (p. 584), and not only do I find a way of agreeing with this shrewd observation; I think it provides the way out of a great deal of traditional perplexity. We are confused about consciousness because of an almost irresistible urge to overestimate the extent of our incorrigibility. Our incorrigibility is real; we feel it in our bones, and being real it is, of course, undeniable, but when we come to characterize it, we generously endow ourselves with capacities for infallibility beyond anything we have, or could possibly have, and even the premonition that we could not possibly have such infallibility comforts rather than warns us, for it ensures us that we are, after all, mysterious and miraculous beings, beyond all explaining. Once we see just how little we are incorrigible about, we can accommodate the claim that this incorrigibility is the crux of our selfhood to the equally compelling claim that we are in the end just physical denizens of a physical universe.

Arbib observes that “it follows from any reasonable theory of the evolution of language that certain types of report will be highly reliable” (p. 584). Indeed, for event-types in a system to acquire the status of reports at all, they must be, in the main, reliable (see Chapter 1, page 15). The trick is not to confuse what we are, and must be, highly reliable about, with what we are incorrigible about. We are, and must be, highly reliable in our reports about what we believe, and desire, and intend, but we are not infallible. We must grant the existence of self-deception, whether it springs from some deep inner motivation, or is the result of rather mundane breakdowns in the channels between our behavior-controlling states and our verbal apparatus.*]] What Arbib suggests, quite plausibly, is that some philosophers have confused the (correct) intuition that we must be authoritative _in general_ in our reports of all our mental states, with the (false) intuition that we are incorrigible or infallible with regard to all our reports of our mental states. Infallibility, if it exists, must be a more modest endowment.

Let us consider how these highly reliable, but not incorrigible, reports of our inner, controlling states might issue from the states they report. Following Arbib, we can grant that one’s controlling state (one’s state of awareness1.5) at any time is immensely rich in functional capacities, and hence in content. Let us suppose that at some moment part of the content of Smith’s state of awareness1.5 is the belief, visually inculcated, that a man is approaching him. Since Smith is a well-evolved creature with verbal capacities, we can expect a further part of the content of this state of awareness1.5 to be a conditional command to report: “I see a man approaching,” or words to that effect. Using Putnam’s analogy, we can say that Smith’s state of awareness1.5 is rather like a Turing Machine state consisting of very many conditional instructions, one of which is the conditional instruction to print: “I see a man approaching.” Let us call Smith’s _whole_ state of awareness1.5 _state A._ Suppose Smith now says, “I see a man approaching.” His verbal report certainly does not do justice to state _A_, certainly represents a partial and perhaps unrepresentative sample of the content of state _A_, and moreover, can occur in situations when Smith is not in state _A_, for there will no doubt be many other states of awareness1.5 that include the conditional instruction to say, “I see a man approaching,” or, due to malfunction or faulty design, Smith’s verbal apparatus may execute that instruction spuriously, when Smith’s state of awareness1.5 would not normally or properly include it. But suppose we break down state _A_ into its component states, one for each instruction. Then being in state _A_ will _ipso facto_ involve being in state _B_, the state of being instructed to report: “I see a man approaching.” Now let us rename state _B_ the state of awareness1 that one sees a man approaching. _Abracadabra_, we have rendered Smith “infallible,” for while his report “I see a man approaching” is only a highly reliable indicator that he is in state _A_ (which would ensure that he would do the other things appropriate to believing a man is approaching, for instance), it is a foolproof indicator that he is in state _B._ This does not leave Smith being infallible about very much, but then we shouldn’t expect him to be—he’s only human, and infallibility about great matters is a Godlike, i.e., inconceivable, power.

Smith’s infallibility has been purchased, obviously, by a cheap trick: it is only by skewing the identity conditions of the state reported so that reportorial truth is guaranteed that we get reportorial infallibility. But the trick, while cheap, is not worthless, for states of awareness1 so defined have a role to play in the description of systems with verbal capacities: we must be able to distinguish the _command states_ of a system’s verbal apparatus, what the system “means to say” in a particular instance, so that subsequent failures in execution—the merely verbal slips—can have a standard against which to be corrected.

Smith has not reported that he sees a man approaching if he makes a verbal slip, or misuses a word, even if the end result is his utterance of the words: “I see a man approaching.” Smith has reported that he sees a man approaching only if he said what he meant to say: that is, only if his actual utterance as executed meets the standards set by his state of awareness1. But if that is what it is for Smith to _report_, and not merely _utter sounds_, then whenever Smith reports, his reports will be guaranteed expressions of his state of awareness1.

This does not mean that we can give independent characterizations of some of Smith’s utterances, namely his reports, that happen to be foolproof signs that Smith is in certain independently characterized internal states, namely his states of awareness1. That would be miraculous. But we wouldn’t want to do that in any case, for if we could tell, by examination, which of Smith’s utterances were his genuine, error-corrected reports, he would not have _privileged_ access, for we could be in a _perfect_ position to determine his states of awareness1. The relationship between internal physical states of Smith, and their external manifestations in utterance is just garden-variety causation, and so any normal linkages between them are subject to all the possibilities of error or malfunction any physical system is subject to. It is just that the concepts of a _report_ and of _awareness1_ are so defined that Smith has an infallible capacity to report his states of awareness1. But does this amount to anything at all of interest? Well, _if_ we happened to want to know what state of awareness1 Smith was in, _we could do no better_ than to wait on Smith’s report, and if we were unsure as to whether what we heard was a genuine report of Smith’s, again we could do no better than to rely on Smith’s word that it was, or on the voucher implicit in his refraining from taking back or correcting what he said. But would we ever want to know what anyone’s state of awareness1 was? If we wanted to know whether what Smith said was what he meant to say, we would. And we might be interested in that, for if Smith said what he meant to say, we have a highly reliable, though not infallible, indicator of Smith’s state of awareness1.5. Or, if we suspected that something was awry in Smith’s perceptual apparatus (because his account of what he saw did not match what was in front of his eyes), we would be interested in his states of awareness1, for if Smith said what he meant to say, then our response to his aberrant perceptual reports would be not, “That _can_’_t_ be what you are aware of, since there is no man approaching,” but, “_Since_ you are aware of a man approaching, when there is no man approaching, there must be something wrong with your eyes or your brain.”

Note that Smith’s access to his states of awareness1 is both privileged and non-inferential, unlike ours. When we want to know what state of awareness1 Smith is in, we must ask him, and then infer on the basis of what happens what the state is. Or we might someday be able to take Smith’s brain apart, and on the basis of our knowledge of its interconnections make a prediction of what Smith would say, were we to ask him, and what he would say, were we further to ask him if his report was sincere, etc., and on the basis of these predictions _infer_ that he was in a particular state of awareness1. But Smith doesn’t have to go through any of this. When we ask him what state of awareness1 he is in, he does not have to ask anyone or investigate anything in turn: he just answers. Being asked, he comes to mean to say something in answer, and whether what he means to say then is right or wrong (relative to what he _ought_ to mean to say, what he would say if his brain were in order, if he were aware1 of what he is aware1.5 of), _if_ he says it, he will thereby say what he is aware1 of.*]] By being a system capable of verbal activity, Smith enters the community of communicators. He, along with the others, can ask and answer questions, make reports, utter statements that are true or false. If we consider this group of persons and ask if there is some area of concern where Smith is the privileged authority, the answer is: in his reports of awareness1.†]] Other persons may make fallible, inferential statements about what Smith is aware1 of. Smith can do better.

I have said that the extent of our infallibility, as opposed to our high reliability, is more restricted than some philosophers have supposed. Our infallible, non-inferential access consists only in our inevitable authority about what we would mean to say at a particular moment, whether we say it or not. The picture I want to guard against is of our having some special, probing, evidence-gathering faculty that has _more_ access to our inner states (our states of awareness1.5 perhaps) than it chooses to express in its reports. Our coming to mean to say something is all the access we have, and while it is infallible access to what we mean to say, it is only highly reliable access to what state is currently controlling the rest of our activity and attitudes. Some philosophers have supposed otherwise. Gunderson, for example says,

> Consider _any_ intentional sentence of the form “I ____ that there are gophers in Minnesota” where ‘____’ is to be filled in by an intentional verb (‘believe’, ‘suppose’, ‘think’, etc., and contrast our way of knowing its truth (or falsity) with any non-first-person variant thereof. … That is, if I know that “I suppose that there are gophers in Minnesota” is true, the way in which I come to know it is radically different from the way I might come to know that “Dennett supposes that there are gophers in Minnesota” is true. (_my italics_)

The verb “suppose” has been nicely chosen; if it is taken in the sense of episodic thinking, what Gunderson at this very moment is supposing to himself, then Gunderson has special, non-inferential incorrigible access to what he supposes, but if it is taken as a synonym for “believe,” then Gunderson is in only a contingently better position than I am to say whether he supposes there are gophers in Minnesota, for he is more acquainted with his own behavior than I happen to be. It would be odd to suppose (in the sense of “judge”) that there are gophers in Minnesota without supposing (in the sense of “believe”) that there are gophers in Minnesota, but not impossible.[^8] That is, Gunderson’s episode of meaning to himself that there are gophers in Minnesota is something to which his access is perfect but it is itself only a highly reliable indicator of what Gunderson believes. Lacking any remarkable emotional stake in the proposition “There are gophers in Minnesota,” Gunderson can quite safely assume that his judgment is not a piece of self-deception, and that deep in his heart of hearts he really does believe that there are gophers in Minnesota, but that is to make a highly reliable inference.

There is more than one verb that straddles the line as “suppose” does. “Think” is another, and a most important one. If one supposes that it is our thinking that actually controls our behavior, then we must grant that we do our thinking subconsciously, beyond our direct access, for we have only fallible and indirect, though highly reliable, access to those states, events, processes that occur in our control systems. If one supposes on the other hand that one’s thinking is one’s “stream of consciousness,” the episodes to which we have privileged access, then we must grant that thinking is an activity restricted to language-users, and only circumstantially related to the processes that account for their self-control. The two notions of thinking can each lay claim to being ordinary. Arbib champions one, and Gunderson the other. When Arbib says of a verbal report that “the phrase is but a projection of the thought, not the thought itself. … Thus utterances like ‘I see a man approaching’ express mere aspects of the robot’s total state,” he seems to be identifying the total state of awareness1.5 with the robot’s thoughts, for he says “many different aspects of its current ‘thoughts’ could have been _elicited_ by different questions” (my italics). The current thoughts, it seems, coexist not _serially_ in a stream of consciousness, not as distinct episodes to which anyone, even the robot, has access in any sense, but in parallel, in the processes of control. I don’t think it is _wrong_ to think of thought in this way, and I also don’t think it is wrong to think of thought as that contentful stream to which I have privileged, non-inferential access. I even think that in the last analysis one is not thinking about _thought_ unless one is thinking of something with both these features.[^9] It is only wrong, I think, to think that this dual prescription can actually be filled by any possible entities, states, or events. In just the same way someone would be mistaken who thought there was some physical thing that was all at once the voice I can strain, lose, recognize, mimic, record, and enjoy.[^10]

There is, then, a sense in which I am saying there is no such thing as a thought. I am not denying that there are episodes whose content we are incorrigible about, and I am not denying that there are internal events that control our behavior and can, in that role, often be ascribed content. I am denying, however, that in providing an account or model of one of these aspects one has provided in any way for the other. And I am insisting that _thoughts_ and _pains_ and other program-resistant features of mentality would have to have both these aspects to satisfy their traditional roles. The pain in my toe, for instance, is surely not just a matter of my meaning to tell you about it, nor is it something I am only inferentially or indirectly aware of, that is disrupting or otherwise affecting the control of my behavior. Then, since I am denying that any entity could have the features of a pain or a thought, so much the worse for the ontological status of such things.

## Notes

*]] These predictions are not _directly_ predictions of _causal_ sequences, as he suggests, since what a system _is programmed to do_ when in a certain state, and what its being in the associated physical state _causes to happen_ can diverge if there is malfunction, but if our hardware is excellent we can safely predict causal sequences from the program.

*]] Cf. G. E. M. Anscombe, _Intention_ (2nd ed. 1963), p. 80: “But if Aristotle’s account [of the practical syllogism] were supposed to describe actual mental processes, it would in general be quite absurd. The interest of the account is that it describes an order which is there whenever actions are done with intentions.” See also Quine “On the Reasons for Indeterminacy of Translation,” _Journal of Philosophy_, LXVII (March 26, 1970).

*]] In subsequent writing about consciousness (see Part III of this volume) I have not exploited this distinction from _Content and Consciousness_, but I have not renounced it either, for the reasons given here. This half of the chapter might be read to more advantage with Part III.

*]] I did not emphasize this sufficiently in _Content and Consciousness_, though it is implicit in the discussions on p. 153 and p. 167.

*]] Smith is not, of course, infallible about _what he means_ by what he says when he says something meaning to say it, not even _that he means_ what he says.

†]] The _truth_ of the utterance as a _report_ of the occurrence of a state of awareness1 is guaranteed by the _success_ of expression (if the utterance _is_ a successful expression), since the _content_ of the state of awareness1 is at the same time its individuating characteristic (what makes it the particular state of awareness1 it is) and the standard against which success in utterance-execution is measured.

1.]] Their papers, “Consciousness: the Secondary Role of Language,” and “_Content and Consciousness_ and the Mind-Body Problem,” were published, along with an abstract of this essay, in _Journal of Philosophy_, LXIX, 18 (October 5, 1972): 579–604.

2.]] See E. A. Feigenbaum and F. Feldman, _Computers and Thought_ (New York, 1963), for this distinction. Less is made of it in the field today (1978) than ten years ago.

3.]] “Mental Events,” in L. Foster and J. W. Swanson, eds., _Experience and Theory_ (University of Massachusetts Press, 1970).

4.]] In his review of _Content and Consciousness_, _Journal of Philosophy_, LXIX (April 12, 1972).

5.]] K. Gunderson, _Mentality and Machines_ (Doubleday Anchor, 1971).

6.]] _Content and Consciousness_, pp. 118–19.

7.]] “What Is It like to be a Bat?” _Philosophical Review_, LXXXIII (1974): 435–445.

8.]] See John Vickers, “Judgment and Belief,” in K. Lambert, ed., _The Logical Way of Doing Things_ (New Haven: Yale, 1969).

9.]] Ryle makes this point in “A Puzzling Element in the Notion of Thinking” in Strawson, ed., _Studies in the Philosophy of Thought and Action_ (Oxford, 1968).

10.]] See _Content and Consciousness_, pp. 8–15.

Brain Writing and Mind Reading

What are we to make of the popular notion that our brains are somehow libraries of our thoughts and beliefs? Is it _in principle_ possible that brain scientists might one day know enough about the workings of our brains to be able to “crack the cerebral code” and read our minds? Philosophers have often rather uncritically conceded that it _is_ possible in principle, usually in the context of making some point about privacy or subjectivity.[^1] I read Anscombe to deny the possibility. In _Intention_[^2] she seems to be arguing that the _only_ information about a person that can be brought to bear in a determination of his beliefs or intentions is information about his past and future actions and experiences; a person’s beliefs and intentions are whatever they must be to render his behavioral biography coherent, and neurological data could not possibly shed light on this. This is often plausible. Suppose Jack Ruby had tried to defend himself in court by claiming he didn’t know (or believe) the gun was loaded. Given even the little we know about his biography, could we even make sense of a neurologist who claimed that he had scientific evidence to confirm Ruby’s disclaimer? But in other cases the view is implausible. Sometimes one’s biography seems completely compatible with two different ascriptions of belief, so that the Anscombean test of biographical coherence yields no answer. Sam the reputable art critic extols, buys, and promotes mediocre paintings by his son. Two different hypotheses are advanced: (a) Sam does not believe the paintings are any good, but out of loyalty and love he does this to help his son, or (b) Sam’s love for his son has blinded him to the faults of the paintings, and he actually believes they are good. Presumably if (a) were true Sam would deny it to his grave, so his future biography will look the same in either case, and his past history of big-heartedness, we can suppose, fits both hypotheses equally well. I think many of our intuitions support the view that Sam really and objectively has one belief and not the other, and it goes against the grain to accept the Anscombean position that in the absence of telltale behavioral biography there is simply nowhere else to look. Couldn’t the brain scientist (in principle) work out the details of Sam’s belief mechanisms, discover the _system_ the brain uses to store beliefs, and then, using correlations between brain states and Sam’s _manifest_ beliefs as his Rosetta Stone, extrapolate to Sam’s covert beliefs? Having deciphered the brain writing, he could read Sam’s mind. (Of course, if we could establish this practice for Sam the art critic, we would have to reopen the case of Jack Ruby, but perhaps, just perhaps, we could then devise a scenario in which neurologists were able to confirm that Ruby was the victim of a series of unlikely but explainable beliefs—as revealed by his “cerebroscope.”)

I admit to finding the brain-writing hypothesis tempting,*]] but suspect that it is not coherent at all. I have been so far unable to concoct a proof that it is incoherent, but will raise instead a series of difficulties that seem insuperable to me. First, though, it would be useful to ask just why the view is plausible at all. Why, for instance, is the brain-writing hypothesis more tempting than the hypothesis that on the lining of one’s stomach there is a decipherable record of all the meals one has ever eaten? Gilbert Harman offers the first few steps of an answer:

We know that people have beliefs and desires, that beliefs and desires influence action, that interaction with the environment can give rise to new beliefs, and that needs and drives can give rise to desires. Adequate psychological theories must reflect this knowledge and add to it. So adequate models must have states that correspond to beliefs, desires and thoughts such that these states function in the model as psychological states function in the person modeled, and such that they are representational in the way psychological states are representational. Where there is such representation, there is a system of representation; and that system may be identified with the inner language in which a person thinks.

This reduces the claim that there is an inner language, which one thinks in, to the trivial assertion that psychological states have a representational character.[^3]

The first point, then, is that human behavior has proven to be of such a nature that the only satisfactory theories will be those in which inner _representations_ play a role (though not necessarily a role that is not eliminable at another level of theory). Diehard peripheralist behaviorists may still wish to deny this, but that is of concern to historians of science, not us. It is Harman’s next point that strikes me as controversial: where there is representation there is system, and this system may be _identified_ with a person’s inner language. Are all representations bound up in systems? Is _any_ system of representations like a language? Enough like a language to make this identification more useful than misleading? Or is Harman’s claim rather that whatever sorts of representations there may be, the sorts we need for human psychology must be organized in a system, and this system must be more like the system of a language than not? Assuming Harman’s claim survives these questions, we still would not have an argument for the full-fledged brain-writing hypothesis. Two more steps are needed. First, we need the claim that these psychological models with their language-style representations must be realized in brainware, not ectoplasm or other ghostly stuff.*]] This ought to be uncontroversial; though psychologists may ignore the details of realization while elaborating and even testing their models, the model-making is ultimately bound by the restriction that any function proposed in a model must be physiologically or mechanically realizable one way or another. Second, it must be claimed that it will be possible to determine the details of such realizations from an empirical examination of the brainware and its causal role in behavior. This second point raises some interesting questions. Could the functional organization of the brain be so inscrutable from the point of view of the neurophysiologist or other physical scientist that no fixing of the representational role of any part were possible? Could the brain use a system that no outsider could detect? In such a case what would it mean to say the brain used a system? I am not sure how one would go about giving direct answers to these questions, but light can be shed on them, I think, by setting up a crude brain-writing theory and refining it as best we can to meet objections.

Again Harman gives us the first step:

> In a simple model, there might be two places in which representations are stored. Representations of things believed would be stored in one place; representations of things desired in the other. Interaction with the environment would produce new representations that would be stored as beliefs. Needs for food, love, etc., would produce representations to be stored as desires. Inferences could produce changes in both the set of beliefs and the set of desires. (_Ibid_., p. 34)

No doubt we would also want to distinguish more or less permanent storage (belief and desire) from the more fleeting or occurrent _display_ of representations (in perception, during problem solving, sudden thoughts, etc.). In any case we already have enough to set some conditions on the brain-writing hypothesis. Some formulations of it are forbidden us on pain of triviality. For instance, claiming that there is brain writing, but that each representation is written in a different language, is just an oblique way of asserting that there is no brain writing. I think the following six conditions will serve to distinguish genuine brain-writing hypotheses from masqueraders.

- (1) The system of representations must have a generative grammar. That is, the system must be such that if you understand the system and know the finite vocabulary you can generate the representations—the sentences of brain writing—you haven’t yet examined. Otherwise the language will be unlearnable.[^4] Only if there were a generative grammar could the investigator get himself into a position to extrapolate from manifest beliefs and desires to covert beliefs and desires. There need not be a single generative grammar covering all representations, however. Just so long as there is a finite number of different “languages” and “multi-lingual” functional elements to serve as interpreters, the learnability condition will be met.
- (2) Syntactical differences and similarities of the language must be reflected in physical differences and similarities in the brain. That is, the tokens of a syntactical type must be physically distinguishable by finite test from the tokens of other syntactical types. That does not mean that all tokens of a type must be physically similar. What physical feature is peculiar to spoken and written tokens of the word “cat”? There must simply be a finite number of physical sorts of token of each type. Tokens and “strings” of tokens may of course align themselves in physical dimensions other than those of natural language. For instance, lexical items might be individuated not by shape but by spatial location, and ordering in the strings might be accomplished not by a sequence in space or time but by degree of electric potential.
- (3) Tokens must be physically salient. This is a “practical” point. Tokens might bear physical similarities, but similarities so complex, so diffuse and multidimensional that no general _detection_ mechanism could be devised: no frequency filters, stereo-locators, litmus papers, or simple combination of these could be built into a token detector. If tokens turned out not to be physically salient—and this is rather plausible in the light of current research—the brain-writing hypothesis would fail for the relatively humdrum reason that brain writing was illegible. It is worth mentioning only to distinguish it from more important obstacles to the hypothesis.
- (4) The representation store must meet Anscombe’s condition of biographical coherence. The sentences yielded by our neurocryptographer’s radical translation must match well with the subject’s manifest beliefs and desires, and with common knowledge. If too many unlikely beliefs or obvious untruths appear in the belief store, we will decide that we have hit upon something strange and marvelous—like finding the Lord’s Prayer written in freckles on a man’s back—but not his belief store. To give a more plausible example, we might discover that certain features of brain activity could be interpreted as a code yielding detailed and accurate information about the relative tensions of the eye muscles, the orientation of the eyeball, the convexity of the lens, etc., and this might give us great insight into the way the brain controlled the perceptual process, but since a man does not ordinarily have any beliefs about these internal matters, this would not be, except indirectly, a key to his belief store.*]]
- (5) There must be a reader or playback mechanism. It must be demonstrated that the physical system in which the brain writing is accomplished is functionally connected in the appropriate ways to the causes of bodily action, and so forth. Of course, if we were to find the cortex written all over with sentences expressing the subject’s manifest beliefs, we would be convinced this was no coincidence, but until the operation of the mechanisms that utilized the writing was discovered, we would not have a theory. (A person who discovered such a marvel would be roughly in the same evidential position as a clairvoyant who, we can imagine, might be able to predict with uncanny accuracy what a person would say, etc., and yet could not be supposed to have any authority—in a court of law, for instance—about a person’s beliefs.)
- (6) The belief store must be—in the main—consistent. If our translation manual yields sentences like “My brother is an only child” and pairs of sentences like “All dogs are vicious” and “My dog is sweet-tempered” one of several things must be wrong. If the subject declines to assert or assent to these anomalous sentences, we will discredit the translation manual (cf. Quine on radical translation); if, on the other hand, the man issues forth these sentences, we will conclude that we have discovered a pathological condition, and our brain-writing system will be viewed as a sort of assent-inducing tumor.*]]

A more graphic way of looking at this point is to ask whether the neurocryptographer could do a bit of tinkering and thereby _insert_ a belief in his subject: if he can _read_ brain writing he ought to be able to _write_ brain writing. Let us suppose we are going to insert in Tom the false belief: “I have an older brother living in Cleveland.” Now, can the neurocryptographer translate this into brain writing and do a bit of rewiring? Let us suppose he can do any rewiring, as much and as delicate as you wish. This rewiring will either impair Tom’s basic rationality or not. Consider the two outcomes. Tom is sitting in a bar and a friend asks, “Do you have any brothers or sisters?” Tom says, “Yes, I have an older brother living in Cleveland.” “What’s his name?” Now what is going to happen? Tom may say, “Name? Whose name? Oh, my gosh, what was I saying? I don’t have an older brother!” Or he may say, “I don’t know his name,” and when pressed he will deny all knowledge of this brother, and assert things like, “I am an only child and have an older brother living in Cleveland.” In neither case has our neurocryptographer succeeded in wiring in a new belief. This does not show that wiring in beliefs is impossible, or that brain writing is impossible, but just that one could only wire in one belief by wiring in many (indefinitely many?) other cohering beliefs so that neither biographical nor logical coherence would be lost.*]] (I examine this case more fully in Chapter 12.)

Now suppose we have a brain-writing theory that meets all of these conditions: we have a storage facility functionally tied to behavior that is somehow administered to preserve logical and biographical coherence, and the mode of storage involves elements having physically salient syntactical parts for which we have a generative grammar. This system is going to take up some room. How much room do we need? Marvin Minsky has an optimistic answer: “One can’t find a hundred things that he knows a thousand things about. … I therefore feel that a machine will quite critically need to acquire on the order of a hundred thousand elements of knowledge in order to behave with reasonable sensibility in ordinary situations. A million, if properly organized, should be enough for a very great intelligence.”[^5] If Minsky’s estimate were realistic, the brain, with its ten billion neurons or trillions of molecules would be up to the task, no doubt. But surely his figure is much too low. For in addition to all the relatively _difficult_ facts I have mastered, such as that New York is larger than Boston and salt is sodium chloride, there are all the easy ones we tend to overlook: New York is not on the moon, or in Venezuela; salt is not sugar, or green, or oily; salt is good on potatoes, on eggs; tweed coats are not made of salt; a grain of salt is smaller than an elephant. … Surely I can think of more than a thousand things I know or believe about salt, and salt is not one of a hundred, but one of thousands upon thousands of things I can do this with. Then there is my knowledge of arithmetic: two plus two is four, twenty plus twenty is forty. … My beliefs are apparently infinite, which means their storage, however miniaturized, will take up more room than there is in the brain. The objection, of course, seems to point to its own solution: it must be that I _potentially_ believe indefinitely many things, but I _generate_ all but, say, Minsky’s hundred thousand by the activity of an extrapolator-deducer mechanism attached to the core library. So let us attach such a mechanism to our model and see what it looks like.

It has the capacity to extract axioms from the core when the situation demands it, and deduce further consequences. To do this, it needs to have an information store of its own, containing information about what items it would be appropriate at any time to retrieve from the core, and, for instance, the metalinguistic information it needs to analyze the contradiction in “all cats are black” and “my cat is brown.” Perhaps it does this by storing the information that what is black is not brown, or maybe _that_ information is in the core storage, and the metalinguistic information stored in the extrapolator-deducer mechanism is to the effect that the core element “what is black is not brown” is relevant to an analysis of the contradiction. Now how will the extrapolator-deducer mechanism store its information? In its own core library of brain-writing sentences? If it has a core library, it will also need an extrapolator-deducer mechanism to act as librarian, and what of _its_ information store? Recalling Lewis Carroll’s argument in “What the Tortoise Said to Achilles,”[^6] we can see that the extrapolator-deducer will be hamstrung by a vicious regress if it must always rely on linguistically stored beliefs which it must retrieve and analyze about what can be deduced from what. This a priori point has been “empirically discovered” in the field by more than one frustrated model-builder. As one team sums it up: “… a memory that merely stores _propositions_ leads to technological, or organic, monstrosities and frustrates, rather than facilitates inductive operations.”[^7]

The conclusion is that writing—for instance, brain writing—is a _dependent_ form of information storage. The brain must store at least some of its information in a manner not capturable by a brain-writing model. Could it do without brain writing altogether? I think we can get closer to an answer to this by further refining our basic model of belief.

Representations apparently play roles at many different levels in the operation of the brain. I have already mentioned the possibility of codes representing information about the tension of eye muscles and so forth, and these representations do not fall into the class of our beliefs. At another level there is the information we “use” to accomplish depth perception. Psychologists ascribe to us such activities as _analyzing depth cues_ and _arriving at conclusions_ about distance based on _information we have_ about texture gradients, binocular interaction, and so forth. Yet it is nothing conscious that I do in order to perceive depth, and if you ask me what beliefs I have about texture gradients I draw a blank. Closer to home, a child can demonstrate his understanding of addition by reeling off sums without being able to formulate or understand propositions about the commutativity of addition. His performance indicates that he has caught on to commutativity, but should we say that among his beliefs is the belief that addition is commutative? To give one more case, while driving down a familiar road I am suddenly struck by the thought that its aspect has changed—somebody has painted his shutters or a tree has blown down, or something. Do I have a _belief_ about how it used to be that grounds my current judgment that it has changed? If so, it is a belief to which I can give no expression and about which I am quite in the dark. Somehow, though, the information is there to be used.

Suppose we partition our information store into the part that is verbally retrievable and the part that is not. I would not want to claim that this separates our beliefs from everything else. Far from it. Our preanalytical notion of belief would permit young children and dumb animals to have beliefs, which must be verbally irretrievable. Perhaps, though, a strong case can be made out that at least our verbally retrievable beliefs are stored in brain writing. The picture that emerges is not, I think, implausible: there are on the one hand those representations that are available for our conscious, personal use and apprehension, and on the other hand, those that operate behind the scenes to keep us together. If any representations are stored in brain writing, the former will be, for they are in intimate relation to our natural languages. Included in this group will be ‘the bits of factual knowledge we pick up by asking questions and reading books, especially the facts that only language-users could apprehend, such as the fact that Thanksgiving is always on Thursday. With regard to this group of representations Minsky’s figure of a hundred thousand looks more realistic, provided we have an extrapolator-deducer mechanism.

If ever it seems that we are _storing sentences_, it is when we are picking up facts in this verbal manner, but are these things we pick up our beliefs? Sometimes we salt away a sentence because we like the sound of it, or because we will later be rewarded for producing it on demand, or just because it has a sort of staying power in our imagination. In Chekhov’s _Three Sisters_, Tchebutykin, reading a journal, mutters: “Balzac was married in Berditchev,” and then repeats it, saying he must make a note of it. Then Irina dreamily repeats it: “Balzac was married in Berditchev.” Did they acquire a belief on that occasion? Whether _they_ did or not, the sentence has stuck in _my_ mind, and yet I wouldn’t say it was one of my beliefs. I have deliberately not looked it up in the encyclopedia; probably it’s true—why would Chekhov insert a distracting falsehood—for mischief? No doubt if someone offered me a thousand dollars if I could tell him where Balzac was married, I’d say Berditchev (wherever that is), but it would be wrong for him to conclude that this was a belief of mine. (This distinction is developed further in Chapter 16.)

If brain writing served only for such storage of words and sentences that we pick up for various reasons, at least we could all breathe a lot easier about the prospects of evil scientists reading our every seditious thought and turning us over to the authorities. Imagine the Loyalty Commissar asking the neurocryptographer if the man in the cerebroscope is a true patriot. “Let’s see,” says the scientist, “Here is the sentence we’ve been looking for: ‘I pledge allegiance to the flag …’” Would finding the sentence “America’s the greatest land of all” satisfy the Commissar? I think not.

The matter of verbally retrievable beliefs is in any case more complicated than the picture we’ve just been examining. Whereas if I am asked who won the Super Bowl in 1969, it does seem a bit as if I am searching for a ready-made sentence to utter in response, in other cases this sort of account does not ring true at all. Suppose I am watching the shell game, intent on which shell the little pea is under. At any moment it seems to be true that I have a belief about where the pea is, and can tell you if you ask, but it does not seem plausible that this is accomplished by a rapid writing and erasing of successive sentences: “Now it’s left, now it’s center, now right,” and the flashing on and off of the negation sign in front of, “It’s under the center shell.” For one thing, if asked to give you my perceptual beliefs of a moment I may have to work a bit to formulate them, yet the perceptual representation was what it was before I was asked. The representationality—or intentionality—of something (e.g., a belief or perception) is compatible with its being vague or indeterminate in some respects.[^8] The effort of retrieval is often an effort to formulate a sentence that is an approximation of a belief, and we are often distressed by the hard edge of determinacy our verbal output substitutes for the fuzziness of our convictions.

The answer we formulate, the judgment we find an expression for when asked for our belief, is determinate and individuated, because it consists of a specific string of words in our natural language, whether we then speak it aloud or not. _These_ representations, not the beliefs to which we have verbal access, but the occurrent, datable judgments themselves, have the syntactic parts we have been looking for, and about these the brain-writing hypothesis looks much more workable. Not only are judgments determinate; they are, as Harman has pointed out, lexically and syntactically unambiguous.[^9] If it occurs to me that our mothers bore us, I know for sure whether I am thinking of birth or ennui. So it is proper to view a judgment not as a sentence _simpliciter_ but as a deep structure or sentence under an analysis. Judgments, unlike beliefs, occur one at a time; we have at any moment indefinitely many beliefs, but can be thinking just one thought. We saw that the brain-writing hypothesis with regard to storage of beliefs did not really effect any economies of design, because however systematic and efficient one’s grammar is, one still needs infinite space to store infinitely many tokens, but with regard to representation of judgments the situation is different. A finite mechanism incorporating a generative grammar would be an efficient means of representing, one at a time, any of an infinite set of propositions.

The interesting thing about judgments is that although each of us is authoritative about the _content_ of his judgments, and although each of us is authoritative about his sincerity or lack of sincerity in giving outward verbal expression of a judgment, we are not in a privileged position when it comes to the question of whether our judgments are reliable indicators of our beliefs.[^10] Normally there is harmony between our judgments and our behavior, and hence between our judgments and our beliefs, but when we are afflicted by Sartre’s _mauvaise foi_, our sincerest judgments can be lies about our beliefs. I may judge to myself that a man is innocent, while believing him guilty. (This point is explored further in Chapter 16.)

This suggests that even if we were to discover a brain-writing system that represented our judgments, the mind reading that could be accomplished by exploiting the discovery would not uncover our beliefs. To return to the case of Sam the art critic, if our neurocryptographer were able to determine that Sam’s last _judgment_ on his deathbed was, “My consolation is that I fathered a great artist,” we could still hold that the issue between the warring hypotheses was undecided, for this judgment may have been a self-deception. But at this point I think we are entitled to question the intuition that inspired the search for brain writing in the first place. If discovering a man’s judgments still leaves the matter of belief ascription undecided, and if in fact either ascription of belief accounts for, explains, predicts Sam’s behavior as well as the other, are we so sure that Sam determinately had one belief or the other? Are we sure there is a difference between his really and truly believing his son is a good artist, and his deceiving himself out of love while knowing the truth in his heart of hearts? If there were brain writing, of course, there would have to be a physical difference between these two cases, but now, what reasons do we have for supposing there is brain writing?

We are thrown back on our conviction that the brain must be an organ that _represents_, but I hope it is no longer obvious that the brain must represent in sentences. In fact we know that at least some of the representation must be accomplished in some more fundamental way. Must there be a _system_ for such representation? I cannot yet see that there must. In particular, I cannot yet see that there must be a _learnable_ system, in Davidson’s sense, for it is not clear to me that the brain must—or can—learn (the way a child learns a language) its own ways of representing. Certainly information can be transmitted by means of unlearnable languages. Consider a string of nine light bulbs in a row; there are 512 different patterns of lit and unlit bulbs possible in this array, and so we can use the array to transmit the natural numbers from 0 to 511. There are all sorts of systems one might use to assign patterns to numbers. An obvious and efficient one would be binary notation: 000000001 is 1, 000000010 is 2, and so forth. Once a person knows the system, he can generate the numbers he has not yet observed, but suppose instead of using this or any other system, the patterns are assigned numbers randomly; a random assignment will carry the information just as well; the outsider will simply not be in a position to predict the balance of the assignments, having learned some of them. Can the brain “use” information carried by unlearnable systems? At some levels of “coding” it obviously does—where the “codes” carry very specific and limited information. In general can the brain get along without learnable representation systems? Until we can say a lot more about what it is for a system to use representations, I for one cannot see how to answer this question. If the answer is no, then there must be brain writing, but how it overcomes the difficulties I have raised here is beyond me. If the answer is yes, then the only way translation of representation can be accomplished is “sentence by sentence,” assigning meaning to representations by determining their functional role in the behavior of the whole system. But where competing translations are behaviorally indistinguishable, the content of the representation will be indeterminate.

## Postscript, 1978

In the seven years since this essay was written many books and articles have appeared dealing with the nature of mental representation. Compared with the models discussed today, the model described here is naive, but the conclusions I draw from it can still be drawn from today’s sophisticated models, though less obviously. It has become clearer in recent years that strictly “propositional” data-structures lack the powers required to serve as models of the fundamental “mental representations,” but few uncontroversial assertions can be made about the structure or function of non-propositional or quasi-propositional representations. The trade-off between storing information in “compiled” or “propositional” or “coded” form, and storing it “tacitly” in the organization of the representational system, has been fruitfully explored by people in artificial intelligence and cognitive psychology. Discussions of these developments are found in Chapters 6, 7, and 9.

## Notes

*]] I claimed it was a distinct possibility with regard to intentions in “Features of Intentional Actions,” _Philosophy and Phenomenological Research,_ XXIX (1968): 232–244.

*]] Cf. Wilfrid Sellars, “Notes on Intentionality,” _Journal of Philosophy_, LXI (1964): 663, where he discusses mental acts as tokens expressing propositions, and claims that all tokens must be _sorts_ of tokens and must have a determinate factual character, and proposes identifying them with neurophysiological episodes.

*]] Discovering such a code is not establishing that the information the code carries for the scientist is also carried for the person, or even for his brain. D. H. Perkel and T. H. Bullock, in “Neural Coding” in F. Schmitt, T. Melnechuk, et al., eds., _Neurosciences Research Symposium Summaries,_ vol. 3 (Cambridge, Mass.: MIT Press, 1969), discuss the discovery of a code “carrying” phasic information about wing position in the locust; it is accurately coded, but the “insect apparently makes no use of this information.” (Blocking this input and substituting random input produces no loss of flying rhythm, ability, etc.)

*]] This condition of rationality has some slack in it. We do permit some small level of inconsistency, but large-scale illogicality must be indicative of either a defect in the subject so serious as to disqualify him as a believer at all, or a defect in our translation hypotheses. See Chapter 1.

*]] Joan Straumanis has pointed out to me that there is some experimental evidence that suggests that another outcome of the rewiring experiment could be that Tom spontaneously and unconsciously fabricates a web of cohering beliefs to “protect” the inserted belief and his others from each other (a sort of pearl-in-the-oyster effect).

1.]] See, in another context, A. I. Melden’s use of the notion in _Free Action_ (New York: Humanities, 1961): pp. 211–15.

2.]] G. E. M. Anscombe, _Intention_ (2nd ed.; Oxford: Blackwell, 1963).

3.]] G. Harman, “Language Learning,” _Nous_, IV (1970): 35. See also his “Three Levels of Meaning,” _Journal of Philosophy_, LXV (1968): 590–602, especially p. 598.

4.]] See Donald Davidson, “Theories of Meaning and Learnable Languages,” in Y. Bar-Hillel, ed., _Logic_, _Methodology, and Philosophy of Science_ (Amsterdam: North-Holland, 1965): 383–94.

5.]] M. L. Minsky, ed., _Semantic Information Processing_ (Cambridge: MIT Press, 1968): 26.

6.]] _Mind_ (1895), reprinted in I. M. Copi and J. A. Gould, eds., _Readings on Logic_ (New York: Macmillan, 1964). Harman offers a similar argument in “Psychological Aspects of the Theory of Syntax,” _Journal of Philosophy_, XLIV (1967): 75–87.

7.]] H. von Foerster, A Inselberg, and P. Weston, “Memory and Inductive Inference,” in H. L. Oestreicher and D. R. Moore, eds., _Cybernetic Problems in Bionics_ (New York: Gordon and Breach, 1968).

8.]] See G. E. M. Anscombe, “The Intentionality of Sensation: A Grammatical Feature,” in R. J. Butler, ed., _Analytical Philosophy_, _Second Series_ (Oxford: Blackwell, 1965).

9.]] G. Harman, “Language Learning.”

10.]] See John Vickers, “Judgment and Belief,” in K. Lambert, ed., _The Logical Way of Doing Things_ (New Haven: Yale University Press, 1969), and A. W. Collins, “Unconscious Belief,” _Journal of Philosophy_, LXVI (1969): 667–80.The Nature of Theory in Psychology

Skinner Skinned

B. F. Skinner has recently retired, after a long and distinguished career at Harvard, and for better or for worse it appears that the school of psychology he founded, Skinnerian behaviorism, is simultaneously retiring from the academic limelight. Skinner’s army of enemies would like to believe, no doubt, that his doctrines are succumbing at last to their barrage of criticism and invective, but of course science doesn’t behave like that, and the reasons for the decline in influence of behaviorism are at best only indirectly tied to the many attempts at its “refutation.” We could soften the blow for Skinner, perhaps, by putting the unwelcome message in terms he favors: psychologists just don’t find behaviorism very _reinforcing_ these days. Skinner might think this was unfair, but if he demanded _reasons_, if he asked his critics to _justify_ their refusal to follow his lead, he would have to violate his own doctrines and methods. Those of us who are not Skinnerians, on the other hand, can without inconsistency plumb the inner thought processes, reasons, motives, decisions and beliefs of both Skinner and his critics, and try to extract from them an analysis of what is wrong with Skinnerian behaviorism and why.

This is not an easy task, in large measure because of a spiraling escalation of vituperation between Skinner and his critics. Skinner began as a naive and achingly philistine social thinker, so the first rounds of _humanist_ criticism of his position were contemptuous, and largely conducted in ignorance of Skinner’s technical work or the background of theories against which it was developed. Skinner, recognizing this, did not conceal his contempt in turn for his arrogant and ignorant humanist opponents, and so it has continued, with both sides willfully misreading and misattributing, secure in the knowledge that the other side is vastly underestimating its opponent. Somewhat surprisingly, Skinner’s scientific critics have often fallen into similarly unedifying ruts.

Although counting myself among Skinner’s opponents, I want to try to avoid the familiar brawl and do something diagnostic. I want to show _how_ Skinner goes astray, through a series of all too common slight errors. He misapplies some perfectly good principles (principles, by the way, that his critics have often failed to recognize); he misdescribes crucial distinctions by lumping them all together; and he lets wishful thinking cloud his vision—a familiar enough failure. In particular, I want to show the falsehood of what I take to be Skinner’s central philosophical claim, on which all the others rest, and which he apparently derives from his vision of psychology. The claim is that _behavioral science proves that people are not free, dignified, morally responsible agents_. It is this claim that secures what few links there are between Skinner’s science and his politics. I want to show how Skinner arrives at this mistaken claim, and show how tempting in fact the path is. I would like to proceed by setting out with as much care as I can the steps of Skinner’s argument for the claim, but that is impossible, since Skinner does not present arguments—at least, not wittingly. He has an ill-concealed disdain for arguments, a bias he feeds by supposing that brute facts will sweep away the most sophisticated arguments, and that the brute facts are on his side. His impatience with arguments does not, of course, prevent him from relying on arguments, it just prevents him from seeing that he is doing this—and it prevents him from seeing that his brute facts of behavior are not facts at all, but depend on an interpretation of the data which in turn depends on an argument, which, finally, is fallacious. To get this phantom—but utterly central—argument out in the open will take a bit of reconstruction.

The first step in Skinner’s argument is to characterize his enemy, “mentalism.” He has a strong gut intuition that the _traditional_ way of talking about and explaining human behavior—in “mentalistic” terms of a person’s beliefs, desires, ideas, hopes, fears, feelings, emotions—is somehow utterly disqualified. This way of talking, he believes, is disqualified in the sense that not only is it not science as it stands; it could not be turned into science or used in science; it is inimical to science, would _have_ to be in conflict with _any_ genuine science of human behavior. Now the first thing one must come to understand is this antipathy of Skinner’s for all things “mentalistic.” Once one understands the antipathy, it is easy enough to see the boundaries of Skinner’s enemy territory.

Skinner gives so many different reasons for disqualifying mentalism that we may be sure he has failed to hit the nail on the head—but he does get close to an important truth, and we can help him to get closer. Being a frugal Yankee, Skinner is reluctant to part with _any_ reason, however unconvincing, for being against mentalism, but he does disassociate himself from some of the traditional arguments of behaviorists and other anti-mentalists at least to the extent of calling them relatively unimportant. For instance, perhaps the most ancient and familiar worry about mentalism is the suspicion that

(1) mental things must be made of _non-physical_ stuff

thus raising the familiar and apparently fatal problems of Cartesian interactionism. Skinner presents this worry,[^1] only to downplay it,[^2] but when all else fails, he is happy to lean on it.[^3] More explicitly, Skinner rejects the common behaviorist claim that it is

(2) the _privacy_ of the mental

in contrast to the public objectivity of the data of behavior that makes the mental so abhorrent to science. “It would be foolish to deny the existence of that private world, but it is also foolish to assert that because it is private it is of a different nature from the world outside.”[^4] This concession to privacy is not all that it appears, however, for his concept of privacy is not the usual one encountered in the literature. Skinner does not even consider the possibility that one’s mental life might be _in principle_ private, _non-contingently_ inaccessible. That is, he supposes without argument that the only sort of privacy envisaged is the sort that could someday be dispelled by poking around in the brain, and since “the skin is not that important as a boundary,”[^5] what it hides is nothing science will not be able to handle when the time comes. So Skinner suggests he will _not_ object to the privacy of mental events, since their privacy would be no obstacle to science. At the same time Skinner often seeks to discredit explanations that appeal to some inner thing “we cannot see,” which seems a contradiction.[^6] For if we read these as objections to what we cannot _in principle_ see, to what is necessarily unobservable, then he must after all be appealing tacitly to a form of the privacy objection. But perhaps we should read these disparagements of appeals to what we cannot see merely as disparagements of appeals to what we cannot _now_ see, but whose existence we are _inferring._ Skinner often inveighs against appealing to

(3) events whose occurrence “can only be inferred.”[^7]

Chomsky takes this to be Skinner’s prime objection against mentalistic psychology,[^8] but Skinner elsewhere is happy to note that “Science often talks about things it cannot see or measure”[^9] so it cannot be that simple. It is not that all inferred entities or events are taboo, for Skinner himself on occasion explicitly infers the existence of such events; it must be a particular sort of inferred events. In particular,

(4) _internal_ events

are decried, for they “have the effect of diverting attention from the external environment.”[^10] But if “the skin is not that important as a boundary,” what can be wrong with internal events as such? No doubt Skinner finds _some_ cause for suspicion in the mere internality of some processes; nothing else could explain his persistent ostrich-attitude toward physiological psychology.[^11] But in his better moments he sees that there is nothing intrinsically wrong with inferring the existence of internal mediating events and processes—after all, he admits that someday physiology will describe the inner mechanisms that account for the relations between stimuli and responses, and he could hardly deny that in the meantime such inferences may illuminate the physiological investigations.[^12] It must be only when the internal mediators are of a certain sort that they are anathema. But what sort? Why, the “occult,” “prescientific,” “fictional” sort, the _“mental_ way station” sort,[^13] but these characterizations beg the question. So the first four reasons Skinner cites are all inconclusive or contradicted by Skinner himself. If there is something wrong with mentalistic talk, it is not necessarily because mentalism is dualism, that mentalism posits nonphysical things, and it is not _just_ that it involves internal, inferred, unobservable things, for he says or implies that there is nothing wrong with these features by themselves. If we are to go any further in characterizing Skinner’s enemy we must read between the lines.[^14]

In several places Skinner hints that what is bothering him is the _ease_ with which mentalistic explanations can be concocted.[^15] One _invents_ whatever mental events one needs to “explain” the behavior in question. One falls back on the “miracle-working mind,” which, just because it _is_ miraculous, “explains nothing at all.”[^16] Now this is an ancient and honorable objection vividly characterized by Molière as the _virtus dormitiva_. The learned “doctor” in _Le Malade Imaginaire_, on being asked to explain what it was in the opium that put people to sleep, cites its _virtus dormitiva_ or sleep-producing power. Leibniz similarly lampooned those who forged

> expressly occult qualities or faculties which they imagined to be like little demons or goblins capable of producing unceremoniously that which is demanded, just as if watches marked the hours by a certain horodeictic faculty without having need of wheels, or as if mills crushed grains by a fractive faculty without needing any thing resembling millstones.[^17]

By seeming to offer an explanation, Skinner says, inventions of this sort “bring curiosity to an end.” Now there can be no doubt that convicting a theory of relying on a _virtus dormitiva_ is fatal to that theory, but getting the conviction is not always a simple matter—it often has been, though, in Twentieth Century psychology, and this may make Skinner complacent. Theories abounded in the early days of behaviorism which posited curiosity drives, the reduction of which explained why rats in mazes were curious; untapped reservoirs of aggressiveness to explain why animals were aggressive; and invisible, internal punishments and rewards that were postulated solely to account for the fact that unpunished, unrewarded animals sometimes refrained from or persisted in forms of behavior. But mentalistic explanations do not _seem_ to cite a _virtus dormitiva._ For instance, explaining Tom’s presence on the uptown bus by citing his desire to go to Macy’s and his belief that Macy’s is uptown does not look like citing a _virtus dormitiva:_ it is not as empty and question-begging as citing a special uptown-bus-affinity in him would be. Yet I think it is clear that Skinner does think that all mentalistic explanation is infected with the _virtus dormitiva_.[^18] This is interesting, for it means that _mentalistic_ explanations are on a par for Skinner with a lot of bad _behavioristic_ theorizing, but since he offers no discernible defense of this claim, and since I think the claim is ultimately indefensible (as I hope to make clear shortly), I think we must look elsewhere for Skinner’s best reason for being against mentalism.

There is a special case of the _virtus dormitiva,_ in fact alluded to in the Leibniz passage I quoted, which is the key to Skinner’s objection: sometimes the thing the desperate theoretician postulates takes the form of a little man in the machine, a _homunculus_, a demon or goblin as Leibniz says. Skinner often alludes to this fellow. “The function of the inner man is to provide an explanation which will not be explained in turn.”[^19] In fact, Skinner identifies this little man with the notion of an autonomous, free and dignified moral agent: he says we must abolish “the autonomous man—the inner man, the homunculus, the possessing demon, the man defended by the literature of freedom and dignity.”[^20] This is a typical case of Skinner’s exasperating habit of running together into a single undifferentiated lump a number of distinct factors that are related. Here the concept of a moral agent is identified with the concept of a little man in the brain, which in turn is identified with the demons of yore. Skinner, then, sees superstition and demonology every time a claim is made on behalf of moral responsibility, and every time a theory seems to be utilizing a homunculus. It all looks the same to him: bad. Moreover, he lumps _this_ pernicious bit of superstition (the moral-autonomous-homunculus-goblin) with all the lesser suspicions we have been examining; it turns out that “mental” means “internal” means “inferred” means “unobservable” means “private” means _“virtus dormitiva”_ means “demons” means “superstition.” Psychologists who study physiology (and hence look at _internal_ things), or talk of _inferred_ drives, or use mentalistic terms like “belief” are all a sorry lot for Skinner, scarcely distinguishable from folk who believe in witches, or, perish the thought, in the freedom and dignity of man. Skinner brands them all with what we might call guilt by free association. For instance, in _Beyond Freedom and Dignity,_ after all Skinner’s claims to disassociate himself from the lesser objections to mentalism, on p. 200 he lets all the sheep back into the fold:

> Science does not dehumanize man; it de-homunculizes him … Only by _dispossessing_ him can we turn to the _real_ causes of human behavior. Only then can we turn from the _inferred_ to the observed, from the miraculous to the natural, from the _inaccessible_ to the manipulable. (_my italics_)[^21]

But I was saying that hidden in this pile of dubious and inconsequential objections to mentalism is something important and true. What is it? It is that Skinner sees—or almost sees—that there is a special way that questions can be begged in psychology, and this way is _akin to_ introducing a homunculus. Since psychology’s task is to account for the intelligence or rationality of men and animals, it cannot fulfill its task if anywhere along the line it _presupposes_ intelligence or rationality. Now introducing a homunculus does just that, as Skinner recognizes explicitly in “Behaviorism at Fifty”:

> … the little man … was recently the hero of a television program called “Gateways to the Mind” … The viewer learned, from animated cartoons, that when a man’s finger is pricked, electrical impulses resembling flashes of lightning run up the afferent nerves and appear on a television screen in the brain. The little man wakes up, sees the flashing screen, reaches out, and pulls the lever … More flashes of lightning go down the nerves to the muscles, which then contract, as the finger is pulled away from the threatening stimulus. _The behavior of the homunculus was, of course, not explained_. An explanation would presumably require another film. And it, in turn, another. (_my italics_)[^22]

This “explanation” of our ability to respond to pin-pricks depends on the intelligence or rationality of the little man looking at the TV screen in the brain—and what does _his_ intelligence depend on? Skinner sees clearly that introducing an unanalyzed homunculus is a dead end for psychology, and what he sees dimly is that a homunculus is hidden in effect in your explanation _whenever you use a certain vocabulary_, just because the use of that vocabulary, like the explicit introduction of a homunculus, presupposes intelligence or rationality. For instance, if I say that Tom is taking the uptown bus because he _wants_ to go to Macy’s and _believes_ Macy’s is uptown, my explanation of Tom’s action _presupposes_ Tom’s intelligence, because if Tom weren’t intelligent enough to put two and two together, as we say, he might fail to see that taking the uptown bus was a way of getting to Macy’s. My explanation has a suppressed further premise: expanded it should read: Tom believes Macy’s is uptown, and Tom wants to go to Macy’s, so _since Tom is rational_ Tom wants to go uptown, etc. Since I am relying on Tom’s rationality to give me an explanation, it can hardly be an explanation of what makes Tom rational, even in part.

Whenever an explanation invokes the terms “want,” “believe,” “perceive,” “think,” “fear”—in short the “mentalistic” terms Skinner abhors—it must presuppose in some measure and fashion the rationality or intelligence of the entity being described.[^23] My favorite example of this is the chess-playing computer. There are now computer programs that can play a respectable game of chess. If you want to predict or explain the moves the computer makes you can do it mechanistically (either by talking about the opening and closing of logic gates, etc., or at a more fundamental physical level by talking about the effects of the electrical energy moving through the computer) or you can say, “If the computer _wants_ to capture my bishop and _believes_ I wouldn’t trade my queen for his knight, then the computer will move his pawn forward one space,” or something like that. We need not take seriously the claim that the computer _really_ has beliefs and desires in order to use this way of reasoning. Such reasoning about the computer’s “reasoning” may in fact enable you to predict the computer’s behavior quite well (if the computer is well-programmed), and in a sense such reasoning can even explain the computer’s behavior—we might say: “Oh, now I understand why the computer didn’t move its rook.”—but in another sense it doesn’t explain the computer’s behavior at all. What is awesome and baffling about a chess-playing computer is how a mere mechanical thing could be made to be so “smart.” Suppose you were to ask the designer, “How did the computer ‘figure out’ that it should move its knight?” and he replied: “Simple; it recognized that its opponent couldn’t counterattack without losing a rook.” This would be highly unsatisfactory to us, for the question is, how was he able to make a computer that _recognized_ anything in the first place? So long as our explanation still has “mentalistic” words like “recognize” and “figure out” and “want” and “believe” in it, it will presuppose the very set of capacities—whatever the capacities are that go to make up intelligence—it ought to be accounting for. And notice: this defect in the explanation need have nothing to do with postulating any non-physical, inner, private, inferred, unobservable events or processes, because it need not postulate any processes or events at all. The computer designer may know exactly what events are or are not going on inside the computer, or for that matter on its highly visible output device: in choosing to answer by talking of the computer’s _reasons_ for making the move it did, he is not asserting that there are any extra, strange, hidden processes going on; he is simply explaining the _rationale_ of the program without telling us how it’s done. Skinner comes very close to seeing this. He says:

> Nor can we escape … by breaking the little man into pieces and dealing with his wishes, cognitions, motives, and so on, bit by bit. The objection is not that those things are mental but that they offer no real explanation and stand in the way of a more effective analysis.[^24]

The upshot of this long and winding path through Skinner’s various objections to mentalism is this: if we ignore the inconsistencies, clear away the red herrings, focus some of Skinner’s vaguer comments, and put a few words in his mouth, he comes up identifying the enemy as a certain class of terms—the “mentalistic” terms in his jargon—which when used in psychological theories “offer no real explanation” because using them is something like supposing there is a little man in the brain. Skinner never says the use of these terms presupposes rationality, but it does. Skinner also never gives us an exhaustive list of the mentalistic terms, or a definition of the class, but once again we can help him out. These terms, the use of which presupposes the rationality of the entity under investigation, are what philosophers call the _intentional idioms_.[^25] They can be distinguished from other terms by several peculiarities of their logic, which is a more manageable way of distinguishing them than Skinner’s.[^26] Thus, spruced up, Skinner’s position becomes the following: _don’t use intentional idioms in psychology_.

Spruced-up Skinner is not alone in being opposed to intentional idioms in psychology. His Harvard colleague, Quine, has been most explicit on the topic.[^27] One might suppose their congruence on this issue came out of discussion or collaboration, but Skinner is so apparently oblivious of Quine’s arguments against intentional psychology, and so diffuse in his own objections to “mentalism” that this is most unlikely. For Quine’s objections to intentional idioms have never had anything to do with their presupposing rationality or offering no explanation; rather he has argued that intentional idioms are to be foresworn because, as Chisholm argues, we cannot translate sentences containing intentional idioms into sentences lacking them.[^28] Sentences containing intentional idioms refuse to “reduce” to the sentences of the physical sciences, so we must learn to do without them; Skinner on the other hand is blithely confident that such translations are possible,[^29] and indeed _Beyond Freedom and Dignity_ consists in large measure of samples of Skinner’s translations.[^30]

If Skinner never avails himself of the Chisholm-Quine untranslatability argument, and never makes explicit the presupposition of rationality argument, he does nevertheless muddy the water with a few other inconclusive objections. Intentional explanations tend to be “unfinished,” he says, in that an action is explained, for instance, by reference to an opinion, without the existence of the opinion being explained in turn. But explaining an explosion by citing a spark is similarly incomplete, and since Skinner admits that both the former and the latter explanation could be completed, this is hardly a telling objection.[^31] He also suggests that intentional explanations are not predictive, which is manifestly false. (See Chapters 1 and 15 of this volume.) Knowing that Tom wants to go to Macy’s and believes the uptown bus will take him there, my prediction that he will take the uptown bus is, while not foolproof, highly reliable. Skinner sometimes hints that intentional explanations are only _vaguely_ predictive, but this does not distinguish them from his own explanations until we are given some parameters by which to measure vagueness, which for human behavior are not forthcoming.

So let us put words in Skinner’s mouth, and follow the phantom argument to its conclusion. We can, then, “agree” with Skinner when we read him between the lines to be asserting that no satisfactory psychological theory can _rest_ on any use of intentional idioms, for their use presupposes rationality, which is the very thing psychology is supposed to explain. So if there is progress in psychology, it will inevitably be, as Skinner suggests, in the direction of eliminating ultimate appeals to beliefs, desires, and other intentional items from our explanations. So far so good. But now Skinner appears to make an important misstep, for he seems to draw the further conclusion that _intentional idioms therefore have no legitimate place in any psychological theory_. But this has not been shown at all. There is no reason why intentional terms cannot be used provisionally in the effort to map out the functions of the behavior control system of men and animals, just so long as a way is found eventually to “cash them out” by designing a mechanism to function as specified (see Chapters 1, 5 and 7). For example, we may not now be able to describe mechanically how to build a “belief store” for a man or animal, but if we specify how such a belief store must function, we can use the notion in a perfectly scientific way pending completion of its mechanical or physiological analysis. Mendelian genetics, for instance, thrived as a science for years with nothing more to feed on than the concept of a gene, a whatever-it-turns-out-to-be that functions as a transmitter of a heritable trait. All that is required by sound canons of scientific practice is that we not suppose or claim that we have reached an end to explanation in citing such a thing. Skinner, or rather phantom-Skinner, is wrong, then, to think it follows from the fact that psychology cannot make any _final appeal_ to intentional items, that there can be no place for intentional idioms in psychology.

It is this misstep that leads Skinner into his most pervasive confusion. We have already seen that Skinner, unlike Quine, thinks that translation of intentional into non-intentional terms is possible. But if so, why can’t intentional explanations, in virtue of these bonds of translation, find a place in psychology? Skinner vacillates between saying they can and they can’t, often within the space of a few pages.

> Beliefs, preferences, perceptions, needs, purposes, and opinions are possessions of autonomous man which are _said to_ change when we change minds. What _is_ changed in each case is a probability of action. (_my_ _italics_)[^32]

How are we to interpret this? As meaning that we change probabilities, _not_ beliefs, or as meaning that changing beliefs _is just_ changing probabilities of action? Skinner’s very next sentence strongly suggests the latter:

> A person’s belief that the floor will hold him as he walks across it depends upon his past experience.

But a few sentences later he hedges this by putting “belief” in scare-quotes:

> We build “belief” when we increase the probability of action by reinforcing behavior.

Does this passage mean that it is _all right_ to talk of building belief, so long as we understand it as increasing action probabilities, or that it is _wrong_ to talk that way since _all_ we are doing is increasing action probabilities?[^33] On the next page he takes the hard line:

> We change the relative strengths of responses by differential reinforcement of alternative courses of action; we do _not_ change something called a preference. We change the probability of an act by changing a condition of deprivation or aversive stimulation; we do _not_ change a need. We reinforce behavior in particular ways; we do _not_ give a person a purpose or an intention. (_my italics_)

This vacillation is typical of Skinner. The exclusivity expressed in the last quotation is rampant in _Beyond Freedom and Dignity_: “Our age is not suffering from anxiety but from the accidents, crimes, …” (p. 14). Young people refuse to get jobs “not because they feel alienated but because of defective social environments …” (p. 15). A man “makes a distinction not through some mental act of perception but because of prior contingencies” (p. 187). (See also pp. 26, 30, 157, 101, 189, 190, 204.) Yet the contrary claim that these intentional terms can all be translated, and hence, presumably, can be used to make true statements in psychology, is just about as widespread. We have just seen what may be Skinner’s definition of “believe”; “want” is defined on p. 37, and “intend” on p. 72, and p. 108. Intentional idioms occur by the dozens in crucial roles in all of Skinner’s books, and Skinner explicitly justifies or excuses this practice in several places. For instance, in _Beyond Freedom and Dignity_ (p. 24), he says, “No doubt many of the mentalistic expressions imbedded in the English language cannot be as rigorously translated as ‘sunrise’, but acceptable translations are not out of reach.” In _About Behaviorism_ (p. 17) he says, “Many of these expressions I ‘translate into behavior’. I do so while acknowledging that _traduttori traditori_—translators are traitors—and that there are perhaps no exact behavioral equivalents …” But the context shows that Skinner thinks he only loses the flavor—the connotations—not the predictive or inferential power or referential accuracy of the terms.

It is unfathomable how Skinner can be so sloppy on this score, for reflection should reveal to him, as it will to us, that this vacillation is over an absolutely central point in his argument.[^34] For surely Skinner is right in seeing that the validity of our conceptual scheme of moral agents having dignity, freedom and responsibility stands or falls on the question: can men ever be _truly_ said to have beliefs, desires, intentions? If they can, there is at least some hope of retaining a notion of the dignity of man; if they cannot, if men never can be said truly to want or believe, then surely they never can be said truly to act responsibly, or to have a conception of justice, or know the difference between right and wrong. So Skinner’s whole case comes down to the question: can intentional explanations (citing beliefs, desires, etc.) on the one hand, and proper, ultimate, scientific explanations on the other hand, _co-exist_? Can they ever _both_ be true, or would the truth of a scientific explanation always _exclude_ the other?

In spite of his vacillation in print, it is clear that Skinner must come down in favor of the exclusive view, if his argument is to work. Certainly the majority of his remarks favor this view, and in fact it becomes quite explicit on p. 101 of _Beyond Freedom and Dignity_ where Skinner distinguishes the “pre-scientific” (i.e., intentional) view of a person’s behavior from the scientific view, and goes on to say, “Neither view can be proved, but it is in the nature of scientific inquiry that the evidence should shift in favor of the second.” Here we see Skinner going beyond the correct intuition that it is in the nature of scientific inquiry that ultimate appeals to intentional idioms must disappear as progress is made, to the bolder view that as this occurs intentional explanations will be rendered false, not reduced or translated into other terms.

I argue at length in Chapter 12 that intentional and mechanistic or scientific explanations _can_ co-exist, and have given here an example supposed to confirm this: we know that there is a purely mechanistic explanation of the chess-playing computer, and yet it is _not false_ to say that the computer _figures out_ or _recognizes_ the best move, or that it _concludes_ that its opponent cannot make a certain move, any more than it is false to say that a computer _adds_ or _multiplies_. There has often been confusion on this score. It used to be popular to say, “A computer can’t really think, or course; all it can do is add, subtract, multiply and divide.” That leaves the way open to saying, “A computer can’t really multiply, of course; all it can do is add numbers together very, very fast,” and that must lead to the admission: “A computer cannot really add numbers, of course; all it can do is control the opening and closing of hundreds of tiny switches,” which leads to: “A computer can’t really control its switches, of course; it’s simply at the mercy of the electrical currents pulsing through it.” What this chain of claims adds up to “prove,” obviously, is that computers are really pretty dull lumps of stuff—they can’t do anything interesting at all. They can’t really guide rockets to the moon, or make out paychecks, or beat human beings at chess, but of course they can do all that and more. What the computer programmer can do if we give him the chance is not _explain away_ the illusion that the computer is doing these things, but _explain how_ the computer truly is doing these things.

Skinner fails to see the distinction between explaining and explaining away. In this regard he is succumbing to the same confusion as those who suppose that since color can be explained in terms of the properties of atoms which are not colored, nothing is colored. Imagine the Skinner-style exclusion claim: “The American flag is _not_ red, white and blue, but rather a collection of colorless atoms.” Since Skinner fails to make this distinction, he is led to the exclusive view, the view that true scientific explanations will exclude true intentional explanations, and typically, though he asserts this, he offers no arguments for it. Once again, however, with a little extrapolation we can see what perfectly good insights led Skinner to this error.

There are times when a mechanistic explanation obviously does exclude an intentional explanation. Wooldridge gives us a vivid example:

> When the time comes for egg laying the wasp _Sphex_ builds a burrow for the purpose and seeks out a cricket which she stings in such a way as to paralyze but not kill it. She drags the cricket into her burrow, lays her eggs alongside, closes the burrow, then flies away, never to return. In due course, the eggs hatch and the wasp grubs feed off the paralyzed cricket, which has not decayed, having been kept in the wasp equivalent of deep freeze. To the human mind, such an elaborately organized and seemingly purposeful routine conveys a convincing flavor of logic and thoughtfulness—until more details are examined. For example, the wasp’s routine is to bring the paralyzed cricket to the burrow, leave it on the threshold, go inside to see that all is well, emerge, and then drag the cricket in. If, while the wasp is inside making her preliminary inspection the cricket is moved a few inches away, the wasp, on emerging from the burrow, will bring the cricket back to the threshold, but not inside, and will then repeat the preparatory procedure of entering the burrow to see that everything is all right. If again the cricket is removed a few inches while the wasp is inside, once again the wasp will move the cricket up to the threshold and re-enter the burrow for a final check. The wasp never thinks of pulling the cricket straight in. On one occasion, this procedure was repeated forty times, always with the same result.[^35]

In this case what we took at first to be a bit of intelligent behavior is unmasked. When we see how simple, rigid and mechanical it is, we realize that we were attributing too much to the wasp. Now Skinner’s experimental life has been devoted to unmasking, over and over again, the behavior of pigeons and other lower animals. In “Behaviorism at Fifty” he gives an example almost as graphic as our wasp. Students watch a pigeon being conditioned to turn in a clockwise circle, and Skinner asks them to describe what they have observed. They all talk of the pigeon _expecting_, _hoping_ for food, _feeling_ this, _observing_ that, and Skinner points out with glee that they have observed nothing of the kind; he has a simpler, more mechanical explanation of what has happened, and it _falsifies_ the students’ unfounded _inferences_. Since in this case explanation is unmasking or explaining away, it always is.[^36] Today pigeons, tomorrow the world. What Skinner fails to see is that it is not the fact that he has an explanation[^37] that unmasks the pretender after intelligence, but rather that his explanation is so simple (see Chapter 12 of this volume). If Skinner had said to his students, “Aha! You think the pigeon is so smart, but here’s how it learned to do its trick,” and proceeded to inundate them with hundreds of pages of detailed explanation of highly complex inner mechanisms, their response would no doubt be that yes, the pigeon did seem, on his explanation, to be pretty smart.

The fact that it is the simplicity of explanations that can render elaborate intentional explanations false is completely lost to Skinner for a very good reason: the only _well-formulated_, _testable_ explanations Skinner and his colleagues have so far come up with have been, perforce, relatively simple, and deal with the relatively simple behavior controls of relatively simple animals. Since all the explanations he has so far come up with have been of the unmasking variety (pigeons, it turns out, do not have either freedom or dignity), Skinner might be forgiven for supposing that all explanations in psychology, including all explanations of human behavior, must be similarly unmasking.

It might, of course, turn out to be the case that all human behavior could be unmasked, that all signs of human cleverness are as illusory as the wasp’s performance, but in spite of all Skinner’s claims of triumph in explaining human behavior, his own testimony reveals this to be wishful thinking. Even if we were to leave unchallenged all the claims of operant conditioning of human beings in experimental situations,[^38] there remain areas of human behavior that prove completely intractable to Skinner’s mode of analysis. Not surprisingly, these are the areas of deliberate, intentional action. The persistently recalcitrant features of human behavior for the Skinnerians can be grouped under the headings of novelty and generality. The Skinnerian must explain all behavior by citing the subject’s past history of similar stimuli and responses, so when someone behaves in a novel manner, there is a problem. Pigeons do not exhibit very interesting novel behavior, but human beings do. Suppose, to borrow one of Skinner’s examples, I am held up and asked for my wallet.[^39] This has never happened to me before, so the correct response cannot have been “reinforced” for me, yet I do the smart thing: I hand over my wallet. Why? The Skinnerian must claim that this is not truly novel behavior at all, but an instance of a _general sort_ of behavior which has been previously conditioned. But what sort is it? Not only have I not been trained to hand over my wallet to men with guns, I have not been trained to empty my pockets for women with bombs, nor to turn over my possessions to armed entities. None of these things has ever happened to me before. I may never have been threatened before at all. Or more plausibly, it may well be that most often when I have been threatened in the past, the “reinforced” response was to _apologize_ to the threatener for something I’d said. Obviously, though, when told, “Your money or your life!” I don’t respond by saying, “I’m sorry. I take it all back.” It is perfectly clear that what experience has taught me is that if I _want_ to save my skin, and _believe_ I am being threatened, I should do what I _believe_ my threatener _wants_ me to do. But of course Skinner cannot permit this intentional formulation at all, for in ascribing wants and beliefs it would presuppose my rationality. He must insist that the “threat stimuli” I now encounter (and these are not defined) are similar in some crucial but undescribed respect to some stimuli encountered in my past which were followed by responses of some sort similar to the one I now make, where the past responses were reinforced somehow by their consequences. But see what Skinner is doing here. He is positing an external _virtus dormitiva_. He has no record of any earlier experiences of this sort, but _infers_ their existence, and moreover _endows_ them with an automatically theory-satisfying quality: these postulated earlier experiences are claimed to resemble-in-whatever-is-the-crucial-respect the situation they must resemble for the Skinnerian explanation to work. Why do I hand over my wallet? Because I must have had in the past some experiences that reinforced wallet-handing-over behavior in circumstances like this.

When Skinner predicts pigeon behavior he makes use of his knowledge of their reinforcement history, but when he predicts human behavior, he does not. This can be vividly seen if we consider once again the chess-playing computer. Suppose we set up a contest between Skinner and an intentionalist to see who could make the best predictions of the computer’s moves. Skinner would proceed by keeping a careful cumulative record of every move the computer ever made, keeping track of each move’s consequences, to see which moves were “reinforced “by their consequences. Would he have a chance of making good predictions? Mathematically, it can be shown that there is no guarantee that he would get anywhere with this method unless he knew the _internal_ starting state of the computer; the computer’s past biography of moves is not enough.[^40] But for the sake of argument we can suppose that the importance of the initial state would recede as the computer made more and more moves (not a universally plausible supposition), so that Skinner could get closer and closer to good predictions in spite of his ignorance of this crucial variable. Skinner’s predictions would take this form: there is a high probability that the computer will move queen to king’s bishop-4 because when stimulated by similar (not necessarily identical) board positions in the past, the computer has been reinforced for making similar (not necessarily identical) moves. There is obviously much that is problematical about such formulations—e.g., what are the shared features of the similar board positions and similar moves?—but let us suppose Skinner succeeds, after years of cumulative recording, in arriving at good predictions. This would not be as flashy and easy a method as the intentionalist’s, who would simply ask himself at each point in the game: “Now if I were the computer, knowing what I know and wanting what I want, which move would I believe to have the best consequences?” but Skinner could comfort himself by recalling Russell’s phrase, and claiming that his opponent’s method had all the advantages of theft over honest toil.

But suppose we complicate the picture. Suppose we wrote some chess-playing programs that could “learn” as they played, and improve as a result of “experience”—by the relatively simple expedient of adjusting weightings in their evaluation formulae for positions as a function of their “track record.” Now suppose we set two different chess-playing computers to playing a series of games against each other, but do not provide for the recording of the games. We turn them on at night, and in the morning discover two very much improved chess-playing computers (one of them, probably, would have established its mastery over the other—something we couldn’t discover until we watched a few games). It must be possible to determine mathematically what these evolved programs are now (if we know exactly what program each had to begin with, and that there is no randomizing element in either program, and that no uncorrected malfunctions occurred during the night, and if we know exactly how many games were played in the time available)—or we could, in principle, take the computers apart, figuring out what their programs were in that way. But for practical purposes both of these methods are ruled out. Can we still make predictions of their behavior? Of course. The intentionalist can predict their behavior just as well (no better) than he can predict the behavior of a novel human opponent, a stranger in town. He would assume his opponent had some intelligence, and hence would expect him to make the most intelligent moves available. But Skinner would have to claim ignorance; the fact that the biography of the computers would be lost would mean that Skinner would not be able to use his method. He would say that too much conditioning of which he had no record had intervened overnight. But he _could_ do this: he could make the same predictions (roughly, depending on his ability at chess) as the intentionalist, and _on the same grounds,_ namely that it was the best move he could see, and then “deduce” the fact that the computer during the night _must have_ been “reinforced” for making moves “similar in some respect” to the one he is now predicting. Here it would be crystal clear that Skinner would have no grounds for such a hypothesis except that his theory required it, and no way of being specific about the “similarity” of the overnight experiences.[^41]

I am suggesting that once Skinner turns from pigeons to people, his proffered “explanations” of human behavior are no better than this. If Skinner complains that mentalistic explanations are too easy, since we always know exactly what mental events to postulate to “explain” the behavior, the same can be said of all the explanation sketches of complex human behavior in Skinner’s books. They offer not a shred of confirmation that Skinner’s basic mode of explanation—in terms of reinforcement of operants—will prove fruitful in accounting for human behavior. It is hard to be sure, but Skinner even seems to realize this. He says at one point, “The instances of behavior cited in what follows are not offered as ‘proof’ of the interpretation,” but he goes right on to say, “The proof is to be found in the basic analysis.” But insofar as the “basic analysis” proves anything, it proves that people are not like pigeons, that Skinner’s unmasking explanations will not be forthcoming. Certainly if we discovered that people only handed over their wallets to robbers after being conditioned to do this, and, moreover, continued to hand over their wallets after the robber had shown his gun was empty, or when the robber was flanked by policemen, we would have to admit that Skinner had unmasked the pretenders; human beings would be little better than pigeons or wasps, and we would have to agree that we had no freedom and dignity.

Skinner’s increasing reliance, however, on a _virtus dormitiva_ to “explain” complex human behavior is a measure of the difference between pigeons and persons, and hence is a measure of the distance between Skinner’s premises and his conclusions. When Skinner speculates about the past history of reinforcement in a person in order to explain some current behavior, he is saying, in effect, “I don’t know which of many possible equivalent series of events occurred, but one of them did, and that explains the occurrence of this behavior now.” But what is the equivalence class Skinner is pointing to in every case? What do the wide variety of possible stimulus histories have in common? Skinner can’t tell us in his vocabulary, but it is easy enough to say: the stimulus histories that belong to the equivalence class have in common the fact that they _had the effect of teaching the person that p,_ of storing certain information. In the end Skinner is playing the same game with his speculations as the cognitivist who speculates about internal representations of information. Skinner is simply relying on a more cumbersome vocabulary.

Skinner has failed to show that psychology without mentalism is either possible or—in his own work—actual, and so he has failed to explode the myths of freedom and dignity. Since that explosion was to have been his first shot in a proposed social revolution, its misfiring saves us the work of taking seriously his alternately dreary and terrifying proposals for improving the world.

## Notes

1.]] _Beyond Freedom and Dignity_ (New York: Knopf, 1971), p. 11. See also Skinner’s _About Behaviorism_ (New York: Random House, 1974): p. 31: “Almost all versions (of mentalism) contend that the mind is a non-physical space in which events obey non-physical laws.”

2.]] _Beyond Freedom and Dignity_, pp. 12 and 191.

3.]] In the film, _Behavior Control: Freedom and Morality_ (Open University Film Series). This is a conversation between Skinner and Geoffrey Warnock, reviewed by me in _Teaching Philosophy_, I, 2 (Fall, 1975): 175–7. See also _About Behaviorism_, p. 121: “By attempting to move human behavior into a world of non-physical dimensions, mentalistic or cognitivistic psychologists have cast the _basic_ issues in insoluble form.” Note that here he countenances no exceptions to the cognitivist-dualist equation.

4.]] _Beyond Freedom and Dignity_, p. 191. See also Skinner’s _Science and Human Behavior_ (Free Press paperback edition, 1953): p. 285 and 82.

5.]] “Behaviorism at Fifty,” in T. W. Wann, ed., _Behaviorism and Phenomenology_ (University of Chicago Press, 1964): 84.

6.]] _Beyond Freedom and Dignity_: pp. 1, 14 and 193. In _About Behaviorism_ Skinner countenances _covert_ behavior (p. 26) and “private consequences” as reinforcers (p. 106), but on other pages insists “the environment stays where it is and where it has always been—outside the body” (p. 75), and “Neither the stimulus nor the response is ever _in_ the body in any literal sense” (p. 148). See also “Why Look Inside,” _About Behaviorism_, 165–69.

7.]] _Beyond Freedom and Dignity_, p. 14.

8.]] “The Case Against B. F. Skinner,” _New York Review of Books_ (December 30, 1971).

9.]] “Behaviorism at Fifty,” p. 84.

10.]] _Beyond Freedom and Dignity_, p. 195; see also pp. 8 and 10. _About Behaviorism_, p. 18 and 170; _Cumulative Record_ (1961): pp. 274–75.

11.]] In “Operant Behavior,” in W. K. Honig, ed., _Operant Behavior: Areas of Research and Application_ (New York: Appleton Century Crofts, 1966), Skinner disparages theories that attempt to order the behavioral chaos by positing “some mental, physiological or merely conceptual inner system which by its nature is neither directly observed in nor accurately represented on any occasion by, the performance of an organism. There is no comparable inner system in an operant analysis” (p. 16). Here sheer internality is apparently the bogy. See also _Science and Human Behavior_, p. 32ff.

12.]] He could hardly deny this, but he comes perilously close to it in _About Behaviorism_, where a particularly virulent attack of operationalism tempts him to challenge the credentials of such innocuous “scientific” concepts as the _tensile strength_ of rope and the _viscosity_ of fluids (pp. 165–66). Before philosophers scoff at this, they should remind themselves where psychologists caught this disease. A few pages later (p. 169) Skinner grants that a molecular explanation of viscosity is “a step forward” and so are physiological explanations of behavior. In “What is Psychotic Behavior?” (in _Cumulative Record_) he disparages “potential energy” and “magnetic field.”

13.]] _Beyond Freedom and Dignity_, pp. 9 and 23; _Cumulative Record_, pp. 283–84; “Behaviorism at Fifty.”

14.]] A patient and exhaustive review of these issues in Skinner’s writings up to 1972 can be found in Russell Keat, “A Critical Examination of B. F. Skinner’s Objections to Mentalism,” _Behaviorism_, vol. I (Fall, 1972).

15.]] “Behaviorism at Fifty,” p. 80; _Beyond Freedom and Dignity_, Chapter 1, and p. 160.

16.]] _Beyond Freedom and Dignity_, p. 195.

17.]] _New Essays on the Understanding_ (1704): Preface. See also Leibniz’s _Discourse on Metaphysics_, X.

18.]] Skinner finds a passage in Newton to much the same effect as Leibniz: _Beyond Freedom and Dignity_, p. 9.

19.]] Ibid., p. 14.

20.]] Ibid., p. 200.

21.]] In _About Behaviorism_ (pp. 213–14), Skinner provides a marvelous list of the cognitivistic horrors—together with the hint that they are all equally bad, and that the use of one implicates one in the countenancing of all the others: “… sensations … intelligence … decisions … beliefs … a death instinct … sublimation … an id … a sense of shame … reaction formations … psychic energy … consciousness … mental illnesses …”.

22.]] “Behaviorism at Fifty,” p. 80.

23.]] See _Content and Consciousness_ (1969), and Chapter 1 of this volume, where I argue at length for this claim.

24.]] “Behaviorism at Fifty,” p. 80.

25.]] See, e.g., Roderick Chisholm, _Perceiving_, _A Philosophical Study_ (1957), and numerous articles since then; also Quine, _Word and Object_ (1960); W. G. Lycan, “On Intentionality and the Psychological,” _American Philosophical Quarterly_ (October, 1969).

26.]] There is a great deal of literature to suggest, however, that specifying the logical marks of intentional idioms is hardly uncontroversial. See, e.g., Lycan, op. cit.

27.]] _Word and Object_, p. 221.

28.]] Quine has recently confirmed to me in conversation that he has been unsuccessful in attempts to convey this portion of the Quinean creed to his close friend William James Hall.

29.]] See especially _Beyond Freedom and Dignity_, pp. 24 and 72.

30.]] A careful reading of these “translations” should do more than reading the combined works of Chisholm and Quine to convince the reader that translating the intentional in behavioral terms is impossible.

31.]] “Behaviorism at Fifty,” p. 93.

32.]] _Beyond Freedom and Dignity_, p. 93.

33.]] See also _Beyond Freedom and Dignity_, p. 118.

34.]] A defender of Skinner, in response to this sentence, sought to explain and justify this curious behavior of Skinner’s by suggesting that the reason Skinner overlooked this crucial vacillation is that he had no idea he was conducting any such argument. According to this apologist, Skinner’s entire case against freedom and dignity boils down to an uncomplicated allegiance to hard determinism. If so, I’ve vastly overestimated Skinner. However, if I understand the principles of the new hermeneutics (a dubious antecedent!), I am entitled to ignore refractory biographical details like these and press ahead with my interpretation of the texts.

35.]] _The Machinery of the Brain_ (New York: McGraw Hill, 1963): p. 82.

36.]] In “Behaviorism at Fifty” Skinner discusses Lloyd Morgan’s Canon of Parsimony, which is a special case, in a way, of Occam’s Razor: never assume any more intellect or powers of rationality than are required to explain the behavior. He seems in the end to think this grounds his conviction that “it is in the nature of scientific inquiry” that ascriptions of rationality must all in the end be falsified.

37.]] True or false—I am not prepared to concede the truth of his operant conditioning theory, even for pigeons.

38.]] But we shouldn’t. See W. F. Brewer, “There is No Convincing Evidence for Operant or Classical Conditioning in Adult Humans,” in W. B. Weimer, ed., _Cognition and the Symbolic Processes_ (Hillsdale, New Jersey: Erlbaum, 1974).

39.]] See _Science and Human Behavior_, p. 177, and Chomsky’s amusing _reductio ad absurdum_ of Skinner’s analysis of “your money or your life” in his review of _Verbal Behavior_, in _Language_ (1959), reprinted in J. Fodor and J. Katz, ed., _The Structure of Language_, _Readings in the Philosophy of Language_ (New York: Prentice Hall, 1964).

40.]] R. J. Nelson, “Behaviorism is False,” _Journal of Philosophy_ (July 24, 1969); see also Michael Arbib, “Memory Limitations of Stimulus-Response Models,” _Psychological Review_, LXXVI, 5 (1969): 507–11, and Patrick Suppes, “Stimulus-Response Theory of Automata and TOTE Hierarchies: A Reply to Arbib,” _Psychological Review_, LXXVI, 5 (1969): 511–14.

41.]] In _About Behaviorism,_ Skinner says: “When … histories are out of reach, very little prediction and control is possible, but a behavioristic account is still more useful than a mentalistic one in interpreting what a person is doing and why. … [If] we are going to guess, it is more helpful to guess about genetic endowment and environmental history than about the feelings which have resulted from them” (pp. 2 and 5–6). Here again we see Skinner’s failure of stimulus discrimination betraying him. He speaks of the _feelings_ which have resulted. Feelings are a wishy-washy, unpredictive lot, no doubt, of scant use to the cognitive psychologist, but beliefs, expectations and intentions, for instance (or better still, their theoretical counterparts) are much more powerfully organizable into predictive and explanatory structures.

Why the Law of Effect Will Not Go Away

The poet Paul Valéry said: “It takes two to invent anything.” He was not referring to collaborative partnerships between people but to a bifurcation in the individual inventor. “The one,” he says, “makes up combinations; the other one chooses, recognizes what he wishes and what is important to him in the mass of the things which the former has imparted to him. What we call genius is much less the work of the first one than the readiness of the second one to grasp the value of what has been laid before him and to choose it.”[^1] This is a plausible claim. Why? Is it true? If it is, what kind of truth is it? An empirical generalization for which there is wide scale confirmation? Or a “conceptual truth” derivable from our concept of invention? Or something else?

Herbert Simon, in _The Sciences of the Artificial_, makes a related claim: “human problem solving, from the most blundering to the most insightful, involves nothing more than varying mixtures of trial and error and selectivity.”[^2] This claim is also plausible, I think, but less so. Simon presents it as if it were the conclusion of an inductive investigation, but _that_ I think, is not plausible at all. An extensive survey of human problem solving may have driven home this thesis to Simon, but its claim to our assent comes from a different quarter.

I want to show that these claims owe their plausibility to the fact that they are implications of an abstract principle whose “necessity” (such as it is) consists in this: we can know independently of empirical research in psychology that any adequate and complete psychological theory must exploit some version or other of the principle. The most familiar version of the principle I have in mind is the derided darling of the behaviorists: the Law of Effect. “The rough idea,” Broadbent observes,[^3] “that actions followed by reward are repeated, is one which is likely to occur to most intelligent people who think about possible explanations of behavior.” This rough idea, refined, is the Law of Effect, and my claim is that it is not just part of _a_ possible explanation of behavior, but of _any_ possible adequate explanation of behavior.

In order to establish this condition of adequacy for psychological theories, we must first be clear about the burden of psychology. Consider the way the rest of the social sciences depend on the more basic science of psychology. Economics, or at any rate classical economics, assumes at the outset an ontology of rational, self-interested agents, and then proposes to discover generalizations about how such agents, the “atoms” of economics, will behave in the market-place. This assumption of intelligence and self-interest in agents is not idle; it is needed to ground and explain the generalizations. Consider the law of supply and demand. There is no mystery about why the law holds as reliably as it does: _people are not fools_; they want as much as they can get, they know what they want and how much they want it, and they know enough to charge what the market will bear and buy as cheap as they can. If that didn’t explain why the law of supply and demand works, we would be utterly baffled or incredulous on learning that it did. Political science, sociology, anthropology and social psychology are similarly content to _assume_ capacities of discrimination, perception, reason and action based on reason, and then seek interesting generalizations about the exploitation of these capacities in particular circumstances. One way of alluding to this shared feature of these social sciences is to note that they are all _intentional_: they utilize the intentional or “mentalistic” or “cognitive” vocabulary—they speak of belief, desire, expectation, recognition, action, etc.—and they permit explanations to come to an end, at least on occasion, with the citation of a stretch of practical reasoning (usually drastically enthymematic): the voters elected the Democrat _because_ they were working men and believed the Republican candidate to be anti-labor; the stock market dropped _because_ investors believed other havens for their money were safer. These sciences leave to psychology the task of explaining _how there come to be_ entities—organisms, human beings—that can be so usefully assumed to be self-interested, knowledgeable and rational. A fundamental task of psychology then is to explain intelligence. For the super-abstemious behaviorist who will not permit himself to speak even of intelligence (that being too “mentalistic” for him) we can say, with Hull, that a primary task of psychology “is to understand … why … behavior … is so generally adaptive, i.e., successful in the sense of reducing needs and facilitating survival. …”[^4] The account of intelligence required of psychology must not of course be question-begging. It must not explain intelligence in terms of intelligence, for instance by assigning responsibility for the existence of intelligence in creatures to the munificence of an intelligent Creator, or by putting clever homunculi at the control panels of the nervous system (see Chapter 4).[^5] If that were the best psychology could do, then psychology could not do the job assigned it.

We already have a model of a theory that admirably discharges just this _sort_ of burden in the Darwinian theory of evolution by natural selection, and as many commentators have pointed out, the Law of Effect is closely analogous to the principle of natural selection. The Law of Effect presumes there to be a “population” of stimulus-response pairs, more or less randomly or in any case arbitrarily mated, and from this large and varied pool, reinforcers _select_ the well-designed, the adaptive, the fortuitously appropriate pairs in an entirely mechanical way: their recurrence is made more probable, while their maladaptive or merely neutral brethren suffer “extinction,” not by being _killed_ (all particular stimulus-response pairs come to swift ends), but by _failing to reproduce_. The analogy is very strong, very satisfying, and very familiar.

But there has been some misinterpretation of the nature of its appeal. Broadbent observes:

> The attraction both of natural selection and of the Law of Effect, to certain types of mind, is that they do not call on explanatory principles of a quite separate order from those used in the physical sciences. It is not surprising therefore that the Law of Effect had been seized on, not merely as a generalization which is true of animals under certain conditions, but also as a fundamental principle which would explain all adaptive behaviour.[^6]

It is certainly true that these analogous principles appeal to physicalists or materialists because they are mechanistically explicable, but there is a more fundamental reason for favoring them: they both can provide clearly non-question-begging accounts of _explicanda_ for which it is very hard to devise non-question-begging accounts. Darwin explains a world of final causes and teleological laws with a principle that is, to be sure, mechanistic but—more fundamentally—utterly independent of “meaning” or “purpose.” It assumes a world that is _absurd_ in the existentialist’s sense of the term: not ludicrous but pointless, and this assumption is a necessary condition of any non-question-begging account of _purpose_. Whether we can imagine a _non_-mechanistic but also non-question-begging principle for explaining design in the biological world is doubtful; it is tempting to see the commitment to non-question-begging accounts here as tantamount to a commitment to mechanistic materialism, but the priority of these commitments is clear. It is not that one’s prior prejudice in favor of materialism gives one reason to accept Darwin’s principle because it is materialistic, but rather that one’s prior acknowledgment of the constraint against begging the question gives one _reason to adopt materialism_, once one sees that Darwin’s non-question-begging account of design or purpose in nature is materialistic. One argues: Darwin’s materialistic theory may not be the only non-question-begging theory of these matters, but it is one such theory, and the only one we have found, which is quite a good reason for espousing materialism.

A precisely parallel argument might occur to the psychologist trying to decide whether to throw in with the behaviorists: theories based on the Law of Effect may not be the only psychological theories that do not beg the question of intelligence, but they _are_ clearly non-question-begging in this regard, and their rivals are not, which is quite a good reason for joining the austere and demanding brotherhood of behaviorists. But all is not well in that camp, and has not been for some time. Contrary to the claims of the more optimistic apologists, the Law of Effect has not been knit into any theory with anything remotely like the proven power of the theory of natural selection. The Law of Effect has appeared in several guises since Thorndike introduced it as a principle of learning; most influentially, it assumed centrality in Hull’s behaviorism as the “law of primary reinforcement” and in Skinner’s as the “principle of operant conditioning,”*]] but the history of these attempts is the history of ever more sophisticated failures to get the Law of Effect to _do enough work._ It may account for a _lot_ of learning, but it can’t seem to account for it all. Why, then, not look for another fundamental principle of more power to explain the balance? It is not just mulishness or proprietary pride that has kept behaviorists from following this suggestion, but rather something like the conviction that the Law of Effect is not just _a_ good idea, but the only possible good idea for this job. There is something right in this conviction, I want to maintain, but what is wrong in it has had an ironic result: allegiance to the Law of Effect in its behavioristic or peripheralistic versions has forced psychologists to beg small questions left and right in order to keep from begging the big question. One “saves” the Law of Effect from persistent counter-instances by the _ad hoc_ postulation of reinforcers and stimulus histories for which one has not the slightest grounds except the demands of the theory. For instance one postulates curiosity drives, the reduction of which is reinforcing, in order to explain “latent” learning, or presumes that when one exhibits an apparently novel bit of intelligent behavior, there _must have been_ some “relevantly similar” responses in one’s past for which one was reinforced. These strategies are not altogether bad; they parallel the evolutionist’s speculative hypothetical ancestries of species, which are similarly made up out of whole cloth to begin with, but which differ usually in being clearly confirmable or disconfirmable. These criticisms of behaviorism are not new,[^7] and not universally fair in application either. I am convinced, nevertheless, that no behaviorism, however sophisticated, can elude all versions of these familiar objections, but that is not a claim to be supported in short compass. It will be more constructive to turn to what I claim is right about the Law of Effect, and to suggest another way a version of it can be introduced to take up where behaviorism leaves off.

The first thing to note is that the Law of Effect and the principle of natural selection are not just analogues; they are designed to work together. There is a kind of intelligence, or pseudo-intelligence, for which the principle of natural selection itself provides the complete explanation, and that is the “intelligence” manifest in tropistic, “instinctual” behavior control. The environmental appropriateness, the biological and strategic wisdom, evident in bird’s-nest-building, spider-web-making and less intricate “innate” behavioral dispositions is to be explained by the same principle that explains the well-designedness of the bird’s wings or the spider’s eyes. We are to understand that creatures so “wired” as to exhibit useful tropistic behavior in their environmental niches will have a survival advantage over creatures not so wired, and hence will gradually be selected by the vicissitudes of nature. Tropistic behavior is not plastic in the individual, however, and it is evident that solely tropistically controlled creatures would not be evolution’s final solution to the needs-versus-environment problem. _If_ creatures with some plasticity in their input–output relations were to appear, _some_ of them might have an advantage over even the most sophisticated of their tropistic cousins. Which ones? Those that were able to distinguish good results of plasticity from bad, and preserve the good. The problem of selection reappears and points to its own solution: let some class of events in the organisms be genetically endowed with the capacity to increase the likelihood of the recurrence of behavior-controlling events upon which they act. Call them reinforcers. Some mutations, we can then speculate, appear with inappropriate reinforcers, others with neutral reinforcers, and a lucky few with appropriate reinforcers. Those lucky few survive, of course, and their progeny are endowed genetically with a capacity to _learn_, where learning is understood to be nothing more than a change (in the environmentally appropriate direction) in stimulus-response probability relations. The obviously adaptive positive reinforcers will be events normally caused by the presence of food or water, by sexual contact, and by bodily well-being, while the normal effects of injury and deprivation will be the obvious negative reinforcers, though there could be many more than these.*]]

The picture so far is of creatures well endowed by natural selection with tropistic _hard-wiring,_ including the hard-wiring of some reinforcers. These reinforcers, in turn, permit the further selection and establishment of adaptive soft-wiring, such selection to be drawn from a pool of essentially arbitrary, _undesigned_ temporary interconnections. Whenever a creature is fortunate enough to have one of its interconnections be followed by an environmental effect that in turn produces a reinforcer as “feedback,” that interconnection will be favored. Skinner is quite explicit about all this. In _Science and Human Behavior_ he notes that “The process of conditioning has survival value,” but of course what he means is that the _capacity_ to be conditioned has survival value. “Where inherited behavior leaves off, the inherited modifiability of the process of conditioning takes over.”[^8] So let us use the term “Skinnerian creatures” for all creatures that are susceptible to operant conditioning, all creatures whose learning can be explained by the Law of Effect. Skinnerian creatures clearly have it over merely tropistic creatures, but it seems that there are other creatures, e.g., at least ourselves and many other mammals, that have it over merely Skinnerian creatures.

The trouble, intuitively, with Skinnerian creatures is that they can learn only by actual behavioral trial and error in the environment. A useful bit of soft-wiring cannot get selected until it has had an opportunity to provoke some reinforcing feedback from the environment, and the problem seems to be that merely _potential_, as yet _unutilized_ behavioral controls can _ex hypothesi_ have no environmental effects which could lead to their being reinforced. And yet experience seems to show that we, and even monkeys, often think out and select an adaptive course of action without benefit of prior external feedback and reinforcement. Faced with this dilemma, we might indulge in a little wishful thinking: if only the Law of Effect could provide for the reinforcement of merely potential, unutilized bits of behavior control wiring! If only such unutilized controls could have some subtle effect on the environment (i.e., if only merely “thinking about the solution” could have some environmental effect) and if only the environment were benign enough to bounce back the appropriate feedback in response! But that, it seems, would be miraculous.

Not so. We can have all that and more by simply positing that creatures have _two_ environments, the outer environment in which they live, and an “inner” environment they carry around with them. The inner environment is just to be conceived as an input–output box for providing feedback for events in the brain.*]] Now we can run just the same speculative argument on Skinnerian creatures that we earlier ran on tropistic creatures. Suppose there appear among the Skinnerian creatures of the world mutations that have inner environments of the sort just mentioned. Some, we can assume, will have maladaptive inner environments (the environments will make environmentally inappropriate behavior more likely); others will have neutral inner environments; but a lucky few will have inner environments that happen to reinforce, by and large, only adaptive _potential_ behavioral controls. In a way we are turning the principle of natural selection on its head: we are talking of the evolution of (inner) environments to suit the organism, of environments that would have survival value in an organism. Mutations equipped with such benign inner environments would have a distinct survival advantage over merely Skinnerian creatures in any exiguous environment, since they could learn faster and _more safely_ (for trial and error learning is not only tedious; it can be dangerous). The advantage provided by such a benign inner environment has been elegantly expressed in a phrase of Karl Popper’s: it “permits our hypotheses to die in our stead.”

The behaviorist, faced with the shortcomings of the Law of Effect, insisted that all we needed was more of the same (that only more of same could explain what had to be explained), and that is what we have given him. He was just construing “the same” too narrowly. The _peripheralism_ of behaviorist versions of the Law of Effect turns out to be not so essential as they had thought. For instance, our talk of an inner _environment_ is merely vestigial peripheralism; the inner environment is just an inner something that selects. Ultimately of course it is environmental effects that are the measure of adaptivity and the mainspring of learning, but the environment can delegate its selective function to something in the organism (just as death had earlier delegated its selective function to pain), and if this occurs, a more intelligent, flexible, organism is the result.

It might be asked if behaviorists haven’t already, in fact long ago, taken this step to inner reinforcement or selection. I think the fairest answer is that some have and some have not, and even those that have have not been clear about what they are doing. On the one hand there are the neo-Skinnerians who have no qualms about talking about the operant conditioning that results in the subject who _imagines_ courses of action followed by reinforcing results, and on the other hand you have the neo-Skinnerians that still rail against the use of such mentalistic terms as “imagine.” Skinner himself falls into both camps, often within the compass of a single page.*]] “The skin,” says Skinner, “is not that important as a boundary,”[^9] but it is hard to believe he sees the implications of this observation. In any event it will be clearer here to suppose that behaviorists are “classical” peripheralists who do not envisage such a reapplication of the Law of Effect via an inner environment.

At this point it is important to ask whether this proposed principle of selection by inner environment hasn’t smuggled in some incoherency or impossibility, for if it has not, we can argue that since our hypothesized mutations would clearly have the edge over merely Skinnerian creatures, there is no reason to believe that operant conditioning was evolution’s final solution to the learning or intelligence problem, and we could then safely “predict” the appearance and establishment of such mutations. Here we are, we could add. We could then go on to ask how powerful our new principle was, and whether there was learning or intelligence _it_ couldn’t explain. And we could afford to be more open-minded about this question than the behaviorist was, since if we thought there _was_ learning it couldn’t handle, we would know where to look for yet a stronger principle: yet a _fourth_ incarnation of our basic principle of natural selection (or, otherwise viewed, yet a _third_ incarnation of our basic psychological principle of the Law of Effect). In fact we can already see just what it will be. Nothing requires the inner environment to be entirely genetically hard-wired. A more versatile capacity would be one in which the inner environment _itself_ could evolve in the individual as a result of—for starters—operant conditioning. We not only learn; we learn better how to learn, and learn better how to learn better how to learn.*]]

So is there anything incoherent about the supposition of inner environments that can select adaptive features of _potential_ behavior control systems (and favor their incorporation into _actual_ behavior controls—for that is what reinforcement amounts to in this application)? Is anything miraculous or question-begging being assumed here? The notion of an inner environment was _introduced_ in explicitly nonintentional language: the inner environment is simply any internal region that can affect and be affected by features of potential behavioral control systems. The benign and hence selected inner environments are simply those in which the result of these causal interactions is the increased conditional probability of the actualization of those potential controls that would be adaptive under the conditions in which they are probable. The way the notion is introduced is thus uncontaminated by covert appeal to intelligence, but it is still not obvious that an inner environment could “work.”

What conditions must we put on features of bits of brain design to ensure that their selection by an optimally designed selector-mechanism will yield a better than chance improvement in ultimate performance? Since selection by inner environment is ultimately a mechanical sorting, which can key only on physical features of what is sorted, at the very least there would have to be a _normal_ or _systematic_ correlation between the physical event types selected and what we may call a _functional role_ in some control program. A physically characterized type of wiring could not consist in the main of reliably adaptive tokens unless those tokens normally played a particular function.*]] This is the same condition, raised one level, that we find on operant conditioning: if physically characterized _response_ classes do not produce a normally uniform environmental effect, reinforcement cannot be adaptive. So if and when this principle works, it works to establish high probabilities that particular appropriate functional roles will be filled at the appropriate times in control programs. Functional roles will be _discriminated_, and thereby control programs will become well designed.

It is hard to keep track of these purported functions and effects while speaking in the sterilized vocabulary of the behaviorist, but there is an easier way of talking: we can say that physical event tokens of a selected type have—in virtue of their normally playing a certain role in a well-designed functional organization—a _meaning_ or _content_. We have many familiar examples of _adaptive potential behavior control elements_: accurate _maps_ are adaptive potential behavior control elements, and so are true _beliefs_, warranted _expectations_, clear _concepts_, well-ordered _preferences,_ sound _plans of action_, in short all the favorite tools of the cognitive psychologist. As Popper says, it is _hypotheses—events_ or states endowed with an intentional characterization—that die in our stead. Is _cognitive_ psychology then bound ultimately to versions of the Law of Effect? That it is, I hope to show by looking at artificial intelligence (AI) research.

AI program designers work backward on the same task behaviorists work forward on. We have just traced the behaviorists’ cautious and self-denying efforts to build from mechanistic principles toward the levels of complexity at which it becomes apt and illuminating to speak in intentional terms about what they claim is going on. The AI researcher _starts_ with an intentionally characterized problem (e.g., how can I get a computer to _understand_ questions of English?), breaks it down into sub-problems that are also intentionally characterized (e.g., how do I get the computer to _recognize_ questions, _distinguish_ subjects from predicates, _ignore_ irrelevant parsings?) and then breaks these problems down still further until finally he reaches problem or task descriptions that are obviously mechanistic. Here is a way of looking at the process. The AI programmer begins with an intentionally characterized problem, and thus frankly views the computer anthropomorphically: if he _solves_ the problem he will say he has designed a computer that can understand questions in English. His first and highest level of design breaks the computer down into subsystems, each of which is given intentionally characterized tasks; he composes a flow chart of evaluators, rememberers, discriminators, overseers and the like. These are _homunculi_ with a vengeance; the highest level design breaks the computer down into a committee or army of intelligent homunculi with purposes, information and strategies. Each homunculus in turn is analyzed into _smaller_ homunculi, but, more important, into _less clever_ homunculi. When the level is reached where the homunculi are no more than adders and subtractors, by the time they need only the intelligence to pick the larger of two numbers when directed to, they have been reduced to functionaries “who can be replaced by a machine.” The aid to comprehension of anthropomorphizing the elements just about lapses at this point, and a mechanistic view of the proceedings becomes workable and comprehensible. The AI programmer uses intentional language fearlessly because he _knows_ that if he succeeds in getting his program to run, any questions he has been begging provisionally will have been paid back. The computer is more unforgiving than any human critic; if the program works then we can be certain that all homunculi have been discharged from the theory.*]]

Working backward in this way has proved to be a remarkably fruitful research strategy, for powerful principles of design have been developed and tested, so it is interesting to note that the overall shape of AI models is strikingly similar to the organization proposed for our post-Skinnerian mutations, and the problems encountered echo the problems faced by the behaviorist. A ubiquitous strategy in AI programming is known as _generate-and-test_, and our opening quotation of Paul Valéry perfectly describes it. The problem solver (or inventor) is broken down at some point or points into a generator and a tester. The generator spews up candidates for solutions or elements of solutions to the problems, and the tester accepts or rejects them on the basis of stored criteria. Simon points out the analogy, once again, to natural selection (_op_. _cit_., pp. 95–98).

The tester of a generate-and-test subroutine is none other than a part of the inner environment of our post-Skinnerian mutations, so if we want to know how well the principle of selection by inner environment can work, the answer is that it can work as well as generate-and-test methods can work in AI programs, which is hearteningly well.*]] Simon, as we saw at the outset, was prepared to go so far as to conclude that _all_ “human problem solving, from the most blundering to the most insightful” can be captured in the net of generate-and-test programming: “varying mixtures of trial and error and selectivity.” This claim is exactly analogous to the behaviorists’ creed that the Law of Effect could explain all learning, and again we may ask whether this is short-sighted allegiance to an idea that is good, but not the only good idea. Generate-and-test programs can simulate, and hence account for (in one important sense)†]] a lot of problem-solving and invention; what grounds have we for supposing it is powerful enough to handle it all? The behaviorist was in no position to defend his creed, but the AI researcher is in better shape.

Some AI researchers have taken their task to be the _simulation_ of particular cognitive capacities “found in nature”—even the capacities and styles of particular human individuals[^10]—and such research is known as CS or “cognitive simulation” research, but others take their task to be, not simulation, but the construction of intelligent programs _by any means whatever_. The only constraint on design principles in AI thus viewed is that they should _work_, and hence any boundaries the AI programmer keeps running into are arguably boundaries that restrict _all possible_ modes of intelligence and learning. Thus, if AI is truly the study of all possible modes of intelligence, and if generate-and-test is truly a necessary feature of AI learning programs, then generate-and-test is a necessary feature of all modes of learning, and hence a necessary principle in any adequate psychological theory.

Both premises in that argument need further support. The first premise was proposed on the grounds that AI’s guiding principle is that _anything is permitted that works_, but isn’t AI really more restrictive than that principle suggests? Isn’t it really that AI is the investigation of all possible _mechanistically realizable_ modes of intelligence? Doesn’t AI’s claim to cover all possible modes beg the question against the vitalist or dualist who is looking for a non-question-begging but also non-mechanistic psychology? The AI researcher is a mechanist, to be sure, but a mechanist-_malgré-lui_. He typically does not know or care what the hardware realizations of his designs will be, and often even relinquishes control and authorship of his programs at a point where they are still replete with intentionalistic constructions, still several levels away from machine language. He can do this because it is merely a clerical problem for compiler programs and the technicians that feed them to accomplish the ultimate “reduction” to a mechanistic level. The constraints of mechanism do not loom large for the AI researcher, for he is confident that any design he can state _clearly_ can be mechanized. The operative constraint for him, then, is something like clarity, and in practice clarity is ensured for anything expressible in a programming language of some level. Anything thus expressible is clear; what about the converse? Is anything clear thus expressible? The AI programmer believes it, but it is not something subject to proof; it is, or boils down to, some version of Church’s Thesis (e.g., anything computable is Turing-machine computable). But now we can see that the supposition that there might be a non-question-begging non-mechanistic psychology gets you nothing, unless accompanied by the supposition that Church’s Thesis is false. For a non-question-begging psychology will be a psychology that makes no ultimate appeals to unexplained intelligence, and that condition can be reformulated as the condition that whatever functional parts a psychology breaks its subjects into, the smallest, or most fundamental, or least sophisticated parts must not be supposed to perform tasks or follow procedures requiring intelligence. That condition in turn is surely strong enough to ensure that any procedure admissible as an “ultimate” procedure in a psychological theory falls well within the intuitive boundaries of the “computable” or “effective” as these terms are presumed to be used in Church’s Thesis. The intuitively computable functions mentioned in Church’s Thesis are those that “any fool can do,” while the admissible atomic functions of a psychological theory are those that “presuppose _no_ intelligence.” If Church’s Thesis is correct, then the constraints of mechanism are no more severe than the constraint against begging the question in psychology, for any psychology that stipulated atomic tasks that were “too difficult” to fall under Church’s Thesis would be a theory with undischarged homunculi.*]] So our first premise, that AI is the study of all possible modes of intelligence, is supported as much as it could be, which is _not quite_ total support, in two regards. The first premise depends on two unprovable but very reasonable assumptions: that Church’s Thesis is true, and that _there can be_, in principle, an adequate and complete psychology.

That leaves the second premise to defend: what reason is there to believe that generate-and-test is a necessary and not merely handy and ubiquitous feature of AI learning programs? First, it must be granted that many computer programs of great sophistication do not invoke any variety of generate-and-test. In these cases the correct or best steps to be taken by the computer are not selected but _given_; the program’s procedures are completely designed and inflexible. These programs are the analogues of our merely tropistic creatures; their design is _fixed_ by a prior design process. Sometimes there is a sequence of such programs, with the programmer making a series of changes in the program to improve its performance. Such genealogical developments do not so much represent problems solved as problems deferred, however, for the trick is to get the program to become self-designing, “to get the teacher out of the learner.” As long as the programmer must, in effect, reach in and rewire the control system, the system is not _learning_. Learning can be viewed as _self-design_, and Simon suggests we “think of the design process as involving first the generation of alternatives and then the testing of these alternatives against a whole array of requirements and constraints” (_op_. _cit_., p. 74). Of course he would suggest this, and we can follow his suggestion, but are there any alternatives? Is there any way of thinking (coherently) about the design process that is incompatible with (and more powerful than) thinking of it as an evolution wrought by generate-and-test? It seems not, and here is an argument supposed to show why. I suspect this argument could be made to appear more rigorous (while also, perhaps, being revealed to be entirely unoriginal) by recasting it into the technical vocabulary of some version of “information theory” or “theory of self-organizing systems.” I would be interested to learn that this was so, but am content to let the argument, which is as intuitive as it is sketchy, rest on its own merits in the meantime.

We are viewing learning as ultimately a _process_ of self-design. That process is for the purposes of this argument defined only by its _product_, and the product is a _new_ design. That is, as a result of the process something comes to have a design it previously did not have. This new design “must come from somewhere.” That is, it takes _information_ to distinguish the new design from all other designs, and that information must come from somewhere—either all from outside the system, or all from inside, or a bit of both. If all from outside, then the system does not redesign itself; this is the case we just looked at, where the all-knowing programmer, who _has_ the information, _imposes_ the new design on the system from without. So the information must all come from inside, or from both inside and outside. Suppose it all comes from inside. Then either the information already exists inside or it is created inside. What I mean is this: either the new design _exists ready made_ in the old design in the sense that its implementation at this time is already guaranteed by its old design, or the old design does not determine in this way what the new design will be. In the former case, the system has not really redesigned itself; it was designed all along to go into this phase at this time, and we must look to a prior design process to explain this. In the latter case, the new design is _underdetermined_ by the old design. This is a feature shared with the one remaining possibility: that the information comes from both inside and outside. In both of these cases the new design is underdetermined by the old design by itself, and only in these cases is there “genuine” learning (as opposed to the merely “apparent” learning of the merely tropistic creature). In any such case of underdetermination, the new design is either underdetermined period—there is a truly random contribution here; nothing takes up all the slack left by the underdetermination of the old design—_or_ the new design is determined by the combination of the old design and contributions (from either inside or outside or both) that are themselves _arbitrary_, that is, _undesigned_ or _fortuitous_. But if the contribution of arbitrary elements is to yield a better than chance probability of the new design being an improvement over the old design, the old design must have the capacity to _reject_ arbitrary contributions on the basis of design features—information—already present. In other words, there must be a _selection_ from the fortuitous contributions, based on the old design. If the arbitrary or undesigned contribution comes from within, what we have is a non-deterministic automaton.*]] A non-deterministic automaton is one such that at some point or points its further operations must wait on the result of a procedure that is undetermined by its program and input. In other words, some tester must wait on some generator to produce a candidate for its inspection. If the undesigned contribution comes from the outside, the situation is much the same; the distinction between _input_ and _random contribution_ is just differently drawn. The automaton is now deterministic in that its next step is a determinate function of its program and its input, but what input it gets is a fortuitous matter. In either case the system can _protect itself_ against merely fortuitous response to this merely fortuitous input only by _selecting_ as a function of its old design from the fortuitous “stimulation” presented. Learning must tread the fine line between the idiocy of pre-programmed tropism on the one hand and the idiocy of an over-plastic domination by fortuitous impingements on the other. In short, every process of genuine learning (or invention, which is just a special sort of learning) must invoke, at at least one but probably many levels, the principle of generate-and-test.

The moral of this story is that cognitivist theoreticians of all stamps may proceed merrily and _fruitfully_ with temporarily question-begging theoretical formulations, but if they expect AI to _pay their debts_ some day (and if anything can, AI can), they must acknowledge that the _processes_ invoked will inevitably bear the analogy to natural selection exemplified by the Law of Effect. The moral is _not_, of course, that behaviorism is the road to truth in psychology; even our hypothesized first-generation mutations of Skinnerian creatures were too intelligent for behaviorism to account for, and we have every reason to believe actual higher organisms are much more complicated than that. The only solace for the behaviorist in this account is that his theoretical paralysis has been suffered in a Good Cause; he has not begged the question, and if the high-flying cognitivists ever achieve his probity, it will only be by relying on principles fundamentally analogous to his.

This leaves open where these inevitable principles of selection will be invoked, and how often. Nothing requires generate-and-test formats to be simple and obviously mechanistic in any of their interesting realizations. On the contrary, _introspective_ evidence, of a sort I will presently illustrate, seems to bear out the general claim that generate-and-test is a common and recognizable feature of human problem solving, at the same time that it establishes that the generators and testers with which we are _introspectively_ familiar are themselves highly sophisticated—highly intelligent homunculi. As Simon points out, generate-and-test is not an efficient or powerful process unless the _generator_ is endowed with a high degree of selectivity (so that it generates only the most likely or most plausible candidates in a circumstance), and since, as he says, “selectivity can always be equated with some kind of feedback of information from the environment” (p. 97), we must ask, of each sort and degree of selectivity in the generator, where _it_ came from—is it learned or innate—and at the end of any successful answer to that question will be a generate-and-test process, either of natural selection if the selectivity is innate, or of some variety of learning, if it is not. A consequence of this is that we cannot tell by any simple inspection or introspection whether a particular stroke of genius we encounter is a bit of “genuine” invention at all—that is, whether the invention occurred just _now_, or is the result of much earlier processes of invention that are now playing out their effects. Did Einstein’s genetic endowment guarantee his creativity, or did his genetic endowment together with his nurture, his stimulus history, guarantee his creativity or did he genuinely create (during his own thought processes), his great insights? I hope it is clear how little hinges on knowing the answer to this question.

At this point I am prepared to say that the first part of Valéry’s claim stands vindicated: it takes two to invent anything: the one makes up combinations; the other one chooses. What of the second part of this claim: “What we call genius is much less the work of the first one than the readiness of the second one to grasp the value of what has been laid before him and to choose it”? We have seen a way in which this must be true, in the strained sense that the _ultimate_ generators must contain an element of randomness or arbitrariness. “The original solution to a problem must lie in the category of luck.”*]] But it does not seem that Valéry’s second claim is true on any ordinary interpretation. For instance, it does not seem to be true of all _inter-personal_ collaborations that the choosers are more the geniuses than their “idea-men” are. Some producers seldom offer poor suggestions; their choosers are virtual yes-men. Other producers are highly erratic in what they will propose, and require the censorship of severe and intelligent editors. There appears to be a trade-off here between, roughly, spontaneity or fertility of imagination on the one hand, and a critical eye on the other. A task of invention seems to require both, and it looks like a straightforwardly empirical question subject to continuous variation how much of each gets done by each collaborator.

Valéry seems to slight the contribution of the first, but perhaps that is just because he has in mind a collaboration at one end of the spectrum, where a relatively undiscriminating producer of combinations makes a lot of work for his editor. Of course, as said at the outset, Valéry is not talking about actual interpersonal collaboration, but of a bifurcation in the soul. He is perhaps thinking of his own case, which suggests that he is one of those who are _aware_ of considering and rejecting many bad ideas. He does not credit _his_ producer-homunculus with much genius, and is happy to identify with the _responsible_ partner, the chooser. Mozart, it seems, was of the same type: “When I feel well and in a good humor, or when I am taking a drive or walking after a good meal, or in the night when I cannot sleep, thoughts crowd into my mind as easily as you would wish. Whence and how do they come? I do not know and _I have nothing to do with it_. Those which please me I keep in my head and hum them; at least others have told me that I do so.”[^11] In such cases the producer-chooser bifurcation lines up with the unconscious and conscious selves bifurcation. One is conscious only of the _products_ of the producer, which one then consciously tests and chooses.

Poincaré, in a famous lecture of 1908, offers an “introspective” account of some mathematical inventing of his own that is more problematic: “One evening, contrary to my custom, I drank black coffee and could not sleep. Ideas rose in crowds; I felt them collide until pairs interlocked, so to speak, making a stable combination.”[^12] In this instance the chooser seems to have disappeared, but Poincar**é** has another, better interpretation of the incident. In this introspective experience he has been given a rare opportunity to glimpse the _processes_ in the generator; what is normally accomplished out of sight of consciousness is witnessed on this occasion, and the ideas that form stable combinations are those few that would normally be presented to the conscious chooser for further evaluation. Poincaré supposes he has watched the selectivity within the generator at work. I am not a little sceptical about Poincaré’s claimed _introspection_ here (I think all introspection involves elements of rational reconstruction, and I smell a good deal of that in Poincaré’s protocol), but I like his categories. In particular, Poincaré gives us, in his discussion of this experience, the key to another puzzling question.

For I have really had two burdens in this paper. The first, which I take to have discharged, is to explain why the Law of Effect is so popular in its various guises. The other is to explain why it is so _unpopular_ in all its guises. There is no denying that the Law of Effect seems to be an affront to our self-esteem, and a lot of the resistance—even hatred—encountered by behaviorists is surely due to this. Poincaré puts his finger on it. He was, if anyone ever has been, a creative and original thinker, and yet his own analysis of how he accomplished his inventions seemed to deny him _responsibility_ for them. He saw only two alternatives, both disheartening. One was that his unconscious self, the generator with whom he does not or cannot _identify_ “is capable of discernment; it has tact, delicacy; it knows how to choose, to divine. What do I say? It knows better how to divine than the conscious self since it succeeds where that has failed. In a word, is not the subliminal self superior to the conscious self? I confess that, for my part, I should hate to accept this.”[^13] The other is that the generator is an automaton, an ultimately absurd, blind trier of all possibilities. That is of course no better a homunculus with whom to identify oneself. One does not want to be the generator, then. As Mozart says of his musical ideas: “Whence and how do they come? I do not know and I have nothing to do with it.”*]] Nor does one want to be just the tester, for then one’s chances of being creative depend on the luck one has with one’s collaborator, the generator. The fundamental passivity of the testing role leaves no room for the “creative self.”†]] But we could not have hoped for any other outcome. If we are to have any adequate _analysis_ of creativity, invention, intelligence, it must be one in which intelligence is analyzed into something none of whose parts is intelligence, and at that level of analysis, of course, no “self” worth identifying with can survive.

The mistake in this pessimism lies in confusing explaining with explaining away. Giving a non-question-begging account of _how_ creatures are intelligent can hardly prove that they aren’t intelligent. If we want to catch a glimpse of a creative self, we should look, for instance, at M. Poincaré, for _he_ (and not any _of_ his proper parts) was certainly a genius.

Finally, I cannot resist passing on a wonderful bit of incidental intelligence reported by Hadamard: the Latin verb _cogito_ is derived, as St. Augustine tells us, from Latin words meaning _to shake together_, while the verb _intelligo_ means _to select among_. The Romans, it seems, knew what they were talking about.

## Notes

*]] Skinner explicitly identifies his principle with the Law of Effect in _Science and Human Behavior_ (1953), p. 87.

*]] Cf. Skinner, _Science and Human Behavior_, p. 83. Skinner speaks of food and water _themselves_ being the reinforcers, but commenting on this difference would entail entering the familiar and arid “more peripheral than thou” controversy. A point of Skinner’s that is always worth reiterating, though, is that negative reinforcers are not _punishments_; they are events the cessation of which is positively reinforcing, that is, their cessation _increases_ the probability of recurrence of the behavior followed by cessation.

*]] This is not Simon’s distinction between inner and outer environment in _The Sciences of the Artificial_, but a more restrictive notion. It also has _nothing whatever_ to do with any distinction between the “subjective” or “phenomenal” world and the objective, public world.

*]] See Chapter 4 for detailed confirmation of this and similar vacillation in Skinner.

*]] At a glance it seems that ultimately we want one-shot learning to change the inner environment. In ordinary perspective, we want to account for the fact that if I am trying to solve a problem, _someone can tell me_, once, what won’t work and I can take this lesson to heart immediately.

*]] See Simon, _op. cit._, p. 73, also pp. 90–2. He argues that _efficient_ evolution of design also requires a hierarchical organization of design elements. My treatment of these issues is heavily indebted to Simon’s illuminating and lucid account.

*]] Cf. Chapters 1 and 11. In _Content and Consciousness_ I scorned theories that replaced the little man in the brain with a committee. This was a big mistake, for this is just how one gets to “pay back” the “intelligence loans” of intentionalist theories. Several levels of homuncular discharge are pictured in the flow charts from Colby’s _Artificial Paranoia_ reproduced on pp. 119–121.

*]] Hubert Dreyfus would disagree—see _What Computers Can’t Do: A Critique of Artificial Reason_ (Harper & Row, 1973)—but Dreyfus has not succeeded in demonstrating any a priori limits to generate-and-test systems hierarchically organized, so his contribution to date is salutary scepticism, not refutation.

†]] There is a tradition of overstating the import of successful AI or CS (cognitive simulation) programs (e.g., “programs are theories and successful programs are confirmed theories”). For the moment all we need accept is the minimal claim that a successful program proves a particular sort of capacity to be in principle mechanistically realizable and hence mechanistically explicable. Obviously much more can be inferred from successful programs, but it takes some detailed work to say what, where and why.

*]] Note that this does _not_ commit the AI researcher to the view that “men are Turing machines.” The whole point of generate-and-test strategies in program design is to _permit_ computers to _hit on_ solutions to problems they cannot be _guaranteed_ to solve either because we can prove there is no algorithm for getting the solution or because if there is an algorithm we don’t know it or couldn’t use it. Hence the utility of generate-and-test and heuristics in programming (see also Chapter 13).

*]] Gilbert Harman points out in _Thought_ (New Jersey: Princeton, 1973), that non-deterministic automata can be physically deterministic (if what is random relative to the program is determined in the machine).

*]] Arthur Koestler, in _The Act of Creation_ (1964): p. 559, quotes the behaviorist E. R. Guthrie to this effect, but it is a misquotation, sad to say, for had Guthrie said what Koestler says he said, he would have said something true and important. Perhaps he did say it, but not on the page, or in the book, where Koestler says he said it.

*]] Peter Kivy has drawn my attention to the fact that Mozart scholars agree that the famous letter quoted above is spurious. Mozart, then, may not have been the Valéry type, but the author of the letter did in any case accurately describe one sort of creative process.

†]] This passivity is curiously evoked by Koestler in his account of “underground games” in _The Act of Creation._ It is a tell-tale sign of the inescapability of the principle of selectivity discussed here that Koestler, the arch-enemy of behaviorism, can do no better, when he sets himself the task of composing a rival account of creativity, than to accept the generate-and-test format and then endow the generator with frankly mysterious effects of uncoincidental coincidence.

1.]] Quoted by Jacques Hadamard, in _The Psychology of Inventing in the Mathematical Field_ (Princeton University Press, 1949): p. 30.

2.]] Herbert Simon, _The Sciences of the Artificial_ (M.I.T.): p. 97.

3.]] D. E. Broadbent, _Behaviour_ (University Paperbacks edn., 1961): p. 75.

4.]] Clark Hull, _Principles of Behavior_ (1943): p. 19.

5.]] Cf. also B. F. Skinner, “Behaviorism at Fifty,” in T. W. Wann, ed., _Behaviorism and Phenomenology_ (University of Chicago Press, 1969): 80.

6.]] Broadbent, _op. cit._, p. 56.

7.]] Cf., e.g., Charles Taylor, _The Explanation of Behaviour_ (1964); Chomsky’s reviews of Skinner’s _Verbal Behavior_, in _Language_ (1959); and _Beyond Freedom and Dignity_, in _New York Review of Books_ (Dec. 30, 1971); Broadbent, _op. cit_.

8.]] _Science and Human Behavior_, p. 55.

9.]] “Behaviorism at Fifty,” in Wann, _op. cit._, p. 84.

10.]] See, for instance, the computer-copy of a _particular_ stockbroker in E. A. Feigenbaum and J. Feldman, eds., _Computers and Thought_ (New York: McGraw Hill, 1964).

11.]] Quoted in Hadamard, _op. cit._, p. 16, italics added.

12.]] Quoted in Hadamard, _op. cit._, p 14.

13.]] Arthur Koestler, _The Act of Creation_ (New York: Dell, 1964): p. 164.

A Cure for the Common Code?

We and other creatures exhibit intelligent behavior, and since the regular production of such behavior requires thought, and since thought requires representation, and since nothing can represent except within a system, we must be endowed with and utilize a system of internal representation having its own “grammar” and “vocabulary,” which we might call the language of thought.

This argument has seldom been brought into the open and examined, but behind the scenes it has motivated and flavored large bodies of philosophical doctrine, and strongly influenced research strategies and theories in psychology, linguistics, computer science and neurophysiology. It is worth asking why such an influential move has been so comfortably ignored until recently. It is not plausibly one of those drifts of thought that seem too obvious to need spelling out; perhaps it has been avoided because once one attempts to put the argument in proper and explicit shape, incoherencies, paradoxes, infinite regresses and other alarming implications seem to arise at every turning. Now Jerry Fodor, in _The Language of Thought_, Crowell, 1975, has done us the fine service of propounding and defending a vigorous, unblinking, and ingenious version of the argument. Many of his conclusions seem outrageous, and the threats of incoherency are now close to the surface, but Fodor argues with great persuasiveness that these are in fact parts of the foundation of deservedly esteemed schools of thought in philosophy, cognitive psychology and linguistics. If he has produced an unintended _reductio ad absurdum_ (a possibility he cheerfully admits), some of our favorite edifices will topple with him. He may be wrong, of course, but the challenge is well presented, and since recently thinkers in all the jeopardized fields have been converging on just the perplexities Fodor discusses, the challenge will not be ignored. The main issue treated in Fodor’s book is fast becoming a major topic of interdisciplinary interest, and philosophers of mind who have squeezed the last drops of enlightenment out of the debate over the identity theory or the individuation of actions should be pleased to find here some important and fascinating problems to engage their talents. What is needed is nothing less than a completely general theory of representation, with which we can explain how words, thoughts, thinkers, pictures, computers, animals, sentences, mechanisms, states, functions, nerve impulses, and formal models (_inter alia_) can be said to represent one thing or another. It will not do to divide and conquer here—by saying that these various things do not represent in the same sense. Of course that is true, but what is important is that there is something that binds them all together, and we need a theory that can unify the variety. Producing such a theory is surely a philosophical endeavor, but philosophers must recognize that some of the most useful and suggestive work currently being done on the problems is being done by psychologists, linguists, and workers in artificial intelligence.

For what it is worth, Fodor probably holds uniquely strong professional credentials for the task of consolidating the insights from these fields, for he holds a joint appointment in psychology and philosophy (at M.I.T., a major center for work in linguistics and artificial intelligence) and has made important contributions to experimental psycholinguistics and linguistics in addition to his work in philosophy. Fodor is not beset by the philosophical naiveté of many of his colleagues in psychology and he has as powerful a grasp of current thinking in linguistics and psychology as anyone in philosophy. Indeed, the overall savvy of his book is one of its most striking characteristics, mainly for good but also for ill. There cannot be many readers well equipped or disposed to appreciate all his knowing nudges; the uninitiated will perhaps be the unpersuaded (and unamused) as well. I fear that Fodor’s unfailing high spirits and jocosity may hurt his cause by irritating as many readers as they amuse. I find the book genuinely witty, however, and can only urge those who resent being tickled while engaged in such serious business to make an extra effort to distinguish the medium from the message.

Fodor’s message has three parts. First he describes and promotes a brand of theorizing he calls cognitive psychology, but clearly he means to cast his net wider, and in places narrower, than that term would suggest. The distinguishing mark of this theorizing is the unapologetic utilization of intentional characterizations of processes and “intellectualist” analysis of perception and other “cognitive processes” in terms of information-flow, hypothesis-testing, inference and decision-making. Within its boundaries fall much current psychology, linguistics, artificial intelligence, and some strains of thought in current philosophy. Let us call it neo-cognitivism, for it is not markedly continuous with earlier schools of cognitive psychology, nor is it all clearly psychology. It has developed largely in recognition of the impotence of (psychological and logical) behaviorism, and its inspiration is drawn largely from linguistics, computer science and (come to think of it) the last three hundred years of epistemology. Fodor attempts to establish the credentials of neo-cognitivism by showing how it avoids the doldrums of Rylean logical behaviorism, steers between the Scylla of dualism and the Charybdis of reductionism to emerge as the only straw floating—as Jerome Lettvin once put it. In this first part Fodor has a strong and persuasive case on almost all counts.

Fodor’s second task is to show that this best hope for a confirmed, powerful psychological theory inescapably requires the postulation of internal representational systems. These systems, though designed for computation rather than communication, have structures—and other features—so like those of natural languages that we may—and should—speak of the language of thought: the medium in which the computational transactions are performed that ultimately govern our behavior and the behavior of other intelligent creatures as well. This is the philosophic heart of Fodor’s book, and will receive detailed attention below.

Third, Fodor completes his book with two lengthy chapters purporting to show how evidence from linguistics and psychology establishes answers to an impressive variety of questions about ‘the structure of the internal code’. Having proved the existence of Planet X, he proceeds to detail its climate and geography for us, using data that had been available but hitherto mute. These chapters are undeniably compelling, for every now and then one gets glimmers of the sort of fruitful falling-into-place so seldom encountered in psychology or philosophy of mind. Whereas, for instance, behaviorism has always worn the guise of a properly endorsed method (a “methodology”) in dogged search of results, here we seem to see an abundance of results and tempting hypotheses to test for which we must somehow concoct methodological permission.

For example, linguists have devised a variety of competing formal systems for more or less algorithmically generating or analyzing sentences, and a question the psycholinguist asks is which if any proposed formalism has “psychological reality,” or in other words describes or mirrors real psychological processes occurring in the production or comprehension of sentences. This empirical question is to be settled independently of the elegance or power of the formal systems. (One way of dividing 67 by 12 is to subtract 12 from 67, then subtract 12 from 55, and so forth, while counting the subtractions; another is long division; which if either has “psychological reality” for an individual human calculator is surely an empirical question, and _asking the calculator_ is not the only, or always the best, way of answering the question.) Subtle studies of reaction times, relative difficulty of comprehension, patterns of errors, and so forth often provide satisfyingly clear-cut verdicts on these questions, but often only if we make just the sorts of assumptions about representational machinery Fodor is attempting to vindicate. Whether these investigations will continue to ramify nicely is far from assured, however, and there is an abundance of danger signals for skeptics to make of what they can.

The conclusion Fodor wishes to draw from this examination is bracingly unqualified: “… having a propositional attitude is being in some _computational_ relation to an internal representation.” “Attitudes to propositions are … ‘reduced’ to attitudes to formulae, though the formulae are couched in a proprietary inner code” (p. 198). The inner code is innate, and one’s innate vocabulary of predicates must be sufficient to represent, by logical construction, any predicate of any natural language one can learn. Once one learns such a predicate one may augment one’s inner code with a synonym, as it were, of the natural language predicate and henceforward use this non-native inner word as an abbreviation for the cumbersome truth-functional molecule of native mentalese (p. 152). We are not born with an inner code word for “airplane” but if we couldn’t form at the outset a predicate of inner mentalese at least coextensive with “airplane” we could never learn what “airplane” meant, could never add an “airplane”-synonym to our basic stock. So there is a sense in which one cannot “acquire new concepts” by learning a language, even one’s mother tongue.

All this (and there is more) is hard to swallow, but what are the alternatives? Thinkers as diverse as B. F. Skinner, Norman Malcolm and Hubert Dreyfus have insisted that the very concept of neural systems of representation is a monstrous error. Let us call that the extreme right wing view. On the extreme left, then, would be researchers such as McConnell and Ungar, who take brain-writing so literally that they suppose one might physically extract token sentences of the inner code from one creature and teach another by injection or ingestion. (Ungar reports he has trained cats to fear the dark and then isolated a substance in them, “scotophobin,” which injected into untrained cats causes them to fear the dark!) Middle-of-the-road positions have yet to be formulated in satisfactory detail, but it is safe to say that Fodor has laid claim to a position far to the left of center and is insisting that no less extreme position can provide the foundations for the promising theories of neo-cognitivism.

Let us return to the beginning and examine Fodor’s case in some detail. Fodor takes his first task to be protecting neo-cognitivism from two philosophic threats: Ryle’s attack on intellectualist theorizing, and the physicalist demand that all theoretical terms be reducible somehow to the terms of physics. Fodor sees these as in different ways suggesting the charge that neo-cognitivism is dualistic (a verdict Fodor would view as at least discouraging and probably fatal). The charge is familiar: the characteristic predicates of cognitive psychology are intentional or “mentalistic” idioms, and since mentalism is dualism, cognitive psychology is dualistic. Certainly in the past this has been an influential train of thought; Brentano did after all reintroduce the concept of intentionality precisely as the distinguishing mark of the non-physical, and (though probably not influenced by Brentano) Skinner has for years seen the spectre of dualism in every variety of “mentalistic” theorizing. The claim has not however figured influentially in recent philosophic work in the area. On the contrary, the coexistence of physicalistic doctrine with intentional or mentalistic vocabulary, while perhaps not having received the justification it ought to have, is a typically undefended and unattacked feature of current discussions.

It is a bit curious then that a rebuttal of the dualism charge should find pride of place in Fodor. Perhaps he is addressing the many psychologists who haven’t heard and are still swayed by Skinner’s suspicions. More curious still is Fodor’s choice of Ryle as the initial target of his rebuttal. In _Psychological Explanation_ (1968) Fodor went to great lengths to refute his version of Ryle’s “logical behaviorism” and in 1975 he has still not been able to remove his hands from this tar-baby. Now it is clearer why Ryle should exercise him so, for he has clarified his interpretation with a cute example: “Why are Wheaties (as the ads say) the breakfast of champions?” “Because,” says the dietician, “they contain vitamins, etc.” “Because,” says the Rylean, “they are eaten for breakfast by a non-negligible number of champions.” The former is a “causal” explanation, the latter a “conceptual” explanation and, according to Fodor, Ryle’s view is that the latter explanation is in competition with the former. When a question should have a conceptual answer, it cannot have a causal answer as well. Questions like “What makes the clown’s clowning clever?” have conceptual answers and, according to Fodor’s Ryle, therefore cannot have causal answers—“Alas for the psychology of clever clowning.”

Fodor’s demolition of this notion should be, and is, obvious, and as an interpretation of Ryle it is almost right: the Wheaties example does most effectively illuminate a central Rylean distinction, and there are many passages in _The Concept of Mind_ that could be cited to support the claim that Ryle deserves to be so interpreted. But Ryle does not, as Fodor thinks, offer _The Concept of Mind_ as a psychological theory or as a substitute for psychology or as a proof that psychology can’t be done. Fodor seems to be pointing out that questions like “What makes the clowning clever?” are ambiguous, but he does not see, or accept, the implication that in such cases there are two questions one can be asking. If there are two questions, it can be true that one cannot answer a question requiring a conceptual answer with a causal answer, which is Ryle’s point, without it being true that psychology and philosophy of mind are in competition. Fodor has construed Ryle’s attack on intellectualist theorizing (involving the postulation of inner cognitive processes) as an attack on intellectualist solutions to problems in psychology, while Ryle intended it primarily as an attack on intellectualist solutions to the conceptual problems of philosophy. In fairness to Fodor’s interpretation, Ryle does strongly suggest that cognitivistic or “para-mechanical” hypotheses and the like are bankrupt as psychology as well (see especially the last chapter of _The Concept of Mind_) and against that excessive strain in Ryle’s thought Fodor’s arguments—and indeed the whole book—are a welcome antidote. But in the process of magnifying and rebutting the worst in Ryle, Fodor misconstrues Ryle in another fashion that leads him to overlook a more penetrating Rylean objection to his enterprise. “Ryle assumes,” Fodor tells us, “… that a mentalist must be a dualist; in particular, that mentalism and materialism are mutually exclusive.” Hence the “tendency to see the options of dualism and behaviorism as exhaustive in the philosophy of mind” (p. 4). Were we to replace “Ryle” with “Skinner” and “philosophy of mind” with “psychology” in this passage there would be no quarrel, but in the sense of the term in which behaviorism is the chief rival empirical theory to Fodor’s mentalism, Ryle is no behaviorist but a sort of mentalist himself. Ryle does not attempt, as Skinner does, to explicate mentalistic predicates “(just) in terms of stimulus and response variables” (p. 8). On the contrary, his explications are typically replete with intentional idioms. Ryle’s familiar account of vanity, for instance (whatever its problems) is not that vanity is a disposition to perform certain locomotions, utter certain sounds, respond to certain stimuli, but that it is a disposition to try to make oneself prominent, to ignore criticism, talk about oneself, avoid recalling past failures, “indulge in roseate daydreams about his own successes” (_The Concept of Mind_, p. 86). What kind of behaviorism is that? Not any kind to be found in psychology. Ryle’s disagreements with Fodor are fundamental, but they are not to be discovered by allying Ryle with Skinner.

Perhaps Ryle’s view can again be illuminated by a fanciful example. Suppose someone were benighted enough to think the monthly bank statement he received was a historical description of actual transfers of currency among thousands of labeled boxes in bank vaults. He is informed of an overdraft and puts forward a theory of anti-dollars, vacuums and vortices to explain it. The Rylean explains that _nothing like that_ is what makes it the case that the account is overdrawn and gives a “conceptual” account of the situation. The “logical behaviorist” account of overdrafts is the one we are usually interested in. Of course there is a mechanical story about what happens at the bank that can be told as well, and perhaps knowing it will help us understand the conceptual account, but the two are distinct.

Fodor does not seem to see this point in application to psychology, for he wishes to maintain with regard to the clever clown “that it is the fact that the behavior was caused by such [inner cognitive] events that makes it the kind of behavior it is; that intelligent behavior _is_ intelligent because it has the kind of etiology it has” (p. 3—but see also n. 2 of p. 29, where Fodor qualifies this). This claim burkes Ryle’s distinction and leads Fodor, I hope to show later, to a mistaken account of what makes it the case that something represents something.

Setting aside this difficulty, Fodor has shown, _contra_ Ryle, that there is some real work that the mentalistic terms of cognitive psychology might do, but could they do this work while being faithful to the spirit of materialism? Fodor argues that the reasonable belief in the generality of physics, and the reasonable desire that the various sciences be somehow unified, have engendered unreasonably strong demands that the theoretical predicates of the “special” sciences, and psychology in particular, be “reducible” to the predicates of physics. Fodor’s critique of reductionism and concomitant defense of functionalism is consonant with other recent accounts, especially Putnam’s, but makes important additions of detail to this emerging orthodoxy. The unreasonableness of reductionism is nicely illustrated by a discussion of its application to Gresham’s Law; a very clear account is given of type and token physicalism and natural kinds, and there is an especially useful development of the claim that it is a mistake to try to make the laws of the unreduced sciences exceptionless. We should look for the laws of physics to be exceptionless, but these laws should not, as the reductionist requires, guarantee that the laws of the reduced sciences have no exceptions, but rather provide an explanation of the exceptions encountered. To reconstrue the laws of the special sciences so that their predicates were locked with the predicates of physics would be to abandon the very utility of the predicates that gave birth to the special sciences in the first place.

Fodor offers a specific positive account of the logical relations that may hold between the terms of a special science (say psychology) and a reducing science (either physiology or physics) which goes far toward establishing the proper independence of the former. I think it could go farther. Fodor shows how a special science can be neutral with regard to variation in physical realization, and can tolerate variety in the physical tokenings of its types even within the individual, but I think he unnecessarily rules out the possibility that there could be a law of a special science, even an exceptionless law, where there were no _laws_ of the reducing sciences relating all the tokenings (because the regularities in token sequences could only be described by conditionals with highly disjunctive antecedents and consequents). It seems essential that he allow for and explain this possibility, for it is fundamental to the capacity of well designed systems to “absorb” random or merely fortuitous noise, malfunction, interference. The account Fodor gives does not permit the brain to tolerate typographical errors in the inner code, so far as I can see.

Fodor supposes his arguments obtain methodological permission to use mentalistic predicates in theory construction. Why should we want them? Because it is “self-evident that organisms often believe the behavior they produce to be of a certain kind and that it is often part of the explanation of the way that an organism behaves to advert to the beliefs it has about the kind of behavior it produces” (p. 28). In other words, Fodor does not believe another reasoned obituary of behaviorism would be worth space in his book. Very well, but what, exactly, is “self-evident”? Fodor believes that the everyday, lay explanations of behavior (of both people and beasts) in terms of beliefs and desires are of a piece with the sophisticated information-flow explanations of the neo-cognitivists, so that the self-evident acceptability of “the dog bit me because he thought I was someone else” ensures the inevitable theoretical soundness of something like “the dog’s executive routine initiated the attack subroutine because in the course of perceptual analysis it generated and misconfirmed a false hypothesis about the identity of an object in its environment.” Fodor recognizes that it is a fairly large step from everyday, personal-level intentional explanations to theory-bound sub-personal level intentional explanations but impatiently dismisses the worry that anything important to his enterprise might hinge on how he took the step: “There is, obviously, a horribly difficult problem about what determines what a person (as distinct from his body, or parts of his body) did. Many philosophers care terrifically about drawing this distinction … but … there is no particular reason to suppose that it is relevant to the purposes of cognitive psychology” (p. 52). We shall see.

Fodor’s next task is to show how neo-cognitivist theory is unavoidably committed to a language of thought. He begins by offering three different but related demonstrations, and similar problems attend each. First, Fodor presents a schema for neo-cognitivist theories of “considered action.” Any such theory will suppose the

> agent finds himself in a certain situation (S) … believes that a certain set of behavioral options … are available to him …, computes a set of hypotheticals roughly of the form if B1 is performed in S, then, with a certain probability, C1. … A preference ordering is assigned to the consequences . … The organism’s choice of behavior is determined as a function of the preferences and the probabilities assigned. (p. 28–29)

In other words, a normative decision theory is to be adapted as a natural history of cognitive processes in the organism, and for such a history to be true, agents must “have means for representing their behavior to themselves.” “For, according to the model, deciding is a computational process; the act the agent performs is the consequence of computations defined over representations of possible actions. No representations, no computations. No computations, no model” (p. 31). Moreover, “an infinity of distinct representations must belong to the system” for “there is no upper bound to the complexity of the representation that may be required to specify the behavioral options available to the agent” (p. 31).

Note that this argument assumes there is a clear line between computational processes and other processes, and another between considered action and mere reactivity. Fodor does not intend his argument to apply only to the psychology of human beings, but how plausible is it that a mole or a chicken or a fish is capable of representing behavioral options of unbounded complexity? The famous four F’s (fighting, fleeing, feeding and sexual intercourse) would seem to be a plausible initial tally of options, and even if we allow, say, a dozen variations on each theme, we hardly need a productive representation system to provide internal vehicles for them all, and the process that led to the appropriate “choice” in such a case would not often appear to be computational, unless all processes are. Presumably a diving bell does not compute its equilibrium depth in the water, though it arrives at it by a process of diminishing “corrections.” Does a fish compute its proper depth of operations? Is there an important qualitative difference between the processes in the fish and the diving bell?

Fodor’s way of dealing with these problems is best understood by contrast with the paths not taken. Fodor could have claimed that only the behavior of human beings (and other smart creatures of his choosing) is governed by truly computational processes, or he could have gone to the other extreme and granted the diving bell its computational processes. Or he could have defended an intermediate position along these lines: all creatures of noticeable intelligence make decisions (I who “care terrifically” would insist that at best something decision-like occurs within them) and as we ascend the phylogenetic scale the decision-processes are more and more aptly characterized as computational; and creatures of noticeable intelligence have at least rudimentary representational systems, but only in higher creatures are these systems language-like in being productive or generative. Instead, he adopts the line that there is a radical discontinuity between computational and non-computational processes: “What distinguishes what organisms do … is that a _representation of the rules they follow constitutes one of the causal determinants of their behavior_” (p. 74, n. 15). If Fodor is to distinguish this claim from the other options he must mean that these rules are _explicitly_ represented (not implicitly represented in virtue of functional organization, that is), and this is the radical heart of Fodor’s position. I will discuss some problems with it later.

Fodor’s second demonstration concerns what he calls concept learning: roughly, coming to distinguish and attach importance to some particular class of things or stimuli in one’s environment (e.g., learning about green apples, or learning not to press the bar until the buzzer sounds, or learning to put the red circles in one pile and everything else in another). Fodor claims that “there is only one kind of theory that has ever been proposed for concept learning—indeed, there would seem to be only one kind of theory that is conceivable—and this theory [that concept learning proceeds by hypothesis formation and confirmation] is incoherent unless there is a language of thought” (p. 36). Why? Because the hypotheses formed must be “couched” in representations. A striking point Fodor makes about this is that experiments can distinguish between _logically equivalent_ but “notationally” different formulations of hypotheses in concept learning. The idea is that taking the spades out of a deck of cards is easier, oddly enough, than leaving all the cards that are not spades in the deck. When presented with the latter task if one does not think “in other words …” one’s performance will suffer. Does this consideration, and similar ones, not establish beyond a shadow of a doubt the “psychological reality” of the representations? As in the first case, it all depends on how far Fodor is prepared to descend with his talk of representations. What is somewhat plausible in the case of human beings is not at all plausible in the case of lower animals, and it seems that even insects can achieve _some_ concept learning. Either some very primitive concept learning does not require hypothesis formation and confirmation, or if it all does, some hypotheses are formed but not “couched,” or just about any feature on the inside of a creature can be considered a representation. Certainly the psychological reality of something functioning in some ways rather like a representation is established by the results Fodor cites, but Fodor has prepared a buttered slide for us and anyone who does not want to get on it might well dig in the heels at this point and ask for more details.

Fodor’s third demonstration concerns perception. Here his point is that, as empiricists have insisted (for all the wrong reasons) “the sensory data which confirm a given perceptual hypothesis are typically internally represented in a vocabulary that is impoverished compared to the vocabulary in which the hypotheses themselves are couched” (p. 44). For instance, the empiricists would say that my hypothesis is that there is an apple out there, and my data are that I seem to see this red round patch. Fodor likes the idea of perception proceeding by a series of computational processes taking descriptions in one vocabulary and using them to confirm hypothesized descriptions in another vocabulary, and the main thing he sees wrong with empiricist versions of this is their penchant for couching the given in the “theory-free language of qualia” rather than the “theory-laden language of values of physical parameters” (p. 48). What is given is the excitation of a sensory mechanism sensitive to a physical property. “Hence, there is no reason to believe that the organism cannot be mistaken about what sensory descriptions apply in any given case” (p. 48). But here Fodor seems to me to have lost track of the important distinction between the content of a signal to the system it informs, and the content we on the outside can assign it when we describe the signal and the system of which it is a part. For instance, one badly misconceives the problem of perception if one views the retinal receptors as “telling” the first level of hypotheses testers “red wavelength light at location L again,” for that level does not utilize or understand (in _any_ impoverished sense) information of that sort. What it gets in the way of data are at best reports with uninterpreted dummy predicates (“it is intensely F at location L again”) and out of these it must confirm its own dummy hypotheses.*]] In fact is either way of talking appropriate? The _vocabulary_ of the signals is not something that is to be settled by an examination of tokens, at least at this level, and when we turn to indirect evidence of “psychological reality” any evidence we turn up will perforce be neutral between interpreted and uninterpreted predicates.

What content is to be assigned to events in the nervous system subserving perception? That, I take it, is a rather important question for cognitive psychology to answer. It cannot be answered, I submit, until one gets quite careful about who (or what, if anything) has access to the candidate representation—for whom or for what the thing in question is a representation. As Michael Arbib has suggested, what the frog’s eye tells the frog’s brain is not what the frog’s eye tells the frog.

Fodor rather nonchalantly dismisses such distinctions. Why are they important? Suppose we make the following extension of his main argument. The only psychology that could possibly succeed is neo-cognitivist, which requires the postulation of an internal system of representations. However, nothing is intrinsically a representation of anything; something is a representation only _for_ or _to_ someone; any representation or system of representations requires at least one _user_ of the system who is external to the system. Call such a user an _exempt agent._ Hence, in addition to a system of internal representations, neo-cognitivism requires the postulation of an inner exempt agent or agents—in short, undischarged homunculi. Any psychology with undischarged homunculi is doomed to circularity or infinite regress, hence psychology is impossible (see Chapters 5 and 7 of this volume, where these claims are expanded and examined at length).

The problem is an old one. Hume wisely shunned the notion of an inner self that would intelligently manipulate the ideas and impressions, but this left him with the necessity of getting the ideas to “think for themselves.” His associationistic couplings of ideas and impressions, his pseudo-chemical bonding of each idea to its predecessor and successor, is a notorious non-solution to the problem. Fodor’s analogous problem is to get the internal representations to “understand themselves,” and one is initially inclined to view Hume’s failure as the harbinger of doom for all remotely analogous enterprises. But perhaps the _prima facie_ absurd notion of self-understanding representations is an idea whose time has come, for what are the “data structures” of computer science if not just that: representations that understand themselves? In a computer, a command to dig goes straight to the shovel, as it were, eliminating the comprehending and obeying middleman. Not _straight_ to the shovel, of course, for a lot of sophisticated switching is required to get the right command going to the right tools, and for some purposes it is illuminating to treat parts of this switching machinery as analogous to the displaced shovellers, subcontractors and contractors. The beauty of it all, and its importance for psychology, is precisely that it promises to solve Hume’s problem by giving us a model of vehicles of representation that function without exempt agents for whom they are ploys. Alternatively, one could insist that the very lack of exempt agents in computers to be the users of the putative representations shows that computers do not contain representations—real representations—at all, but unless one views this as a rather modest bit of lexicographical purism, one is in danger of discarding one of the most promising conceptual advances ever to fall into philosophers’ hands.

Fodor almost parenthetically makes these points (in a footnote on p. 74, where he roundly rebuts an ill-considered version of the homunculus argument of mine). He is justly unafraid of homunculi, for they are at most just picturesquely described parts of the switching machinery that ensures the functional roles of the inner messages, but fails to recognize that they still play the theoretical role of fixing the “topic” and “vocabulary” of the messages they communicate. If viewing messages of the inner code as self-understanding representations in this fashion can save Fodor’s enterprise from incoherence—and in principle I think it can—it does so by adding constraints to the notion of an internal representation system that emphasize rather than eliminate the distinction between personal level attributions of beliefs and desires and sub-personal level attributions of content to intrasystemic transactions. If there is any future for internal systems of representation it will not be for languages of thought that “represent our beliefs to us,” except in the most strained sense. Fodor notices the strain (p. 52) but decides to tolerate it. The result, for all its vividness, is at least misleading in a way that has an analogy in the history of science. The problem of genetic inheritance used to look all but insoluble. Did the sperm cell contain a tiny man, and if so did the tiny man have sperm cells containing tiny men and so forth _ad infinitum?_ Or did the sperm cell contain a picture or description of a human being, and if so, what looked at the picture or read the description? The truth turns out to be scarcely less marvelous than the “absurd” speculations, what with self-reading, self-duplicating codes and their supporting machinery, but anyone who had insisted all along that somehow the mother finds out from the sperm what sort of baby the father wants would not have been pointing in just the right direction. (This sidelong glance at DNA serves the additional purpose of reminding the skeptics who view the contraptions of artificial intelligence as hopelessly inefficient and ‘inorganic’ that nature has proved not to be stingy when it comes to micro-engineering solutions to hard problems.)

Earlier I claimed that Fodor’s view of computational processes commits him to a radical view of representation. The problems with this hard line on the psychological reality of explicit representations are apparent—indeed, are deliberately made apparent, to Fodor’s credit—in his discussion of language learning. His argument is that the process of learning the meaning of a word, even the initial words of one’s native tongue, is and must be a process of hypothesis formation and confirmation, and in particular,

> among the generalizations about a language that the learner must hypothesize and confirm are some which determine the extensions of the predicates of that language. A generalization that effects such a determination is, by stipulation, _a truth rule_. (p. 59)

For instance, the truth rule for “is a chair” is “˹y is a chair˺ is true iff*]] Gx” where “G” is a predicate of one’s internal code. Fodor seems to think that the only hypotheses which could _determine_ the extension of a natural language predicate would have to be confirmed hypotheses explicitly about that predicate and having the explicit form of a truth rule. But to play Fodor’s own game for a moment, couldn’t a child learn _something that determined_ the extension of “is a chair” by _disconfirming_ the following hypotheses (and others):

- ˹x is a chair˺ is true iff x is red
- ˹x is a chair˺ is true iff x is in the living room
- ˹x is a chair˺ is true iff x has a cushion

and, perhaps, confirming—even _mis_confirming—others, e.g., “˹x is a chair˺ is true if x is this object here or that object there,” and “˹x is a chair˺ is true only if x would support Daddy’s weight,” without ever _explicitly_ representing a confirmed truth rule for “is a chair”? I suspect that Fodor’s reply would be that to learn something determining the extension of “is a chair” the child must explicitly conjoin all the confirmed hypotheses and the negations of the disconfirmed hypotheses and that somehow this amounts to the confirmation of the explicit truth rule for “is a chair,” but aside from the implausibility of this as a story of real computational processes (but remember DNA) one wants to know what could conceivably count against the presumably empirical claim that whatever could determine the extension of a predicate has the explicit form of a truth rule, if this example did not.

Perhaps Fodor has gratuitously overstated his own best case, for it seems as if he is committed to the impossible view that only explicit representation is representation, and (roughly) nothing can be believed, thought about or learned without being explicitly represented.

> That is, one might think of cognitive theories as filling in explanation schema[ta] of, roughly, the form: _having the attitude R to proposition P is contingently identical to being in computational relation C to the formula (or sequence of formulae) F_. A cognitive theory, insofar as it was both true and general, would presumably explain the productivity of propositional attitudes by entailing infinitely many substitution instances of this schema: one for each of the propositional attitudes that the organism can entertain. (p. 77)

Perhaps we “entertain” propositional attitudes either seriatim or at least in manageably small numbers at any one time, but the propositional attitudes we _have_ far outstrip those we (in some sense) actively entertain. For instance, it should come as no news to any of you that zebras in the wild do not wear overcoats, but I hazard the guess that it _hadn’t occurred_ to any of you before just now. We all have believed it for some time but were not born believing it, so we must have come to believe it between birth and, say, age fifteen, but it is not at all plausible that this is a hypothesis any of us has explicitly formed or confirmed in our childhood, even unconsciously. It is not even plausible that having formed and confirmed other hypotheses entailing this fact about zebras, we (in our spare time?) explicitly _computed_ this implication.

Fodor does seem to be committed to some such view as this, however. He backs into this corner by underestimating the viability of what he takes to be the only alternative, which he characterizes as a dispositional behavioral analysis of propositional attitudes. “A number of philosophers who ought to know better do, apparently, accept such views” Fodor says, never doubting that he has seen clearly to the very heart of such silliness. His version of dispositional analysis is so simplistic, however, that he thinks the notion is adequately buried by a quip: “Pay me enough and I will stand on my head iff you say chair. But I know what ‘is a chair’ means all the same” (p. 63). It is of course true that the arduous piecemeal composition of dispositional definitions of propositional attitudes would be a bootless methodology for psychology (Ryle knew better than to attempt to say, precisely, just what his “multi-track” dispositions were), but if, as Fodor supposes, the representation-talk of cognitive psychology ultimately gets vindicated by such ploys as computer modeling of cognitive systems and processes, he must be committed in spite of himself to a version of Rylism. For a computer program is just a very complicated specification of a multitrack disposition (a disposition to be disposed under conditions A, B, C to be disposed under conditions X, Y, Z to be disposed … to give output O … etc.). Notationally distinct but equivalent programs are equivalent precisely in that they determine the same multi-track disposition.

Suppose research reveals all the psychologically real computational processes in Mary, and artificial intelligencers program a robot, Ruth, whose internal processes “model” Mary’s as perfectly as you like. Suppose that Mary believes that _p._ So then does Ruth. But suppose the artificial intelligencers then give another robot, Sally, a program equivalent to Ruth’s, but notationally and computationally different. Sally may not be a good psychological model of Mary, but Sally, like Ruth and Mary, believes that _p._*]] That is, the ascription of all Mary’s beliefs and desires (etc.) to Sally will be just as predictive as their ascription to Ruth so far as prediction of action goes. Sally’s response delays, errors, and the like may not match Mary’s, but this is not what belief ascription is supposed to predict or explain (cf. Fodor, p. 123). If one agrees with Fodor that it is the job of cognitive psychology to map the psychologically real processes in people, then since the ascription of belief and desire is only indirectly tied to such processes, one might well say that beliefs and desires are not the proper objects of study of cognitive psychology. Put otherwise, cognitivist theories are or should be theories of the subpersonal level, where beliefs and desires disappear, to be replaced with representations of other sorts on other topics.

But unless I am misreading Fodor, he will have none of this. His position simply is that since believing that snow is white could not be having a disposition to behave, it must be having a token of the mentalese translation of “snow is white” installed in some wonderful way in one’s head. Perhaps I am misreading him by interpreting “being in a computational relation to a formula of the inner code” as implying the existence of a real token of that formula in some functionally characterized relation to the rest of the machinery, but the weaker alternative, _viz.,_ that one is in a computational relation to a formula if one _can or would produce or use_ a token of that formula in some way under some circumstance, invokes dispositionalism of just the sort Fodor has presumably forsworn.

None of this is to say that neural representations, even tokens of brain-writing, are impossible. It is not even to deny that the existence of such representations is a necessary condition for cognition. It may well turn out to be. But Fodor, by making explicit coding _criterial_ for representation or contentfulness, has committed the very sin he imputes to Ryle: he has confused a conceptual answer with a causal answer. Like neo-cognitivists generally, Fodor wants to be able to assign content to events or other features of systems, to treat them as information-bearers or messages. What makes it the case ultimately that something in this sense represents something within a system is that it has a function within the system, in principle globally specifiable.*]] To say that it has the function of bearing a certain message or transmitting certain information is to talk in circles, but often in useful circles for the time being. Content is a function of function, then, but not every structure can realize every function, can reliably guarantee the normal relationships required. So function is a function of structure. There are, then, strong indirect structural constraints on things that can be endowed with content. If our brains were as homogeneous as jelly we could not think. Fodor, however, makes a direct leap from content to structure and seems moreover to make structure in the end criterial for content.

On his view a prescriptive theory (e.g., natural deduction or decision theory) can be predictive of behavior only if it is descriptive of inner processes. When we predict and explain the behavior of a system at the intentional level _our_ calculations have a certain syntactic structure: to oversimplify, they are formal proofs or derivations, e.g., of descriptions of best actions to take given certain beliefs and preferences. We predict that the physical states or events to which we assign the premises as formulae will _cause_ those states or events whose formulae are the later lines of our calculations (see, e.g., p. 73). Fodor seems to suppose that the only structures that could guarantee and explain the predictive power of our intentionalistic calculations (and permit us to assign formulae to states or events in a principled way) _must_ mirror the syntax of those calculations. This is either trivially true (because the “syntactic” structure of events or states is defined simply by their function) or an empirical claim that is very interesting, not entirely implausible, and as yet not demonstrated or even argued for, so far as I can tell. For instance, suppose hamsters are interpretable as good Bayesians when it comes to the decisions they make. Must we in principle be able to find some saliencies in the hamsters’ controls that are interpretable as tokens of formulae in some Bayesian calculus? If that is Fodor’s conclusion, I don’t see that he has given it the support it needs, and I confess to disbelieving it utterly.

In a recent conversation with the designer of a chess-playing program I heard the following criticism of a rival program: “It thinks it should get its queen out early.” This ascribes a propositional attitude to the program in a very useful and predictive way, for as the designer went on to say, one can usually count on chasing that queen around the board. But for all the many levels of explicit representation to be found in that program, nowhere is anything roughly synonymous with “I should get my queen out early” explicitly tokened. The level of analysis to which the designer’s remark belongs describes features of the program that are, in an entirely innocent way, emergent properties of the computational processes that have “engineering reality.” I see no reason to believe that the relation between belief-talk and psychological-process talk will be any more direct.

Are all these doubts about Fodor’s radical view swept away by the material in the second half of this book, where evidence is adduced about the structure, vocabulary and utilization of the inner code? The challenge of these chapters to the skeptic is to find a way of recasting what cannot be denied in them in terms less radical than Fodor’s. I do not see that this cannot be done, but saying it is not doing it, and doing it would require a monograph. Fodor’s account of the inner code in action is packed with detail and bold speculation, and is supported by a variety of elegant experiments and ingenious arguments. Fodor puts together a more or less Gricean theory of communication and a more or less Chomskyan view of the relation between surface features of utterances and deeper levels, but comes out forcefully against semantic primitives (at least in their familiar role in the production and comprehension of sentences). He defends _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett as inner representational vehicles in addition to his code formulae, and claims to show that the inner code can represent its own representations and has a vocabulary about as rich as that of English—to mention a few highlights. There are a few dubious links in the argumentation (e.g., Fodor’s cat and mouse example on p. 142 seems obviously mis-analyzed, but it may not matter), but time and again Fodor succeeds, in my estimation, in parrying the “obvious” philosophical objections. One exception is in his account of communication. Unless I am reading him too literally, he seems committed to the view that for A to communicate verbally with B, A and B must not only share a natural language but have the same version of mentalese as well. Once again this is a claim that might be trivial or might be almost certainly false, and we can’t tell until Fodor is more explicit.

Faulting Fodor for not being sufficiently explicit in this instance is a bit ungenerous, for Fodor has offered a detailed theory in an area hitherto bereft of detailed theories, and has been more explicit than anybody else about many of the murky issues. The book is exceptionally clear, with excellent summaries of arguments and conclusions at just the right places. The view Fodor has put forward is a remarkably full view; seldom have stands on so many different issues been so staunchly taken in this area, and even where I think he is wrong, it is usually the crispness of his expression that suggests for the first time just exactly what is wrong. Fodor challenges us to find a better theory, and I fully expect that challenge to be met, but when better theories emerge they will owe a good deal to Fodor’s reconnaissance.

## Notes

*]] Cf. J. J. C. Smart, _Philosophy and Scientific Realism_ (London: Routledge & Kegan Paul, 1963), on “topic-neutral reports.” The epistemic status of reports with uninterpreted predicates and reports with qualia-predicates would seem to be the same.

*]] The term “iff” is logicians’ shorthand for “if and only if.”

*]] Perfect equivalence of programs is a very strong condition. I would hold it is a sufficient but not necessary condition for sharing intentional characterizations.

*]] Ignoring for the moment the normative element in all intentional attributions (see Chapter 1 of this volume).

Artificial Intelligence as Philosophy and as Psychology

Philosophers of mind have been interested in computers since their arrival a generation ago, but for the most part they have been interested only in the most abstract questions of principle, and have kept actual machines at arm’s length and actual programs in soft focus. Had they chosen to take a closer look at the details I do not think they would have found much of philosophic interest until fairly recently, but recent work in Artificial Intelligence, or AI, promises to have a much more variegated impact on philosophy, and so, quite appropriately, philosophers have begun responding with interest to the bold manifestos of the Artificial Intelligentsia.[^1] My goal in this chapter is to provide a sort of travel guide to philosophers pursuing this interest. It is well known that amateur travelers in strange lands often ludicrously miscomprehend what they see, and enthusiastically report wonders and monstrosities that later investigations prove never to have existed, while overlooking genuine novelties of the greatest importance. Having myself fallen prey to a variety of misconceptions about AI, and wasted a good deal of time and energy pursuing chimaeras, I would like to alert other philosophers to some of these pitfalls of interpretation. Since I am still acutely conscious of my own amateur status as an observer of AI, I must acknowledge at the outset that my vision of what is going on in AI, what is important and why, is almost certainly still somewhat untrustworthy. There is much in AI that I have not read, and much that I have read but not understood. So traveler, beware; take along any other maps you can find, and listen critically to the natives.

The interest of philosophers of mind in Artificial Intelligence comes as no surprise to many tough-minded experimental psychologists, for from their point of view the two fields look very much alike: there are the same broad generalizations and bold extrapolations, the same blithe indifference to the hard-won data of the experimentalist, the same appeal to the deliverances of casual introspection and conceptual analysis, the aprioristic reasonings about what is impossible in principle or what must be the case in psychology. The only apparent difference between the two fields, such a psychologist might say, is that the AI worker pulls his armchair up to a console. I will argue that this observation is largely justified, but should not in most regards be viewed as a criticism. There is much work for the armchair psychologist to do, and a computer console has proven a useful tool in this work.

Psychology turns out to be very difficult. The task of psychology is to explain human perception, learning, cognition, and so forth in terms that will ultimately unite psychological theory to physiology in one way or another, and there are two broad strategies one could adopt: a _bottom-up_ strategy that starts with some basic and well-defined unit or theoretical atom for psychology, and builds these atoms into molecules and larger aggregates that can account for the complex phenomena we all observe, or a _top-down_ strategy that begins with a more abstract decomposition of the highest levels of psychological organization, and hopes to analyze these into more and more detailed smaller systems or processes until finally one arrives at elements familiar to the biologists. It is a commonplace that both endeavors could and should proceed simultaneously, but there is now abundant evidence that the bottom-up strategy in psychology is unlikely to prove very fruitful. The two best developed attempts at bottom-up psychology are stimulus-response behaviorism and what we might call “neuron signal physiological psychology,” and both are now widely regarded as stymied, the former because stimuli and responses prove not to be perspicuously chosen atoms, the latter because even if synapses and impulse trains are perfectly good atoms, there are just too many of them, and their interactions are too complex to study once one abandons the afferent and efferent peripheries and tries to make sense of the crucial center (see Chapters 4 and 5).[^2] Bottom-up strategies have not proved notably fruitful in the early development of other sciences, in chemistry and biology for instance, and so psychologists are only following the lead of “mature” sciences if they turn to the top-down approach. Within that broad strategy there are a variety of starting points that can be ordered in an array. Faced with the practical impossibility of answering the empirical questions of psychology by brute inspection (how _in fact_ does the nervous system accomplish _X_ or _Y_ or Z?), psychologists ask themselves an easier preliminary question:

How could any system (with features _A, B,_ C, …) possibly accomplish _X_?*]]

This sort of question is easier because it is less “empirical”; it is an _engineering_ question, a quest for a solution _(any_ solution) rather than a discovery. Seeking an answer to such a question can sometimes lead to the discovery of general constraints on all solutions (including of course nature’s as yet unknown solution), and therein lies the value of this style of aprioristic theorizing. Once one decides to do psychology this way, one can choose a degree of empirical difficulty for one’s question by filling in the blanks in the question schema above.[^3] The more empirical constraints one puts on the description of the system, or on the description of the requisite behavior, the greater the claim to “psychological reality” one’s answer must make. For instance, one can ask how any neuronal network with such-and-such physical features could _possibly_ accomplish human color discriminations, or we can ask how any finite system could _possibly_ subserve the acquisition of a natural language, or one can ask how human memory could _possibly_ be so organized so as to make it so relatively easy for us to answer questions like “Have you ever ridden an antelope?,” and so relatively hard to answer “What did you have for breakfast last Tuesday?” Or, one can ask, with Kant, how anything at all could _possibly_ experience or know anything at all. Pure epistemology thus viewed, for instance, is simply the limiting case of the psychologists’ quest, and is _prima facie_ no less valuable _to psychology_ for being so neutral with regard to empirical details. Some such questions are of course better designed to yield good answers than others, but _properly carried out,_ any such investigation can yield constraints that bind all more data-enriched investigations.

AI workers can pitch their investigations at any level of empirical difficulty they wish; at Carnegie Mellon University, for instance, much is made of paying careful attention to experimental data on human performance, and attempting to model human performance closely. Other workers in AI are less concerned with that degree of psychological reality and have engaged in a more abstract version of AI. There is much that is of value and interest to psychology at the empirical end of the spectrum, but I want to claim that AI is better viewed as sharing with traditional epistemology the status of being a most general, most abstract asking of the top-down question: how is knowledge possible?*]] It has seemed to some philosophers that AI cannot be plausibly so construed because it takes on an additional burden: it restricts itself to _mechanistic_ solutions, and hence its domain is not the Kantian domain of all possible modes of intelligence, but just all possible mechanistically realizable modes of intelligence. This, it is claimed, would beg the question against vitalists, dualists and other anti-mechanists. But as I have argued elsewhere, the mechanism requirement of AI is not an additional constraint of any moment, for if psychology is possible at all, and if Church’s Thesis is true, the constraint of mechanism is no more severe than the constraint against begging the question in psychology, and who would wish to evade that? (See Chapter 5.)[^4]

So I am claiming that AI shares with philosophy (in particular, with epistemology and philosophy of mind) the status of most abstract investigation of the principles of psychology. But it shares with psychology _in distinction from philosophy_ a typical tactic in _answering_ its questions. In AI or cognitive psychology the typical attempt to answer a _general_ top-down question consists in designing a _particular_ system that does, or appears to do, the relevant job, and then considering which of its features are necessary not just to one’s particular system but to any such system. Philosophers have generally shunned such elaborate system-designing in favor of more doggedly general inquiry. This is perhaps the major difference between AI and “pure” philosophical approaches to the same questions, and it is one of my purposes here to exhibit some of the relative strengths and weaknesses of the two approaches.

The system-design approach that is common to AI and other styles of top-down psychology is beset by a variety of dangers of which these four are perhaps the chief:

- (1) designing a system with component subsystems whose stipulated capacities are _miraculous_ given the constraints one is accepting. (E.g., positing more information-processing in a component than the relevant time and matter will allow, or, at a more abstract level of engineering incoherence, positing a subsystem whose duties would require it to be more “intelligent” or “knowledgeable” than the supersystem of which it is to be a part.
- (2) mistaking _conditional_ necessities of one’s particular solution for completely general constraints (a trivial example would be proclaiming that brains use LISP; less trivial examples require careful elucidation).
- (3) restricting oneself artificially to the design of a subsystem (e.g., a depth perceiver or sentence parser) and concocting a solution that is systematically incapable of being grafted onto the other subsystems of a whole cognitive creature.
- (4) restricting the performance of one’s system to an artificially small part of the “natural” domain of that system and providing no efficient or plausible way for the system to be enlarged.

These dangers are altogether familiar to AI, but are just as common, _if harder to diagnose conclusively,_ in other approaches to psychology. Consider danger (1): both Freud’s ego subsystem and J. J. Gibson’s invariance-sensitive perceptual “tuning forks” have been _charged_ with miraculous capacities. Danger (2): behaviorists have been _charged_ with illicitly extrapolating from pigeon-necessities to people-necessities, and it is often claimed that what the frog’s eye tells the frog’s brain is not at all what the person’s eye tells the person’s brain. Danger (3): it is notoriously hard to see how Chomsky’s early _syntax-driven_ system could interact with semantical components to produce or comprehend purposeful speech. Danger (4): it is hard to see how some models of nonsense-syllable rote memorization could be enlarged to handle similar but more sophisticated memory tasks. It is one of the great strengths of AI that when one of its products succumbs to any of these dangers this can usually be quite conclusively demonstrated.

I now have triangulated AI with respect to both philosophy and psychology (as my title suggested I would): AI can be (and should often be taken to be) as abstract and “unempirical” as philosophy in the questions it attempts to answer, but at the same time, it should be as explicit and particularistic in its models as psychology at its best. Thus one might learn as much of value to psychology or epistemology from a _particular_ but highly _un_realistic AI model as one could learn from a detailed psychology of, say, Martians. A good psychology of Martians, however unlike us they might be, would certainly yield general principles of psychology or epistemology applicable to human beings. Now before turning to the all important question: “What, so conceived, has AI accomplished?,” I want to consider briefly some misinterpretations of AI that my sketch of it so far does not protect us from.

Since we are viewing AI as a species of top-down cognitive psychology, it is tempting to suppose that the decomposition of function in a computer is intended by AI to be somehow isomorphic to the decomposition of function in a brain. One learns of vast programs made up of literally billions of basic computer events and somehow so organized as to produce a simulacrum of human intelligence, and it is altogether natural to suppose that since the brain is known to be composed of billions of tiny functioning parts, and since there is a _gap of ignorance_ between our understanding of intelligent human behavior and our understanding of those tiny parts, the ultimate, millennial goal of AI must be to provide a hierarchical breakdown of parts in the computer that will mirror or be isomorphic to some hard-to-discover hierarchical breakdown of brain-event parts. The familiar theme of “organs made of tissues made of cells made of molecules made of atoms” is to be matched, one might suppose, in electronic hardware terms. In the thrall of this picture one might be discouraged to learn that some functional parts of the nervous system do not seem to function in the digital way the atomic functioning parts in computers do. The standard response to this worry would be that one had looked too deep in the computer (this is sometimes called the “grain problem”). The computer is a digital device at bottom, but a digital device can simulate an “analogue” device to any degree of continuity you desire, and at a higher level of aggregation in the computer one may find the analogue elements that are mappable onto the non-digital brain parts. As many writers have observed,[^5] we cannot gauge the psychological reality of a model until we are given the commentary on the model that tells us which features of the model are intended to mirror real saliencies in nature, and which are either backstage expediters of the modeling or sheer inadvertent detail. (In the Eighteenth Century, scientists built beautiful brass clockwork models of the solar system called orreries. The _gears_ of an orrery are not intended to represent anything real in the firmament, nor are the reflections of one brass globe in another.) When one looks at AI programs in detail one sees mountains of digital calculation or “number crunching”; if this looks terribly unbiological, this is to be excused since that part of the model’s structure is to be viewed as backstage, not part of the model we are supposed to compare with nature.

This story is fine in principle, I think, and the day may come when this imagined isomorphism down to the neuronal level (but not backstage) is proclaimed, but in fact we see nothing remotely like this in current AI work. We see instead models that diverge from nature _very_ early on the downward path. AI programs typically model relatively high-level cognitive features, such as concept learning or revision of goals or beliefs, and it does indeed often take millions of basic computer events to simulate just a handful of these _intended model features—_but no one really knows or cares what these basic events are, for no one supposes that _any_ of the saliencies in the mass of activity that subserves the intended features are to be taken seriously as psychology or physiology. The psychological reality of an AI program is thus usually intended to be only skin deep; only the highest levels of program description and commentary are to be counted as the model.

The _thin_ness of the psychological reality of AI models is obscured for the uninitiated by a particularly misleading feature of most of the AI programs that get into the limelight. What especially attracts audiences and evokes wonder is the marvelous verisimilitude of some of the programs. Consider this excerpt of conversation (minus the programmer’s excellent commentary) between Terry Winograd’s SHRDLU and its interlocutor. SHRDLU is a program that lives in an imaginary “block world” where it can manipulate colored blocks on a table in response to commands and answer questions about its life in this world. Computer responses are capitalized.

## 

`Had you touched any pyramid before you put the green one on the little cube?`  
`YES, THE GREEN ONE.`  
`When did you pick it up?`  
`WHILE I WAS STACKING UP THE RED CUBE, A LARGE RED BLOCK AND A LARGE GREEN CUBE.`  
`Why?`  
`TO GET RID OF IT.`  
`Why did you do that?`  
`TO CLEAR OFF THE RED CUBE.`  
`Why did you clear off that cube?`  
`TO PUT IT ON A LARGE GREEN CUBE.`  
`Why did you do that?`  
`BECAUSE YOU ASKED ME TO.``[^6]`

When SHRDLU replies to the last question with “Because you asked me to,” one can hardly help being struck by the apparent _humanity_ of the response—“SHRDLU is _one of_ us!” it is tempting to exclaim. If one is naive, one will be awestruck; if one is not, one will still be titillated by the illusion, for that is largely what it is. SHRDLU’s response, though perfectly appropriate to the occasion (and not by coincidence!) is “canned.” Winograd has simply given SHRDLU this whole sentence to print at times like these. If a child gave SHRDLU’s response we would naturally expect its behavior to manifest a general capacity which might also reveal itself by producing the response: “Because you told me to,” or, “Because that’s what I was asked to do,” or on another occasion: “Because I felt like it,” or “Because your assistant told me to,” but these are dimensions of subtlety beyond SHRDLU.[^7] Its behavior is remarkably versatile, but it does not reveal a rich knowledge of interpersonal relations, of the difference between requests and orders, of being cooperative with other people under appropriate circumstances. (It should be added that Winograd’s paper makes it very explicit where and to what extent he is canning SHRDLU’s responses, so anyone who feels cheated by SHRDLU has simply not read Winograd. Other natural language programs do not rely on canned responses, or rely on them to a minimal extent.)

The fact remains, however, that much of the antagonism to AI is due to resentment and distrust engendered by such legerdemain. Why do AI people use these tricks? For many reasons. First, they need to get some tell-tale response back from the program and it is as easy to can a mnemonically vivid and “natural” response as something more sober, technical and understated (perhaps: “REASON: PRIOR COMMAND TO DO THAT”). Second, in Winograd’s case he was attempting to reveal the _minimal_ conditions for correct analysis of certain linguistic forms (note all the “problems” of pronominal antecedents in the sentences displayed), so “natural” language _output_ to reveal correct analysis of natural language _input_ was entirely appropriate. Third, AI people put canned responses in their programs because it is fun. It is fun to amuse one’s colleagues, who are not fooled of course, and it is especially fun to bamboozle the outsiders. As an outsider, one must learn to be properly unimpressed by AI verisimilitude, as one is by the chemist’s dazzling forest of glass tubing, or the angry mouths full of teeth painted on World War II fighter planes. (Joseph Weizenbaum’s famous ELIZA program,[^8] the computer “psychotherapist” who apparently listens so wisely and sympathetically to one’s problems, is intended in part as an antidote to the enthusiasm generated by AI verisimilitude. It is almost all clever canning, and is not a psychologically realistic model of anything, but rather a demonstration of how easily one can be gulled into attributing too much to a program. It exploits syntactic landmarks in one’s input with nothing approaching genuine understanding, but it makes a good show of comprehension nevertheless. One might say it was a plausible model of a Wernicke’s aphasic, who can babble on with well-formed and even semantically appropriate responses to his interlocutor, sometimes sustaining the illusion of comprehension for quite a while.)

The AI community pays a price for this misleading if fascinating fun, not only by contributing to the image of AI people as tricksters and hackers, but by fueling more serious misconceptions of the point of AI research. For instance, Winograd’s real contribution in SHRDLU is _not_ that he has produced an English speaker and understander that is psychologically realistic at many different levels of analysis (though that is what the verisimilitude strongly suggests, and what a lot of the fanfare—for which Winograd is not responsible—has assumed), but that he has explored some of the deepest demands on any system that can take direction (in a natural language), plan, change the world and keep track of the changes wrought or contemplated, and in the course of this exploration he has clarified the problems and proposed ingenious and plausible _partial_ solutions to them. The real contribution in Winograd’s work stands quite unimpeached by the perfectly true but irrelevant charge that SHRDLU doesn’t have a _rich_ or human understanding of most of the words in its very restricted vocabulary, or is terribly slow.

In fact, paying so much attention to the performance of SHRDLU (and similar systems) reveals a failure to recognize that AI programs are not _empirical_ experiments but _thought_-experiments prosthetically regulated by computers. Some AI people have recently become fond of describing their discipline as “experimental epistemology.” This unfortunate term should make a philosopher’s blood boil, but if AI called itself thought-experimental epistemology (or even better: _Gedanken-_experimental epistemology) philosophers ought to be reassured. The questions asked and answered by the thought-experiments of AI are about whether or not one can obtain certain sorts of information processing—recognition, inference, control of various sorts, for instance—from certain sorts of designs. Often the answer is no. The process of elimination looms large in AI. Relatively plausible schemes are explored far enough to make it clear that they are utterly incapable of delivering the requisite behavior, and learning this is important progress, even if it doesn’t result in a mind-boggling robot.

The hardware realizations of AI are almost gratuitous. Like dropping the cannonballs off the Leaning Tower of Pisa, they are demonstrations that are superfluous to those who have understood the argument, however persuasive they are to the rest. Are computers then irrelevant to AI? “In principle” they are irrelevant (in the same sense of “in principle,” diagrams on the blackboard are in principle unnecessary to teaching geometry), but in practice they are not. I earlier described them as “prosthetic regulators” of thought-experiments. What I meant was this: it is notoriously difficult to keep wishful thinking out of one’s thought-experiments; computer simulation _forces_ one to recognize all the costs of one’s imagined design. As Pylyshyn observes, “What is needed is … a technical language with which to discipline one’s imagination.”[^9] The discipline provided by computers is undeniable (and especially palpable to the beginning programmer). It is both a good thing—for the reasons just stated—and a bad thing. Perhaps you have known a person so steeped in, say, playing bridge, that his entire life becomes in his eyes a series of finesses, end plays and cross-ruffs. Every morning he draws life’s trumps and whenever he can see the end of a project he views it as a lay-down. Computer languages seem to have a similar effect on people who become fluent in them. Although I won’t try to prove it by citing examples, I think it is quite obvious that the “technical language” Pylyshyn speaks of can cripple an imagination in the process of disciplining it.[^10]

It has been said so often that computers have huge effects on their users’ imaginations that one can easily lose sight of one of the most obvious but still underrated ways in which computers achieve this effect, and that is the sheer speed of computers. Before computers came along the theoretician was strongly constrained to ignore the possibility of truly massive and complex processes in psychology because it was hard to see how such processes could fail to _appear_ at worst mechanical and cumbersome, at best vegetatively slow, and of course a hallmark of mentality is its swiftness. One might say that the speed of thought defines the upper bound of subjective “fast,” the way the speed of light defines the upper bound of objective “fast.” Now suppose there had never been any computers but that somehow (by magic, presumably) Kenneth Colby had managed to dream up these flow charts as a proposed model of a part of human organization in paranoia. (The flow charts are from his book, _Artificial Paranoia,_ Pergamon, 1975; figure 7.1]] represents the main program; figures 7.2]] and 7.3]] are blow-ups of details of the main program.) It is obvious to everyone, even Colby I think, that this is a vastly oversimplified model of paranoia, but had there not been computers to show us how all this processing and much much more can occur in a twinkling, we would be inclined to dismiss the proposal immediately as altogether toe clanking and inorganic, a Rube Goldberg machine. Most programs look like that in slow motion (hand simulation) but speeded up they often reveal a dexterity and grace that appears natural, and this grace is entirely undetectable via a slow analysis of the program (cf. time lapse photography of plants growing and buds opening). The grace in operation of AI programs may be mere illusion. Perhaps nature is graceful _all the way down,_ but for better or for worse, computer speed has liberated the imagination of theoreticians by opening up the possibility and plausibility of very complex interactive information processes playing a role in the production of cognitive events so swift as to be atomic to introspection.

![[11146_007_fig_001.jpg]]

Figure 7.1

![[11146_007_fig_002.jpg]]

Figure 7.2

![[11146_007_fig_003.jpg]]

Figure 7.3

At last I turn to the important question. Suppose that AI is viewed as I recommend, as a most abstract inquiry into the possibility of intelligence or knowledge. Has it solved any very general problems or discovered any very important constraints or principles? I think the answer is a qualified yes. In particular, I think AI has broken the back of an argument that has bedeviled philosophers and psychologists for over two hundred years. Here is a skeletal version of it: _First_, the only psychology that could possibly succeed in explaining the complexities of human activity must posit internal representations. This premise has been deemed obvious by just about everyone except the radical behaviorists (both in psychology and philosophy—both Watson and Skinner, and Ryle and Malcolm). Descartes doubted almost everything _but_ this. For the British Empiricists, the internal representations were called ideas, sensations, impressions; more recently psychologists have talked of hypotheses, maps, schemas, _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, propositions, engrams, neural signals, even holograms and whole innate theories. So the first premise is quite invulnerable, or at any rate it has an impressive mandate (see Chapter 6). But, _second,_ nothing is intrinsically a representation of anything; something is a representation only _for_ or _to_ someone; any representation or system of representations thus requires at least one _user_ or _interpreter_ of the representation who is external to it. Any such interpreter must have a variety of psychological or intentional traits (see Chapter 1): it must be capable of a variety of _comprehension,_ and must have beliefs and goals (so it can _use_ the representation to _inform_ itself and thus assist it in achieving its goals). Such an interpreter is then a sort of homunculus.

Therefore, psychology _without_ homunculi is impossible. But psychology _with_ homunculi is doomed to circularity or infinite regress, so psychology is impossible.

The argument given is a relatively abstract version of a familiar group of problems. For instance, it seems (to many) that we cannot account for perception unless we suppose it provides us with an internal image (or model or map) of the external world, and yet what good would that image do us unless we have an inner eye to perceive it, and how are we to explain _its_ capacity for perception? It also seems (to many) that understanding a heard sentence must be somehow _translating_ it into some internal message, but how will this message in turn be understood: by translating it into something else? The problem is an old one, and let’s call it _Hume’s Problem_, for while he did not state it explicitly, he appreciated its force and strove mightily to escape its clutches. Hume’s internal representations were impressions and ideas, and he wisely shunned the notion of an inner _self_ that would intelligently _manipulate_ these items, but this left him with the necessity of getting the ideas and impressions to “think for themselves.” The result was his theory of the self as a “bundle” of (nothing but) impressions and ideas. He attempted to set these impressions and ideas into dynamic interaction by positing various associationistic links, so that each succeeding idea in the stream of consciousness dragged its successor onto the stage according to one or another principle, all without benefit of intelligent _supervision._ It didn’t work, of course. It couldn’t conceivably work, and Hume’s failure is plausibly viewed as the harbinger of doom for any remotely analogous enterprise. On the one hand, how could _any_ theory of psychology make sense of representations that _understand themselves_, and on the other, how could _any_ theory of psychology avoid regress or circularity if it posits at least one representation-understander in addition to the representations?

Now no doubt some philosophers and psychologists who have appealed to internal representations over the years have believed in their hearts that somehow the force of this argument could be blunted, that Hume’s problem could be solved, but I am sure no one had the slightest idea _how to do this_ until AI and the notion of data-structures came along. Data-structures may or may not be biologically or psychologically realistic representations, but they are, if not living, breathing examples, at least clanking, functioning examples of representations that can be said in the requisite sense to understand themselves.*]]

How this is accomplished can be metaphorically described (and any talk about internal representations is bound to have a large element of metaphor in it) by elaborating our description (see Chapter 5) of AI as a top-down theoretical inquiry. One starts, in AI, with a specification of a whole person or cognitive organism—what I call, more neutrally, an intentional system (see Chapter 1)—or some artificial segment of that person’s abilities (e.g., chess-playing, answering questions about baseball) and then breaks that largest intentional system into an organization of subsystems, each of which could itself be viewed as an intentional system (with its own specialized beliefs and desires) and hence as formally a homunculus. In fact, homunculus talk is ubiquitous in AI, and almost always illuminating. AI homunculi talk to each other, wrest control from each other, volunteer, sub-contract, supervise, and even kill. There seems no better way of describing what is going on.[^11] Homunculi are _bogeymen_ only if they duplicate _entirely_ the talents they are rung in to explain (a special case of danger (1)). If one can get a team or committee of _relatively_ ignorant, narrow-minded, blind homunculi to produce the intelligent behavior of the whole, this is progress. A flow chart is typically the organizational chart of a committee of homunculi (investigators, librarians, accountants, executives); each box specifies a homunculus by prescribing a function _without saying how it is to be accomplished_ (one says, in effect: put a little man in there to do the job). If we then look closer at the individual boxes we see that the function of each is accomplished by subdividing it via another flow chart into still smaller, more stupid homunculi. Eventually this nesting of boxes within boxes lands you with homunculi so stupid (all they have to do is remember whether to say yes or no when asked) that they can be, as one says, “replaced by a machine.” One _discharges_ fancy homunculi from one’s scheme by organizing armies of such idiots to do the work.

When homunculi at a level interact, they do so by sending _messages,_ and each homunculus has representations that it uses to execute its functions. Thus typical AI discussions _do_ draw a distinction between representation and representation-user:[^12] they take the _first step_ of the threatened infinite regress, but as many writers in AI have observed,[^13] it has gradually emerged from the tinkerings of AI that there is a trade-off between sophistication in the representation and sophistication in the user. The more raw and uninterpreted the representation—e.g., the mosaic of retinal stimulation at an instant—the more sophisticated the interpreter or user of the representation. The more interpreted a representation—the more _procedural_ information is _embodied in it_, for instance—the less fancy the interpreter need be. It is this fact that permits one to get away with _lesser_ homunculi at high levels, by getting their earlier or lower brethren to do some of the work. One never quite gets _completely_ self-understanding representations (unless one stands back and views all representation in the system from a global vantage point), but all homunculi are ultimately discharged. One gets the advantage of the trade-off only by sacrificing versatility and universality in one’s subsystems and their representations,[^14] so one’s homunculi cannot be too versatile nor can the messages they send and receive have the full flavor of normal human linguistic interaction. We have seen an example of how homuncular communications may fall short in SHRDLU’s remark, “Because you asked me to.” The context of production and the function of the utterance makes clear that this is a sophisticated communication and the product of a sophisticated representation, but it is not a full-fledged Gricean speech act. If it were, it would require too fancy a homunculus to use it.

There are two ways a philosopher might view AI data structures. One could grant that they are indeed self-understanding representations or one could cite the various disanalogies between them and prototypical or _real_ representations (human statements, paintings, maps) and conclude that data-structures are not really internal representations at all. But if one takes the latter line, the modest successes of AI simply serve to undercut our first premise: it is no longer obvious that psychology needs internal representations; internal pseudo-representations may do just as well.

It is certainly tempting to argue that since AI has provided us with the only known way of solving Hume’s Problem, albeit for very restrictive systems, it must be on the right track, and its categories must be psychologically real, but one might well be falling into Danger (2) if one did. We can all be relieved and encouraged to learn that there is _a_ way of solving Hume’s Problem, but it has yet to be shown that AI’s way is the only way it can be done.

AI has made a major contribution to philosophy and psychology by revealing a particular way in which simple cases of Hume’s Problem can be solved. What else has it accomplished of interest to philosophers? I will close by just drawing attention to the two main areas where I think the AI approach is of particular relevance to philosophy.

For many years philosophers and psychologists have debated (with scant interdisciplinary communication) about the existence and nature of mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett. These discussions have been relatively fruitless, largely, I think, because neither side had any idea of how to come to grips with Hume’s Problem. Recent work in AI, however, has recast the issues in a clearly more perspicuous and powerful framework, and anyone hoping to resolve this ancient issue will find help in the AI discussions.[^15]

The second main area of philosophical interest, in my view, is the so-called “frame problem.”[^16] The frame problem is an abstract _epistemological_ problem that was in effect discovered by AI thought-experimentation. When a cognitive creature, an entity with many beliefs about the world, performs an act, the world changes and many of the creature’s beliefs must be revised or updated. How? It cannot be that we perceive and notice _all_ the changes (for one thing, many of the changes we _know_ to occur do not occur in our perceptual fields), and hence it cannot be that we rely entirely on perceptual input to revise our beliefs. So we must have internal ways of updating our beliefs that will fill in the gaps and keep our internal model, the totality of our beliefs, roughly faithful to the world.

If one supposes, as philosophers traditionally have, that one’s beliefs are a set of propositions, and reasoning is inference or deduction from members of the set, one is in for trouble, for it is quite clear (though still controversial) that systems relying only on such processes get swamped by combinatorial explosions in the updating effort. It seems that our entire conception of belief and reasoning must be radically revised if we are to explain the undeniable capacity of human beings to keep their beliefs roughly consonant with the reality they live in.

I think one can find an _appreciation_ of the frame problem in Kant (we _might_ call the frame problem Kant’s Problem) but unless one disciplines one’s thought-experiments in the AI manner, philosophical proposals of solutions to the problem, including Kant’s of course, can be viewed as at best suggestive, at worst mere wishful thinking.

I do not want to suggest that philosophers abandon traditional philosophical methods and retrain themselves as AI workers. There is plenty of work to do by thought-experimentation and argumentation, disciplined by the canons of philosophical method and informed by the philosophical tradition. Some of the most influential recent work in AI (e.g., Minsky’s papers on “Frames”) is loaded with recognizably philosophical speculations of a relatively unsophisticated nature. Philosophers, I have said, should study AI. Should AI workers study philosophy? Yes, unless they are content to reinvent the wheel every few days. When AI reinvents a wheel, it is typically square, or at best hexagonal, and can only make a few hundred revolutions before it stops. Philosopher’s wheels, on the other hand, are perfect circles, require _in principle_ no lubrication, and can go in at least two directions at once. Clearly a meeting of minds is in order.*]]

## Notes

*]] George Smith and Barbara Klein have pointed out to me that this question can be viewed as several ways ambiguous, and hence a variety of quite different responses might be held to answer such a question. Much of what I say below about different tactics for answering a question of this form can be construed to be about tactics for answering different (but related) questions. Philosophers who intend a question of this sort rhetorically can occasionally be embarrassed to receive in reply a detailed answer of one variety or another.

*]] This question (and attempts to answer it) constitutes one main branch of epistemology; the other main branch has dealt with the problem of skepticism, and its constitutive question might be: “Is knowledge possible?”

*]] Joseph Weizenbaum has pointed out to me that Turing saw from the very beginning that computers could in principle break the threatened regress of Hume’s Problem, and George Smith has drawn my attention to similar early wisdom in Von Neumann. It has taken a generation of development for their profound insights to be confirmed, after a fashion, by detailed models. It is one thing—far from negligible—to proclaim a possibility in principle, and another to reveal how the possibility might be made actual in detail. Before the relatively recent inventions of AI, the belief that Hume’s Problem could be dissolved somehow by the conceptual advances of computer science provided encouragement but scant guidance to psychologists and philosophers.

*]] I am indebted to Margaret Boden for valuable advice on an early draft of this paper. Her _Artificial Intelligence and Natural Man_ (Harvester, 1977) provides an excellent introduction to the field of AI for philosophers.

1.]] J. Weizenbaum, _Computer Power and Human Reason_ (San Francisco: Freeman, 1976): p. 179, credits Louis Fein with this term.

2.]] Cf. also _Content and Consciousness._

3.]] Cf. Zenon Pylyshyn, “Complexity and the Study of Artificial and Human Intelligence,” in Martin Ringle, ed., _Philosophical Perspectives on Artificial Intelligence_ (Humanities Press and Harvester Press, 1978), for a particularly good elaboration of the top-down strategy, a familiar theme in AI and cognitive psychology. Moore and Newell’s “How can MERLIN Understand?” in Lee W. Gregg, _Knowledge and Cognition_ (New York: Academic Press, 1974), is the most clear and self-conscious employment of this strategy I have found.

4.]] See also Judson Webb, “Gödel’s Theorem and Church’s Thesis: A Prologue to Mechanism,” _Boston Studies in the Philosophy of Science_, XXXI (Reidel, 1976).

5.]] Wilfrid Sellars, _Science, Perception and Reality_ (London: Routledge & Kegan Paul, 1963): pp. 182ff.

6.]] Terry Winograd, _Understanding Natural Language_ (New York: Academic Press, 1972), pp. 12ff.

7.]] Cf. Correspondence between Weizenbaum, et al. in _Communications of the Association for Computing Machinery_; Weizenbaum, CACM, XVII, 7 (July 1974): 425; Arbib, CACM, XVII, 9 (Sept. 1974): 543; McLeod, CACM, XVIII, 9 (Sept. 1975): 546; Wilks, CACM, XIX, 2 (Feb. 1976): 108; Weizenbaum and McLeod, CACM, XIX, 6 (June 1976): 362.

8.]] J. Weizenbaum, “Contextual Understanding by Computers,” CACM, X, 8 (1967): 464–80; also _Computer Power and Human Reason_.

9.]] Cf. Pylyshyn, _op. cit_.

10.]] Cf. Weizenbaum, _Computer Power and Human Reason_, for detailed support of this claim.

11.]] Cf. Jerry Fodor, “The Appeal to Tacit Knowledge in Psychological Explanation,” _Journal of Philosophy_, LXV (1968); F. Attneave, “In Defense of Homunculi,” in W. Rosenblith, _Sensory Communication_ (Cambridge: MIT Press, 1960); R. DeSousa, “Rational Homunculi,” in A. Rorty, ed., _The Identities of Persons_ (University of California Press, 1976); Elliot Sober, “Mental Representations,” in _Synthese_, XXXIII (1976).

12.]] See, e.g., Daniel Bobrow, “Dimensions of Representation,” in D. Bobrow and A. Collins, eds., _Representation and Understanding_ (New York: Academic Press, 1975).

13.]] W. A Woods, “What's In a Link?” in Bobrow and Collins, _op. cit._; Z. Pylyshyn, “Imagery and Artificial Intelligence,” in C. Wade Savage, ed., _Minnesota Studies in the Philosophy of Science_, IX (forthcoming), and Pylyshyn, “Complexity and the Study of Human and Artificial Intelligence,” _op. cit._; M. Minsky, “A Framework for Representing Knowledge,” in P. Winston, ed., _The Psychology of Computer Vision_ (New York, 1975).

14.]] Cf. Winograd on the costs and benefits of declarative representations in Bobrow and Collins, _op. cit._: 188.

15.]] See, e.g., Winston, _op. cit_., and Pylyshyn, “Imagery and Artificial Intelligence,” _op. cit._; “What the Mind's Eye Tells the Mind's Brain,” _Psychological Bulletin_ (1972); and the literature referenced in these papers.

16.]] See e.g., Pylyshyn’s paper _op. cit._; Winograd in Bobrow and Collins, _op. cit._; Moore and Newell, _op. cit._; Minsky, _op. cit_.Objects of Consciousness and the Nature of Experience

Are Dreams Experiences?

The “received view” of dreams is that they are _experiences that occur during sleep_, experiences which we can often recall upon waking. Enlarged, the received view is that dreams consist of sensations, thoughts, impressions, and so forth, usually composed into coherent narratives or adventures, occurring somehow in awareness or consciousness, though in some other sense or way the dreamer is _un_conscious during the episode.*]] _Received_ it certainly is; as Norman Malcolm pointed out in his book, _Dreaming_, not only has it been virtually unchallenged, it has been explicitly endorsed by Aristotle, Descartes, Kant, Russell, Moore, and Freud.[^1] That was in 1959, and I think it is fair to say that in spite of Malcolm’s arguments against the received view, it is still the received view. I want to reopen the case, and though my aims and presuppositions are quite antagonistic to Malcolm’s, those familiar with his attack will see many points at which my discussion agrees with and gains insight and direction from his. I will not, though, go into a detailed extraction and defense of what I find valuable in Malcolm’s book. My immediate purpose in what follows is to undermine the authority of the received view of dreams. My larger purpose is to introduce a view about the relationship between experience and memory that I plan to incorporate into a physicalistic theory of consciousness, a theory considerably different from the theory I have hitherto defended.[^2]

The most scandalous conclusion that Malcolm attempted to draw from his analysis of the concept of dreaming was to the effect that contemporary dream research by psychologists and other scientists was conceptually confused, misguided, ultimately simply _irrelevant_ to dreaming.[^3] This conclusion strikes many as bizarre and impertinent. If scientists can study waking experience, waking sensation, thought, imagination, consciousness, they can surely study the varieties of these phenomena that occur during sleep, in dreaming. This riposte is not, of course, a consideration that would impress Malcolm, for it is simply an announcement of faith in the received view, the view that dreams do consist of sensations, thoughts, and so forth occurring during sleep, and Malcolm already knows that the view he is attacking inspires such faith. In any event, as everyone expected, Malcolm’s words have had little or no discouraging effect on dream researchers. Their work continues apace to this day, apparently with a degree of fruition that makes a mockery of Malcolm’s view. So let us suppose, _contra_ Malcolm, that the researchers are neither the perpetrators nor the victims of a conceptual crime, and see where it leads us. Let us suppose that the dream researcher’s concept of dreaming is not only received, but the true and unconfused concept of dreaming. What are the prospects, then, for the scientific elaboration of the received view?

It is well known that periods of rapid eye movements (REMs) occur during sleep, and correlate well with subsequent reports of having dreamed. There are also characteristic EEG patterns usually concurrent with the REM episodes, and other physiological correlates that go to suggest that dreams do indeed occur during sleep, and can now be timed, confirmed to occur, and measured in all manner of ways. One tantalizing finding has been the apparent occasional content-relativity of the REMs. A person whose REMs are predominantly horizontal is awakened and reports a dream in which he watched two people throwing tomatoes at each other. A predominantly vertical pattern in REMs is correlated with a dream report of picking basketballs off the floor and throwing them up at the basket.[^4] A neurophysiological model*]] of dreaming would plausibly construe these REMs as relatively gross and peripheral effects of a more determinate content-relative process deeper in the brain, which we might hope some day to _translate,_ in this sense: we might be able to _predict_ from certain physiological events observed during sleep that the subsequent dream report would allude to, for example, fear, falling from a height, eating something cold, even (in the Golden Age of neurocryptography) buying a train ticket to New Haven for $12.65 and then forgetting which pocket it was in. The prospect of a _generalized_ capacity to predict dream narratives in such detail would be vanishingly small in the absence of a highly systematic and well-entrenched theory of representation in the brain, but let us suppose for the nonce that such a theory is not only in principle possible, but the natural culmination of the research strategies that are already achieving modest success in “translating” relatively gross and peripheral nervous-system activity.*]]

Now some people claim never to dream, and many people waken to report that they have dreamed but cannot recall any details. The latter usually have a strong conviction that the dream _did_ have details, though they cannot recall them, and even when we can recall our dreams, the memories fade very fast, and the mere act of expressing them seems to interfere, to speed up the memory loss. Here the impression of details _there then_ but _now lost_ is very strong indeed. REM researchers now confidently state that their research shows that _everybody_ has dreams (and every night); some of us just seldom—or never—recall them. It must be unsettling to be assured that one has dreamed when one is positive one has not; Malcolm could be expected to diagnose one’s reaction to such an assurance as the shudder of conceptual violation,[^5] but that would be an overstatement. The data of common experience strongly suggest a gradation in people’s capacities to recall (both dreams and other items), and it should be nothing worse than an odd but obvious implication of the received view that one could _dream_ without recalling, just as one can promise without recalling, or be raucously drunk without recalling.

Guided by common experience and the received view, then, we can imagine our scientists of the future isolating the memory mechanisms responsible for dream recall, and finding ways of chemically facilitating or inhibiting them. This is surely plausible; research into the chemistry of memory already suggests which chemicals might have these powers. We would expect that the scientists’ claims to a theory of the dream-recall mechanism would be buttressed by systematic ties to a theory of memory mechanisms in general and by results, such as, perhaps, their ability to cure the dream-amnesiac.

So we imagine future dream theory to posit two largely separable processes: first, there are neural events during sleep (more specifically during REM periods having certain characteristic EEG correlates) that systematically represent (are systematically correlatable with) the “events occurring in the dream,” and during this process there is a second, memory-loading process so that these events can be recalled on waking (when the memory process works). Dreams are _presented_, and simultaneously _recorded_ in memory, and we might be able to interfere with or prevent the recording without disturbing the presentation.

This posited process of memory-loading and playback must be saved from simplistic interpretation if we are to maintain any vestige of realism for our fantasy. It is rarely if ever the case that a dreamer awakens and proceeds to recite with vacant stare a fixed narrative. Dream recall is like recall generally. We interpret, extrapolate, revise; it sometimes seems that we “relive” the incidents and _draw conclusions_ from this reliving—conclusions that are then expressed in what we actually _compose_ then and there as our recollections. It is not easy to analyze what must be going on when this happens. What is the _raw material_, the evidence, the basis for these reconstructions we call recollections?

Consider a fictional example. John Dean, a recently acclaimed virtuoso of recollection, is asked about a certain meeting in the Oval Office. Was Haldeman present? Consider some possible replies.

- (1) “No.”
- (2) “I can’t (or don’t) recall his being there.”
- (3) “I distinctly recall that he was not there.”
- (4) “I remember noticing (remarking) at the time that he was not there.”

If Dean says (1) we will suspect that he is saying less than he _can_ say, even if what he says is sincere and even true. At the other extreme, (4) seems to be a nearly _complete_ report of the relevant part of Dean’s memory. Answer (2), unlike all the others, reports an inability, a blank. Under the right circumstances, though, it carries about as strong a pragmatic implication of Haldeman’s absence as any of the others (we ask: could Dean conceivably fail to recall Haldeman’s presence if Haldeman had been there?). The stronger these pragmatic implications, the more disingenuous an answer like (2) will seem. Consider: “Was Dan Rather at that meeting in the Oval Office?” “I can’t _recall_ his being there.” The answer is seen to be disingenuous because we know Dean knows, and we know, the additional supporting premises which, in conjunction with (2), imply something like (1), and we expect Dean to be reasonable and draw this conclusion for—and with—us. Then what should Dean say, if asked the question about Dan Rather? Certainly not (4), unless the paranoia in the White House in those days knew no bounds, but (3) can be heard to carry a similar, if weaker, implication. We would not expect Dean to say this because it suggests (presumably misleadingly) that his answer is closer to being _given_ in his recollection, less a conclusion quickly drawn. (1) is clearly the best answer on the list under these circumstances. It _looks_ like a conclusion he reaches on the basis of things he remembers. He remembers Nixon and Ehrlichman talking with him, forming a sort of triangle in the room, and on the basis of _this_ he concludes that Haldeman, and Rather, were absent, though he took no notice of the fact at the time, or if he did he has forgotten it. Now suppose Dean says (1). Perhaps when he does this he recalls in his mind this triangle, but does not bother to tell us that—he does not close his eyes on the witness stand and do a little phenomenology for us; he simply offers up his conclusion as a dictate of memory. But he need not have gone through this conscious process of reliving and reasoning at all. He may say, directly, “No,” and if he is pressed to be more forthcoming, any reasoning he offers based on other things he recalls will not be expressing any reasoning he knows he went through before his initial negative reply. He may not even be able to explain why or how his memory dictates this answer to the question, and yet be sure, and deservedly sure, that his reply is a sincere and reliable dictate of memory.

To summarize: sometimes we can sincerely answer a question of recollection with an answer like (4), but often we cannot, and sometimes we draw a blank, but in _all_ these cases there are conclusions we can draw based on what in some sense we directly remember in conjunction with common and proprietary knowledge, and these conclusions need not be drawn in a process of _conscious_ reasoning. Whatever it is that is directly remembered can play its evidentiary role in prompting an answer of recollection without coming into consciousness. This suggests that when we remember some event, there is some limited amount of information that is _there_, not necessarily in consciousness but available in one way or another for utilization in composing our recollections and answering questions we or others raise. Perhaps what occupies this functional position is an immensely detailed recording of our experience to which our later access is normally imperfect and partial (although under hypnosis it may improve). Perhaps there is enough information in this position to reconstitute completely our past experience and present us, under special circumstances, with a vivid hallucination of reliving the event.*]] However much is in this position in Dean, however, it is not possible that Dan Rather’s absence is there except by implication, for his absence was not experienced by Dean at the time, any more than up to this moment you have been experiencing Rather’s absence from this room. What the posited memory-loading process records, then, is whatever occupies this functional position at a later time. The “playback” of dream recollections, like other recollections, is presumably seldom if ever complete or uninterpreted, and often bits of information are utilized in making memory claims without being played back in consciousness at all.

In dreaming there is also a third process that is distinguished both in the layman’s version of the received view and in fancier theories, and that is the _composition_ of what is presented and recorded. In various ways this process exhibits intelligence: dream stories are usually coherent and realistic (even surrealism has a realistic background), and are often gripping, complex, and of course loaded with symbolism. Dream composition utilizes the dreamer’s general and particular knowledge, her recent and distant experience, and is guided in familiar ways by her fears and desires, covert and overt.

Studying these three processes will require tampering with them, and we can imagine that the researchers will acquire the technological virtuosity to be able to influence, direct, or alter the composition process, to stop, restart, or even transpose the presentation process as it occurs, to prevent or distort the recording process. We can even imagine that they will be able to obliterate the “veridical” dream memory and substitute for it an undreamed narrative. This eventuality would produce a strange result indeed. Our dreamer would wake up and report her dream, only to be assured by the researcher that she never dreamed _that_ dream, but rather another, which they proceed to relate to her. Malcolm sees that the scientific elaboration of the received view countenances such a possibility-in-principle and for him this amounts to a _reductio ad absurdum_ of the received view,[^6] but again, this is an overreaction to an admittedly strange circumstance. Given the state of the art of dream research today, were someone to contradict my clear recollection of what I had just dreamed, my utter skepticism would be warranted, but the science-fictional situation envisaged would be quite different. Not only would the researchers have proved their powers by correctly predicting dream recollections on numerous occasions, but they would have a theory that explained their successes.

And we need not suppose the dream they related to the dreamer would be entirely _alien_ to her ears, even though she had no recollection of it (and in fact a competing recollection). Suppose it recounted an adventure with some secretly loved acquaintance of hers, a person unknown to the researchers. The stone wall of skepticism would begin to crumble.

The story told so far does not, I take it, exhibit the conceptual chaos Malcolm imagines; strange as it is, I do not think it would evoke in the layman, our custodian of ordinary concepts, the nausea of incomprehension. As a premise for a science-fiction novel it would be almost pedestrian in its lack of conceptual horizon-bending.

But perhaps this is not at all the way the theory of dreaming will develop. Malcolm notes in passing that it has been suggested by some researchers that dreams may occur during the moments of waking, not during the prior REM periods. Why would anyone conjecture this? Perhaps you have had a dream leading logically and coherently up to a climax in which you are shot, whereupon you wake up and are told that a truck has just backfired outside your open window. Or you are fleeing someone in a building, you climb out a window, walk along the ledge, then fall—and wake up on the floor having fallen out of bed. In a recent dream of mine I searched long and far for a neighbor’s goat; when at last I found her she bleated _baa-a-a—_and I awoke to find her bleat merging perfectly with the buzz of an electric alarm clock I had not used or heard for months. Many people, I find, have anecdotes like this to relate, but the scientific literature disparages them, and I can find only one remotely well-documented case from an experiment: different stimuli were being used to waken dreamers, and one subject was wakened by dripping cold water on his back. He related a dream in which he was singing in an opera. Suddenly he _heard and saw_ that the soprano had been struck by debris falling from above; he ran to her and as he bent over her, felt water dripping on his back.[^7]

What are we to make of these reports? The elaboration of the received view we have just sketched can deal with them, but at a high cost: precognition. If the terminal events in these dreams are strongly _prepared for_ by the narrative, if they do not consist of radically juxtaposed turns in the narrative (for example, the goat turns into a telephone and starts ringing), then the composition process must have been directed by something having “knowledge” of the future. That is too high a price for most of us to pay, no doubt. Perhaps all these anecdotes succumb to a mixture of reasonable skepticism, statistics (coincidences do happen, and are to be “expected” once in a blue moon), the discovery of subtle influences from the environment, and various other deflating redescriptions. But if all else failed we could devise any number of variant dream theories that accommodated these “miracles” in less than miraculous ways. Perhaps, to echo the earlier conjecture, dreams are composed and presented _very fast_ in the interval between bang, bump, or buzz and full consciousness, with some short delay system postponing the full “perception” of the noise in the dream until the presentation of the narrative is ready for it. Or perhaps in that short interval dreams are composed, presented, and recorded _backward_, and then remembered front to back. Or perhaps there is a “library” in the brain of undreamed dreams with various indexed endings, and the bang or bump or buzz has the effect of retrieving an appropriate dream and inserting it, cassette-like, in the memory mechanism.

None of these theories can be viewed as a mere variation or rival elaboration of the received view. If one of them is true, then the received view is false. And since these rival theories, including the theory inspired by the received view, are all empirical, subject to confirmation and refutation, and since the rival theories even have some (admittedly anecdotal) evidence in their favor, we are constrained to admit that the received view might simply turn out to be false: dreams, it might turn out, are not what we took them to be—or perhaps we would say that it turns out that there are no dreams after all, only dream “recollections” produced in the manner described in our confirmed theory, whichever it is. Malcolm sees that all this is implied by the received view, and takes it to be yet another _reductio ad absurdum_ of it: any view that could permit the discovery that “we are always only under the _illusion_ of having had a dream” is “senseless.”[^8] But again, Malcolm’s response to this implication is too drastic. The claim that we had been fooled for millennia into believing in dreams would be hard to swallow, but then we would not have to swallow it unless it had the backing of a strongly confirmed scientific theory, and then this claim would put no greater strain on our credulity than we have already endured from the claims of Copernicus, Einstein, and others. It would be rather like learning that dream-recall was like _déjà vu—_it only _seemed_ that you had experienced it before—and once you believed _that_, it would no longer even seem (as strongly) that you were recalling. The experience of “dream recall” would change for us.*]]

My attack on the received view is not, however, a straightforward empirical attack. I do not wish to aver that anecdotal evidence about dream anticipation disproves the received view, but I do want to consider in more detail what the issues would be were a rival to the received view to gain support. I hope to show that the received view is more vulnerable to empirical disconfirmation than its status as the received view would lead us to expect. Of the rival theories, the cassette-library theory runs most strongly against our pretheoretical convictions, for on the other two there still is some vestige of the presumed presentation process: it is just much faster than we had expected, or happens backward. On the cassette view, our “precognitive” dreams are never dreamed at all, but just spuriously “recalled” on waking. If our memory mechanisms were empty until the moment of waking, and then received a whole precomposed dream narrative in one lump, the idea that precognitive dreams are _experienced episodes_ during sleep would have to go by the board.

Suppose we generalize the cassette theory to cover all dreams: all dream narratives are composed directly into memory banks; which, if any, of these is available to waking recollection depends on various factors—precedence of composition, topicality of waking stimulus, degree of “repression,” and so forth. On this view, the process of presentation has vanished, and although the dream cassettes would have to be filled at some time by a composition process, that process might well occur during our waking hours, and spread over months (it takes a long time to write a good story). The composition might even have occurred aeons before our birth; we might have an _innate_ library of undreamed dream cassettes ready for appropriate insertion in the playback mechanism. Stranger things have been claimed. Even on the received view, the composition process is an unconscious or subconscious process of which we normally have no more _experience_ than of the processes regulating our metabolism; otherwise dreams could not be suspenseful. (I say “normally” for there does seem to be the phenomenon of self-conscious dreaming, where we tinker with a dream, run it by several times, attempt to resume it where it left off. Here the theatrical metaphor that enlivens the received view seems particularly apt. After tinkering like the playwright, we must sit back, get ourselves back into the audience mood, suspend disbelief, and re-enter the play. Some researchers call these occasions _lucid dreams._ But usually we are not privy to the composition process at all, and so have no inkling about when it might occur.) Research might give us good grounds for believing that dream narratives that were composed onto cassettes in the morning decayed faster than cassettes composed in the afternoon, or during meals.

A more likely finding of the cassette-theorist would be that the composition process occurs during sleep, and more particularly, during periods of rapid eye movements, with characteristic EEG patterns. One might even be able to “translate” the composition process—that is, predict dream recollections from data about the composition process. This theory looks suspiciously like the elaboration of the received theory, except that it lacks the presentation process. Cassette narratives, we are told, are composed in narrative order, and long narratives take longer to compose, and the decay time for cassettes in storage is usually quite short; normally the dream one “recalls” on waking was composed just minutes earlier, a fact attested to by the occasional cases of content-relativity in one of the by-products of cassette composition: rapid eye movements. On this theory dream memories are produced just the way the received theory says they are, except for one crucial thing: the process of dream-memory production is entirely unconscious, involves no awareness or experiencing at all. Even “lucid dreams” can be accommodated easily on this hypothesis, as follows: although the composition and recording processes are entirely unconscious, on occasion the composition process inserts traces of itself into the recording via the literary conceit of a dream within a dream.*]]

Now we have a challenge to the received view worth reckoning with. It apparently accounts for all the data of the REM researchers as well as the received view does, so there is no reason for sober investigators not to adopt the cassette theory forthwith if it has any advantages over the received view. And it seems that it does: it has a simple explanation of precognitive dreams (if there are any) and it posits one less process by eliminating a presentation process whose point begins to be lost.

But what greater point could a process have? In its presence we have experience; in its absence we have none. As Thomas Nagel would put it, the central issue between these two theories appears to be whether or not it is like anything to dream.[^9] On the cassette theory it is not like anything to dream, although it is like something _to have dreamed._ On the cassette theory, dreams are not experiences we have during sleep; where we had thought there were dreams, there is only an unconscious composition process and an equally unconscious memory-loading process.

A few years ago there was a flurry of experimentation in learning-while-you-sleep. Tape recordings of textbooks were played in the sleeper’s room, and tests were run to see if there were any subsequent signs of learning. As I recall, the results were negative, but some people thought the results were positive. If you had asked one of them _what it was like_ to learn in one’s sleep, the reply would presumably have been: “It was not like anything at all—I was sound asleep at the time. I went to sleep not knowing any geography and woke up knowing quite a bit, but don’t ask me what it was like. It wasn’t like anything.” If the cassette theory of dreams is true, dream-recollection production is a similarly unexperienced process. If asked what it is like to dream one _ought_ to say (because it would be the truth): “It is not like anything. I go to sleep and when I wake up I find I have a tale to tell, a ‘recollection’ as it were.” It is Malcolm’s view that this is what we ought to say, but Malcolm is not an explicit champion of the cassette theory or any other empirical theory of dreaming. His reasons, as we shall see, are derived from “conceptual analysis.” But whatever the reasons are, the conclusion seems outrageous. _We all know better_, we think. But do we? We are faced with two strikingly different positions about what happens when we dream, and one of these, the received view, we are not just loath to give up; we find it virtually unintelligible that we could be wrong about it. And yet the point of difference between it (as elaborated into a theory by scientists) and its rival, the cassette theory, is apparently a technical, theoretical matter about which the layman’s biases, his everyday experience, and even his personal recollections of dreams are without authority or even weight. What should we do? Sit back and wait for the experts to tell us, hoping against hope that dreams will turn out to be, after all, experiences? That seems ridiculous.

If that seems ridiculous, perhaps it _is_ ridiculous. Can some way be found to protect the received view from the possibility of losing this contest? If we do not for a minute believe it could lose, we must suppose there is some principled explanation of this. One might set out in a verificationist manner.*]] What could possibly settle the issue between the received view and the cassette theory if subjects’ recollections were deemed neutral? The conclusion of one view is that dreams are experiences, and of the other, that they are not; but if subjects’ recollections were not held to be _criterial_, nothing else could count as evidence for or against the rival theories, at least with regard to this disputed conclusion. Therefore the claimed difference between the two theories is illusory, or perhaps we should say they are both pseudo-theories. This will not do. We can easily imagine the two theories to share a concept of experience, and even to agree on which data would go to show that dreams were, in this shared technical sense, experiences. Nor would this technical concept of experience have to look all that unordinary. We have many common ways of distinguishing which among the events that impinge on us are experienced and which are not, and we can imagine these theories to build from these ordinary distinctions a powerful shared set of well-confirmed empirically necessary and sufficient conditions for events to be experienced. If, for instance, some part of the brain is invariably active in some characteristic way when some event in waking life is, as we ordinarily say, experienced, and if moreover we have a theory that says why this should be so, the absence of such brain activity during REM periods would look bad for the received view and good for the cassette view.

But if that is what we should look for, the received view is in trouble, for one routinely recognized condition for having an experience is that one be conscious, or awake, and dreamers are not. A well-confirmed physiological condition for this is that one’s reticular activating system be “on,” which it is not during sleep. The fact that one is in a sound sleep goes a long way to confirming that one is _not_ having experiences, as _ordinarily_ understood. Malcolm would make this criterial, but that is one more overstatement. Lack of reticular system activity strongly suggests that nothing is being experienced during REMs, but the defender of the received view can plausibly reply that reticular activation is only a condition of normal _experience_, and can point to the frequent occurrence during REM periods of the normal physiological accompaniments of fear, anxiety, delight, and arousal as considerations in favor of an extended concept of experience. How could one exhibit an emotional reaction to something not even experienced? The debate would not stop there, but we need not follow it further now. The fact remains that the physiological data would be clearly relevant evidence in the dispute between the theories, and not all the evidence is on the side of the received view.

Still, one might say, the very relevance of physiological evidence shows the dispute not to involve our ordinary concept of experience at all, but only a technical substitute. For suppose we were told without further elaboration that the theory inspired by the received view had won the debate, had proved to be the better theory. We would not know what, if anything, had been confirmed by this finding. Which of our hunches and biases would be thereby vindicated, and are any of them truly in jeopardy?

This plausible rhetorical question suggests that none of our precious preconceptions about dreaming _could_ be in jeopardy, a conclusion that “conceptual analysis” might discover for us. How might this be done? Let us return to the comparison between the cassette view of dreams and the speculation that one might learn in one’s sleep. I suggested that subjects in either circumstance should, on waking, deny that it was like anything to have undergone the phenomenon. But there would be a crucial difference in their waking states, presumably. For the dreamer, unlike the sleep learner, would probably want to add to his disclaimer: “Of course it _seems to me_ to have been like something!” The sleep learner has new knowledge, or new beliefs, but not new _memories._ This is surely an important difference, but just what difference does it make? Is it that the claim:

- (5) It was not like anything, but it seems to me to have been like something,

is a covert contradiction? Can one sustain the following principle?

- (6) If it seems to have been like something, it was like something. The present tense version of the principle is unassailable:
- (7) If it seems to me to be like something to be _x,_ then it is like something to me to be _x._

That is what we mean when we talk of what it is like: how it seems to us.*]] When we try to make the principle extend through memory to the past, however, we run into difficulties. There is no good reason to deny that memories can be spurious, and there is plenty of confirmation that they can. This is somewhat obscured by some looseness in our understanding of the verb “remember.” Sometimes we draw a distinction between remembering and seeming to remember such that remembering, like knowing, is veridical. On this reading it follows that if you remember something to have been _x,_ it was _x._ If it was not _x,_ you only seem to remember that it was. But when I say, about a restaurant we are dining in, “This isn’t the way I remember it,” my claim is equivocal. I may not be claiming the restaurant has changed—it may be that my memory is at fault. On this reading of “remember” there is still a distinction between remembering and seeming to remember, but it is not a distinction with veridicality on one side: for example one tells a tale of one’s childhood that is shown to be false and one wonders whether one has mistaken fantasizing or confabulating for (mis)remembering. On _either_ reading, however, there is no claim that can be made of the form:

- (8) Since I remember it to have been like something, it was like something.

On the first reading of “remember,” the claim, while logically impeccable, does no work unless one claims a capacity to tell one’s memories from one’s seeming memories that one simply does not have. On the second reading, even if we could always tell fantasy from memory the consequent would not follow. So (5) represents a possible state of affairs. We had in fact already countenanced this state of affairs as an abnormality in supposing that the dream researchers could, by tampering, insert a spurious dream recollection. Now we are countenancing it as a possible and not even improbable account of the normal case.

Malcolm sees that nothing like (6) or (8) can be exploited in this context; we can seem to have had an experience when we have not, and for just this reason he denies that dreams are experiences! His argument is that _since_ one can be under the impression that one has had an experience and yet not have had it, and since if one is under the impression that one has had a dream, one _has_ had a dream,*]] having had a dream cannot be having had an experience; hence, dreams are not experiences.

This “criteriological” move has a curious consequence: it “saves” the authority of the wakened dream-recaller, and this _looks like_ a rescue of subjectivity from the clutches of objective science, but it “saves” dreaming only at the expense of experience. What Malcolm sees is that if we permit a distinction between “remembering” and “seeming to remember” to apply to dream recollections, the concept of dreaming is cast adrift from any criterial anchoring to first-person reports, and becomes (or is revealed to be) a theoretical concept. Once we grant that subjective, introspective or retrospective evidence does not have the authority to settle questions about the nature of dreams—for instance, whether dreams are experiences—we have to turn to the other data, the behavior and physiology of dreamers, and to the relative strengths of the theories of these, if we are to settle the question, a question which the subject is not in a privileged position to answer. Malcolm avoids this by denying that dreams are experiences, but this only concedes that one does _not_ have a privileged opinion about one’s own past experiences.*]] This concession is unavoidable, I think, and Malcolm’s is not the only philosophic position caused embarrassment by it. A defender of the subjective realm such as Nagel must grant that in general, whether or not it was like something to be _x,_ whether or not the subject _experienced_ being x—questions that _define_ the subjective realm—are questions about which the subject’s subsequent subjective opinion is not authoritative. But if the subject’s own convictions do not settle the matter, and if, as Nagel holds, no objective considerations are conclusive either, the subjective realm floats out of ken altogether, except perhaps for the subject’s convictions about the specious present. Dreams are particularly vulnerable in this regard only because, as Malcolm observes, sleepers do not and cannot _express_ current convictions about the specious present (if they have any) while they are dreaming. Since our only expressible access to dreams is retrospective, dreams are particularly vulnerable, but they are not alone. The argument we have been considering is more general; the dispute between the rival theories of memory-loading can be extended beyond dreaming to all experience. For instance, just now, while you were reading my remarks about Nagel, were you experiencing the peripheral sights and sounds available in your environment? Of course you were, you say, and you can prove it to your own complete satisfaction by closing your eyes and recalling a variety of events or conditions that co-occurred with your reading those remarks. While not _central_ in your consciousness at the time, they were certainly _there,_ being _experienced,_ as your recollections show. But the cassette theorist, emboldened by the success with dreams, puts forward the _subliminal peripheral recollection-production_ theory, the view that the variety of peripheral details in such cases are not consciously experienced, but merely unconsciously recorded for subsequent recall. Events outside our immediate attention are not experienced at all, our theorist says, but they do have subliminal effects on short-term memory. Our capacity to recall them for a short period does not establish that they were experienced, any more than our capacity to “recall” dreams shows that they were experienced. But this is nonsense, you say: _recording those peripheral items for subsequent recollection just is experiencing them._

If only this bold claim were true! Look what it would do for us. The difference between the received view of dreams and the cassette theory would collapse; the presumably unconscious memory-loading process of the cassette theory would turn out to be the very presentation process dear to the received view. A “conceptual relationship” could be established between experience and memory that avoided the difficulties heretofore encountered in such claims, as follows: The conceptual relationship is _not_ between experiencing and subsequent subjective convictions of memory (the latter are _not_ criterial), but between experiencing and something perfectly objective: the laying down thereupon of a memory trace—for however short a time and regardless of subsequent success or failure at recollection.*]] The conceptual relationship would be identity. _Experire est recordare._

Much can be said in support of this principle, but at this time I will restrict myself to a few brief persuasions. First, is remembering a _necessary_ condition for experiencing? Arguably, yes, if you grant that memories may not last long. The idea of a subject, an “I,” experiencing each successive state in a stream of consciousness with _no_ recollection of its predecessors, is a hopelessly impoverished model of experience and experiencers. The _familiarity_ and _continuity_ in the world of current experiences is a necessary background for recognition and discrimination, and only short-term memory can provide this. Items that come and go so fast, or so inconspicuously, as to leave no reverberations behind in memory at all, are plausibly viewed as simply not experienced. So if remembering is a necessary condition, is it also a _sufficient_ condition for experiencing? Yost and Kalish say so, without supporting argument: “Dreaming is a real experience. And since dreams can be remembered they must be conscious experiences.”[^10] Martin and Deutscher, in their article “Remembering,” concur:

> So long as we hold some sort of ‘storage’ or ‘trace’ account of memory, it follows that we can remember only what we have experienced, for it is in our experience of events that they “enter” the storehouse.[^11]

So remembering, in the sense of storing away in the memory for some time, is arguably a necessary and sufficient condition for experiencing. These are, I think, philosophically respectable arguments for the claimed identity, and to them can be added an ulterior consideration which will appeal to physicalists if not to others. The proposed identity of experiencing and recording promises a striking simplification for physicalist theories of mind. The problematic (largely because utterly vague) presentation process vanishes as an extra phenomenon to be accounted for, and with it goes the even more mysterious _audience_ or _recipient_ of those presentations. In its place is just a relatively prosaic short-term memory capacity, the sort of thing for which rudimentary but suggestive physical models already abound.

The principle as it stands, however, is too strong, on two counts. Consider again Martin and Deutscher’s commentary on the “storehouse” model of memory: “It is in our experience of events that they ‘enter’ the storehouse.” What, though, of forcible or illegal entry? We need an account of something like _normal_ entry into memory so that we can rule out, as experiences, such abnormally entered items as the undreamed dream surgically inserted by the dream researchers. We want to rule out such cases, not by declaring them impossible, for they are not, but by denying that they are experiences for the subject. As we shall see in a moment, the best way of doing this may have a surprising consequence. The second failing of our principle is simply that it lacks the status we have claimed for it. It is not self-evident; its denial is not a contradiction. We must not make the mistake of asserting that this is a discovered conceptual truth about experience and memory. We must understand it as a proposal, a theoretically promising adjustment in our ordinary concepts for which we may have to sacrifice some popular preconceptions. For instance, whether animals can be held to dream, or to experience anything, is rendered an uncertainty depending on what we mean by _recall._ Can animals _recall_ events? If not, they cannot have experiences. More radically, subjective authority about experience goes by the board entirely. Still, we get a lot in return, not the least of which is a way of diagnosing and dismissing the Pickwickian hypothesis of subliminal peripheral recollection-production.

We are still not out of the woods on dreaming, though, for we must define normal memory-entry in such a way as to admit ordinary experience and exclude tampering and other odd cases.

When the memory gets loaded by accident or interference we will not want this to count as experience, and yet we want to grant that there is such a thing as nonveridical experience. The memory-loading that occurs during a hallucination occurs during abnormal circumstances, but not so abnormal as to lead us to deny that hallucinations are experiences. But look at a slightly different case (I do not know if this ever occurs, but it might). Suppose at noon Jones, who is wide awake, suffers some event in her brain that has a delayed effect: at 12:15 she will “recall” having seen a ghost at noon. Suppose her recollection is as vivid as you like, but suppose her actual behavior at noon (and up until recollection at 12:15) showed no trace of horror, surprise, or cognizance of anything untoward. Had she shown any signs at noon of being under the impression that something bizarre was happening, we would be strongly inclined to say she had had a hallucination then, was experiencing it then, even if she could not recount it to us until fifteen minutes later. But since she did not _react_ in any such telling way at noon, but proceeded about her business, we are strongly inclined to say the hallucination occurred later, at 12:15, and was a _hallucination of recollection_ of something she had never experienced, even though the cause of the hallucination occurred at noon. Since the events responsible for her later capacity to recall did not contribute to her behavior-controlling state at the time, they did not enter her experience then, whatever their later repercussions. But then when we apply this distinguishing principle to dreams, we find that it is quite likely that most dreams are not experiences. Whereas nightmares accompanied by moans, cries, cowering, and sweaty palms _would_ be experiences, bad dreams dreamed in repose (though remembered in agony) would not be, unless, contrary to surface appearances, their entry into memory is accomplished by engagements of the whole behavior-controlling system sufficiently normal to distinguish these cases sharply from our imaginary delayed hallucination.*]]

If it turns out that sleep, or at least that portion of sleep during which dreaming occurs, is a state of more or less peripheral paralysis or inactivity; if it turns out that most of the functional areas that are critical to the governance of our wide awake activity are in operation, then there will be good reason for drawing the lines around experience so that dreams are included. If not, there will be good reason to deny that dreams are experiences.

Some of the relevant data are already familiar. The occurrence of REMs suggests that more than a little of the visual processing system is active during dream periods, and it should be a fairly straightforward task—perhaps already accomplished—to determine just how much is. Even strongly positive results would not be overwhelming grounds for deciding that dreams are experiences, however, for in various sorts of hysteric or psychosomatic blindness there is substantial apparently normal activity in the visual processing system, and in so-called subliminal perception the same is true, and in neither case are we inclined to suppose that visual experience occurs. More compelling, in many ways, is the evidence that dreams serve a purpose: they seem to be used to redress emotional imbalances caused by frustrating experiences in waking life, to rationalize cognitive dissonances, allay anxieties, and so forth. When this task is too difficult, it seems, the dream mechanisms often go into a looping cycle; troubled people often report recurring obsessive dreams that haunt them night after night. It is implausible that such recurrent dreams must be recomposed each night,*]] so if a recurrent physiological process can be correlated with these dreams, it will appear to be a presentation process, and the presentation process will have a point: namely, to provide the emotional and cognitive-processing functional parts with the raw material for new syntheses, new accommodations, perhaps permitting a more stable or satisfying self-image for the dreamer. But even this function could easily be seen to be accomplished entirely _unconsciously._ The self-presentation tactics and perceptual interpretation ploys posited by theorists as diverse as Freud and Erving Goffman are no less plausible for being presumed to be entirely unconscious, and they serve a similar self-protective maintenance function. As Malcolm points out, dreamers’ narratives can be used by Freudians and others as a valuable source of information about the internal processes that shape us, without our having to suppose that these are recollections of experiences.†]]

It is an _open_, and _theoretical_ question whether dreams fall inside or outside the boundary of experience.*]] A plausible theory of experience will be one that does justice to _three_ distinguishable families of intuitions we have about experience and consciousness: those dealing with the role of experience in guiding current behavior, those dealing with our _current_ proclivities and capacities to _say_ what we are experiencing, and those dealing with the _retrospective_ or _recollective_ capacity to say. In earlier work I have sharply distinguished the first and second of these, but underestimated the distinctness and importance of this third source of demands on a theory of consciousness. A theory that does justice to these distinct and often inharmonious demands must also do justice to a fourth: the functional saliencies that emerge from empirical investigation. In the end, the concept of experience may not prove to differentiate any one thing of sufficient theoretical interest to warrant time spent in determining its boundaries. Were this to occur, the received view of dreams, like the lay view of experience in general, would not be so much disproved as rendered obsolete. It may seem inconceivable that this could happen, but armchair conceptual analysis is powerless to establish this.

## Notes

*]] Cf. Hilary Putnam’s version of “a natural lexical definition”: “a series of impressions (visual, etc.) occurring during sleep; usually appearing to the subject to be of people, objects, etc.; frequently remembered upon awakening” (“Dreaming and ‘Depth Grammar’” in R. J. Butler, ed., _Analytical Philosophy_ (Oxford, 1962): 224.

*]] Putnam (_op. cit._) points out that a crucial lacuna in Malcolm’s verificationist arguments against REMs as evidence confirming the received view is his failure to consider the confirmation relations arising from the use of developed theories and models (p. 226). At a number of points this paper attempts to fill that gap.

*]] I have in mind such work as Hubel and Wiesel’s “translation” of optic nerve signals in the cat. I argue against optimism regarding the prospects for a generalized neural theory of representation in “Brain Writing and Mind Reading” (Chapter 3 of this volume).

*]] Cf. Wilder Penfield’s descriptions of electrode-induced memory hallucinations, in _The Excitable Cortex in Conscious Man_ (Springfield, Illinois, 1958).

*]] Cf. Putnam, _op. cit_., p. 227. The naive subject of _déjà vu_ says, “I vaguely remember experiencing all this before”; the sophisticated subject is not even tempted to say this, but says, perhaps, “Hm, I’m having a _déjà vu_ experience right now.” The experience has changed.

*]] For more on lucid dreams and their accommodation by the cassette theory, see Kathleen Emmett, “Oneiric Experiences” and my “The Onus Re Experiences: a Reply to Emmett,” both forthcoming in _Philosophical Studies._

*]] This argument is inspired by the verificationist arguments of Malcolm, and its rebuttal is inspired by Putnam’s objections, but Malcolm does not commit himself to this argument.

*]] Cf. Nagel, “What Is It Like to Be a Bat?”: 440n. “[T]he analogical form of the English expression ‘what is it _like’_ is misleading. It does not mean ‘what (in our experience) it resembles’, but rather ‘how it is for the subject himself’.”

*]] “That he really had a dream and that he is under the impression that he had a dream: these are the same thing” (“Dreaming and Skepticism”: 32). This is the central premise of Malcolm’s work on dreaming, and one he gets from Wittgenstein: “The question whether the dreamer’s memory deceives him when he reports the dream after waking cannot arise unless indeed we introduce a completely new criterion for the report’s ‘agreeing’ with the dream, a criterion which gives us a concept of ‘truth’ as distinguished from ‘truthfulness’ here” (_Philosophical Investigations_: 222–223). It is Malcolm’s unswerving loyalty to this remark that forces his account into such notorious claims.

*]] Sometimes Malcolm seems to want to “save” all “private states” in this way, thus either having to deny that experiences are private states, or having to adopt after all some principle like (8). See _Dreaming_, p. 55.

*]] Not completely regardless of subsequent success or failure at recollection, for identifying some process as the laying down of a memory trace is identifying some process by its function, and nothing that did not have as its normal effect enabling the subject to report truly about the past could be picked out functionally as the memory-loading process.

*]] Malcolm too sees an important distinction between “violent nightmares” and normal dreams dreamed in repose, a distinction that forces him to claim we have several different concepts of sleep. Only thus can he save as a _conceptual_ truth the claim that we have no experiences while we sleep.

*]] I am indebted to Robert Nozick for raising this consideration.

†]] _Dreaming_, p. 122. Malcolm quotes with approval this methodological suggestion of Freud’s from _A General Introduction to Psychoanalysis_ (Garden City: Norton, 1943), p. 76:

Any disadvantage resulting from the uncertain recollection of dreams may be remedied by deciding that exactly what the dreamer tells is to count as the dream, and by ignoring all that he may have forgotten or altered in the process of recollection.

*]] Foulkes, _The Psychology of Sleep_ (New York: Scribners, 1966) cites a number of telling, if inconclusive, further observations: in one study no association was found between “the excitement value of dream content and heart or respiration rate” (p. 50), a datum to be balanced by the curious fact that there are usually action-potentials discoverable in the motor-neurons in the biceps of one who is asked to _imagine_ bending one’s arm; similar action-potentials are found in the arms of deaf mute dreamers—people who talk with their hands. There are also high levels of activity in the sensory cortex during dreaming sleep.

1.]] Norman Malcolm, _Dreaming_ (London: Routledge & Kegan Paul, 1959): p. 4.

2.]] In _Content and Consciousness._

3.]] _Dreaming_, _op. cit_., p. 82.

4.]] David Foulkes, _The Psychology of Sleep_ (New York: Scribners, 1966).

5.]] See Norman Malcolm, “Dreaming and Skepticism,” _Philosophical Review_, LXV (1956): 14–37, especially section VIII.

6.]] “Dreaming and Skepticism,” _loc. cit_., section VIII, especially p. 30.

7.]] William Dement and E. A. Wolpert, “The Relation of Eye Movements, Bodily Motility and External Stimuli to Dream Content,” _Journal of Experimental Psychology_, LV (1958): 543–53.

8.]] “Dreaming and Skepticism,” _loc. cit_., p. 31.

9.]] Thomas Nagel, “What Is It Like to Be a Bat?” _Philosophical Review_, LXXXIII (1974): 45–51.

10.]] R. M. Yost and Donald Kalish, “Miss MacDonald on Sleeping and Waking,” _Philosophical Quarterly_ (April 1955): 109–24. Malcolm discusses this claim in _Dreaming_, _op. cit_.

11.]] C. B. Martin and Max Deutscher, “Remembering,” _Philosophical Review_, LXXV (1969): 189.

Toward a Cognitive Theory of Consciousness

## I

Philosophers of mind and epistemologists have much to learn from recent work in cognitive psychology, but one of philosophy’s favorite facets of mentality has received scant attention from cognitive psychologists, and that is consciousness itself: full-blown, introspective, inner-world, phenomenological consciousness. In fact if one looks in the obvious places (the more ambitious attempts at whole theories, overviews of recent research, and more specialized work in such areas as attention and “mental imagery”) one finds not so much a lack of interest as a deliberate and adroit avoidance of the issue. I think I know why. Consciousness appears to be the last bastion of occult properties, epiphenomena, immeasurable subjective states—in short, the one area of mind best left to the philosophers, who are welcome to it. Let them make fools of themselves trying to corral the quicksilver of “phenomenology” into a respectable theory.

This would permit an acceptable division of labor were it not for the fact that cognitive psychologists have skirted the domain of consciousness by so wide a margin that they offer almost no suggestions about what the “interface” between the models of cognitive psychology and a theory of consciousness should be. I propose to fill this gap and sketch a theory of consciousness that can be continuous with and help to unify current cognitivist theories of perception, problem-solving, and language use. I fear that to the extent that the view I put forward is seen to meet these desiderata, it will _seem_ not to do justice to the phenomena, so it would help if first I said just what I am trying to do justice to. Nagel has epitomized the problem of consciousness with the question: “What is it like to be something?”[^1] It is certainly not like anything to be a brick or a hamburger; it certainly is like something to be you or me; and it seems to be like something to be a bat or a dog or a dolphin, if only we could figure out what. The question, “Is it like something to be an _X_?” may in the end be the wrong question to ask, but it excellently captures the intuitions that constitute the challenge to a theory of consciousness. Until one’s psychological or physiological or cybernetic theory explains how it can be like something to be something (or explains _in detail_ what is wrong with this demand), one’s theory will be _seriously_ incomplete. It is open to the theorist, of course, to reject the challenge out of hand. One can emulate those behaviorists who (it has been charged) “feign anesthesia” and categorically deny that anyone _has_ an inner life. This course has little or nothing to recommend it. Some behaviorists may find this comfortable ground to defend, but it would be awkward at the very least for the cognitivist, who has to explain what is going on when, for example, one asks one’s experimental subjects to form a mental image, or to give an introspective account of problem-solving, or to attend to the sentences in the left earphone rather than the sentences in the right earphone. The cognitivist must take consciousness seriously, but there are relatively noncommittal ways of doing this. One can somewhat paradoxically treat consciousness itself as something of a “black box” from which introspective and retrospective statements issue (with their associated reaction times, and so forth), but how is this black box fastened to the other boxes in one’s model? I shall propose an answer to this question, one that will also be a partial account of what is going on inside the black box.

## II

There is much that happens to me and in me of which I am _not_ conscious, which I do not experience, and there is much that happens in and to me of which I _am_ conscious. That of which I am conscious is that to which I have _access,_ or (to put the emphasis where it belongs), that to which _I_ have access. Let us call this sort of access the access of personal consciousness, thereby stressing that the _subject_ of that access (whatever it is) which exhausts consciousness is the _person,_ and not any of the person’s parts. The first step in characterizing this access is to distinguish it from two other sorts of access that play important roles in cognitive theories. The first of these can be called _computational access._ When a computer program is composed of subroutines (typically governed by an “executive” routine) one can speak of one routine having access to the output of another. This means simply that there is an information link between them: the results of computation of one subroutine are available for further computation by another subroutine. A variety of interesting issues can be couched in terms of computational access. For instance, Marvin Minsky faults the design of current chess-playing programs by pointing out that the executive programs typically do not have enough _access_ (of the right sort) to the routines that evaluate the various lines of play considered. Typically, the evaluator “has to summarize the results of all the search … and compress them into a single numerical quantity to represent the value of being at node A … [but] we want S [the output of the evaluator] to tell the Move Generator which kinds of moves to consider. But if S is a mere number, this is unsuitable for much reasoning or analysis.”[^2] It would be better if the higher executive had more access to the details of the line of play evaluated, and not just a summary judgment.

In a very different context, Julesz’ perception experiments using randomly generated dot displays show that at least some perceptual information about depth, for instance, is computed by a process that has _access_ to highly uninterpreted information about the pattern of light stimulating the retinas.[^3] Lines of computational access are currently being studied in cognitive psychology and related fields, and there are useful characterizations of direct and indirect access, variable access, gated access, and so forth. Computational access has nothing _directly_ to do with the access of personal consciousness, for _we_ do not have access to many things that various parts of our nervous systems are shown to have access to. For instance, some levels of the visual processing system must have computational access to information about inner ear state changes and saccadic eye movements, but _we_ do not, and _we_ have virtually no access to the information our autonomic nervous systems must have access to in order to maintain the complex homeostases of health.

The second sort of access to distinguish from both computational access and the access of personal consciousness might be called _public access._ Often it is useful to a programmer to have access to what the computer is doing, so that the computer’s progress on the program can be monitored; and to this end a “trace” is provided for in the program so that the computer can print out information about the intermediate steps in its own operations. One provides for public access of this sort by designing a print-out subroutine and giving it _computational_ access to whatever one wants public access to. This is a nontrivial additional provision in a program, for there is a difference between, say, the access the executive routine has to its subroutines, and the access the print-out routine has to the access the executive routine has. The domain of computational access for a system and the domain of public access for the system user are as distinct as the functions and offices of Secretary of State Henry Kissinger and Press Secretary Ron Nessen. Kissinger has computational access to much information that we the public have no access to, because Nessen, our avenue of access, has no computational access to the information. What is used for control is one thing, and what is available to the public is another, and there is at best a contingently large overlap between these domains, both in computer systems and in the White House.

The notion of public access seems to bring us closer to the personal access of consciousness, for we are speaking creatures (we have a sort of print-out faculty), and—at least to a first approximation—that of which we are conscious is that of which we can tell, introspectively or retrospectively. There is a problem, however. So far, the _subject_ of public access has not been identified. On the one hand we can speak of the _public’s_ access via print-out or other publication to what is going on in a system, and on the other we can speak of the _print-out faculty’s_ computational access to the information it publishes; but surely neither of these subjects is the “I” who has access to my contents of consciousness, nor does any more suitable subject appear likely to be found in this neighborhood.

There are other worries as well, of course. Nonhuman, nonverbal creatures have no print-out faculties, or at best very rudimentary and unexpressive print-out faculties, yet some philosophers—notably Nagel—insist that full-blown, phenomenological consciousness is as much their blessing as ours. I think one can be skeptical of this claim without thereby becoming the Village Verificationist, but the issue deserves an unhurried treatment of its own.

The picture of a human being as analogous to a large organization, with intercommunicating departments, executives, and a public relations unit to “speak for the organization” is very attractive and useful. The basic idea is as old as Plato’s _Republic_, but it seems to have a fatal flaw: it is not like anything to be such an organization. What is it like to be the Ford Administration? Nothing, obviously, even if it is like something to be a certain part of that administration. The whole is a very clever assemblage of coordinated _parts_ that at its best acts with a unity not unlike the unity of a single person,[^4] but still, it has no soul of its own, even if some of its parts do.

This apparently decisive shortcoming threatens a wide spectrum of theory-building enterprises currently receiving favorable attention in philosophy and psychology. Any philosopher of mind who (like myself) favors a “functionalist” theory of mind must face the fact that the very feature that has been seen to recommend functionalism over cruder brands of materialism—its abstractness and hence neutrality with regard to what could “realize” the functions deemed essential to sentient or intentional systems—permits a functionalist theory, however realistically biological or humanoid in flavor, to be instantiated not only by robots (an acceptable or even desirable consequence in the eyes of some), but by suprahuman organizations that would seem to have minds of their own only in the flimsiest metaphorical sense.

Davis has raised a graphic version of this objection with regard to functionalist theories of pain.[^5] Let a functionalist theory of pain (whatever its details) be instantiated by a system the subassemblies of which are not such things as C-fibers and reticular systems but telephone lines and offices staffed by people. Perhaps it is a giant robot controlled by an army of human beings that inhabit it. When the theory’s functionally characterized conditions for pain are now met we must say, if the theory is true, that the robot is in pain. That is, real pain, as real as our own, would exist in virtue of the perhaps disinterested and businesslike activities of these bureaucratic teams, executing their proper functions. It does _seem_ that there must be more to pain than that.[^6]

Psychologists cannot escape this embarrassment merely by declining to embrace philosophers’ versions of functionalism, for their own theories are vulnerable to a version of the same objection. Functionalist theories are theories of what I have called the sub-personal level. Subpersonal theories proceed by analyzing a person into an organization of subsystems (organs, routines, nerves, faculties, components—even atoms) and attempting to explain the behavior of the whole person as the outcome of the interaction of these subsystems. Thus in the present instance the shortcoming emerged because the two access notions introduced, computational access _simpliciter_ and the computational access of a print-out faculty, were defined at the sub-personal level; if introduced into a psychological theory they would characterize relations not between a person and a body, or a person and a state of affairs or a person and anything at all, but rather, at best, relations between _parts_ of persons (or their bodies) and other things. So far as I can see, however, every cognitivist theory currently defended or envisaged, functionalist or not, is a theory of the sub-personal level. It is not at all clear to me, indeed, how a psychological theory—as distinct from a philosophical theory—could fail to be a sub-personal theory.*]] So the functionalists’ problem of capturing the person as subject of experience must arise as well for these cognitivist theories. At best a sub-personal theory will seem to give us _no grounds_ for believing its instantiations would be subjects of experience, and at worst (as we have seen) a sub-personal theory will seem to permit instantiations that _obviously are not_ subjects of experience. Take your favorite inchoate cognitivist theory and imagine it completed and improved along the lines of its infancy; is it not always easy to imagine the completed theory instantiated or “realized” by an entity—an engineer’s contraption, for instance, or some kind of zombie—to which we have no inclination to grant an inner conscious life?

Intuition then, proclaims that any sub-personal theory must leave out something vital, something unobtainable moreover with subpersonal resources. Intuitions can sometimes be appeased or made to go away, however, and that is the task I set myself here. I propose to construct a full-fledged “I” out of sub-personal parts by exploiting the subpersonal notions of access already introduced.

This unpromising enterprise is forced on me, as students, colleagues, and other critics have insisted over the last few years, if I am to salvage the sort of functionalist theory of the mind I have heretofore defended. Since I have no other theory of the mind to fall back on, since in fact I see no remotely plausible alternatives to tempt me, I accept this problem as mine. It is not mine alone, though, as I hope I have made clear. This is fortunate, for the problem begs for a cooperative solution; my attempt trespasses deep in psychologists’ territory, and I would hope to stimulate assistance, not a boundary dispute, from that quarter.

The first step is to sketch a sub-personal flow chart, a cognitivistic model that by being sub-personal “evades” the question of personal consciousness but, unlike cognitivistic psychologies with which I am familiar, prepares attachment points for subsequent explicit claims about consciousness. The flow chart will be a philosopher’s amateur production, oversimplified in several dimensions, but I think it will be fairly clear how one could go about adding complications.

## III

For clarity I restrict attention to six of the functional areas to which a theory of consciousness must do justice (see Figure 9.1]]). At the output end we have the print-out component, and since this is our own Ron Nessen analogue I shall call it _PR. PR_ takes as input _orders to perform speech acts,_ or _semantic intentions,_ and executes these orders. The details of the organization of the _PR_ component are hotly contested by psycholinguists and others, and I do not wish to adjudicate the debates. Roughly, I suppose the breakdown to be as follows: the speech act command gets turned into an _oratio obliqua_ command (to say that _p_), and this gets turned into a “deep structure” specification—in “semantic markerese” perhaps—which in turn yields a surface structure or _oratio recta_ specification. We can imagine this to branch into either a phonological or graphological specification, depending on whether the initial command was to speak or write. These specifications, finally, drive motor subroutines that drive the vocal or writing apparatus to yield an ultimate execution of the input intention. There is a good deal of interaction between the levels: if one has difficulty pronouncing a certain word, this may count against its inclusion in the surface structure if one intends to speak, but not if one is writing.

![[11146_009_fig_001.jpg]]

Figure 9.1

_PR_ gets all its directions from a higher executive or _Control_ component, but the pool of information to which _PR_ has access is a special short-term memory store or buffer memory, which I shall simply call _M_. The lines of communication between Control, _M,_ and _PR_ are roughly as follows: suppose Control “decides” for various reasons to “introspect”:

- (1) it goes into its introspection subroutine, in which
- (2) it directs a question to _M;_
- (3) when an answer comes back (and none may) it assesses the answer: it may
    - (a) censor the answer
    - (b) “interpret” the answer in the light of other information
    - (c) “draw inferences” from the answer, or
    - (d) relay the answer as retrieved direct to _PR_
- (4) The outcome of any of (a-d) can be a speech command to _PR._

The point of the buffer memory _M_ is that getting some item of information into _M_ is a necessary but not sufficient condition for getting it accessed by _PR_ in the form of the _content_ of some speech act command.

Now what gets into _M_ and how? First let us look at perception. I assume a tier of perceptual analysis levels beginning with sense-organ stimulation and arriving ultimately at highly interpreted information about the perceived world, drawing often on more than one sense modality and utilizing large amounts of stored information. The entire process is variably goal-dependent. Again, the details of this stack of processes are controversial, but I shall venture a few relatively safe points. At the lowest levels we have what Neisser calls “iconic memory,”[^7] a very short storage of the stimuli virtually uninterpreted. “Parallel processing” by “feature detectors” takes us up several levels and yields crude but local-specific information about edges, corners, shapes, patches of color and so forth. From there a process of “hypothesis generation and confirmation” takes over, a _sequential,_ not parallel, process that utilizes both stored “world knowledge” (in the “expectation-driven mode”) and the results of the parallel feature detectors (in the “data-driven mode”) to determine the generation of hypotheses and their confirmation and disconfirmation. Perhaps the “data structures” at the highest levels of this process are Minsky’s “frames,” and perhaps they are not. All the processes of perceptual analysis, but especially the higher, sequential levels, are governed by complicated instruction from Control. As Neisser argues convincingly, with limited cognitive resources with which to perform this sophisticated task of perceptual analysis, Control must budget wisely, allocating the available cognitive resources to the sensory modality or topic of most current importance. This allocation of cognitive resources is the essence of _attention,_ Neisser argues, and I partially concur. There is a notion of attention that is very definitely a matter of allocation of cognitive resources.*]] This notion of attention, important as it is, is only very indirectly connected with consciousness, as can be seen at a glance if one considers the fact that any problem-solving or game-playing computer pays attention, in this sense, first to one candidate course of action and then to another, and presumably it would not on this ground be deemed conscious. Or consider the fact that a somnambulist must no doubt allocate considerable cognitive resources to the job of navigating successfully and maintaining balance while being, in some important sense, unconscious (and unconscious _of_ all this calculation) at the time. In this sense of attention, _unconscious attention_ is no contradiction in terms, and in fact no hints at all have been given to suggest what _conscious_ attention might be.

Now the perceptual analysis component sends information to _M from many levels._ Why? Because when one sees a complex scene and analyzes it as, say, a chair and a table in the middle of the room, one sees more than just that there are that chair and table there. One sees the shapes, colors, local details, and periphery too. I do not want to _identify_ what one experiences with what one _can say,_ but at least _if_ one can say something about some current feature of the perceivable world, one has experienced it. This is vividly brought out by tachistoscopic experiments.[^8] If one sees a string of four alphabetic letters flashed for a few milliseconds on a screen, one cannot usually identify them. Although the stimulus pattern persists in iconic storage after the actual external flash has ceased, this storage decays before the higher-level processors can complete their work; and once the data are lost, analysis must terminate. But one sees _something;_ one can say that one has seen a flash, or a flash with some dark objects, or even four letters or symbols. Something is experienced, even though perceptual analysis is not completed. In such a case, I am supposing, the results of however much analysis gets accomplished normally go to _M._ These results will go to other places of great importance as well, no doubt, but for our purposes all that matters is what gets into _M._*]]

Perception, then, sends a variety of inputs to _M._ Perceptual experience is not the only conscious experience we have, though, so what else must we suppose gets into _M?_ We are normally conscious of our thinking when we set out to solve problems, so let us very artificially isolate a _problem-solving component_ that sends its results to _M._ (At least for some sorts of problem-solving—“imagistic” problem-solving—it is tempting to suppose the processes utilize a lot of the machinery of perceptual analysis; hence the dotted lines in Figure 9.1]].) We shall return later to this component and its interactions with _M._ Another unit that sends information to _M_ is Control itself. A partial record of its goals, plans, intentions, beliefs gets installed in _M_ for occasional publication when the situation demands it.

These are the essential units of the system for my purposes here, but just to illustrate how the model could be extended, I add the dream-production unit. It loads _M_ as well, and, as I have argued in the preceding chapter, the question whether dreams are experiences is to be answered by assessing the nature of this memory-loading process (the “route taken” by the access arrow).

Before turning to the question of how such a sub-personal model could possibly say anything about consciousness, let me illustrate briefly how it is supposed to handle various phenomena. Fodor discusses an experiment by Lackner and Garrett.[^9] In dichotic listening tests, subjects listen through earphones to two different channels and are instructed to _attend_ to just one channel. They can typically report with great accuracy what they have heard through the attended channel, but not surprisingly they can typically say little about what was going on concomitantly in the unattended channel. Thus, if the unattended channel carries a spoken sentence, the subjects can typically report they heard a voice, or even a male or female voice. Perhaps they even have a conviction about whether the voice was speaking in their native tongue, but they cannot report _what was said._ One hypothesis, based on Broadbent’s filtering theory,[^10] is that a control decision is made to allocate virtually all the cognitive resources to the analysis of the attended channel, with only low-level (“preattentive”) processing being done on the input from the unattended channel. Processing of the unattended channel at the level of semantic analysis, for instance, is on this hypothesis _just not done._ Lackner and Garrett’s experiments disconfirm the Broadbent model in this instance, however. In the _attended_ channel subjects heard ambiguous sentences, such as, “He put out the lantern to signal the attack.” In the unattended channel one group of subjects received disambiguating input (e.g., “He extinguished the lantern”), while another group had neutral or irrelevant input. The former group _could not report_ what they heard through the unattended channel, but they favored the suggested reading of the ambiguous sentences more than the control group. The influence of the unattended channel on the interpretation of the attended signal can be explained only on the hypothesis that the unattended input is processed all the way to a semantic level, even though the subjects have no awareness of this—that is, cannot report it. On my model, this suggests that although higher-level processing of the unattended channel goes on, only low-level results are sent to _M._ This nicely illustrates the independence of computational access for _control_ (in this case, influencing perceptual set in the attended channel) from computational access for _publication,_ and gives an instance of, and an interpretation of, the well-known unreliability of introspective evidence. The _absence_ of introspective evidence that a certain analysis has been performed is never reliable evidence that no such analysis has been performed. The analysis in question may simply be one of the many processes that contribute in other ways to control, perception, and action, without loading _M_ with its results. I shall discuss more subtle cases of the relationship between such processing and introspective access later.

To pave the way for this, I want to say a bit more about the interaction proposed between _PR,_ Control, and _M._ I will settle for relatively crude suggestions, but the possibilities of such interactions can be—and to some extent have been—studied systematically. Relative retrieval times, lexical biases, the reliability of “tip-of-the-tongue” judgments, similarity spaces, and the like can provide an abundance of clues to guide the model builder. Consider James’ introspective account of having a forgotten name on the tip of one’s tongue:

> There is a gap therein, but no mere gap. It is a gap that is intensely active. A sort of wraith of the name is in it, beckoning us in a given direction, making us at moments tingle with the sense of our closeness, and then letting us sink back without the longed for term. If wrong names are proposed to us, this singularly definite gap acts immediately so as to negate them. They do not fit into the mold. … The rhythm of a lost word may be there without a sound to clothe it.[^11]

This passage, for all its phenomenological glories, is strikingly suggestive of purely functional interrelationships that might realistically be postulated to hold between the components of the model—or the components of a better model of course. Suppose a functionalistic model inspired by this passage were developed and supported in the usual ways: it would be part of the burden of this essay to mitigate resistance to the claim that an instantiation of such a model could _assert (knowing_ what it meant and _meaning_ what it said) just what James asserts in this passage.

Returning to the proposed interactions between _PR,_ Control and _M,_ suppose _PR_ gets a speech act command that for one reason or another it cannot execute. Words fail it. I propose that a failure discovery like this feeds back to Control, which will deal with the situation in a number of ways. It can alter its directions to Perceptual Analysis, producing a new perceptual set. This _may_ result in a reinterpretation of the incoming stimulation, producing a _changed_ input (at any level) to _M,_ and then a changed speech act command to _PR._ Being unable at first to describe one’s perceptual experience could lead in this way to a _change_ in one’s perceptual experience. This would help explain, for instance, the heightened capacity to discriminate—and experience—wines that comes from learning to use the exotic vocabulary of the wine-taster. (What I am proposing is, of course, a very Kantian bit of machinery, designed in effect to knit intuitions and concepts together. Any psychological theory must address this problem; in some models the Kantian perspective is more readily seen.) But if perceptual revision did not occur, Control could send to _PR_ a direction to say that one finds the experience ineffable or indescribable and this might be followed by a series of commands to say various things about what the experience was more or less like, about just how one’s words are betraying one’s true semantic intentions and so forth. What I am granting is that there is no guarantee that information loaded into _M_ has a publication in the native tongue that is acceptable to the system.

What kind of _information_ might fail to find expression in one’s native tongue? Although _M_ has been characterized as an information store, nothing has been said about the _form_ the information must take. What sort of “data structures” are involved? Is the information encoded “propositionally” or “imagistically” or “analogically”? These important questions deserve answers, but not here. It is important here, however, to explain why I refrain from answering them, and that will require a digression.

The current debate in cognitive psychology between the propositionalists and the lovers of _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett (see Chapter 10) is multifariously instructive to philosophers, not only because it contains echoes of philosophic controversies, but also because it clearly illustrates the close and systematic relationship between “pure” philosophy—especially epistemology—and empirical psychology. Psychologists, faced with the practical impossibility of answering the empirical questions of psychology by brute inspection (how does the human nervous system accomplish perception or cognition?) very reasonably ask themselves an easier preliminary question: how could _any_ (physical or mechanical or biological) system accomplish perception or cognition? This question is easier because it is “less empirical”; it is an _engineering_ question, a quest for a solution (any solution) rather than a discovery, but it is still dominated by a mountain of empirical facts—in particular, facts about the powers, limits, and idiosyncrasies of actual human perceivers under a wide range of conditions.

The psychological question becomes: how could any system _do all that?_ It is a question one is ill-equipped to answer if one does not know what _all that_ is—for instance, if one is a philosopher largely unacquainted with the psychologists’ data. Yet there is a strong aprioristic element in the psychologists’ investigations, because it turns out to be very difficult to compose any model at all that could conceivably _do all that._ What is wrong with most models is that they fail to satisfy some quite general constraint or constraints on all solutions. The charge often leveled against such models is not just that they fail to account for some body of data, but that they _could not conceivably_ account for human perception or cognition (for instance), since they violate some proclaimed necessary condition on all solutions. This aprioristic thinking is not peculiar to psychology. Engineers can enumerate necessary conditions for being an amplifier or a motor, and biologists can set down constraints on all possible solutions to the problem of the mechanics of genetic inheritance, to give just two examples. Once one decides to do psychology this way at all, one can address oneself to the problems raised by the most particular constraints, by middling constraints, or by the most general constraints. One can ask how any neuronal network of such-and-such features can possibly accomplish human color discriminations, or one can ask how any finite organic system can possibly subserve the acquisition of a natural language, or one can ask, with Kant, how anything at all could possibly experience or know anything. Pure epistemology thus viewed is simply the limiting case of the psychologists’ quest, and any constraints the philosopher finds in that most general and abstract investigation bind all psychological theories as inexorably as constraints encountered in more parochial and fact-enriched environments.

Notice, too, that the philosophers’ most abstract question is not asked in a factual vacuum; when we ask aprioristically how experience is possible, or what knowledge is, or how anything can be a symbol or have meaning, we appeal to, and are thus constrained by, an enormous body of commonplace facts: the facts that anchor what we mean by “experience,” “symbol,” and so forth. All the philosopher need know in the way of facts is what can be learned at mother’s knee, but that is not nothing. The psychologist says: “The experimental results bear me out, don’t they?” The philosopher says: “That’s what it is to understand an utterance, isn’t it?” If recently many philosophers of mind, knowledge, and language have found it useful or imperative to descend in the direction of more data, the reason is that the issues at the less general levels are proving to be fascinating, manipulable, and apparently useful in illuminating the more abstract level.

This is particularly apparent in the current controversy over propositions and _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett as vehicles of information in cognitive systems, a controversy of protean guise, sometimes appearing to be pure philosophy (and hence no business of psychologists!), sometimes an abstract engineering question for cyberneticists and the like, and sometimes a question of hard psychological, biological, or phenomenological fact. It has grown popular to the point of becoming second nature to talk of _information_-processing and transmission in the nervous system, but there is uncertainty and disagreement about the _a priori_ constraints on any such talk of information. There is often the illusion that no problems attend the psychologists’ talk of information, since information theory has presumably provided a rigorous foundation for such talk, but it is not often that psychologists have in mind any hard-edged information-theoretic sense of the term; usually what is being alluded to is the information or content an event within the system has _for the system as a (biological) whole_ (what the frog’s eye tells the frog’s brain, or better, as Arbib has suggested—in conversation—what the frog’s eye tells the frog). The _content_ (in this sense) of a particular vehicle of information, a particular information-bearing event or state, is and must be a function of its function in the system. This is the sense of “information” utilized in our model (and in psychological models generally); so when I assert that, for instance, there is a transfer of information from some perceptual analysis area to _M_, I endow that transfer event with content, and the content it has is to be understood as a function of the function within the whole system of that event. So far, that event’s function has been only circularly characterized: it has the function of conveying information about the results of analysis at that level to a functional area that is accessible to another functional area whose function is to express in a natural language just that information obtained by that level of perceptual analysis. Such a characterization is circular, but not necessarily viciously circular. The circle is a high-level holding pattern, which permits us to consider the constraints on any theory without descending to the next, more empirical level.

We can say, though, just what that next level down is. The content of a psychological state or event is a function of its function, and its function is—in the end, must be—a function of the _structure_ of the state or event and the systems of which it is a part. Not just any structures can realize the functions that we determine must be realized, but the step from functional constraint to structural constraint is treacherous[^12] and takes a philosopher quite far from home. When the question of “form of information” takes on this (quite proper) guise as a question of engineering, I leave it—reluctantly—to the engineers. I shall address myself shortly to the question in its “purely philosophical” or “phenomenological” guises. So, to end the digression, it would be best, for the time being, to stay in our circle and talk only of the _content_ of states and events, and not the structure of the vehicles of content.

Stopping at a level of description above the stern demands of structural realizations is thus engaging in an extended exercise in more or less well-motivated handwaving,*]] but this handwaving may well be saved by ultimate realizations of these information-processing components, and if it is, it will have been not only not in vain, but an essential propaedeutic to such theorizing.

One can never be sure, however. For instance, the Control component in my model is awfully fancy. It has a superb capacity to address just the right stored information in its long-term memory, a talent for asking _M_ just the right questions, and an ability to organize its long-and short-term goals and plans in a very versatile way. This is no homunculus that any AI researcher has the faintest idea how to realize at this time. The ever-present worry is that as we devise components—lesser homunculi—to execute various relatively menial tasks near the periphery, we shall be “making progress” only by driving into the center of our system an all-powerful executive homunculus whose duties require an almost Godlike omniscience. I can make no firm claims for the soundness of my components in this model. The most I shall venture for them is that they seem to me not to reproduce the problems at deeper levels, thus merely postponing solution.

## IV

With those qualifications and excuses behind me, I turn to the decisive question. Suppose an entity were all wired up in some fashion so as to realize the flow chart in Figure 9.1]]. What would it be like (if anything) to be such an entity? At first glance the answer seems to be: not like anything. The whole system has been designed to operate in the dark, as it were, with the various components accomplishing their tasks unperceived and unperceiving. In particular, we have not supposed any inner introspecting eye to be watching the perceptual analysis processes, the control decisions, the efforts of _PR_ to execute its orders. And yet to us on the outside, watching such an entity, engaging it in conversation, listening to its efforts to describe the effects on it of various perceptual environments, there will be at least the illusion that it is like something to be the entity. In fact it will tell us (or at least seem to be telling us) just what it is like. But inside it is all darkness, a hoax. Or so it seems. Inside your skull it is also all darkness, and whatever processes occur in your gray matter occur unperceived and unperceiving. Can it be said that just as there is some other point of view that _you_ have, there is some other point of view that _it_ has?

It is hard to know how to answer that question. But the following may help. Suppose I put forward the bold hypothesis that you are a realization of this flow chart, and that it is in virtue of this fact that it seems—to us and to you—that there is something it is like to be you. Can you give good grounds for denying the hypothesis, and if so, what are they? What personal access do you have, and to what? Here I must abruptly shift the perspective of this paper and wax phenomenological for a while. I want to draw your attention to a class of phenomena. If you have ever had a sudden _presentiment_ that someone was looking over your shoulder, or a _premonition_ that something dire was about to happen, you are acquainted with the phenomena. These events are propositional episodes, thinkings that _p;_ there is normally some inclination to express them (although the inclination is easily suppressed or canceled), and we may not even express them to ourselves in “inner speech.” When they occur in us, we have not the faintest idea what their etiology is (unless we have some theory about the causes of premonitions; my point is that “to introspection” they arrive from we know not where). There are other more familiar examples of coming to want to say something without knowing how or why. Witticisms “occur to us,” but we do not know how we produce them (the example is Ryle’s). Lashley long ago pointed out that if asked to think a thought in dactylic hexameter we (many of us) can oblige, but we have no awareness of how we do it: the _result_ arrives, and that is the extent of our direct access to the whole business. Lashley’s provocative comment on his example was that “no activity of the mind is ever conscious,” and the interpretation of this I am supporting is that we have access—conscious access—to the _results_ of mental processes, but not to the processes themselves.

My contention is that far from being rare and anomalous occurrences, propositional episodes, these thinkings that _p_, are our normal and continuous avenue to self-knowledge, that they exhaust our immediate awareness, and that the odd varieties, such as the presentiment that someone is looking over one’s shoulder, are striking only because of their isolation from the preceding and following presentiments, only because of our inability to follow them up with related propositional episodes about the same topic.*]] Right now it occurs to me that there are pages in front of me, a presentiment whose etiology is not known directly by me, but which is, of course, perfectly obvious. It is my visual system that gives me this presentiment, along with a host of others. I _can say_ all sorts of things to elaborate on and supplement my initial report. But if I am put in an abnormal perceptual environment—for instance, in a tachistoscopic experiment—I may be less sure _why_ I want to say what I do. I sort of have a hunch that it was an English word you flashed on the screen, but did I really _see_ it? I cannot say what word it was, or describe it in any detail.

Instead of cajoling you with further phenomenological persuasions, I shall enlarge upon my view by drawing an analogy to Hume. Hume’s revolutionary step in the analysis of causation was to suggest that we had it all backward. Earlier attempts at an account of our belief in causation supposed that when we saw a cause and then an effect we _saw_ the necessary connection between them, and _thereupon and therefore_ inferred or expected the effect when we saw the cause. Hume examined the cause (“turned it on all sides”) and could _find_ no such necessary connection to be observed, so he suggested that it was the other way around: having been conditioned, in effect, to infer or expect the effect when seeing the cause, we _found ourselves_ drawing the inference, and this gave rise to an illusion of sorts that we were _seeing_ a necessary connection that explained and grounded the inference we were compelled to make. The inference itself, Hume says, is psychologically and epistemically prior, and it gives rise to the belief in a “perceived” necessary connection. I am proposing a parallel account of “introspection”: we find ourselves _wanting to say_ all these things about what is going on in us; this gives rise to _theories_ we hold about how we come to be able to do this—for instance, the notorious but homespun theory that we “perceive” these goings on with our “inner eye,” and that this _perception_ grounds and explains the semantic intentions we have.

Hume might almost have arrived at this extension of his view. He claims (_Treatise,_ I.iv.2) that there is a difference between our “internal impressions” and our sensations. The latter require the positing of continuously existing external bodies in order to preserve the coherence and constancy of our discontinuous impressions of sensation. (Hume’s example is the fire in his study fireplace that gradually burns down, turning slowly to embers in the periods between the interrupted and different sensations he receives from the fire.) But, says Hume, “internal impressions” do not require this postulating; “on no occasion is it necessary to suppose that they have existed and operated, when they were not perceived, in order to preserve the same dependance and connexion, of which we have had experience.” This claim is virtually unavoidable for Hume—given his allegiance to a Lockean doctrine of the “transparency” of the mind to itself—but it is a fundamental error. Not only must we come to accept all manner of covert influences, unconscious problem-solving processes, and the like (recall Lashley’s dictum), but _mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett themselves_ are the creatures of a “posit,” an inference or extrapolation exactly analogous to Hume’s “posit” about external bodies. This is graphically illustrated by Shepard’s experiments with “rotating mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett.”[^13] The subjects in these experiments are shown pairs of line drawings like those in Figure 9.2]], and asked whether or not the pair are different views of the same shape. In this case, as you can quickly determine, the answer is yes. How did you do it? A typical answer is, “I rotated the image in my mind’s eye.” Amazingly, Shepard set out to see if he could determine the normal angular velocity of rotation of such _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett! How could he do this? The subjects were given buttons to press to give their answers. After tentative standard latency times were subtracted from both ends of the duration between display and answer, Shepard was left with durations that should, on his hypothesis, vary linearly (ignoring acceleration and deceleration) with the degrees of rotation required to bring the figures into superimposition. It should take roughly twice as long to rotate an image through 100 degrees as through 50 degrees. Shepard claims to have obtained significant positive results: he himself can rotate such mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett at an angular velocity of 62.6 degrees per second.

![[11146_009_fig_002.jpg]]

Figure 9.2

Now how can my view possibly accommodate such phenomena? Aren’t we directly aware of an image rotating in phenomenal space in this instance? No. And that much, I think, you can quickly ascertain to your own satisfaction. For isn’t it the case that if you attend to your experience more closely when you say you rotate the image you find it moves in discrete jumps—it flicks through a series of orientations. You cannot gradually speed up or slow down the rotation, can you? But now “look” again. Isn’t it really just that these discrete steps are discrete propositional episodes? Now it looks like _this,_ but if I imagine it turned _that_ much, it would look like _that_ … ah yes, it would eventually look just like the other one. But the flicking, you may insist, is clearly part of a motion observed—the axis of rotation is, perhaps, vertical, not horizontal. But your reason for saying this is just that your intermediate judgments define the rotation. They are judgments that fall in an order that would be the proper order of perceptual judgments in the case of watching a real image rotate around a vertical axis. If you are inclined to argue that only an internal system that actually did proceed by some rotation in space of a representation or image could explain the sequence of judgments and their temporal relations in such cases, you might be right, but your grounds are hardly overwhelming. In fact, these discrete series of judgments bear a striking resemblance to the discrete series of small flashing lights that create the illusions of perceived motion, which have received so much attention from psychologists.[^14]

We know that in these situations we all “perceive” motion—even elaborate orbital motions in three dimensions—when there is no motion. When we are confronted by a small group of these sequentially flashing lights we experience an illusion; we are led irresistibly to a nonveridical perceptual judgment that there is a single light moving in a particular way. What I am suggesting is that as the discrete series of flashes is to that nonveridical judgment, so our series of judgments in the image rotation case is to the judgment that something is really rotated in our minds (or in our brains, or anywhere). There _may_ be motion of something “behind” our judgments in the image rotation case, but if there is, it is something quite outside our present ken, and its very existence is suggested only by the most tenuous inference, however psychologically irresistible it may be.

My account of the Shepard phenomenon is that _however_ the problem-solving process is accomplished, it yields _results_, both final and intermediate, that are available in _M_ to be accessed by _PR._ These results, by the time they reach _PR_, are unproblematically propositional in nature: they are intentions to say that _p_. They are one product of perception or problem-solving. Another product is ducking when you see a flying object coming at you, but this is neither propositional nor imagistic so far as I can see. These products are perhaps only _indirect_ products of perceptual processes; the _direct_ or _immediate_ product, one might wish to say, is experience itself, and the question is whether experience is propositional or imagistic or something else. My answer, counterintuitive as it may seem at first, is that if that question has any admissible interpretation at all, introspection cannot answer it. We have no direct personal access to the _structure_ of _contentful_ events within us.

## V

Having given some suggestion about how the model I propose operates with a variety of phenomena, I now want to make some proposals about how the traditional categories of consciousness are to be superimposed on the model. These proposals are not supposed to be a priori truths about consciousness, or the dictates of conceptual analysis of our ordinary concepts, but rather suggestions about the best fit we can achieve between our pretheoretical intuitions (which are not entirely consistent) and a cognitive theory of the sort I have been sketching.

(1) One perceives more than one experiences. Perceptual analysis provides information about the world that is utilized in the control of behavior but is not accessible to introspection or consciousness, on any familiar understanding of these terms. In other words, there is nothing repugnant to theory in the notion of unconscious or subliminal perception or “subception,” and any intuitions to the contrary should be discarded.

(2) The content of one’s experience includes whatever enters (by normal routes) the buffer memory _M_. What one experiences may decay before it is in fact accessed by _PR_, or it may be garbled in transition to _PR_, or it may be relatively inaccessible to _PR_.*]] In virtue of this possibility of error or malfunction between _M_ and _PR_, _what one wants to say_ is not an infallible or incorrigible determinant of what one has experienced or is currently experiencing. So the content of one’s experience is given an _objective_ characterization, and any intuition we have to the contrary that we are the sole and perfect arbiters of what we experience should be discarded.

(3) One experiences more at any time than one wants to say then. What fills the “periphery,” adds detail to one’s “percepts,” inhabits “fringe consciousness,” is, as phenomenologists have insisted, _there_. Where? In _M_. No more mysterious process of presentation or apprehension of inhabitants of phenomenal space is needed.

(4) One experiences more than one attends to—in either of two senses of attention. One experiences more than what results from higher-level allocations of cognitive resources, and one experiences, as (3) asserts, more than one is currently thinking. These are entirely different ways in which there are unattended contents of consciousness, even though there is a strong contingent link between them. Usually Control fixes things so that what one is attending to in the former sense is what one is attending to in the latter. Put otherwise, our conscious access to what we are attending to is normally excellent.

(5) One’s access to one’s experience is accomplished via the access relations between _M_ and _PR_. As Anscombe would put it, we simply _can say_ what it is we are experiencing, what it is we are up to. This is accomplished without any inner eye or introspective faculty beyond the machinery invoked in the model.

(6) Our feelings of special authority in offering introspective reports—the basis for all the misbegotten theses of introspective incorrigibility and infallibility—arise from the fact that our semantic intentions, which determine what we want to say, are the standards against which we measure our own verbal productions; hence if we say what we mean to say, if we have committed no errors or infelicities of expression, then our actual utterances cannot fail to be expressions of the content of our semantic intentions, cannot fail to do justice to the access we have to our own inner lives.

(I claimed in _Content and Consciousness_ that this fact explained how we were, in a very limited and strained sense, incorrigible with regard to the contents of our awareness or consciousness. Now, thanks to the relentless persuasions of John Bender, William Talbott, Thomas Blackburn, Annette Baier and others, I wish to claim that this fact explains not how we are in fact incorrigible, but rather why people—especially philosophers—so often think we are. The fact does provide for what Gunderson calls the investigational asymmetry of some first-person claims, but the asymmetry is not profitably to be viewed, as I used to claim, as any sort of even limited incorrigibility.)

## VI

Having an inner life—being something it is like something to be—is on this account a matter of having a certain sort of functional organization, but the only natural entities that could be expected to have such functional organizations would be highly evolved and _socialized_ creatures. The prospect of a robot artificially constructed to replicate such a functional structure is not ruled out, but when one reflects on the activities such an entity would have to engage in to be more than an instantaneous version of such a system, the claim that it would be conscious loses—at least for me—its implausibility. We might not have the imagination to engage such a thing in interpersonal relations; it might not _seem_, in its metallic skin, to have an inner life or any prospect of an inner life. Such appearances are unreliable, however, for consider the possibility of there being a truly conscious entity (whatever one supposes this involves) that was just like us except that it operated on a time scale ten thousand times slower than ours. We would have a very hard time recognizing any of its day-long emissions as speech acts, let alone witty, cheery, doleful, heartfelt speech acts, and its ponderous responses to cuts and bruises would not easily enliven our sympathies; but if so, we would _ex hypothesi_ be ignoring a genuine, conscious person among us.

When we wonder if something or someone is conscious, it is tempting to view this as wondering whether or not a special light is turned on inside. This is an error, however, as we can see by asking questions about our own cases: was I conscious (or conscious of X) at time _t_? When we see that what settles the issues in our own case is a consideration of facts about our current capacities and past activities, and the best theory that can account for these, we are less reluctant to let the same considerations settle the issues in the case of others.

There is no _proving_ that something that seems to have an inner life does in fact have one—if by “proving” we understand, as we often do, the evincing of evidence that can be seen to establish by principles already agreed upon that something is the case. In this paper I set myself the task of constructing an “I,” a something it was like something to be, out of sub-personal parts of the sort encountered in cognitivistic theories. I do not now wish to claim that I have _demonstrably_ succeeded in this. Suppose we consider the two questions:

(1) Would an entity instantiating this theory sketch _seem_ (to “others,” to “us”) to have an inner conscious life?

(2) Would such an entity _in fact_ have an inner conscious life? Question (1) is an agreeably straightforward question of engineering. Perhaps the thing whose design I have sketched would impress the keenest skeptic, and perhaps it would be lamentably (or comically) unrealistic or mute or self-defeating. Whatever flaws the design has might have philosophical or psychological significance, or might be rather trivial blunders on my part. (Is my model akin to the blueprint for a perpetual motion machine, or have I merely forgotten to provide a way out for the exhaust gases?) Most if not all objections to _details_ in my model can be cast—even if not so intended—as grounds for denying (1), thus:

(3) Such an entity would not even _seem_ to have an inner conscious life because

- … it lacks any provision for such human phenomena as …,
- … it ignores …,
- … it would respond in situation - - - by doing …

I must take such objections seriously because part of my goal in this paper is to reveal, by imagined counterinstance, the implausibility of the charge that no entity describable solely by the resources of cognitivistic theory could _possibly_ seem to have an inner conscious life. If that charge is nevertheless true (I cannot imagine how that could be _shown_—but perhaps I shall live and learn), then cognitivism is forlorn, and this would be a fact of great importance to philosophy and psychology.

Suppose, however, that some cognitivistic model of consciousness (not mine, no doubt, but its kin, I like to think) encouraged a positive answer to question (1). Suppose some model passed all the appearance tests we could devise. How on earth should one then address question (2)? Is there a better course than mere doctrinaire verificationism on the one hand, or shoulder-shrugging agnosticism on the other? This is of course just “the problem of other minds,” and I propose that progress can be made on it by reexamining _what one knows about one’s own case_ in the light of the most promising theories of psychology. What convinces _me_ that a cognitivistic theory could capture all the dear features I discover in my inner life is not any “argument,” and not just the programmatic appeal of thereby preserving something like “the unity of science,” but rather a detailed attempt to describe to myself exactly those features of my life _and the nature of my acquaintance with them_ that I would cite as my “grounds” for claiming that I _am—_and do not merely _seem to be_—conscious. What _I_ discover are facts quite congenial to cognitivistic theorizing, and my tactic here has been to try, by persuasive redescription, to elicit the same discoveries in others. Skeptics can view the form of the argument, such as it is, as a challenge—to produce a rival description of some feature of conscious experience that is both acceptable to many (better, it should evoke enthusiastic agreement, it should ring a bell) and unassimilable by cognitivistic theorizing. I am aware of the irony of recommending something so reminiscent of the battle of descriptions that embarrassed the early introspectionists to death, but how else could anyone plausibly support the claim that one’s theory was a theory of _consciousness_?

## Notes

*]] Ryle and Wittgenstein are the preeminent modern theorists of the personal level. In fact, in their different ways they invent the enterprise, by showing that there is work to be done, that there are questions that arise purely at the personal level, and that one misconceives the questions if one offers sub-personal hypotheses or theories as answers. Typically readers who do not understand, or accept, these difficult claims see them as evading or missing the point, and complain that neither Ryle nor Wittgenstein has any positive psychological theory to offer at all. That is true: the personal level “theory” of persons is not a psychological theory.

*]] See p. 33ff above.

*]] _M_ is a special hypothesized memory location, defined functionally by its access relations to _PR,_ and it should not be confused with any already familiar functionally or anatomically defined variety of buffer memory, short-term memory, or echoic memory posited by cognitive theories to date. It may, for all I know, _coincide_ nicely with some variety of memory already proposed and studied, but eclectic as my model is, I do not intend here to be appropriating any existing notion from psychology.

*]] It only appears to be more specific handwaving when one talks not simply of contentful states and events but of cognitive _maps_, say, as opposed to stored _propositions._ People who like _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett say they are talking about _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett but not (of course!) about _pictures in the brain;_ people who like propositions say they are talking about propositions—which are not _at all_ like _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett—but also, of course, not _sentences in the brain._ There is plenty of doctrine about what _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett and propositions are _not_, but very little about what they are.

*]] Cf. Ryle’s illuminating account of “unstudied utterances” in _The Concept of Mind_ (London: Hutcheson, 1949) and Sellars’ treatments of “thinkings out loud” and “proximate propensities” to think out loud in _Science, Perception and Reality_ (London: Routledge & Kegan Paul, 1963). Ryle claims that our unstudied utterances “constitute our primary evidence for making self-comments (p. 183), and I am claiming that it is our proximate propensities to make unstudied utterances that constitute this primary evidence. Ryle probably would not deny this, for he says (p. 194): “One of the things often signified by ‘self-consciousness’ is the notice we take of our own unstudied utterances, including our explicit avowals, whether these are spoken aloud, muttered or said in our heads. We eavesdrop on our own voiced utterances and our own silent monologues.” Ryle offers no account, however, of just what one is doing when one eavesdrops on one’s silent monologues, nor could he give such an account without descending to the sub-personal level he wishes to avoid; _we_ do not do anything in order so to eavesdrop; we _just are_ aware of our own thinkings.

*]] Ryle says: “There is, however, a proper sense in which I can be said generally to know what has just been engaging my notice or half-notice, namely that I generally could give a memory report of it, if there was occasion to do so. This does not exclude the possibility that I might sometimes give a misreport, for even short-term reminiscence is not exempt from carelessness or bias” (_The Concept of Mind_, p. 160). Ryle permits himself to call this capacity “log-keeping,” and my _M_ is apparently just Ryle’s log-keeping system “paramechanized.” But surely Ryle’s own remarks on log-keeping, if taken seriously, constitute just the sort of paramechanical hypothesis he typically condemns. Why is Ryle led to such an uncharacteristic account? Not because he is _aware_ of keeping logs, and not because he finds himself or observes himself keeping logs. (A Rylean would be quick to ask him embarrassing questions about how many entries he writes in his log before breakfast, and how he writes them.) Ryle is led to this (happy) lapse, I suppose, because _what must be explained_, _viz._, our ability to report on so many different things that were just now happening, demands an explanation somehow in terms of an information—or memory—model. Another precursor of _M_ that may have occurred to the reader is Freud’s _preconscious_: “The question, ‘How does a thing become conscious?’ would be more advantageously stated: ‘How does a thing become preconscious?’, and the answer would be: ‘Through becoming connected with the word-presentations corresponding to it.’” (_The Ego and the Id_, New York: Norton, 1962, p. 10.)

1.]] Thomas Nagel, “What Is It Like to Be a Bat?” _Philosophical Review_, LXXXIII (1974): 435–51.

2.]] Marvin Minsky, “A Framework for Representing Knowledge,” MIT AI Lab Memo #306, 1974, subsequently published in P. Winston, ed., _The Psychology of Computer Vision_ (New York: Academic Press, 1975).

3.]] B. Julesz, _Foundations of Cyclopean Vision_ (Chicago: University of Chicago Press, 1971).

4.]] John Rawls, _A Theory of Justice_ (Cambridge: Harvard University Press, 1971).

5.]] Lawrence Davis, unpublished paper delivered to the Association for Philosophy and Psychology, M.I.T., October, 1974.

6.]] See also Sydney Shoemaker, “Functionalism and Qualia,” _Philosophical Studies_, XXVII (1975): 291–315; and Ned Block, “Troubles with Functionalism,” in C. Wade Savage, ed., _Perception and Cognition: Issues in the Foundations of Psychology_, _Minnesota Studies in the Philosophy of Science_, vol. IX (1978).

7.]] Ulric Neisser, _Cognitive Psychology_ (New York: Appleton Century Crofts, 1967). Neisser now wishes to banish iconic memory from a position of importance in the theory of perceptual processes—see his _Cognition and Reality_ (San Francisco: W. H. Freeman, 1976)—but I cannot see that he can go so far as to deny its existence, and I am not convinced it is not important.

8.]] Neisser, _Cognitive Psychology_, _op. cit._

9.]] Jerry Fodor, _The Language of Thought_ (New York: Crowell, 1975); J. R. Lackner and M. Garrett, “Resolving Ambiguity: Effects of Biasing Context in the Unattended Ear,” _Cognition,_ I (1973): 359–72.

10.]] D. E. Broadbent, _Perception and Communication_ (Oxford: Pergamon Press, 1958).

11.]] William James, _The Principles of Psychology_, vol. I, 1890; edition cited (New York: Dover Publications, 1950): 251–52.

12.]] The clearest account of false steps from functional premises to structural conclusions is to be found in Zenon Pylyshyn, “What the Mind’s Eye Tells the Mind’s Brain: A Critique of Mental Imagery,” _Psychological Bulletin_, LXXXVI (1973): 1–24, and “Representation of Knowledge: Non-linguistic Forms,” in _Theoretical Issues in Natural Language Processing_, Association for Computational Linguistics Preprint (1975).

13.]] R. N. Shepard and J. Metzler, “Mental Rotation of Three-Dimensional Objects,” _Science_, CLXXI (1971): 701–3.

14.]] I owe this observation to Michael Hooker.

Two Approaches to Mental Images

> “You don’t believe in me,” observed the Ghost. “I don’t,” said Scrooge.
> 
> “What evidence would you have of my reality, beyond that of your senses?”
> 
> “I don’t know,” said Scrooge. “Why do you doubt your senses?”
> 
> “Because,” said Scrooge, “a little thing affects them. A slight disorder of the stomach makes them cheats. You may be an undigested bit of beef, a blot of mustard, a crumb of cheese, a fragment of an underdone potato. There’s more of gravy than of grave about you, whatever you are!”
> 
> Scrooge was not much in the habit of cracking jokes, nor did he feel, in his heart, by any means waggish then. The truth is, that he tried to be smart, as a means of distracting his own attention, and keeping down his terror; for the spectre’s voice disturbed the very marrow in his bones.
> 
> _—_Dickens, _A Christmas Carol_

Of all the controversies currently raging in philosophy and psychology, none is being conducted with more vigor—if not rigor—than the debate over the nature of, and even the very existence of, mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett. Although the issues are various and complex, and although the arguments exhibit at least some of the variety and complexity the issues demand, one can describe the situation with negligible sociological distortion by saying that there is a _single_ war being fought on many fronts (or at least there seems to be in the minds of the participants), a war between the believers and the skeptics, the lovers of mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett—let us call them _iconophiles—_and those who decry or deny them—the _iconophobes._ Both sides have apparently decided to pretend to know what _mental representations_ are, and the issue is whether there are mental representations with properties peculiar to _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett.[^1] Whatever _mental_ representations are, they must be understood by analogy to non-mental representations, such as words, sentences, maps, graphs, pictures, charts, statues, telegrams, etc. The question is whether any of one’s mental representations are more like _pictures_ or _maps_ than like _sentences_, to take the favored alternative.

A curious feature of the debate is the passion it evokes, which is unlike the normal passion of scientific controversy in being as accessible to the layman and spectator as to the proprietors of the various theories. People often take a lively interest in a controversy in physics or biology or astronomy without feeling the need to take sides, and indeed without deeming themselves equipped to have an opinion worth promoting, but everyone, it seems, has a fiercely confident opinion about the nature and existence of mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett. This manifests itself in remarkable ways: in unhesitating predictions of the results of novel psychological experiments, in flat disbelief in the integrity of recalcitrant experiments, in gleeful citation of “supporting” experimental evidence, coupled with bland imperviousness to contrary evidence. Since this relatively uninformed or pretheoretical partisanship comes in both varieties—iconophile and iconophobe—one breathtakingly simple explanation of the phenomenon, and one that is often proposed, is that in fact some people do have mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett and others don’t. Each side naively extrapolates from its own experience. There are a variety of data that independently support this hypothesis. It is very likely that there are quite radical differences in people’s “imagistic” powers. That cannot, however, be the whole explanation of this confident partisanship. I wish to show that a subtle misconception of the issue underlies this curious phenomenon, a misconception that is as apt to beguile the theorist as the innocent bystander.

I will try to bring out the misconception by first setting out a picture of the issue from the point of view of the iconophile. This picture is familiar, but _out of focus, untuned_. I will then show how the issue is transformed by attention to a few distinctions. Here, then, is the _untuned_ version of the iconophile’s case.

The iconophile asserts that _there are_ mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, and moreover is prepared to back up this ontological claim by saying just _where_ in the scheme of things the mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett are. First, they are the typical _effects_ of certain sorts of causes. Mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett are, let us say, the typical effects of veridical visual perception, the ingestion of hallucinogens, the desire to solve geometry problems in one’s head, and so forth. There are all manner of mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, but let us take a particular case for discussion. On being asked whether it is possible to touch one’s right elbow to one’s left knee while touching one’s left elbow to one’s right knee, Lucy refrains from contortions and instead _forms a mental image_ of herself attempting this feat. Call the image itself, the target item in this debate, _a_. Here we have a certain train of events including, _inter alia_, Lucy’s (A) hearing the question, (B) coming to understand it, (C) coming to desire to answer it, (D) deciding not to contort herself, and finally (E) attempting to frame an image. These are all plausibly held to be events in the causal ancestry of _α_. No one of them itself is, or contains, or need be held to be or contain, a mental image, but as a sequence they form a particular natural prelude to a particular mental image, namely, _α_.

Moreover, says our iconophile, mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett are the typical _causes_ of certain sorts of effects. That is to say, the occurrence of a mental image such as _α makes a difference_: had it not occurred, later things that transpired would not have transpired, or would have transpired differently. In making this claim, our iconophile rejects epiphenomenalism, wisely, since no version of that bizarre doctrine merits attention.*]] Of all the effects typically produced by the occurrence of a mental image, one is singled out by its immediacy and the consequent dependence of other effects on it; let us call it the _apprehension_ of the mental image, being as neutral as possible about what apprehension might be. Perhaps there could be unapprehended mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett. If so, the people visited by them _do not believe_ they are visited by them, do not _make use_ of them (e.g., to solve problems, to answer questions, simply to perceive), do not remember them. That is what we shall mean by apprehension. A mental image that fails to be apprehended is like a stimulus that fails to stimulate; one can rule either out by definition if one wishes. Note that we have left it open whether one can _mis_apprehend a mental image or _unconsciously_ apprehend a mental image. As I said, we are being as neutral as possible about what apprehension might be.

But whether or not there might be unconsciously apprehended mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, there certainly are mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett that are consciously apprehended, supposing for the moment that all we mean by “consciously apprehended” is “believed by the subject to have occurred in virtue of having been apprehended at occurrence.” Thus, one of the effects of _α_ is Lucy’s subsequent belief that _α_ has occurred. Now we could say that this belief (partially) _constituted_ Lucy’s apprehension of _α_,

A→B→C→D→E→_α_→Apprehension of _α_ (= belief that _α_ occurred & …), or we could say that this belief was _caused by_ an intervening apprehension of _α_.

A→B→C→D→E→_α_→Apprehension of _α_ → belief that _α_ occurred.

In either case, we will acknowledge that such a belief is at least a typical consequence of the occurrence of a mental image such as _α_. It is a familiar fact that beliefs spawn beliefs with great fecundity, and no sooner would Lucy believe that _α_ had occurred than she would also believe a host of other things about _α_. Let us both arbitrarily and vaguely distinguish the manifold of beliefs _quite_ immediately produced by and “focussed on” or “about” _α_, and call this assemblage of beliefs _β_.

A→B→C→D→E→_α_→_β_ (= a manifold of beliefs about _α_).

Up to now, “_α_” has been the proper name of a particular mental image of Lucy’s, and “_β_” the proper name of one of its effects. Now generalize this bit of nomenclature. Let _α_ be any mental image and _β_ be its relatively direct effect in the realm of belief. There can be debate and disagreement among iconophiles over the scope and population of particular _β_-manifolds, but surely the conscious apprehension of an _α_ will produce _some_ beliefs about _α_. Put the uncontroversial beliefs in that _α_’s _β_-manifold and ignore the rest, since nothing will hinge on them. The outer boundary of a _β_-manifold will not be important to us, but the joint between an _α_ and its _β_-manifold will.

To some theorists, beliefs are themselves _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett or at least _like_ _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett. Armstrong, for instance, likens beliefs to maps[^2] and contrasts them to sentences. Since I want the _β_-manifolds to be agreed by all to be purely _non_-imagistic (whatever that means) believers in the imagistic nature of belief, if there are any, are asked to direct their attention to a slightly different item among the causal descendants of any _α_, namely to the causally-first entirely non-imagistic typical psychological effect of that _α_, and call _it_ the _β_-manifold of that _α_. It might, for instance, be a manifold of _dispositions to say and do various things_ in virtue of one’s (imagistic) beliefs about _α_.

This concept of a _β_-manifold is unavoidably lax, but perhaps its flavor can be conveyed by another example. Suppose I am inspired by my current mental image of a unicorn to draw a picture of a unicorn, indeed to _render_ my current mental image on the page. There are then _two_ _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett: my mental image, and its good or bad copy, the drawn image. Something intervenes between them. Of all that intervenes there are myriad descriptions possible, no doubt, most of them quite unknown to us. A plausible sketch of one such description would include in roughly this sequence: first, the mental image of the unicorn; then apprehension of that image, followed by (or constituting) belief that one is having just such an image; then delight in it; a subsequent desire to produce it for others; then an intention to draw, leading to an intention to grasp the pencil _just_ so—and so forth. Of all of this, I want to focus on the earliest and most immediate non-imagistic products of (or constituents of) apprehension, the manifold of beliefs about the mental image that one might suppose would remain relatively constant over changes in artistic desire, delight, boredom, and incidental knowledge, provided only that the image remained constant (supposing that notion to make sense—remember, this is the iconophile’s story). One cannot get entirely precise about this. For instance, the _desire to draw_ arrives hand in hand with the _belief_ that the image is, shall we say, art-worthy, and does _this_ belief belong in the _β_-manifold or not? I don’t think it makes any difference to my case one way or another.

Now I have tried to define _β_-manifolds in such a way that any iconophile will be happy to acknowledge the existence of _β_-manifolds as the first cognitive but non-imagistic difference the having of a mental image typically makes in us. By characterizing the _β_-manifold as itself non-imagistic, I mean to obtain a similar acquiescence from the iconophobe, for iconophobes do not for a moment wish to deny the existence of those manifolds of belief called by the iconophile _β-_manifolds. To deny that would be to deny the one truly obvious fact about mental imagery: people generally do think they have mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett and are disposed to say so. Iconophobes differ from iconophiles in denying that the normal or typical proximate causes of those manifolds are things or events having the earmarks of, and hence deserving to be called, __resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett_. The debate is entirely over the nature of the occupants of the _α_ role, _the nature of the normal causes of_ _β_-manifolds.

So far neither category is all that well characterized—not by me, and not by the participants in the psychological and philosophical debates, so it is not surprising that disagreement between iconophile and iconophobe should persist, should lack agreed-upon methods of resolution, should look like a _philosophical_ dispute. It is not. It is merely _embryonic-scientific._ What is remarkable is that so many people find it so difficult to let this issue be what it ought to be: a scientific, empirical question, amenable to experimental probing. Why should anyone feel differently? Most of us laymen are quite content to be not only ignorant but opinionless about the normal causes of dandruff, inflation, earthquake and schizophrenia. We will let science tell us if only it will. But, one is mightily tempted to say, the normal cause of my _β-_manifold to the effect that I am having a mental image of sort _x_ is (of course!) a mental image of sort _x_, and I already know more about that mental image of mine, simply in virtue of having it, than science could ever tell me. This temptation is at the heart of the misconception I hope to uncover and dispell. I hope to dispell it by pointing out and clarifying two approaches to mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett that are very different, but not at war. The _first_ approach has just in fact been sketched, if fuzzily. It can be called, without intending thereby to do it special honor or dishonor, the _scientific approach._ It proceeds by defining mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett as the _normal causes_ of _β_-manifolds. (The existence of _β_-manifolds is uncontroversial, however murky and ill-defined the concept is.) This approach treats the hints, avowals and protestations of image-havers as issuing in the normal way from their _β_-manifolds. Having various beliefs about what is going on in them, people come to say various things, and these utterances are useful _data_ about the nature of mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett. The subjects have no more special authority about the nature of their own mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, on this approach, than about the nature of their genes or germs. This approach calmly ignores the claim of the image-haver to be _authoritative_ about the nature of his or her mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett.

By defining mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett as the normal causes of _β_-manifolds, the scientific approach virtually guarantees the _existence_ of mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett—since it is highly probably that _β_-manifolds have some normal pattern of causation—and conducts an investigation to see whether mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett are well named. Are mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett __resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett_ of any sort? Similarly, atoms came to be defined as the smallest units of a chemical element, and then it remained a question of whether atoms were well named. They weren’t, as it turned out, and the scientific approach to mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett is just as prepared to discover non-imagistic mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett as physicists were to discover splittable atoms.

The stance of the scientific approach to the avowals of image-havers will benefit from further characterization. The scientist confronted by an organism that “talks” is free to treat the vocal emissions of the specimen as mere sounds, the causes of which are to be investigated. This is the way we treat human snores, for instance. If one makes this choice, one is not then confronted with questions concerning the semantic interpretation of the sounds, their meaning, truth or falsity. For snores this proves to be no hardship, but all attempts to treat those sounds we call human utterances in this non-semantic way have failed to produce credible accounts of their etiology, to put it mildly. Once one makes the decision to treat these sounds as utterances with a semantic interpretation on the other hand, one is committed to an intentionalistic interpretation of their etiology, for one has decided to view the sounds as the products of communicative intentions, as the expressions of beliefs, or as lies, as requests, questions, commands and so forth. Under the scientific approach to mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, the image-haver is _not_ subjected to the indignity of having her avowals and commentaries treated as mere noise: they are granted the status of expressions of belief, assertations made with both reason and sincerity. The scientific approach simply refrains from granting at the outset that the beliefs being expressed are _true_ or even well grounded. In this the scientific approach deviates slightly from the normal mode of interpersonal communication. If in the course of conversation I assert that _p_ and you set to wondering out loud what might _cause_ me to believe that _p_, you bid fair to insult me, for your wondering leaves open the gaping possibility that my belief that _p_ might lack the proper epistemic credentials, might be _merely_ caused (might be caused in some way other than that mysteriously _right_ sort of causal way alluded to in the various causal theories of perception, reference, memory, inference, etc.). It is this possibility that the scientific approach to mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett leaves explicitly open, just in order to investigate it. What normally causes people to believe that the sun is shining is the sun’s shining, but what normally causes people to believe that everything they are now experiencing they have experienced once before is not their having experienced it all once before. The scientific approach to mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett leaves open the possibility that mental image beliefs, _β_-manifolds, might have an explanation more akin to the explanation of _déjà vu_ than to that of normal visual perception.

The _other_ approach to mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, which I will name in due course, is initially more congenial in _not challenging_ or questioning the epistemic credentials of image-havers’ beliefs. Indeed, the _β_-manifold of a person can be endowed with a certain authority: the authority to _create a world_, the world that is the logical construct of the manifold of beliefs. Any set of beliefs determines a world; if the beliefs are all true, the world thus determined coincides with a portion of the real world. If any are false, the set determines a world that is at least partly fictional (e.g., the world of Dickens’ London). If the set of beliefs is inconsistent, the world determined will contain objects with contradictory properties, but that is all right, since the objects are not real objects but merely _intentional objects._[^3]

The second approach to mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett defines them not as the _normal causes_ of _β_-manifolds, but as the _intentional objects_ of _β_-manifolds.

![[11146_e010_001.jpg]]

By defining mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett as the intentional objects of _β_-manifolds, the second approach guarantees the _existence_ of mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett _as logical constructs._ That is, it guarantees them the odd but surprisingly robust existence of intentional objects generally, what Brentano called “intentional inexistence.” On this approach, mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett are at least as real as Santa Claus. Just as one might set out to learn all there is to be learned about Santa Claus, the intentional object, so one might set out to learn all there is to be learned about those intentional objects, people’s mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett.

Note that there are truths and falsehoods about Santa Claus. It is true that Santa Claus has a white beard and drives a flying sleigh, false that he is tall and thin. Focussing on intentional objects like this does not require a presumption of fiction or falsehood. Consider the difference between setting out to learn all there is to know about Queen Elizabeth II, and setting out to learn all there is to know about Queen Elizabeth II, the intentional object constructable from the beliefs of British school children under the age of ten. The latter investigation might prove both more interesting and more useful than the former.

But to return to our second approach to mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, why would one be interested in the logical construction route in this instance? The scientific approach was seen to be blandly _uncommitted_ to the truth of _β_-manifolds; _its_ mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, the normal causes, exist with the features they do whether or not people’s beliefs about them are true. This second approach seems blithely _unconcerned_ with the truth of _β_-manifolds; _its_ mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, the logical constructs, or intentional objects, exist (as logical constructs) with precisely the features they are believed to have—whether or not the beliefs are true (true _of_ anything real). Could one claim that this second approach would be a serious pursuit?

An extended analogy may convince us that it could be, by exhibiting in more detail the program of such a study. Suppose anthropologists were to discover a tribe that believed in a hitherto unheard of god of the forest, called _Feenoman._ Upon learning of Feenoman, the anthropologists are faced with a fundamental choice (not a deliberate, intentional choice, but a choice-point, an opportunity): they may convert to the native religion and believe wholeheartedly in the real existence and good works of Feenoman, or they can _study the cult_ with an agnostic attitude. Consider the agnostic path. While not believing in Feenoman, the anthropologists nevertheless decide to study and systematize as best they can the religion of these people. They set down descriptions of Feenoman given by native informants; they look for agreement, seek to explain and eliminate disagreements (some say Feenoman is blue-eyed, others say he—or she—is brown-eyed). Gradually a logical construction emerges: Feenoman, the forest god, complete with a list of traits and habits, and a biography. These infidel scientists, or Feenomanologists as they call themselves, have described, ordered, catalogued, inventoried, the relevant belief-manifolds of the natives, and arrived at the _definitive_ description of Feenoman. Note that the beliefs of the natives are authoritative (he’s _their_ god, after all), but only because Feenoman is being treated as _merely_ an intentional object, a mere fiction as we disbelievers insist, and hence entirely a _creature_ of the beliefs of the natives, a logical construct. Since those beliefs may contradict each other, Feenoman, as logical construct, may have contradictory properties attributed to him. The Feenomanologists try to present the best logical construct they can, but they have no overriding obligation to resolve all contradictions—they are prepared to discover unresolved and undismissible disagreements among the devout.

The believers, of course, don’t see it that way—by definition, for they are the believers, to whom Feenoman is no _mere_ intentional object, but someone as real as you or I. Their attitude toward their own authority about the traits of Feenoman is—or ought to be—a bit more complicated. On the one hand they do believe they _know_ all about Feenoman—they are Feenomanists, after all, and who should know better than they? Yet unless they hold themselves severally to have some sort of papal infallibility, they allow as how they could _in principle_ be wrong in some details. They could just possibly be instructed about the true nature of Feenoman. For instance, Feenoman himself might set them straight about a few details. Thus, a native Feenomanist who fell in with the visiting Feenomanologists and adopted their stance would have to adopt an attitude of distance or neutrality toward his own convictions (or shouldn’t we have to say his own _prior_ convictions?), and would in the process suffer some sort of crisis of faith, and pass from the ranks of the truly devout. (Cf. the old joke about Santayana’s curious brand of Roman Catholicism: Santayana’s creed, it was said, is that there is no God and Mary is His Mother.)

We can imagine another group of anthropologists who study the Feenomanists and their religion, who are also disbelievers or agnostics, like the Feenomanologists, but who set out to plot the normal causes of Feenomanist belief-manifolds. Their first step would be to learn what the Feenomanologists had charted out of those belief manifolds. This might provide valuable clues about the normal causes of the manifolds. This would be especially true if Feenomanism turns out to be true religion; we leave it open, that is, for the scientific cause-seekers to discover Feenoman and confirm the native religion. The whole investigation might, however, prove fruitless; perhaps there are no normal or projectible or salient patterns in the events that lead Feenomanists to their creed. (Cf. the conditioning of “superstitious” behavior in pigeons via random reinforcement schedules by Herrnstein _et al_.) What if these cause-seekers ask the Feenomanists, the believers, about the normal causes of their beliefs? The Feenomanists will insist, naturally, that the normal causes of their Feenomanist belief manifolds are the words and deeds of Feenoman. The anthropologists might discover otherwise. They might discover that the normal cause of the bulk of Feenomanist belief manifolds was the trickery of Sam the Shaman. This would be a particularly interesting case for us, for no matter how directly and reliably Sam the Shaman’s activities determined the content of Feenomanist beliefs about Feenoman, we would not on that account alone be entitled or inclined to _identify_ Feenoman as Sam the Shaman. Identification depends on the truth of the beliefs caused. If an impressive number of the most important traits of Feenoman are traits of Sam the Shaman, we will be tempted to identify the two. So, for that matter, will the Feenomanists themselves—a telling test. But probably two overridingly important traits of Feenoman are his immortality and his supernatural power, and no doubt the lack of these in Sam the Shaman would count decisively, both in our eyes and the Feenomanists’, against identifying the intentional object of their creed with its normal cause. It seems, however, to be a matter almost of taste: we will learn which traits of Feenoman are _essential_, and for whom, when we see which Feenomanists (if any) accept the proposed identity.

It is time to start drawing morals from this extended analogy. I expect the morals I wish to draw are obvious, but since there are so many of them, and since I mean to draw them all, I must try your patience by making them all explicit. The second approach to mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett I shall _call_ the _phenomenological approach,_ but with the following warning: I mean to be _prescribing_ an approach rather than _describing_ an existing discipline. When I speak of the phenomenological approach I shall mean the approach I am here outlining _whether or not_ any people who call themselves phenomenologists would accept it as an accurate description of their enterprise. My position, to put it bluntly, is that if what I call the phenomenological approach does not describe the program of Husserl, Sartre and their followers, so much the worse for their program. I intend to _defend_ the phenomenological program as I describe it, and I call it the phenomenological program because it seems to me—no Husserl scholar, but an amateur of long standing—to do justice to what is best in Husserl.[^4] I am even less of a Sartre scholar, but my reading of Sartre also encourages me to use this name for the approach described.[^5] I would be happy to learn that my description finds favor among self-styled phenomenologists; if it does not, I shall change the name of the approach, not its description.

The phenomenological approach, then, sets out to rationalize the _β_-manifolds of people by describing the intentional objects that are their logical constructs. It proceeds independently of any search for the causes of those _β_-manifolds, and can _afford_ to ignore experimental data about reaction times, interference effects and the like (I don’t recommend that it turn its back on these data, but it may).[^6] Its master, like the master of the New Critics, is the _text_, in this case the protocols, the introspective declarations, avowals, revisions, confessions of _subjects_ or image-havers (see Chapter 9).[^7] It treats these declarations, once care has been taken to allow for editorial revision, as authoritative, as _constituting_ the features of the intentional objects being studied. In so viewing these declarations, the phenomenologists adopt an attitude fundamentally if subtly different from the attitude of the subjects themselves, for the phenomenologists must view the mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett that are the focus of _their_ investigation as possibly only _mere_ intentional objects, while by the subjects the mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett are _believed to be real—_“by definition,” provided the subjects are sincere. Phenomenologists will be interested to discover inconsistencies between the accounts of different subjects, and even within the accounts of a single subject on a single occasion, and will make more or less standard efforts to remove these by queries and challenges designed to provoke thoughtful expression, retraction of misspoken claims, and so forth. But if inconsistencies remain after such purification of the text has gone as far as it can go, then the phenomenologists will be under no obligation to force consistency onto the _β_-manifolds they are cataloguing. Of course, the probing and challenging may well _effect a revision_ in their subjects’ _β_-manifolds, just as the anthropologists’ sophisticated questions might well provoke doctrinal revision, invention, or clarification in the Feenomanists’ creed. Under such probing some subjects might even come, rightly or wrongly, so to alter their _β_-manifolds that they no longer could be said to believe in mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett—and so _in the sense of the phenomenological approach_ they would _cease to have_ mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett. (Under intense anthropological scrutiny from Feenomanologists, Feenoman might cease to exist. That seems to have happened to Satan, for instance.)

Like the Feenomanists, subjects ought to have a complicated attitude toward their own authority about their mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett. On the one hand, believing as they do, they take their introspective descriptions to be truths, but if so, then they ought to grant that the real objects they are attempting to describe might turn out to have properties unrecognized by them, might turn out to be not as they believe them to be, _might_ even turn out not to exist.[^8] There is thus a tension between their attitude as subjects and the attitude of the phenomenologists studying them, and just as the Feenomanist turned Feenomanologist could not both cling unproblematically to his faith and study it, the _auto_phenomenologist studying his own mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett must find a built-in frustration in his efforts at “bracketing” or _epoché_, in divorcing himself from the implications of those of his beliefs he is studying at the very moment he is drawing out those implications. If he succeeds in suspending belief in them, then to that extent he succeeds in altering the _β_-manifold he is attempting to study. Autophenomenology is so difficult that even experts typically fail, and end up studying some artifact of their own enterprise.*]]

The tension between the attitude that takes mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett to be constituted “authoritatively” by _β_-manifolds and the attitude that takes mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett to be the real, normal causes of those _β_-manifolds, helps to create a spurious _third_ approach to mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, an approach that tries to treat mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett as both incorrigibly known and causally efficacious. Such marvelous entities would have to inhabit a medium more transparent to cognition than ordinary physical space, yet more actual and concrete than the mere logical space in which logical constructs, possible worlds, and the like reside. Call it, as many have, _phenomenal space._ It is as if our Feenomanist turned Feenomanologist were to grasp in his confusion at the desperate stratagem of inventing a god-space, or heaven, for his beloved Feenoman to reside in, a space _real_ enough to satisfy the believer in him, but remote and mysterious enough to hide Feenoman from the skeptic in him. Phenomenal space is Mental Image Heaven, but if mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett turn out to be _real_, they can reside quite comfortably in the physical space in our brains, and if they turn out not to be real, they can reside, with Santa Claus, in the logical space of fiction.*]]

This point is so central to my purpose, and people are often so resistant to it, that I wish to take a bit of time to explore its implications. I have been speaking as if _β_-manifolds were uniform from subject to subject, but of course they are not. Not only do people differ in the kinds of imagery they report, but they hold all manner of different _theories_ about what they call their mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, and hence their _β_-manifolds vary widely in content. Some people think their mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett are epiphenomena; some people think their mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett are something-or-others that happen in their brains; some may think their mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett are _merely_ intentional objects, fictitious things they are mightily tempted to believe in, and hence, when they let their guard down, do believe in. (Cf. Scrooge.) Now, if anyone believes his mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett are information-carrying structures in his brain that deserve to be called _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett because they have certain structural and functional properties, and if science eventually confirms that the normal causes of that person’s _β_-manifolds are just such structures, then he can happily _identify_ intentional object with cause. It will turn out for him that imaging is, and has been, like normal veridical perception: just as pigs cause one to see pigs and cows cause one to see cows, _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett have caused him to believe he is having _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett. That is the scientific iconophile’s prediction. If, on the other hand, that person’s beliefs turn out to be false, if they turn out to be caused by things in the brain lacking the peculiar features of _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, then the scientific iconophobe will turn out to be right, and we will have to say that that person’s _β_-manifolds are composed of (largely) false beliefs, what one might call systematically illusory beliefs. We ought to be able, in such a case, to convince the person of this. Success in this attempt would eliminate those beliefs, but not necessarily the temptation or disposition to lapse back into them. (Some illusions are “stronger” than others.) Complete success would “cure” the delusions, and, from the phenomenological point of view, eradicate the mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett.

What if someone holds that his mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett are not physical events or structures in his brain at all, but rather either epiphenomena or items in phenomenal space, having automatically cognizable features, obeying a different logic from that of _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett in physical space, etc.? It is _not_, I am saying, an empirical question whether _he_ is right: he is wrong, and it doesn’t take science to prove it; it takes philosophy to prove it. Philosophy’s job in this entire issue is to clear away hopeless doctrines like these, and leave the issue between scientific iconophile and scientific iconophobe as the only issue with _ontological_ significance. _In principle_ there could be a parallel question to dispute within the purely phenomenological approach. The debate between iconophile and iconophobe phenomenologists could only concern the correct or best extrapolation of intentional objects from _β_-manifolds. Such questions can be discussed with serious intent and rigor, but if scientific iconophobia turned out to be the truth, they would be of rather rarefied and diminished significance. They would be parallel to such questions as “Does Santa Claus have eight or nine reindeer? (Is Rudolph _established_?)” and “Was Apollo a murderer?” If scientific iconophilia turns out to be true, there could be a rich and interesting interaction between the scientists and the phenomenologists. It would not follow from the truth of scientific iconophilia that the _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett of phenomenology could be identified with the newly discovered or confirmed _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett in the _α_-position, for the latter might be, while undeniably __resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett_, very unlike the intentional objects they produced. Of course for any particular mental image creed that turns out to be _true religion_, the scientific approach will answer all the questions the phenomenological approach leaves indeterminate. There is always more to learn about _real_ things about which we have (some) true beliefs.

A third approach of sorts to mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett can be obtained by noting the possibility of an equivocation in the notion of a _logical construct._ Suppose someone says, as I guess many philosophers are tempted to say: “_Whatever_ science discovers, it can’t discover that I don’t have mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett when I think I do, and not merely as intentional objects of my beliefs.” This is a powerful intuition, I grant, but one best combatted. If one wants, however, one can grant the point by a simple logical maneuver: define mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett as logical constructs out of _β_-manifolds in a different sense of logical construct: make having the _β_-manifold _criterial_ for having a mental image; make it tantamount to having a mental image. One could then say, “What it _means_ to say I am having a mental image is that I am having a certain _β_-manifold.” This view might be called logical internal behaviorism. It is an approach of sorts, but an approach that has already reached its goal. It leaves nothing more (of interest) to be said about mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett. It is like the phenomenological approach in not being at war with the scientific approach, though people who hold such doctrines often cast their claims in terms that strongly suggest otherwise. (Cf. Norman Malcolm, _Dreaming_, 1959.) Note too that on this view we already know what mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett are, and one thing is clear: they are _not_ _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett of any sort, or anything _like_ _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett. They are no more like _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett than home runs are like baseballs or fences.

Is my position then best described as iconophile or iconophobe? With regard to the legitimate scientific disagreement about the nature of mental representations, this paper is so far entirely neutral. It has so far merely attempted to clarify that issue by distinguishing it sharply from spurious—if traditional—debates about entirely mythical species of mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett: the various non-physical, phenomenal or epiphenomenal, self-intimating, transparent to cognition, unmisapprehensible, pseudo-extended, quasi-imagistic phantasms that have often been presented as mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett in the past. About these I am a devout iconophobe. What do I put in their place? What is left to be going on in one once these pretenders have been dismissed? In their place there are only the _α_’s—the causes, and the _β_-manifolds—the effects, and about these I have been neutral, for the question whether either of these is properly imagistic is not a pure philosophical question, but a question of psychological theory or meta-theory. At the outset I required a definition of _β_-manifolds that had them non-imagistic, but now we can see that that was a temporary bit of scaffolding; at the time there seemed to be just _one_ question about mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, and that proviso was needed to isolate that one question. Now we can see that in fact there are two—indeed many more than two—roles in cognitive theories that _might_ be filled by information-bearing structures that deserved to be called _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett. Armstrong, you recall, likens beliefs themselves to maps, and perhaps he is right: perhaps when psychological theory develops—if it ever does—to the point where there are well behaved and well attested theoretical entities playing roughly the _α_ and _β_ roles, it might prove appropriate to conceive of the _β_ items as _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett of some sort. That is of course an issue that is far removed from any introspector’s authority to settle. The considerations that count concern the power and versatility of different kinds of information-bearing structures, or data-structures as they are called in computer science. There is much that can already be said by iconophiles and iconophobes in support of their positions, but this is not the time to review that debate.*]] The goal of this essay has just been to clear the decks so that debate can proceed unhindered by misconceptions about what we might call the metaphysical status of mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett.

## Notes

*]] I find there are different senses of the term “epiphenomenal” in currency today. The epiphenomenalism I have just rudely dismissed is the view that there are epiphenomena that are more or less accompanying shadows of events in the brain, but that these epiphenomena _have no physical effects at all._ One might say this is one half of Cartesian interactionism: bodily events cause (or occasion) mental events (epiphenomena), but these are themselves causally inert. Some people, notably psychologists, often use “epiphenomenal” to mean, roughly, “nonfunctional.” Thus if I engrave curlicues and filigrees on the connecting rods of an engine they are merely epiphenomenal in this sense—they don’t contribute or detract from the normal functioning of the engine, but of course they do have effects in the world by which their presence may be determined: they reflect and absorb light, for instance. That there are non-functional physical properties of brain events (or non-functional brain events) we already know; we will need an impressive argument to show there are non-functional (and causally inert) non-physical events or properties.

*]] For example, imagine the plight of the autophenomenologist who set out to study the intentional objects that accompanied his engagement in wildly abandoned sex; he would end up studying the intentional objects of someone engaged in sex while simultaneously performing _epoché_—hardly the same experience at all. According to Schacht, “Phenomenology proper is characterized by Husserl as ‘the critique of transcendental experience’ _(I_[_deas_] 29); ‘transcendental experience’ is said to be ‘a new kind of experience’ (I, 27), distinct from ordinary experience; and the ‘phenomenological reduction’ is held to be the operation through the performance of which this ‘new kind of experience’ becomes accessible to us” (_op. cit_., p. 298).

*]] Nothing I have said here _requires_ materialism to be true. If the followers of the scientific approach wish to be dualists, they can set out to study dualistic causation of _α_, of _β_ by _α_, and of behavior, etc., by _β_. My objections to dualism are of the familiar sort, and are recounted in other writings, but no appeal is being made to them here. Even for the dualist, I am insisting, there is no way to _merge_ the two approaches. The dualist must either be investigating, or talking about, the occupants of the _α_-role, or intentional objects. A non-physical cause would have to bear the same relation to an intentional object as a physical cause.

*]] In _Content and Consciousness_, in spite of my efforts to distinguish what I distinguish here in terms of the two approaches (there I spoke of the personal and sub-personal levels of explanation), I mixed metaphysical iconophobia, the view I have just espoused, and scientific iconophobia, a view I still favor, though cautiously. That is, many of the iconophobic arguments and claims of _Content and Consciousness_ are properly viewed as contributions (good or bad) to psychology, not philosophy.

1.]] The psychological literature on mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett is growing so fast that a citation of current work will be out of date before the ink is dry. There is even a new journal, the _Journal of Mental Imagery_. A few classic papers are R. N. Shepard and J. Metzler, “Mental Rotation of Three-Dimensional Objects,” _Science_, CLXXI (1971): 701–3; L. A. Cooper and R. N. Shepard, “Chronometric Studies of the Rotation of Mental Images,” in W. G. Chase, ed., _Visual Information Processing_ (1973): 75–115; Allan Paivio, _Imagery and Verbal Processes_ (New York: Holt Rinehart and Winston, 1971); Zenon Pylyshyn, “What the Mind’s Eye Tells the Mind’s Brain: a Critique of Mental Imagery,” _Psychological Bulletin_ (1972) and “Imagery and Artificial Intelligence,” in C. Wade Savage, ed., _Minnesota Studies in the Philosophy of Science_, vol. IX (University of Minnesota Press, 1978), and M. Kosslyn, “Information Representation in Visual Images,” _Cognitive Psychology_ (1975), vol. 7 : 341–70.

2.]] D. M. Armstrong, _Belief, Truth and Knowledge_ (Cambridge University Press, 1971).

3.]] Cf. my “Geach and Intentional Identity,” _Journal of Philosophy_ LXV, 11 (1968): 335–41.

4.]] I find support for my reading of Husserl in Richard Schacht’s excellently lucid article, “Husserlian and Heideggerian Phenomenology,” _Philosophical Studies_, XXIII (1972): 293–314. Schacht proposed there to distinguish what he calls _phenomenography_ from _phenomenology._ In the terms of this chapter, phenomenography would be the mere gathering and cataloguing of protocols (one’s own, not others’) without attempting the further extrapolation of intentional objects from them—which is phenomenology.

5.]] I find support especially in Alastair Hannay’s “Sartre’s Illusion of Transcendence,” Chapter 5 of _Mental Images—A Defence_ (London: Allen and Unwin, 1971). “The term ‘image’ itself should be used to refer to the way the object appears, or rather, to the way in which an imaging-consciousness ‘gives itself an object’” (p. 99).

6.]] Husserl speaks of the _hyle_ or hyletic phase, and I am tempted to claim that the occupants of the α-role are Husserl’s _hyle_, but perhaps the hyle are only some constituents of or raw material for the α. See “The Hyletic and Noetic Phases as Real, the Noematic as Non-real Phases of Experience,” Section 97 of Husserl’s _Ideas_, W. R Bryce Gibson, trans. (London: Allen and Unwin, 1931). Bo Dahlbom has both clarified and complicated my thinking on this issue.

7.]] Cf. also _Content and Consciousness_, and for an account of the relation between such utterances and the “propositional episodes” that evince them, see my “On the Absence of Phenomenology” (in B. L. Tapscott and D. Gustafson, eds., _Body, Mind, and Method_: _Essays in Honor of Virgil Aldrich_, Dordrecht: Reidel, 1979).

8.]] Cf. Strawson’s insightful discussions of the dangers and prospects of conceiving of (without believing in) “phenomenal figures,” in “Physical and Phenomenal Geometry,” in _The Bounds of Sense_ (London: Methuen, 1966): pp. 281–92. “The problem is whether the nature of phenomenal geometry can be described without a dangerous reliance on the possibly dubious concept of the phenomenal figure—a concept which, though dubious, is satisfying to one’s simple-minded wish for a set of _objects_ to be the peculiar matter of the study” (p. 288).

Why You Can’t Make a Computer That Feels Pain

It has seemed important to many people to claim that computers cannot _in principle_ duplicate various human feats, activities, happenings. Such aprioristic claims, we have learned, have an embarrassing history of subsequent falsification. Contrary to recently held opinion, for instance, computers can play superb checkers and good chess, can produce novel and unexpected proofs of nontrivial theorems, can conduct sophisticated conversations in ordinary if tightly circumscribed English. The materialist or computerphile who grounds an uncomplicated optimism in this ungraceful retreat of the skeptics, however, is in danger of installing conceptual confusion in the worst place, in the foundations of his own ascendant view of the mind. The triumphs of artificial intelligence have been balanced by failures and false starts. Some have asked if there is a pattern to be discerned here. Keith Gunderson has pointed out that the successes have been with task-oriented, _sapient_ features of mentality, the failures and false starts with _sentient_ features of mentality, and has developed a distinction between program-receptive and program-resistant features of mentality.[^1] Gunderson’s point is not what some have hoped. Some have hoped that he had found a fall-back position for them: _viz.,_ maybe machines can _think_ but they can’t _feel._ His point is rather that the task of getting a machine to feel is a very different task from getting it to think; in particular it is not a task that invites solution simply by sophisticated innovations in _programming,_ but rather, if at all, by devising new sorts of _hardware._ This goes some way to explaining the recalcitrance of mental features like pain to computer simulation, but not far enough. Since most of the discredited aprioristic thinking about the limitations of computers can be seen in retrospect to have stumbled over details, I propose to conduct a more detailed than usual philosophic thought experiment. Let us imagine setting out to prove the skeptic wrong about pain by actually writing a pain program, or designing a pain-feeling robot. I think the complications encountered will prove instructive.

The research strategy of computer simulation has often been misconstrued by philosophers. Contrary to the misapprehensions innocently engendered by Turing’s classic paper, “Computing Machinery and Intelligence,”[^2] it is never to the point in computer simulation that one’s model be _indistinguishable_ from the modeled. Consider, for instance, a good computer simulation of a hurricane, as might be devised by meteorologists. One would not expect to get wet or windblown in its presence. That ludicrous expectation would be akin to a use-mention error, like cowering before the word “lion.” A good computer simulation of a hurricane is a program, which, when you feed in _descriptions_ of new meteorological conditions, gives you back _descriptions_ of subsequent hurricane behavior. The descriptions might be in roughly ordinary English, dealing with clouds, waves and tides, or in some arbitrary notation, dealing with barometric pressure, wind velocities, and yet more esoteric (but measurable) features of hurricanes. The goal is to devise a program that will give you good “predictions” of what a hurricane will do under a great variety of highly complex conditions. Such a program is tantamount to an immense conjunction of complicated conditionals: “if conditions _A_, _B_, _C_, … obtain, then _R_ will result; and if conditions _D_, _E_, _F_, … obtain, S will result; and ….” Obviously the only way to populate that conjunction reliably is by deriving the particular conditionals from general covering laws, all properly meshed and coordinated. So in order to write a good simulation program, one must have a _theory_ of hurricane behavior, and it must be a good theory. But if one must have a theory in the first place, why bother incorporating it into a program? There are several good reasons. First, the demands of program writing force into the open any incoherencies, gaps, or unanswered questions in a theory; it keeps the theoretician honest. Second, once a theory is thus incorporated into a working, “debugged” program, its implications can be quickly determined and assessed. A simulation can be an “experience-generator”; hurricanes are not that numerous, but a simulation program could generate thousands of different storm histories to scrutinize for implausibility or worse. Also, of course, such a program could be used in high-speed real time prediction of current weather. The fact that such a simulation program is ultimately only a high speed generator of the consequences that some theory assigns to various antecedent conditions is often obscured by the _mode of presentation_ of the input and output. It is often useful, convenient, or just plain exciting to use the output to drive a visual display, a raster or TV screen on which appears, say, a swirling vortex moving up a map of the East Coast, but that swirling vortex is a sort of epiphenomenon, the tail that doesn’t wag the dog. The theory incorporated into the program _directs_ the behavior of the presentation, and does not _read off_ the behavior of the presentation, which itself plays no role in the simulation beyond its role as a convenient display.

Now let us consider a similarly inspired computer simulation of human pain. We write a program, based on our theory of pain, such that when we type in _descriptions_ of conditions:

An anvil drops from a height of two feet onto S’s unanesthetized left foot,

the computer types back _descriptions_ of results:

_S_ jumps about on right foot, a tear in his eye, screaming.

We test the program by varying how we fill in the blanks in our permissible input formulae (e.g.: A _____ is dropped from a height of _____ on _S_’s _____) and checking the resulting outputs for plausible variety and dependence on the input. What is unsatisfying about this computer simulation of pain? The skeptic might reply that it is a simulation at best only of pain _behavior,_ but consider our hurricane simulation: what _else_ is there to simulate but the hurricane’s behavior? A better reply is that we have so far only attempted to simulate _external_ pain behavior. This defect is easily remedied. Revised, our program will yield such outputs as

_S_’s _C_-fibers are stimulated, … a pain-memory is laid down; _S_’s attention is distracted; _S_’s heart-rate increases … ; _S_ jumps about on right foot, a tear in the eye, screaming.

(We can be sketchy, for the moment, about the internal “behavior” or effects alluded to in the program.) Suppose, then, that we pack our output descriptions with neurophysiological description or even mentalistic psychological description about effects on memory, belief, desire, etc. Still, the skeptic may insist we have left something—indeed everything—of importance out. We have simulated, perhaps, the internal and external _causes and effects_ of pain, but not the pain itself.[^3] Some identity theorists may wish to retort to this that C-fiber stimulation just _is_ the pain,*]] but we need not take a stand on that point, since there are further ways of obliging the skeptic. We can rewrite our program so it yields such outputs as

There is a pain, _P,_ of the in-the-left-foot variety, in _S; S_’s _C_-fibers are stimulated …

Now we have explicitly included the pain. But, says the skeptic, the program still leaves out the _quality_ of the pain. Very well. We expand our theory, and concomitantly our program, to yield detailed descriptions about even this. Again we feed in:

An anvil is dropped from a height of two feet on S’s left foot.

and this time we get back:

There is a pain, _P_, of the in-the-left-foot variety in _S; P_ begins as a dull, scarcely noticeable pressure, and then commences to throb; _P_ increases in intensity until it explodes into shimmering hot flashes of stabbing stilettoes of excruciating anguish (or words to that effect) …; _S_’s _C_-fibers are stimulated …

I see no reason why our program could not be enlarged to incorporate all this; the biggest problem would seem to be discovering sufficient uniformity and lawfulness in such “phenomenological” effects as reported by sufferers to permit much prediction. Of course if the data we collect suggest a random distribution of these effects within certain boundaries that is easy enough to incorporate into our program as well.†]]

I do not expect this would satisfy the skeptic. He might try to express his doubts by pointing out that there is nothing pain-like going on in the computer when it churns out these reports. But of course not. Nor does the computer hurricane generate an internal low barometric pressure behind its steely facade. At this point it should dawn on the skeptic that he has been barking up the wrong tree. He has no pressing quarrel with _this_ research strategy when it is directed to psychological phenomena, since its guiding presupposition is not that men are computers (any more than hurricanes are) but simply that one can have a rigorous _theory_ of human psychology, materialist, dualist, epiphenomenalist, or whatever. Isn’t there, however, another research strategy that differs significantly from the one we’ve been considering, where the aim of the computer is to _do,_ not _describe_? For instance, “Shakey” at Stanford Research Institute is a robot that can “recognize” simple objects with its television eyes; it pushes cubes and pyramids around in response to typed commands. Such “performance models,” one might say, really do things; they do not so much incorporate theories (as do simulations) as _instantiate_ theories.*]] The skeptic’s challenge is now for us to make such a robot, a _feeler of pain_, not a mechanized theory about feelers of pain. So let us try to design such a robot. Of course our efforts in this task will be as much guided by our _theory_ of pain as were our earlier simulation efforts, and we might ask the skeptic if he had any quarrels with our earlier, programmed theory _as a theory_ of pain. If the skeptic makes no objections to it, we are home free, for it is a relatively straightforward task to build the robot with the help of our earlier “describing” program. The describing program simply becomes the control system for our new robot.

Here is how it is done. Suppose our original program yielded outputs like: “S trembles, a tear in his eye, and says ‘Ouch! My thumb hurts.’” First, we rewrite all outputs in the first person: “I tremble, a tear in my eye, and I say ‘Ouch! My thumb hurts.’” Then we drop the redundant “I say” wherever it occurs and move all direct quotation onto a separate “protocol” terminal, which will then print only “Ouch! My thumb hurts.” The rest of the output is reprogrammed to drive a display of flashing lights with labels. The “tremble” light goes on, the “tear in the eye” light, and so forth. Then we replace the input sentences in a similar manner. We make up magnetized plastic tokens representing different objects—anvils, knives, olives—falling from different heights, and we label an array of slots to accept these tokens: “thumb,” “big toe,” etc., so that _dropping_ the anvil token into the thumb slot simulates dropping the anvil on the thumb. Of course that’s not very realistic, but we can improve it easily. For instance, we can replace the “tremble” light with an eccentric flywheel that makes the whole computer vibrate when it is turning; the tear in the eye problem has already been solved for us by the toy manufacturers, and the other details of verisimilitude are either obviously irrelevant or can be solved by the Disney studios given six months and enough Federal grant money. The result will be a robot that really _does_ things; it trembles and reels and whimpers; it says just where the pain is; it attempts to duck falling objects—perhaps it even kicks back if we kick it.*]]

But what about the rest of our earlier simulation? What happens to the hot flashes and dull throbs mentioned in our descriptive program? These parts of the output we transform into labeled flashing lights and leave them that way: sometimes the “dull throb” light is on (blinking slowly if you like) and sometimes the “hot flash” light is on. If the skeptic insists on more verisimilitude here, what can he be asking for? Remember that these lights are not blinking randomly. The “dull throb” light goes on only at appropriate times, the robot can then say: “There is a dull, throbbing pain,” and the other apposite side effects of dull, throbbing pains are presumed to be arranged to coincide as well. But, the skeptic persists, no amount of _side effects_ can turn what is not a dull, throbbing pain into a dull, throbbing pain, and obviously, calling this event a dull, throbbing pain does not make it one either. This objection, for all its plausibility, is unfair as it stands. The skeptic, we must assume, had no objection to settling for an IBM typewriter as the “speech” element in this robot, and surely typing is not talking—and calling typing talking would not make it talking. Since he has not challenged us to make a _bonafide_ member of the species _homo sapiens_ out of whatever bits and pieces are on the shelves at IBM, he must be permitting us to use some substitutes—the legs can be titanium, not flesh and bones—and since our flashing light (or whatever turns it on) has all the _functional_ features he has demanded of pain, why is he now changing the game? Calling the robot a human being would not make the robot a human being either, but that was never set as a goal. It begins to appear that what the skeptic was after all along was not a _simulation_ or an _analogue_ of pain, but the synthesis of real pain, like the synthesis of urea by Wöhler in 1828 that marked the unification of organic and inorganic chemistry. The synthesis of real pain in a machine would tend to confirm that we human beings are just fancy soft machines, as the materialist contends.

That we might reconstrue our task in this way highlights a peculiarity in our ordinary concept of pain. The word “pain” has both a _sortal_ grammar (“I have a pain,” “pains shooting up my arm”) and a _mass noun_ grammar (“There is more pain now,” “it will cause you some pain”). The mass noun grammar often permits us—even invites us—to view pain as a sort of biological or psychological substance, rather than a process or event or activity or state. For instance, the amount of morphine that can be safely administered depends on the _amount of pain_ it has to kill. For excruciating pain (e.g., that of coronary thrombosis) two to four times the usual therapeutic dose may be given without danger. But in cases of severe pains that can quickly and spontaneously disappear (e.g., those of coronary occlusion or biliary colic) such doses are dangerous, since if the pain disappears suddenly the patient may show signs of morphine poisoning. If such were to happen, one would do well to punch or slap the patient, since, as Stearns observed in 1883, “pain is the antidote for morphine poisoning.” One creates _more pain_ for the morphine to neutralize, and thus prevents the excess of morphine from poisoning.[^4] This suggests that specificity to morphine as an antagonist would be a legitimate test for any robot pain to pass.

This reconstrual of the task might seem, however, to harbor a conceptual confusion. Does one not contradict oneself in speaking of the synthesis of real pain? Synthetic urea _is_ urea, as genuine as any to be found, but synthetic rubber is not rubber.*]] Is artificial intelligence genuine intelligence? Artificial coloring is perfectly genuine _coloring,_ but artificial flowers are not flowers. The field of artificial intelligence trades on this ambiguity. Successes are often heralded as examples of genuine intelligence created by artifice, but in the face of objections this claim can be adjusted; artificial intelligence _works just as well as,_ or is a useful and even theoretically interesting _substitute_ for, genuine intelligence. I do not believe the term “artificial intelligence” is objectionable on these grounds, for I do not believe in the distinction we are invited to make in this instance. Suppose the intelligence of some artifacts does function just as well as human intelligence (an immense supposition, of course); then, since intelligence, like respiration, is a purely functional notion, artificial intelligence, like artificial respiration, is no less genuine for being obtained by artifice. It may not be _just like_ natural, human intelligence, but it is genuine intelligence, as genuine as (we can imagine) the _alien_ intelligence of extra-galactic creatures might be.[^5] But what of artificial or synthetic pain? Is pain like rubber and flowers, or like coloring, respiration and intelligence? Whatever answer we might agree on (and agreement is both unlikely and ultimately unimportant), one lesson is clear: _if_ pain is deemed to be _essentially_ a biological phenomenon, _essentially_ bound up with birth, death, the reproduction of species, and even (in the case of human pain) social interactions and interrelations, then the computer scientist attempting to synthesize real pain in a robot is on a fool’s errand. He can no more succeed than a master cabinetmaker, with the finest tools and materials, can succeed in making, today, a _genuine_ Hepplewhite chair.

Reservations about whether synthetic pain would be real pain may seem overly precious, but it is important to bring them into the open, for several reasons. First, a great deal of the counterintuitiveness of the notion of robot pain no doubt derives from a dim appreciation of this side of our notion of pain. _Real_ pain is bound up with the struggle to survive, with the real prospect of death, with the afflictions of our soft and fragile and warm flesh.[^6] With our concept of pain, as with many others, there is a tug toward _parochiality: real_ Chateau Latour has to have been made in a particular place, in a particular way, by particular people: an artificially concocted fluid indistinguishable to both chemists and connoisseurs from Chateau Latour would still not be _real_ Chateau Latour. (Real vodka, on the other hand, can be made from just about anything, anywhere, by anybody.) The parochiality of the concept of pain, is, moreover, not an irrational feature, or at least not obviously so, for it has a role to play in defining our moral community. There can be no denying (though many have ignored it) that our concept of pain is inextricably bound up with (which may mean something less strong than _essentially connected with_) our ethical intuitions, our senses of suffering, obligation, and evil.[^7] It will not do to suppose that an assessment of any attempt at robot synthesis of pain can be conducted independently of questions about what our moral obligations to this robot might be. One reason, then, why you can’t make a computer that feels pain is that our concept of pain is not a pure psychological concept but also ethical, social, and parochial, so that whatever we _put inside_ our computer or robot will not avail unless it brings in its train these other considerations, a matter over which our control, as computer designers, is worse than limited. This reason is important, and worth developing with more care, but not here, for it is also a bit of a red herring. Even if contextual matters, such questions of origin and “form of life,” make a difference, they do not make enough of a difference. I do not think the skeptic wishes to rest his case at a point where the programmer’s synthetic product might fall short only by these yardsticks (like the clever chemist’s imitation Chateau Latour which only _seems_ to have “good breeding”). Moreover, were the synthetic product that good, the contextual matters might either fall into line (we would start treating the computer very much as one of us, and commiserate with it, comfort it, etc.) or be dislodged in our minds from their position of importance. In any event, what the skeptic finds impossible to imagine is that this thing that happens in and to him (and it happens in and to him quite independently—or so it seems—of his biological origin, destiny, social milieu or ethical status) can be made to happen in and to a robot.

At this point it is easy for the skeptic to fall into extravagant and irrelevant claims to support or flesh out his skepticism. When he says no _robot_ could feel pain as he does, is it the artificiality, the chemistry, or _what_ that makes the difference? A _cloned_ duplicate of himself would presumably be capable of feeling pain, but if we could _construct_ a biochemical duplicate of him, would this artifact be a painless robot? On what grounds, other than the grounds of origin we have just now set aside? Supposing, then, that a manufactured _biochemical_ duplicate _would_ feel pain, on his view, what difference could it make if we use other materials? Only two replies, both insupportable, occur to me: (1) organic compounds are capable of realizing functional structures with capacities of a sophistication or power _in principle_ unrealizable in non-organic materials, or (2) though an inorganic replica might succeed in duplicating a human being’s functional structure, the states in it functionally isomorphic to human pain states would fail to be genuine pain states because the biochemistry of pain-state realization is essential.*]] These are both highly implausible vitalistic claims, and any skeptic led to defend his view in this territory has simply been led astray. That is not to say that murmurs of vitalism do not make a large contribution to the skeptics’ attitudes, but just that the contribution should be first isolated and then ignored. To find something better for the skeptic to say, we must give him more details to work with. We have been assuming, up to now, that the programmer could have at his disposal a fairly satisfactory theory of pain to exploit in designing his robots. We have been assuming, that is, that the _mysteriousness_ of pain might thwart our efforts at synthesizing pain without thwarting our efforts at theorizing about it. But how realistic is that assumption? The best way to examine it is to set down the bare bones of current physiological theory relating to pain, and list some of the attested pain phenomena, the data any acceptable theory of pain must account for, and see if there are any insuperable difficulties presented by them. We can record the known dependencies and interrelations among these phenomena by plotting a “flow-chart” of sorts for a pain program.

## II

The flow-chart to be presented here is merely a sketch, lacking rigor and detail, but its point is to facilitate philosophical scrutiny of pain, not to launch any serious project of programming or theoretical psychology or neurophysiology. I hope it will be clear that the difficulties we encounter would only be exacerbated in a more systematic model.

We can begin with what is known about the functional anatomy of transmission from pain sites into the brain. As is generally the case, the further in from the periphery we move, the murkier the details. The journey begins at the skin, with receptors sometimes called nociceptors that respond with some degree of specificity to a variety of noxious events: mechanical distortion, intensities of heat and cold and chemical changes, for instance. The outputs of these receptors travel brain-ward through at least two very different types of fibers: swiftly through the large myelinated _A_-fibers, and slowly through the narrow, unmyelinated _C_-fibers.*]] Both signals arrive at the substantia gelatinosa, the midbrain gateway, where a complicated interaction takes place. _A_-fibers also send effects inwards via other channels. The _A_- and _C-_fibers seem to make two different functional contributions. On the one hand, it seems that the _C_-fibers are the preponderant transmitters of “slow,” “deep,” “aching,” or “visceral” pains, while _A_-fibers are implicated in “sharp,” “bright,” “stabbing” pains. Recently Melzack and Wall have suggested a more interesting function for the _A_-fibers. They act at the substantia gelatinosa to _inhibit_ the effect of the _C-_fibers, thus _closing_ the gate to pain-impulse transmission, or at least damping the output of that gate. Moreover, the _A_-fiber channels that bypass the Melzack-Wall gate in the substantia gelatinosa seem to initiate more central activity that sends inhibitory signals back down to the gate, further blocking the transmission of impulses from the _C-_fibers. The capacity of the hypothesized Melzack-Wall gate system to explain a variety of pain phenomena is immense, as we will soon see.[^8]

What, then, happens to the output of the gate, the so-called _T_-cell transmissions? In broadest outline we can say that once again there is a split into two channels. One channel carries through the lower, phylogenetically older portion of the brain, the limbic system (hypothalamus, reticular formation, paleocortex, hippocampus), and the other passes through the thalamus and is projected onto the higher, phylogenetically newer, characteristically human part of the brain, the neocortex. Let us simplify by calling these the old low path and the new high path. The new high path is subject to yet another bifurcation: there is both a specific and a non-specific projection of fibers from the thalamus onto the cortex.

![[11146_011_fig_001.jpg]]

Figure 11.1

The new high path, which is relatively undeveloped or non-existent in lower animals, subserves fine-grained perception: location and characterization of pain and other stimuli. The old low path is characterized by orthodoxy as the _aversive_ system, the “motivational-affective processing” system. Orthodoxy is well buttressed by evidence in this instance,*]] and this suggested separation of the hurtfulness or awfulness of pain from its other characteristics—to speak loosely—will loom large in our further discussion.

![[11146_011_fig_002.jpg]]

Figure 11.2

Having charted this far with the aid of anatomical roadmaps, we have reached _terra incognita_, and if we are to proceed with our flow chart we must abandon the pretence that our boxes represent anatomically salient structures, and proceed with a purely functional, abstract breakdown of the system. We can make use of the freedom thus provided to be more boldly speculative, and also, inevitably, vaguer about the nature of the functions and relations we are charting. The only constraint on our design will be that it accommodate the known and presumed phenomena.

Everyone knows, for instance, that distracting one’s attention (e.g., by going to a movie) diminishes or banishes pain. This can be easily provided for if we build in a presenter-receiver filtering system across the pathway for incoming signals from all the sense modalities, subject to the following conditions: the receiver can have its general sensitivity raised or lowered, and the presenter has _selective_ volume controls, so that its various signals can be turned up independently.*]] Then the effect of distracted attention could work this way: paying special attention to one input (the visual and auditory input from the movie) would be accomplished in part by turning up its volume in the presenter. Then the receiver would compensate for this high volume by decreasing its sensitivity, having the effect of muffling everything else, including the pain signals. The same effect might be accomplished by the Melzack-Wall gate, but let’s be generous and draw in a separate filtering system.

Pain signals trigger a variety of “spinal reflexes” causing relatively simple but swift muscular responses without the intervention of higher brain centers (and in some instances without passing through the substantia gelatinosa, the Melzack-Wall gate), and since distracted attention has little or no effect on these, we will put the filter only in the new high path, and draw in the reflex links (R) to the motor output nerves without intervening links of sophisticated control.

There are many transactions between the old low and new high paths. Of particular importance to us is the relation the reticular formulation in the old low brain has to higher centers. The reticular activating system plays a major role in governing sleep and waking, and determining the level of arousal generally; it also plays a role in directing attention, and thus can be considered a versatile alarm system.

So far we have hardly touched on the effect of pain stimuli on “higher centers,” so let us sketch in roughly what is most obvious about these effects. When we have a pain we believe we have a pain (at least normally), and pains can be remembered, usually, for some time. So in our control circle we will place a memory and belief box, and to be on the safe side, _two_ arrows leading to it, one from the old low path and one from the new high path (further investigation might lead us to revise any of this, of course). Also, pains are abhorrent, at least usually. That is, the infliction of pain is a reliable behavior-modifier, tongue loosener, punishment. (Whether punishment is good for anything is another matter. Pain is a good way to punish.) So we should draw in a “goals and desires” box, with appropriate inputs. (If the “aversive” effects of pain are subserved _entirely_ by the old low path, we might not want an arrow from the new high path to the desire center, but again, let’s be generous. No doubt even the most intellectual apprehension of pain stimuli could have _some_ effect on one’s desires, current or long-term.)

![[11146_011_fig_003.jpg]]

Figure 11.3

It is a useful and oft-used myth, at least, that higher controls in human beings are accomplished by something like logical processing of the material in the belief and desire boxes (see Chapter 3), so let us distinguish a ratiocination unit in the control area. We need this in any case, since one effect of pain stimuli on this function (as captured by our box) is not _informational_ but _noisy_: pains interfere with our ability to concentrate, to solve problems, to think clearly, so we should draw a “noise” arrow to the ratiocination unit. (Perhaps we should draw noise arrows to other units as well, but let’s not overcomplicate our diagram.)

![[11146_011_fig_004.jpg]]

Figure 11.4

Finally, let us transmit the control center’s effects on behavior through an action organizing unit (with the specially important speech synthesis unit drawn in) to the motor-effector controls and thence to the muscles. In addition to the control center’s effects on behavior, we must put in the arrows for various “descending effects” on the input system, including those already alluded to: an inhibitory effect on the Melzack-Wall gate, a volume control on the filter, a “perceptual set” or “readiness” determiner to weight the analyzing machinery, and others not worth drawing in. Then of course there should be “feedback” arrows throughout. That should be enough detail to handle the phenomena at least in outline. The sketchiness and idealization of this model should not be forgotten, of course. Some of the functions captured by these boxes may merge in single anatomical structures, and such distortions as are present in the model might seriously misrepresent the actual state of affairs to the point of requiring major revision of the model. In any case, however, we now have _a_ fairly complicated and versatile model to play with; let us see how it runs when it comes to providing for the variety of pain phenomena.

![[11146_011_fig_005.jpg]]

Figure 11.5

Why does it help to rub or press the painful area, or to scratch an itch? Melzack and Wall claim that this increases _A_-fiber transmission, thus increasing the inhibition of the _C_-fiber stimulation in the substantia gelatinosa. A less commonly recognized home remedy for pain is not to _distract,_ but to _concentrate_ one’s attention on the pain. I discovered this for myself in the dentist’s chair, thinking to take advantage of the occasion by performing a phenomenological investigation without the benefit of novocain, and have since learned that this is a highly elaborated technique of Zen Buddhism.[^9] I recommend this enthusiastically. If you can make yourself study your pains (even quite intense pains) you will find, as it were, no room left to _mind_ them: (they stop hurting)—though studying a pain (e.g., a headache) gets boring pretty fast, and as soon as you stop studying them, they come back and hurt, which, oddly enough, is sometimes less boring than being bored by them and so, to some degree, preferable. I am not at all sure that what I just said “makes sense”; that is, I am not at all sure that this loose talk about _pains_ that cease and resume hurting, or even this loose talk about _studying_ one’s pains, is ontologically, metaphysically, scientifically, phenomenologically _sound_, but it is nevertheless just what I want to say. That is the way I would put it if I were unselfconscious and unworried about committing some conceptual gaffe. And that is a crucial part of the problem of pain: we have a baffling variety of such untutored, unstudied accounts of pain phenomena, and it is no easier to give them all credence than it is to revise them by the light of some procrustean theory about what pain experiences must be. But, to return to the effect of attention on pains, whatever the “correct” philosophical analysis is of the variety of first person pain reports, it must have room for the fact that focusing attention can _obtain relief_ (to put the matter neutrally for the moment). Melzack and Wall have a ready explanation of this phenomenon: focusing attention on one’s pains may serve to raise the volume only on the _A_-fiber component of it, thus inhibiting the _C_-fibers at the Melzack-Wall gate. Their experiments tend to confirm this hypothesis, and suggest that analgesia by hypnosis or yoga methods have similar explanations.*]]

We “locate” our pains, but this is not a single thing we do. On the one hand, we react more or less reflexively to jerk the injured part away from contact with harm, and that is accomplished through the old low path. But we also “can say” where our pains are, and this is presumably accomplished on the new high path with the aid of the specific projection system to the perceptual analysis areas of the cortex, and thence through the control system to ultimate speech. Excitation of a specific peripheral nerve fiber at any point on its length normally produces a sensation of pain felt at its normal extremity. “Phantom limb” is, of course, the most vivid manifestation of this phenomenon.*]]

In “referred pain” the pain location does not match the location of the trauma. This must be due to “leakage” or “short-circuits” at crossover points in the specific pathways, probably in the substantia gelatinosa. Oddly enough, however, administering novocain or other local anesthetic to the site where the pain is _felt_ diminishes the referred pain, and pressure on that area increases the pain. This can be accounted for if we suppose the leakage is not a simple _turning-on_ of the wrong fiber, but an _enhancement_ of a resting level of transmission. Under local anesthesia there would be nothing to enhance (since local anesthetics stop _all_ transmission), and pressing the uninjured area would produce a higher pre-existing level to enhance.

Now let us locate on our model the effects of various drugs, especially the anesthetics, which prevent all sensation, and the analgesics, which are specific for pain. Novocain and related local anesthetics act by completely stopping the transmission of nerve cells at their source. In fact, they block nerve activity wherever they are injected: “spinal block” is a local anesthetic administered high in the pathway to the brain, creating a wide area of total but still “local” anesthesia. There are no local analgesics, but aspirin is unique among the common general analgesics in having a peripheral site of action.*]] It antagonizes a metabolite, bradykinin, at the nociceptors; it is bradykinin that persists in stimulating the nociceptors after the initial traumatic event and thereby is responsible for persistent pain. Aspirin, by antagonizing bradykinin, prevents pain at the earliest opportunity. This is interesting because aspirin is also unique among analgesics in lacking the “reactive disassociation” effect. All other analgesics (e.g., the morphine group and nitrous oxide in sub-anesthetic doses) have a common “phenomenology.” After receiving the analgesic subjects commonly report not that the pain has disappeared or diminished (as with aspirin) but that the pain _is as intense as ever_ though they no longer _mind_ it. To many philosophers this may sound like some sort of conceptual incoherency or contradiction, or at least indicate a failure on the part of the subjects to _draw enough distinctions,_ but such philosophical suspicions, which we will examine more closely later, must be voiced in the face of the normality of such first-person reports and the fact that they are expressed in the widest variety of language by subjects of every degree of sophistication. A further curiosity about morphine is that if it is administered _before_ the onset of pain (for instance, as a pre-surgical medication) the subjects claim not to feel any pain subsequently (though they are not _numb_ or anesthetized—they have sensation in the relevant parts of their bodies); while if the morphine is administered _after_ the pain has commenced, the subjects report that the pain continues (and continues to be _pain_), though they no longer mind it.

Our model suggests that morphine and other analgesics must work on the old low path while leaving the new high path relatively in order, and such is the case. While morphine, like anesthetic drugs generally, takes effect first at the higher, cortical levels of the brain and then descends to the old brain, the specific projections to the cortex are especially resistant to damping by drugs, so that the effects of these drugs is more pronounced on the old low aversive path than on the new high path of fine-grained perception. The timing-dependence feature of morphine might be explained this way: once old low pain signals have contributed to the “set” of the perceptual analyzing machinery (via influences on the control center’s “descending effects” which would weight the interpretation machinery in favor of interpreting particular signal patterns as pain-transmitting), this cannot be quickly undone, even after the contribution from the old low path is eliminated by morphine. Lobotomized subjects similarly report feeling intense pain but not minding it, and in other ways the manifestations of lobotomy and morphine are similar enough to lead some researchers to describe the action of morphine (and some barbiturates) as “reversible pharmacological leucotomy [lobotomy].”*]]

When we turn from local anesthesia and analgesia in conscious subjects to general anesthesia, the situation becomes more complicated. The major problem can be approached by way of a curious and terrible incident from the annals of medicine. Curare, the poison used by South American Indians on their blow-pipe darts, was purified (as _d-tubocurarine_) and introduced into medical research in the 1930s, and its action was soon well understood.[^10] It is a paralytic that acts directly on all the neuromuscular junctions, the last rank effectors of the nervous system, to produce total paralysis and limpness of all the voluntary muscles. It has no central effect except for a slight enhancement effect on activity in the cortex. In the l940s, however, some doctors fell under the misapprehension that curare was a general anesthetic, and they administered it as such for major surgery. The patients were, of course, quiet under the knife, and made not the slightest frown, twitch or moan, but when the effects of the curare wore off, complained bitterly of having been completely conscious and in excruciating pain, feeling every scalpel stroke but simply paralyzed and unable to convey their distress. The doctors did not believe them. (The fact that most of the patients were infants and small children may explain this credibility gap.)[^11] Eventually a doctor bravely submitted to an elaborate and ingenious test under curare, and his detailed confirmation of the subjects’ reports was believed by his colleagues: curare is very definitely not any sort of anesthetic or analgesic.[^12]

Recently a puzzle occurred to me: suppose that one were to add to curare a smidgen of _amnestic_, a drug that (we will hypothesize) has no effect on experience or memory during _n_ hours after ingestion but thereafter wipes out all memory of those _n_ hours.*]] Patients administered our compound, curare-cum-amnestic, will not later embarrass their physicians with recountings of agony, and will in fact be unable to tell in retrospect from their own experience that they were not administered a general anesthetic. Of course _during_ the operation they would know, but would be unable to tell us.†]] At least most of our intuitions tell us that curare-cum-amnestic would not be an acceptable substitute for general anesthesia, even if it were cheaper and safer.‡]] But now how do we know that general anesthetics in use today are not really curare-cum-amnestic? We know, in fact, that curare _is_ routinely used in general anesthesia today. Most general anesthetics by themselves in safe doses do not entirely block reflex spasms, so curare or another curariform paralytic is administered to prevent muscle-tearing damage and thrashing about that could interfere with the surgeon’s task. Moreover, a variety of drugs in the anesthesiologist’s bag are known to be amnestics. How do we know that these drugs have the further effect of producing genuine anesthesia or even analgesia? Absence of complaint or other behavioral manifestation, we have seen, is hardly sufficient grounds—though they are routinely and not unreasonably relied on in daily medical practice. To answer this question we will have to look more closely at the _internal_ effects of the so-called general anesthetics, and at other, more indirect clues about their functions.

There are a wide variety of general anesthetics, but they fall into groups, and if we take three important drugs as paradigms, we will have more than enough variation to suggest the problems: (1) nitrous oxide, or laughing gas, which is inhaled; (2) ether, one of many related volatile inhalants; (3) sodium pentothal (or thiopental sodium), an injected “ultra-fast-acting” barbiturate.[^13] These drugs are chemically very different, and have different effects on the central nervous system and the rest of the body. Moreover, in modern practice they are seldom used alone, but are almost always accompanied by preanesthetic medication, such as an analgesic (e.g., morphine), a sedative to combat anxiety and the nausea that often results from ether inhalation, or even a “basal” anesthetic, which produces anesthesia sufficiently deep for preparation for surgery but too shallow for surgery. In spite of this variation we can impose some order by considering the traditional “stages” and “planes” of general anesthesia. In passing from full consciousness into the anesthetized state, one moves through three of four marked stages. In the first, one is conscious but apt to have hallucinations or uncontrollable thoughts; the drug is acting on the neocortex (at least partly as an enhancer or stimulant). In the second or delirium stage one is unconscious (in some sense) but may laugh, shout, swear or thrash about. The drug’s effects are descending through the brain, and one hypothesis is that the drug has reached the higher motor control centers and paralyzed them, “releasing” lower motor activity. In the third stage, called surgical anesthesia, there are four planes, of increasing depth. Depending on the surgery to be done one will be kept in the shallowest permissible plane of surgical anesthesia, since the fourth stage, medullary paralysis, is a short prelude to respiratory failure and death. These temporal stages are all but undetectable with sodium pentothal, where stage three is reached in a few seconds, and their manifestations are largely obliterated by the effects of preanesthetic medication with ether or nitrous oxide (no one wants a hallucinating, thrashing patient to deal with, which is one reason for pre-anesthetic medications). So the importance for practice, if not pedagogy, of the traditional stages of anesthetic induction is virtually nil. The four planes of third-stage surgical anesthesia, however, have well-recognized symptoms relied on by anesthesiologists in maintaining the proper level of anesthesia during surgery. And for all the differences among the drugs, one similarity is clear enough: in doses large enough to produce deep plane surgical anesthesia (or fourth stage medullary paralysis) all the drugs are analgesic and anesthetic if any drug could be, since their effect at those levels amounts to virtual shut-down of the entire central nervous system. Such barely reversible brain death will look plausibly pain-free (by being everything-free) to even the unrepentant interactionist, for there is almost nothing happening to interact with. This is small comfort to the skeptic, however, since because of their very danger such deep levels of anesthesia are shunned. In fact the direction of anesthetic practice is toward ever shallower, safer, more manageable anesthesia, with supplementary medication, such as curare, analgesics, sedatives and—yes—amnestics taking care of any loose ends uncontrolled by the shallow anesthetic.*]]

The persistence of reflex responses to painful stimuli under anesthesia is an obtrusive and unsettling fact, in need of disarming.†]] Goodman and Gilman observe that at the second stage of anesthesia “painful procedures are dangerous because the response to such stimulations (including incidental dreams [!]) is exaggerated and may lead to violent physical activity” (p. 32), and they note further that even at surgical levels, barbiturate anesthetics “do not adequately obtund the reflex responses to impulses which, in the conscious state, would be experienced as painful” (p. 127). Yet they assure us that analgesia in these circumstances is complete, despite the occurrence of “behavior” that is held—by some schools of thought—to be well nigh “criterial” for pain. The presence of the reflexes shows that the paths between nociceptors and muscles are not all shut down. What special feature is absent from those paths whose presence is required for the occurrence of pain? The short answer routinely given is: consciousness. General anesthetics render one unconscious, and when one is unconscious one cannot feel pain, no matter how one’s body may jerk about. What could be more obvious? But this short answer has the smell of a begged question. The principle appealed to (that consciousness is a necessary condition for feeling pain) does not have the status of a well-confirmed empirical hypothesis, or a “law of nature,” and its utility evaporates if we try to construe it as an “analytic truth.” Until an analysis is given of the relatively gross, molar notions of consciousness and pain, the principle has no particular warrant, save what it derives from its privileged position as one of the experience-organizing, pretheoretically received truths of our common lore, and in that unsystematic context it is beyond testing. Until we have a theoretical account of consciousness, for instance, how are we to tell unconsciousness from strange forms of paralysis, and how are we to tell consciousness from zombie-like states of unconscious activity and reactivity? The paradigms of unconsciousness that anchor our acceptance of this home truth principle are insufficiently understood to permit us to make the distinctions we need to make in this instance.

I think it is fair to say that until very recently anesthesiologists had no better defense for their defining professional claim than such an appeal to “intuitive” principle:

> “How do you prevent pain?”
> 
> “We give people drugs that render them unconscious.”
> 
> “And how do you know they are really unconscious?”
> 
> “Try to wake them up; you’ll see. (Besides, when they do wake up, they don’t recall any pain).”

Today, fortunately, better answers can be given; answers that at least have the potential to form the framework of detailed and confirmable theories. The “state of unconsciousness” produced by general anesthetics can be independently characterized, and its importance accounted for. Drugs that cause sleep or deeper levels of “unconsciousness” are called _hypnotics_, and all general anesthetics are hypnotics. Moreover, they all achieve this effect by antagonizing—though in different ways—the normal functioning of the reticular formation, preventing (_inter alia_) the arousal of the neocortex. Some further details are of particular interest. Barbiturate anesthetics in sub-hypnotic doses are not anesthetic or analgesic at all, whereas nitrous oxide in sub-hypnotic doses is a reliable analgesic. This meshes well with our physiological account, since nitrous oxide not only depresses the reticular formation but also depresses transmission between the thalamus and cortex, an effect barbiturates lack. Melzack and Wall report that in cats barbiturate anesthetics produce strong descending inhibitory effects to their gate system in the substantia gelatinosa. So some general anesthetics may overdetermine their essential effect, but being a hypnotic (suppressing general arousal) is sufficient.

A more puzzling matter is the claim (e.g., by Goodman and Gilman) that “pain is totally abolished before the onset of unconsciousness” in the first stage of anesthetic induction; a scalpel incision, they say, feels like a blunt instrument drawn across the skin! One is entitled to view this claim with skepticism; surgical incisions during stage one anesthesia without other medication must be exceedingly rare occurrences in modern medicine, and for good reason, so presumably the grounds for the claim are anecdotal and not of recent vintage. But suppose the claim is in fact well grounded (at least true on occasion). At first blush it appears an embarrassment to our theory, since orthodoxy has it that only the cortex is affected during stage one anesthesia, and the effect on it is enhancement, not depression or blockade. How could cortical enhancement possibly produce analgesia? One possible answer: by evoking a hallucination (e.g., of a blunt instrument being drawn across the skin). The abnormal cortical activity of first stage anesthesia is known to evoke hallucinations, and hallucinations do have the power to overrule and obliterate competing veridical inputs (one’s hallucinations are not simply _superimposed_ on veridical perceptions), so if one were fortunate enough to hallucinate a harmless blunt instrument when the scalpel was plunged in, one would not feel pain. And, of course, one’s being fortunate enough would not be fortuitous; the content of hallucinations is apparently guided by our deepest needs and desires, and what apter or deeper guiding desire than the desire to avoid pain? A similar account suggests itself for analgesia under hypnotic suggestion.

The shutting down of the reticular formation by anesthetics does not “turn off” the cortex nor does it prevent stimuli from reaching it. It prevents or depresses “recruitment” by those stimuli; they arrive at the cortex, but do not produce the normal spreading ripple of effects; they die out. On any plausible account of cortical functioning this should prevent the completion of the process of perceptual analysis. We could of course claim, with the support of orthodoxy, that such an effect on the cortex “produces unconsciousness” and we could then “explain” the absence of pain in such circumstances by an appeal to our common sense principle that consciousness is a necessary condition for pain, but that would require us to explain just how and why failure of cortical recruitment amounts to or causes unconsciousness, which is a step in the wrong direction, a step away from detailed functional analysis toward the haven of vague and unsystematized preconception. The hypothesis that the successful completion of a process of perceptual analysis is a critical feature in our functional account of pain is, in contrast, a generator of a variety of plausible accounts of perplexing phenomena. We have already seen its utility in accounting for the morphine time-dependence phenomenon. It could also be invoked to account for the relation between the amnestic and anesthetic properties of some drugs. Brazier suggests that anesthesia may result from a derangement of some _memory_ functions subserved by the hippocampus, producing a sort of continuous amnesia of the specious present. Such a “forgetting” of each passing moment would cause a complete disability of perceptual analysis and ultimate recognition, and, so goes the theory, a pain not recognized is no pain at all.[^14]

Another application of the hypothesis accounts for the striking fact that soldiers who have been wounded in battle often exhibit no discomfort from their serious injuries while awaiting treatment in the safety of a field hospital, but will complain bitterly of the pain of a clumsy venipuncture when a blood sample is taken.[^15] They are in a state of specific—not general—analgesia, and the specificity is relative not even to bodily location, but to the _import_ of the stimulation. This capacity for import-sensitive analgesia has been exploited rather systematically by the Lamaze natural childbirth technique. Adherents of the Lamaze method claim that by giving the mother a meaningful task to perform, the input which would otherwise be perceived as pain is endowed with a complex action-directing significance; since the patient is not merely a passive or helpless recipient of this input, but rather an _interested_ recipient, a _user_ of the input, it is not perceived as pain, and again, since a pain not recognized as such is no pain at all, the Lamaze method actually promotes a _reduction of pain_ in childbirth.

The content-sensitivity of some forms of analgesia and the time-dependence of morphine’s analgesic effect can only be explained by a theory that treats the experience of pain as somehow the outcome of a process of perceptual analysis. Then, once that process is grossly located (in the neocortex), we can determine a necessary condition for its successful completion (reticular formation arousal), and can provide some grounds for the belief that we are loath to abandon: general anesthetics are not misnamed. They are not misnamed because they prevent the completion of a process that is empirically established as a normally necessary condition of pain. This invocation of perceptual analysis restores the new high path in the cortex to a position of importance in our account, and suggests that activity in the old low path is important not because it _is_ or _amounts to_ pain, but because it is a major contributing condition of pain.

This forces us to acknowledge a far from negligible distinction between the pain we humans experience and the pain experienced by creatures that lack a neocortex (unless we want to maintain that only human beings and perhaps a few other “higher” animals do experience pain). But it should already be obvious to us that there are tremendous functional differences between human and subhuman pain: no one is surprised that yoga, Zen Buddhism and Christian Science are ineffective anodynes for animals. What of anesthetic practice in veterinary surgery and animal experimentation, however? The hypothesis that “saves” shallow anesthesia for human subjects is apparently inapplicable to animals without a neocortex. The curare incident should persuade us not to jump to complacent conclusions about this. Current thinking in veterinary anesthesiology closely follows human anesthesiology in most regards: the Melzack-Wall theory is featured, but the action of drugs on the reticular formation is regarded as central. The reticular formation plays about the same role in animals’ brains, serving to arouse those higher perceptual areas that are precursors, phylogenetically, of the human neocortex. Somewhat disturbing, however, is the common use in animals of “dissociative anesthetics” such as phencycladine and ketamine, which do not depress the reticular formation, but produce a state like cataleptic stupor. These drugs have been discontinued for human administration because their anesthetic properties were called in doubt, and patients frequently reported horrible hallucinations (typically, of dying and then flying through outer space to hell).[^16]

This completes the survey of pain phenomena, and carries our functional, partly anatomical, flow-chart of pain as far as it can profitably be carried. The point of this extended exercise in speculative psychophysiology has been to flesh out a theory sketch to the point where one can plausibly claim to have an account that accommodates the data in all their variety.

## III

Now we can return to the philosophical question that motivated the exercise: is the resulting theory a theory of pain at all; does it capture pain so that any realization of the flow chart would properly be said to be capable of experiencing genuine pain?

A related, but somewhat different question is this: can we locate pain, as distinct from its typical causes and effects, on the flow chart? The flow chart gives us a functional description at what I have called the sub-personal level.[^17] I have labeled the various boxes “belief,” “desire,” “action” and so forth, but that was taking a liberty. The flow-chart deals directly not with a person’s acts, beliefs, thoughts, feelings, but with the behind-the-scenes machinery that governs speech dispositions, motor subroutines, information storage and retrieval, and the like. It has been convenient to talk as if the _objects_ of our attention, what we pay attention to, were impulse trains in the nervous system, to talk as if the muffled outputs from the filter _were_ the diminished pains, to talk as if we recognize or fail to recognize a neural signal _as_ a pain, but this is loose talk, and the conceptual confusions it invites are not inconsequential. When we retell the subpersonal story without taking these liberties we seem to be leaving something out.

Suppose we want to know how an anesthetic about to be administered to us works. The doctor tells us that it prevents mechanisms in the brain from “interpreting” certain impulse trains arriving from the periphery. This, he says, in turn prevents the initiation of motor activity, blocks normal effects on long and short term information storage and goal structures, and … permits surgery to proceed at safe levels of respiration and blood pressure. Yes, we reply, but does it stop the pain? If we are unsatisfied with the answer he has already given us, his further reassurance that of course the anesthetic does stop the pain is not yet another consequence of any theory of anesthesia he knows, so much as a “philosophical” dogma—quite reasonable, no doubt—that plays a useful role in his bedside manner. The sub-personal theory he relies upon, and perhaps helps to confirm or advance, can provide for the phenomena, it seems, while remaining neutral about the “philosophical” puzzles about pain. For instance, not only can it account for the effect of novocain and curare, it also can account for the presence of the “reactive disassociation” effect of morphine without taking a stand on whether the effect is properly described as the _presence_ of pain in the _absence_ of aversion, or as the _absence_ of pain in the _presence_ of peculiar beliefs or speech dispositions. It can explain the placebo effect without settling the question: Does placebo administration promote a belief that _causes_ or _amounts to_ the absence of pain? It can explain the success of the Lamaze method without committing itself to an account of what the success consists in: is it correct to say that the technique turns pains into painless sensations, or should we say it prevents certain pains from ever occurring at all? It can explain why concentrating on one’s pain provides relief, without settling the question of whether such concentration changes the object of attention, and if so, whether the object is so changed it is no longer a pain, or rather a pain one does not mind having, a pain that doesn’t hurt.

The sub-personal account can provide at least a sketchy suggestion of why hypnosis is sometimes an effective method of obtaining relief, but what, exactly, does hypnosis accomplish? Does it manage to prevent the pain that would otherwise occur from occurring, does it prevent its existence, or does it simply permit the subject to ignore or abide the pain? Or does it leave the subject in pain but make him _think_ or _act_ _as_ _if_ he were not? Can it possibly be that these are different ways of saying the same thing? Suppose someone is given the posthypnotic suggestion that upon awakening he will _have_ a pain in his wrist. If the hypnosis works, is it a case of pain, hypnotically induced, or merely a case of a person who has been induced to _believe_ he has a pain? If one answers that the hypnosis has induced real pain, suppose the posthypnotic suggestion has been: “On awakening you will _believe_ you have a pain in the wrist.” If this suggestion works, is the circumstance just like the previous one? (Isn’t believing you are in pain tantamount to being in pain?) Or doesn’t hypnosis induce beliefs at all? Is it rather that in both cases the subject just acts as if (1) he were in pain, (2) he believed he was in pain? What is presumably true in any case is that the effect of the hypnosis was to distort or weight the perceptual analysis machinery so that it produced a certain output, the sort of output that normally produces all or most of the normal pain dispositions: dispositions to avow, dispositions to nurse the wrist, take aspirin, and perhaps even dispositions to respond to stimulation of the wrist with the classic “spinal reflexes” (I do not know how deep hypnosis can reach—hypnotically induced “pain” does not evoke the characteristic palmar skin resistance of pain, but may otherwise be indistinguishable). Even if we knew exactly which of the boxes in our flow-chart were affected by hypnosis, and how, we would not thereby have answers to our philosophical questions (except in the extreme cases: if hypnosis were to produce only a disposition to _say_, “I have a pain in my wrist,” and no other manifestations of pain, or alternatively, if hypnosis produced an observable injury, with swelling, inflammation, bradykinin, etc., in the wrist, we would find easy unanimity in our answers).

The philosophic questions do not seem idle, but our sub-personal theory does not—at least not yet—provide leverage for answering them. The silence of the sub-personal account here is due simply to the fact that pain itself does not appear on our flow chart, which seems to concern itself solely with the _causes and effects_ of pain.[^18] As we trace through the chart, we find the causal contributions include nociceptor and _C_-fiber stimulation, _T_-cell activity, the processes of perceptual analysis and the contributions thereto of old low path activity; and among the effects we find muscle contraction, avoidance reactions, reports, beliefs, disruptive effects on thinking or reasoning, and powerful goal modifications. The absence of a “pain” box might seem to be a simple omission, easily corrected. The most plausible place to insert a pain box is between the perceptual analysis box and the higher control centers. Isn’t pain the _result_ of perceptual analysis and the _cause_ of our reactions to discomfort? Let us call the inserted box the _pain center_. Now what does it do? If one claims its function is simply to serve as the locus for the transmissions just mentioned, the go-between, then contrary to our suspicion, pain was already represented in our model; we simply had not drawn a line around it. If the point is rather that there is a separable and terrible something we had hitherto left out, how could we possibly add it with this box?

How do we get pain into the pain center? Here is a suggestion: there are two little men in the pain center, and when the light goes on one starts beating the other with chains. What is wrong with this? Not that we have introduced homunculi, for there are (somewhat less colorful) homunculi inhabiting all these boxes. That is a legitimate and useful way to comprehend flow-charting (see Chapters 5, 7, and 9). What is wrong is that even if there were pain in the box, it would not be the person’s pain, but the little man’s. And to be crass about it, who cares if the little men in one’s brain are in pain? What matters is whether _I_ am in pain.*]]

There is no way of adding a pain center to the sub-personal level without committing flagrant category mistakes, by confusing the personal and sub-personal levels of explanation.†]] We might toy with the idea that our pain center, somewhat like Descartes’ notorious pineal gland, is the producer of epiphenomena, the _echt_ pains that make all the difference (without of course making any _causal_ difference). The standard rebuttal to this version of epiphenomenalism should suffice. Suppose there were a person of whom our sub-personal account (or a similar one) _without the pain center_ were true. What are we to make of the supposition that he does not experience pain, because the sub-personal theory he instantiates does not provide for it? First we can make the behaviorist’s point that it will be hard to pick him out of a crowd, for his pain behavior will be indistinguishable from that of normal people. But also, it appears _he_ will not know the difference, for after all, under normally painful circumstances he believes he is in pain, he finds he is not immune to torture, he gladly takes aspirin and tells us, in one way or another, of the relief it provides. I would not want to take on the task of telling him how fortunate he was to be lacking the _je ne sais quoi_ that constituted real pain.

But that is a tendentious description of the case. Let us consider instead the hypothesis suggested by it, _viz.,_ that we have simply not seen the woods for the trees, that pain is not to be found in any one box of our flow-chart, but is a function or combination somehow of the elements already present. What function? The chief value of all this somewhat science-fictional flow-charting and compiling of odd phenomena—the reason I have spent so much time on it—is that it serves to drive wedges of contingency between features that are often thought to be conceptually inseparable, simply because they are usually coincident. What I am asserting is that the arrows on the flowchart are the arrows of normal causal relation, and wherever we have seen fit to posit a particular relation or dependency, we can imagine a severing of the normal connections responsible for it. Some of this fragmentation has familiar manifestations, some is to be found only rarely, and some never occurs, so far as I know, though we can conceive of it occurring.

We can locate our pains, for instance, but this is a complex ability of ours that could become discomposed on occasion. Anscombe considers such a case:

> You say that your foot, not your hand, is very sore, but it is your hand you nurse, and you have no fear of or objection to an inconsiderate handling of your foot, and yet you point to your foot as the sore part: and so on. But here we should say that it was difficult to guess what you could mean.[^19]

Pains are also goal modifiers, but they might not be. That is, we can imagine a person who says he is in pain, locates the pain consistently, is in fact being beaten, writhes, cries, trembles, _but is immune to torture_. Is this really imaginable? Of course it is. Perhaps that is what masochists are. Or perhaps they have, as it were, a sign reversed going to the goal box, so they seek out pain instead of avoiding it, at least in certain circumstances.*]]

Pains are abhorrent, but what are we to make of the reports of subjects who are lobotomized or under morphine analgesia, who report pains, rank them in terms of greater and less intensity, but seem and claim not to _mind_ the pains? Are they confused? They say they are in pain, but could they properly be said to believe they were in pain? It is not as if they are speaking parrot-fashion, nor do they exhibit massive conceptual confusions in other areas, so why can it not be that they do believe they are in pain? The only strong presumption against granting them this belief is that a good many “theories” of pain make us “incorrigible” or “privileged” about our pains, and this is often characterized by the stipulation that belief that one is in pain is a sufficient condition for being in pain. If we hold this view of incorrigibility and grant these subjects their belief, then they are in pain, but then pain is not always abhorrent, even when the subjects are experiencing, as they sometimes believe, very intense pain. One might try to claim that such people reveal by their very odd behavior that they do not understand the word “pain,” but that would be hard to support. Before the lobotomy or morphine administration, we can presume, they had a good command of English, including the word “pain,” and there is no evidence, I think, to show that any of these treatments tends to produce lexical amnesia or other verbal confusions.*]] To be sure, they do not understand the word “pain” the way some theories would say they ought to, but to bow to these theories would be to beg the question in very description of the case.

The ordinary use of the word “pain” exhibits incoherencies great and small. A textbook announces that nitrous oxide renders one “insensible to pain,” a perfectly ordinary turn of phrase which elicits no “deviancy” startle in the acutest ear, but it suggests that nitrous oxide doesn’t prevent the occurrence of pain at all, but merely makes one insensible to it when it does occur (as one can be rendered insensible to the occurrence of flashing lights by a good blindfold). Yet the same book classifies nitrous oxide among analgesics, that is _preventers_ of pain (one might say “painkillers”) and we do not bat an eye. Similarly, if “pain is the antidote to morphine poisoning” then morphine cannot be said to prevent pain from occurring. Perhaps what the maxim really means is that _normally painful stimulation_ is the antidote for morphine poisoning, but if that is what it means, that is not what it says, and what it says is easily understood, and understood to be good English. This particular slackness in our ordinary use has provided a playground for interminable philosophic disputation over the issue: can there be unfelt pains? I suggest that our flow-chart handles this traditional question by discrediting it. There can be, in principle, any combination of the normal “causes and effects” of pain in the absence of any others, and intuitions will no doubt clash about which words to use to describe the results. Other philosophical questions about pain might have more interesting answers.

Consider the commonplaces about differences in “pain-threshold.” Some people, it is often claimed, can stand more pain than others: they have a _high_ pain threshold.*]] Suppose I am one of those with a _low_ threshold, and undergo treatment (drugs, hypnosis, or whatever) supposed to change this. Afterwards I report it was a complete success. Here is what I say:

- (1) The treatment worked: the pain of having a tooth drilled is as intense as ever, only now I can stand it easily.

Or I might say something different. I might say:

- (2) The treatment worked: having a tooth drilled no longer hurts as much; the pain is much less severe.

Can we distinguish these claims? Of course. They obviously mean very different things. Can I then know which claim is correct in my own case or in another’s? Wittgenstein is sometimes supposed to have argued in the _Philosophical Investigations_ that I cannot be said to know such a thing—and maybe that there is nothing to know; the claims are, in some sense, equivalent. But I do not think that can be right, whether or not Wittgenstein argued for it (and I do not see that he did). Suppose after my treatment I report the results in the first manner. Someone then chides me: “How do you know it’s not (2)?” Now if I say in reply that there is an inner quality of painfulness that I can recall my past experiences at the dentist’s to have had, and if I now resurrect that quality from my memory, and compare it with the quality of my present experience, I can see that the present experience has that same quality, only I mind it less; then Wittgenstein has a case against me. That sort of supporting claim must be bogus.*]] I could not confirm for myself by such a combination of recall and introspection that (1) was the right way to talk. Yet all the same I could stick to my story. I could say: “All I know is that that’s the way I want to describe it—that’s how it first occurred to me, and your skepticism hasn’t changed my mind: I _still_ want to say that. Nothing _shows_ me I am in pain, and similarly nothing need show me that my pain is as intense as ever, though I mind it less.” Such _things I want to say_ count for something, but not, as we have just seen, for everything (we aren’t _required_ to accept the reports of morphine users or lobotomized subjects).

Could I be supported in my conviction about threshold by further evidence? We might run a survey on those who had had the treatment, and find a consensus. Or we might find that I was an anomaly, or that there were two broad groups of reporters, whose memberships were predictable from some features of the subjects (age, blood type, social background, size of the cortex …). Would a consensus confirm my story, or would it simply give us a general fact about pain-talk under certain conditions? The _truth_ of the pain-talk would still seem open to question. Or, if one holds that the uniformity of this way of speaking is constitutive of the meaning of “pain” and hence ensures the truth of all this pain-talk as truth-by-meaning then at least we can ask if, all things considered, this is an apt or perspicuous way of talking, of dividing up the world. One is inclined to think that there must be some reason for us to say one thing rather than another, even if these “grounds” are not available to us introspectively. It would not be appropriate for us to be so designed that our convictions on this matter were grounded in no distinction of interest at all, but then to what other grounds could one appeal but to internal, sub-personal grounds? Suppose for instance, we were to look inside me and find that the treatment had the effect of diminishing the effects on goal structures, current action-directing sub-routines, and memory, but left unchanged the intensity or magnitude of whatever causally earlier processes normally co-vary with intensity-of-pain-reported. This would support my way of talking at least indirectly, by showing that there is at least one interpretation of the open schema “the magnitude of x is unchanged, but the effect of _x_ on _y_ is diminished” that is true. The truth of one interpretation could be called upon to help explain my desire to assert what might be another interpretation, even if we decline for various reasons to identify the referents of the different interpretations of “_x_” and “_y_.” Suppose, alternatively, that we find normal operation of all systems in the flow-chart after the perceptual analyzer, but a diminuation in amplitude for some events or events earlier in the chain. This would seem in just the same way to support the second style of introspective report, and make my account suspect. But would it? Does the diminishing size of the retinal image of a receding figure make suspect the claims of perceptual size constancy? Only, perhaps, to those who hold extremely naive identity theories. Detailed isomorphisms between personal level talk of pains, beliefs, feelings, and actions and sub-personal level talk about impulse trains and their effects tempt the impatient to drive the silver spike of identity theory prematurely. The result is inevitably a theory that is easily refuted.

The philosophical questions that an identity theory (or other “philosophical” theory of pain) would be designed to answer are generated by our desire to put together an account that consistently honors all, or at any rate most, of our intuitions about _what pain is_. A prospect that cannot be discounted is that these intuitions do not make a consistent set. This would not be a particularly unsettling discovery if we could identify a few peripheral and unbuttressed intuitions as the culprits; they could be presumed to be mistaken or illusory, and dismissed, leaving a consistent core of intuitions as the raw material for philosophical analysis and system-building. Thus one might _legislate_ a neat systematic relationship between sortal talk of pains and mass-term talk of pain, thereby establishing two distinct “concepts of pain,” and ignore any intuitions inharmonious to that scheme however well attested to by ordinary usage. Recommending such a slight (and improving) revision of our ordinary concept would not be, arguably, doing violence to our ordinary concept. But if contradiction is more entrenched, a more radical approach is dictated.

Consider the idea that being in pain is not any mere occurrence of stimuli, but an interpreted reception, a _per_ception that is influenced by many prior cognitive and conative factors. Some will find this intuitive, but pre-theoretically it is hardly compelling. On the contrary, nothing is more “intuitive” to the sufferer than that there is little that is _cognitive_ about pain, that what one wants relief from is not merely an undesirable species of perception, that _in addition to_ one’s state of consciousness, or perceptual or epistemic state, the pain is there, a brute presence, unanalyzable and independent. The apparent disharmony between these two blocs of intuitions can be turned into clear contradiction if theory is permitted to develop along traditional lines. The grammatical grounds for the contradiction have already been noted: it is equally ordinary to speak of drugs that prevent pains or cause them to cease, and to speak of drugs that render one insensitive to the pains that may persist. (Further instances of the latter notion in our ordinary conception of pain can be found in the discussions of people who are “congenitally insensitive to pain.”[^20] Our _prima facie_ obligation not to cause pain in others is surely understood not to exclude these unfortunate individuals from the class of subjects.) So ordinary usage provides support both for the view that for pains, _esse est percipi_,[^21] and for the view that pains can occur unperceived.

What kinds of _objects of perception_ are pains: are they merely _intentional objects_, or have they an independent status? (See Chapter 10) No one can defensibly claim to know. Neither introspection nor physiological research can cast any light on the question, and philosophical analysis can plausibly support or attack either view for the simple reason that there are common intuitions and associated ways of speaking that support the contrary views.*]] If one takes such contradictory testimony to impeach the authority of such intuitions as determinants of our ordinary concept of pain, where else might one look for testimony? Not even contradiction can dislodge our shared intuitions from their role as best manifestations of—constitutive employments of—our ordinary concept. What must be impeached is our ordinary concept of pain. A better concept is called for, and since even the most rudimentary attempt at a unified theory of pain phenomena is led inelectably to the position that pain occurs normally only as the result of a process of perceptual analysis, the _esse est percipi_ position of pain promises to be more theoretically perspicuous, which, faced with the impasse of intuitions, is reason enough to adopt it.*]] This suggests an identification of pain with events—whatever they are—that occur post-interpretation, so that if we can determine where, in our model, interpretation is completed, whatever issues from that will be pain (when the interpretation machinery so interprets). Setting aside the categorical crudities of that formulation, there are still problems, for the interpretation of events in such a system is not an atomic matter, but highly compound. Perception has not one product but many, operating at different levels and in different ways. Has the interpretation machinery interpreted a signal as a pain if it evokes a speech disposition to say one is in pain? Or must it also produce the normal or apposite effects on belief, memory, desire, non-verbal action, and so forth? Looking at all the various effects such an interpretation of signals could produce, we can answer the philosophic questions about pain only by deciding which effects are “essential” to pain and which are not.

What governs our decisions about essentiality, however, is our stock of pretheoretical intuitions, which we have seen to be in disarray. Having countenanced legislation to settle two such conflicts already, we still face incompatibility of well-entrenched intuitions, such as these:

- (1) Pains are essentially items of immediate experience or consciousness; the subject’s access to pain is privileged or infallible or incorrigible.
- (2) Pains are essentially abhorrent or awful—“Pain is perfect misery, the worst of evils …”

Efforts to capture both of these “essential” features in a theory of pain are bound to fail; theories that contrive to maintain both of these claims do so only at the expense of equally well-entrenched claims from other quarters. To see this suppose we attempt to capture at least part of what is compelling about (1) by the thesis:

- (3) It is a necessary condition of pain that we are “incorrigible” about pain; i.e., if you believe you are in pain, your belief is true; you are in pain.*]]

Condition (3) says that belief that one is in pain is a _sufficient_ condition of pain. Such belief may be sufficient, but if we are held to be incorrigible about other states of mind or sensations as well (as incorrigibilists generally hold) there must be some other, distinguishing feature of pains; that they are abhorrent or awful seems as good a candidate as any. But then from (3) and

- (4) It is a necessary condition of pain that pains are awful

it follows that believing one is in pain is a sufficient condition for really experiencing or undergoing (and not merely believing one is experiencing or undergoing) something awful. But the belief itself is not the pain, and it is not awful. Surely it is logically possible to be in a dispositional state bearing all the usual earmarks of belief that one is in pain, and yet not be experiencing or undergoing something awful. Not only is this logically possible, it is instanced routinely by morphine subjects and others. Then is there any way of denying that this consequence of (3) and (4) is false? There is a heroic line available. One could maintain that _whatever_ dispositional state one was in, it could not properly be characterized as the state of belief that one was in pain unless one _understood_ the concept of pain, and hence believed that pains were awful, and hence would never believe one was in pain unless one believed one was experiencing something awful; and then, since we are incorrigible about experience in general, one would never believe one was experiencing something awful unless one was experiencing something awful and, finally, since something _undergone but not experienced_ (presuming that we can make sense of such a distinction) could not be awful (in the right sense), it really is quite defensible to claim that belief that one is in pain is sufficient condition for undergoing something awful.*]] This line can “save” (3) and (4) as conjoined necessary conditions of pain, but only at the expense of other intuitions, about our access to our beliefs or our capacity to say when we are in pain. If asked if I am in pain, I should say: “I am if I believe that I am, but who knows if my apparent belief is a genuine belief?” On this view, those who sincerely report that under morphine their pains are _intense_ but not _awful_ are not mistaken in believing they are in pain when they are not (for that has just been deemed to be logically impossible), but in saying something they do not believe (but only believe they believe?). The counterintuitiveness of this result does not utterly disqualify the heroic line. There are any number of ways of cutting this Gordian knot, and this is one of them. One decides which intuitions must go, and builds one’s theory accordingly.

I do not recommend the course just considered, however. I recommend giving up incorrigibility with regard to pain altogether, in fact giving up _all_ “essential” features of pain, and letting pain states be whatever “natural kind” states the brain scientists find (if they ever do find any) that normally produce all the normal effects. When that day comes we will be able to say whether masochists enjoy pain, whether general anesthetics prevent pain or have some other equally acceptable effect, whether there are unfelt pains, and so forth. These will be discoveries based on a somewhat arbitrary decision about what pain is, and calling something pain doesn’t make it pain. This is especially true of pain, for one of our intuitions about pain is that whether or not one is in pain is a brute fact, not a matter of decision to serve the convenience of the theorist. I recommend against trying to preserve that intuition, but if you disagree, whatever theory I produce, however predictive or elegant, will not be by your lights a theory of pain, but only a theory of what I illicitly choose to _call_ pain. But if, as I have claimed, the intuitions we would have to honor were we to honor them all do not form a consistent set, there can be no true theory of pain, and so no computer or robot could instantiate the true theory of pain, which it would have to do to feel real pain. Human beings and animals could no more instantiate the true theory of pain (there being none), which lands us with the outrageous conclusion that no one ever feels pain. But of course we do. Human suffering and pain cannot be whisked out of existence by such an argument. The parochiality of the concept of pain protects us but not robots (or Martians or at least lower animals) from the skeptical arguments, by fixing the burden of proof: an adequate theory of pain must have normal human beings as instantiations, a demand that presupposes the primacy, but not the integrity, of our ordinary concept of pain.

What then is the conclusion? It is that any robot instantiation of any theory of pain will be vulnerable to powerful objections that appeal to well-entrenched intuitions about the nature of pain, but reliance on such skeptical arguments would be short-sighted, for the inability of a robot model to satisfy all our intuitive demands may be due not to any irredeemable mysteriousness about the phenomenon of pain, but to irredeemable incoherency in our ordinary concept of pain. Physiological perplexities may defy the best efforts of theoreticians, of course, but philosophical considerations are irrelevant to the probability of that. If and when a good physiological sub-personal theory of pain is developed, a robot could in principle be constructed to instantiate it. Such advances in science would probably bring in their train wide-scale changes in what we found intuitive about pain, so that the charge that our robot only suffered what we artificially _called_ pain would lose its persuasiveness. In the meantime (if there were a cultural lag) thoughtful people would refrain from kicking such a robot.

## Notes

*]] They would be—perhaps unwittingly—wrong if they made this claim, as we shall see. Stimulation of the _C_-fibers is neither a necessary nor sufficient condition for the occurrence of pain. (C-fibers _are_ stimulated under general anesthesia, and need not be stimulated for neuralgia or central pain to occur.) The term “_C_-fibers” seems however to have lost, for philosophers, its empirical anchoring in neuroanatomy and become a philosopher’s wild-card referring expression for whatever physical event “turns out to be identical with” pain.

†]] Such an enterprise might be illuminated by a revival of the researches of the Nineteenth Century investigator Hahnemann, who botanized over seventy-three distinct phenomenological varieties of pain. See F. Sauerbruch, _Pain: Its Meaning and Significance_ (1963), p. 74.

*]] The distinction is not as clear-cut as it may first appear. Terry Winograd’s natural language understanding program (see Terry Winograd, “Understanding Natural Language,” _Cognitive Psychology_, 1972, vol. 3, pp. 1–191) “manipulates” the “objects in its environment” and answers questions about them. But its environment is entirely artificial and internal, like the environment of the swirling hurricane of our earlier example. When Winograd’s device “puts a cube on the table,” is it a doer or a describer? Moreover, if we view the theory we incorporate into a program as an _uninterpreted_ theory, we are free to view the computer’s behavior as satisfying one interpretation of the theory, so that any programmed computer can be viewed as instantiating (on one interpretation) the theory incorporated in its program. The tokens of computer behavior that on one interpretation are uttered _descriptions_ of the behavior of some other entity instantiating the theory, can on another interpretation be viewed as themselves instances of behavior predicted by the theory. I owe this observation to Joseph Weizenbaum.

*]] Some of this fantasy has already been turned to fact. SIM ONE, a robot used in training medical students, blinks, breathes, has measurable blood pressure, coughs, twitches, and can become “anesthetized.” See J. S. Denson and S. Abrahamson, “A Computer-controlled Patient Simulator,” _Journal of the American Medical Association_, CCVIll (1969): 504–8.

*]] Herbert Simon, in _The Sciences of the Artificial_ (1969), points to the distinction that is occasionally drawn between “artificial” and “synthetic”; a green glass gem might be called an artificial sapphire, while a manufactured gem chemically identical to genuine sapphire would be called synthetic (p. 4).

*]] Note that we can obtain the specificity of functional reaction to morphine and other drugs without accomplishing this in the same way, chemically, that a human body does.

*]] The difference in transmission speed is considerable: 100 meters per second versus less than two meters per second. If you stick a pin in your finger you can distinguish quite readily two resultant sensations in sequence: roughly, a pricking sensation followed swiftly by a “deeper” pain. This tandem effect is generally thought to be explained by the difference in transmission speed. See R. A. Sternbach, _Pain_, _A Psychophysiological Analysis_ (1968), p. 30 for a discussion of the issue, and references.

*]] Lesions in the old low path are responsible for “central pain” or neuralgia (pain with a central cause but peripheral “location”—one does not necessarily feel a _head_ache in central pain). “Cortical representations” of pain are considered less “important” by researchers generally. Cortical lesions seem almost never to produce central pain (and when they do, descending effects on the old low path are indicated). Penfield, in his research on stimulation of the exposed cortex produced a wide variety of effects, but almost no pain. See V. Cassinari and C. A. Pagni, _Central Pain: A Neurosurgical Survey_ (1969); and Wilder Penfield, _The Excitable Cortex in Conscious Man_ (1958). Moreover, “direct stimulation of the reticular and limbic systems produces strong aversive drive and behavior typical of responses to naturally occurring painful stimuli.” (Melzack and Wall, _op. cit.,_ p. 20.)

*]] Such volume control systems have been posited in the course of many different investigations in the brain sciences. Arguably such a system’s existence has been physiologically confirmed in cats. See Hernandez-Peon, Scherer and Jouvet, “Modification of Electrical Activity in Cochlear Nucleus during ‘Attention’ in Unanesthetized Cats,” _Science_, CXXIII (1956): 331–32.

*]] Can the Melzack-Wall theory also account for acupuncture anesthesia? It is not hard to speculate about mechanisms that could be added to the Melzack-Wall theory to accommodate the acupuncture effects, but I understand Wall is currently at least agnostic about the capacity of the theory to handle it.

*]] To some extent pain locations need to be _learned,_ though, and can be unlearned. In cases of limb amputation performed on children before they developed the use and coordination of the limb, phantom limb is rarely experienced. When amputation occurs just after birth, phantom limb never occurs. See M. Simmel, “Phantom Experiences Following Amputation in Childhood,” _Journal of Neurosurgery and Psychiatry_, XXV (1962): 69–72.

Moreover, locations can be “mislearned.” A variety of pain commonly produced in jet pilots under certain high altitude conditions is positively located by them in either the cheeks or the teeth. Which location is reported depends not on variation in the physiological etiology of the pain (which is constant) but on whether or not the pilots have had recent painful dental work.

*]] Aspirin also probably has central analgesic effects.

*]] A. S. Keats and H. K. Beecher, “Pain Relief with Hypnotic Doses of Barbiturates, and a Hypothesis,” _Journal of Pharmacology_ (1950). Lobotomy, though discredited as a behavior-improving psychosurgical procedure, is still a last resort tactic in cases of utterly intractable central pain, where the only other alternative to unrelenting agony is escalating morphine dosages, with inevitable addiction, habituation and early death. Lobotomy does not excise any of the old low path (as one might expect from its effect on pain perception), but it does cut off the old low path from a rich input source in the frontal lobes of the cortex.

*]] I know of no drug with just these powers, but a number of drugs used in anesthesia are known to have amnestic properties. Scopolamine is the strongest and most reliable amnestic (though it is still unreliable), but it has other effects as well: _not_ anesthesia or analgesia, but it does create hallucinations and a sort of euphoria. Scopolamine and other amnestics are often prescribed by anesthesiologists _for the purpose of creating amnesia._ “Sometimes,” I was told by a prominent anesthesiologist, “when we think a patient may have been awake during surgery, we give scopolamine to get us off the hook. Sometimes it works and sometimes not.” Scopolamine was once widely used in conjunction with a sedative or morphine to produce the “twilight sleep” then recommended for childbirth. One pharmacological textbook, in discussing this practice, uses the phrase “obstetrical amnesia or analgesia” as if amnesia and analgesia were much the same thing. (Goodman and Gilman, _op. cit_., p. 555.)

†]] Unable in fact, not unable in principle. We could quite easily devise signaling systems triggered directly by activity in electrode-monitored motor neurons. My point is not that such a state is in principle indistinguishable from anesthesia; I simply want to consider what, aside from current behavioral evidence (and later memory report) is crucial in making the determination.

‡]] I have found some people who proclaim their untroubled readiness to accept this substitute. I think they have been bewitched by Wittgensteinian logical behaviorism.

*]] For instance, the 1969 edition of Krantz and Carr describes the drawbacks of halothane, a recent popular inhalant anesthetic, as follows: it produces incomplete muscle relaxation, and “it does not produce patent analgesic properties, so it is used with nitrous oxide for analgesia, and a curariform [for paralysis].” One might well wonder just what halothane’s strengths are.

†]] When anesthesia (without curare) is so deep that reflexes are absent, the worry that this absence is due to a curariform effect of the anesthetic by itself has been laid to rest recently by experiments in which twitch responses were directly evoked in deeply anesthetized subjects by electrode stimulation of motor nerves. (Reported by S. H. Ngai, “Pharmacologic and Physiologic Aspects of Anesthesiology,” _New England Journal of Medicine,_ Feb. 26, 1970: 541.) This reassuring datum is somewhat beside the point, however, since under common anesthetic practice, the reflexes are only obliterated by the accompanying curare.

*]] The reason I do not object to positing a homunculus that, e.g., _infers_ on the basis of texture gradients, overlap and perspective clues that a particular object in my visual field is at a particular distance, is that although there are grounds for claiming an inference-like process must be _going on in me_, it is clear enough that _I_ do not draw the inference—though it gets drawn _in me_. But it is important that I be the subject of my pains. If the proper parts of me are for some purposes construable as homunculi, and if on these construals these proper parts are occasionally the subject of pain (an unlikely turn for the theory to take, but not impossible), then those will not or need not be occasions when it is also the case that I am the subject of pain.

†]] “Indeed the concept [of a pain center] is pure fiction unless virtually the whole brain is considered to be the ‘pain center’ because the thalamus, the limbic system, the hypothalamus, the brain stem reticular formation, the parietal cortex, and the frontal cortex are all implicated in pain perception.” Melzack and Wall, “Pain Mechanisms: A New Theory,” _Science,_ CL (1965): 975.

*]] Roger Trigg, in _Pain and Emotion_ (Oxford, 1970), claims, correctly I think, that it would be _abnormal_ but not conceptually impossible to have a very _intense_ pain but not dislike it. Trigg also offers a useful account of intensity of pain in which intensity is sharply distinguished from ‘strength of dislike’.

*]] Trigg, _op. cit._, examines the hypothesis that leucotomes are too confused or imbecilic to know what they are answering.

*]] Two different phenomena have been alluded to by this term. The pain-threshold measured by the Hardy-Wolff-Goodell _dolorimeter_ is presumed to be the minimal level of intensity at which a sensation type is deemed painful by the subject. (See J. D. Hardy, H. G. Wolff, and H. Goodell, _Pain Sensations and Reactions_, Baltimore: Williams and Wilkins, 1952, and also H. K. Beecher, op. cit., a classic critique of this experimental method of “measuring pain.”)

In more common parlance, one’s pain threshold is a _maximum_ level of pain one can “tolerate,” whatever that may be held to mean in the circumstances. The common belief that there is a wide variation in people’s tolerance for pain is expressed repeatedly in the medical literature (see, e.g., Asenath Petrie, _Individuality in Pain and Suffering_, Chicago, 1967), but nowhere that I know of is there a careful attempt to confirm this by any objective tests.

*]] Such a claim might be phenomenologically sincere, but as a justification for my convictions about how to describe the result of treatment it is without merit. I owe to Lawrence Davis the suggestion that we must not rule out the possibility of having such an experience.

*]] See Pitcher, “The Awfulness of Pain,” _loc. cit_., where a debate is presented between the Affirmativist, who holds that all pains are unpleasant, and the Negativist, who denies this. Pitcher claims, correctly I believe, that this debate “has no winner” (p. 485).

*]] In “Pain Perception,” _loc. cit_., Pitcher adopts a similarly pragmatic strategy, defending a “perceptual” theory of pain that “will strike many as bizarre” largely on grounds of theoretical cogency.

*]] Not all versions of “privileged access” (to pains and other items) would maintain, or imply, this thesis, but many do, and it should be clear in what follows that a parallel argument can be addressed against some important versions that do not. For instance, the view that if one _says_, sincerely and with understanding, that one is in pain, one is in pain, succumbs even more directly to a version of my argument. One might think (and I used to claim) that Saul Kripke was committed to the incorrigibility thesis by his claim, in “Naming and Necessity,” D. Davidson and G. Harman, eds., _The Semantics of Natural Language_ (Dordrecht: Reidel, 1972), p. 339: “To be in the same epistemic situation that would obtain if one had a pain _is_ to have a pain; to be in the same epistemic situation that would obtain in the absence of a pain _is_ not to have a pain.” But Kripke denies that this claim entails anything like (3) (A.P.A. Eastern Division Meeting, December 29, 1974). This leaves Kripke’s notion of epistemic situation obscure to me, and I would not hazard a guess about whether a version of my argument applies to his view.

*]] Pitcher discusses a similar argument in “Pain Perception”: 387–88.

1.]] Keith Gunderson, _Mentality and Machines_ (1971).

2.]] A. M. Turing, _Mind_ (1950), reprinted in A. R. Anderson, ed., _Minds and Machines_ (1964), and discussed by Gunderson, _inter alia_, in “The Imitation Game,” in Anderson and in _Mind_ (1964), and revised in _Mentality and Machines._.

3.]] Cf. Gunderson, _Mentality and Machines_, p. 157: “… emotion is different from its effects …”

4.]] See, e.g., Louis S. Goodman and Alfred Gilman, _The Pharmacological Basis of Therapeutics_ (2nd edition, 1955): p. 248.

5.]] On this, and many other matters Joseph Weizenbaum has provided me with illuminating suggestions.

6.]] Cf. Paul Ziff, “The Feelings of Robots,” _Analysis,_ XIX, 3 (1959), reprinted in Anderson, ed., _Minds and Machines._.

7.]] See, in this regard, Stanley Cavell’s suggestive paper, “Knowing and Acknowledging,” in _Must We Mean What We Say?_ (1969).

8.]] George Pitcher discusses the philosophical implications of the Melzack-Wall theory in “The Awfulness of Pain,” _Journal of Philosophy_, LXVIII (July 23, 1970). A more recent and highly developed version of the Melzack-Wall theory is found in R. Melzack and P. D. Wall, “Psychophysiology of Pain,” in H. Yamamura, ed., _Anesthesia and Neurophysiology, International Anesthesiology Clinics_, VIII, 1 (1970).

9.]] A journalistic account of this technique can be found in Marilyn Ferguson, _The Brain Revolution_ (1973).

10.]] See A. R. McIntyre, _Curare: Its History_, _Nature and Clinical Use_ (Chicago: University of Chicago, 1947) and A. E. Bennett, “The History of the Introduction of Curare into Medicine,” _Anesthesia and Analgesia_ (1968): 484–92.

11.]] See Scott M. Smith, et al., “The Lack of Cerebral Effects of d-Tubocurarine,” _Anesthesiology_, VIII, 1(1947): 1–14.

12.]] Ibid.

13.]] Surveys of anesthesiology can be found in the standard medical school pharmacology texts, such as Goodman and Gilman, _op. cit_., or J. C. Krantz and C. J. Carr, _The Pharmacological Principles of Medical Practice_ (7th edn., Baltimore: Williams and Wilkins, 1969). More up-to-date material on the physiology of anesthesia is contained in H. Yamamura, _op. cit_.

14.]] Mary Brazier, “Effects of Anesthesia on Visually Evoked Responses,” in H. Yamamura, _op. cit_.

15.]] H. K. Beecher, “The Measurement of Pain,” _Pharmacological Review_ (1957): 59–191.

16.]] A survey of the literature on animal pain (which I have only begun) could start with William V. Lumb and E. Wynn Jones, _Veterinary Anesthesia_, 1973. See also J. E. Breazille and R. L. Kitchell, “Pain Perception in Animals,” _Federal Proceedings_, XXVIII, 1379 (1969) and G. Corssen, et al., “Changing Concepts in Pain Control During Surgery: Dissociative Anesthesia with Cl-581 [Ketamine] A Progress Report” in _Anesthesia and Analgesia_, LXVII (1968): 6.

17.]] See _Content and Consciousness_, especially Chapter 4.

18.]] Cf. Thomas Nagel’s review of _Content and Consciousness_, _Journal of Philosophy_ (April 20, 1972).

19.]] G. E. M. Anscombe, _Intention_ (Ithaca, New York: Cornell University Press, 1963): p. 14. See also the discussion of this case in Kathryn Pyne Parsons, “Mistaking Sensations,” _Philosophical Review_, LXXIX (April, 1970).

20.]] See, e.g., L. D. Cohen, et al., “Case Report: Observations of a Person with Congenital Insensitivity to Pain,” _Journal of Abnormal Social Psychology_, LI (1955): 333, and B. H. Kirman, et al., “Congenital Insensitivity to Pain in an Imbecile Boy,” _Developmental Medicine and Child Neurology_, X (1968): 57–63.

21.]] See, e.g., George Pitcher, “Pain Perception,” _Philosophical Review_, LXXIX (July, 1970).Free Will and Personhood

Mechanism and Responsibility

## I

In the eyes of many philosophers the old question of whether determinism (or indeterminism) is incompatible with moral responsibility has been superseded by the hypothesis that _mechanism_ may well be. This is a prior and more vexing threat to the notion of responsibility, for mechanism is here to stay, unlike determinism and its denial, which go in and out of fashion. The mechanistic style of explanation, which works so well for electrons, motors and galaxies, has already been successfully carried deep into man’s body and brain, and the open question now is not whether mechanistic explanation of human motion is possible, but just whether it will ultimately have crucial gaps of randomness (like the indeterminists’ mechanistic explanation of electrons) or not (like the mechanistic explanation of macroscopic systems such as motors and billiards tables). In either case the believer in responsibility has problems, for it seems that whenever a particular bit of human motion can be given an entirely mechanistic explanation—with or without the invocation of “random” interveners—any non-mechanistic, rational, purposive explanation of the same motions is otiose. For example, if we are on the verge of characterizing a particular bit of human motion as a well-aimed kick in the pants, and a doctor can show us that in fact the extensor muscles in the leg were contracted by nerve impulses triggered by a “freak” (possibly random?) epileptic discharge in the brain, we will have to drop the search for purposive explanations of the motion, and absolve the kicker from all responsibility. Or so it seems. A more central paradigm might be as follows. Suppose a man is found who cannot, or will not, say the word “father.” Otherwise, we may suppose, he seems perfectly normal, and even expresses surprise at his “inability” to say “that word I can’t say.” A psychoanalyst might offer a plausible explanation of this behavior in terms of unconscious hatred and desires and beliefs about his father, and a layman might say “Nonsense! This man is just playing a joke. I suspect he’s made a bet that he can go a year without saying ‘father’ and is doing all this deliberately.” But if a neurosurgeon were to come along and establish that a tiny lesion in the speech center of the brain caused by an aneurysm (random or not) was causally responsible for the lacuna in the man’s verbal repertory (not an entirely implausible discovery in the light of Penfield’s remarkable research), both the analyst’s and the layman’s candidates for explanation would have the rug pulled out from under them. Since a mere mechanistic happening in the brain, random or not, was the cause of the quirk, the man cannot have had reasons, unconscious or ordinary, for it, and cannot be held responsible for it. Or so it seems.

The principle that seems to some philosophers to emerge from such examples is that _the mechanistic displaces the purposive_, and any mechanistic (or causal) explanation of human motions takes priority over, indeed renders false, any explanation in terms of desires, beliefs, intentions. Thus Hospers says “Let us note that the more _thoroughly_ and _in detail_ we know the causal factors leading a person to behave as he does, the more we tend to exempt him from responsibility.”[^1] And Malcolm has recently supported the view that “although purposive explanations cannot be dependent on non-purposive explanations, they would be refuted by the verification of a comprehensive neurophysiological theory of behavior.”[^2] I want to argue that this principle is false, and that it is made plausible only by focusing attention on the wrong features of examples like those above. The argument I will unwind strings together arguments and observations from a surprisingly diverse group of recent writers, and perhaps it is fair to say that my share of the argument is not much. I will try to put the best face on this eclecticism by claiming that my argument provides a more fundamental and unified ground for these variously expressed discoveries about the relations between responsibility and mechanism.

## II

The first step in reconciling mechanism and responsibility is getting clearer about the nature of the apparently warring sorts of explanations involved. Explanations that serve to ground verdicts of responsibility are couched at least partly in terms of the beliefs, intentions, desires, and reasons of the person or agent held responsible. There is a rough consensus in the literature about the domain of such explanations, but different rubrics are used: they are the “purposive” or “rational” or “action” or “intentional” explanations of behavior. I favor the term ‘intentional’ (from the scholastics, via Brentano, Chisholm, and other revivalists). _Intentional explanations_, then, cite thoughts, desires, beliefs, intentions, rather than chemical reactions, explosions, electric impulses, in explaining the occurrence of human motions. There is a well-known controversy debating whether (any) intentional explanations are ultimately only causal explanations—Melden and Davidson[^3] are the initial protagonists—but I shall avoid the center of this controversy and the related controversy about whether a desire or intention could be identical with a physical state or event, and rest with a more modest point, namely that intentional explanations are at least not causal explanations _simpliciter_. This can best be brought out by contrasting genuine intentional explanations with a few causal hybrids.

Not all explanations containing intentional terms are intentional explanations. Often a belief or desire or other intentional phenomenon (intentional in virtue of being referred to by intentional idioms) is cited as a cause or (rarely) effect in a perfectly Humean sense of cause and effect.

- (1) His belief that the gun was loaded caused his heart attack.
- (2) His obsessive desire for revenge caused his ulcers.
- (3) The thought of his narrow escape from the rattler made him shudder.

These sentences betray their Humean nature by being subject to the usual rules of evidence for causal assertions. We do not know at this time how to go about confirming (1), but whatever techniques and scientific knowledge we might have recourse to, our tactic would be to show that no other conditions inside or outside the man were sufficient to bring on the heart attack, and that the belief (however we characterize or embody it) together with the prevailing conditions brought about the heart attack in a law-governed way. Now this sort of account may be highly suspect, and ringed with metaphysical difficulties, yet it is undeniable that this is roughly the story we assume to be completable in principle when we assert (1). It may seem at first that (1) is not purely causal, for the man in question can tell us, infallibly or non-inferentially, that it was his belief that caused his heart attack. But this is false. The man is in no better position than we to say what caused his heart attack. It may feel to him as if this was the cause of the attack, but he may well be wrong; his only _knowledge_ is of the temporal juxtaposition of the events. Similarly, (2) would be falsified if it turned out that the man’s daily consumption of a quart of gin was more than sufficient to produce his ulcers, however strong and sincere his intuitions that the vengefulness was responsible. We are apt to think we have direct, non-inferential experience of thoughts causing shudders, as asserted in (3), but in fact we have just what Hume says we have: fallible experience over the years of regular conjunction.

These explanations are not intentional because they do not explain by _giving a rationale_ for the _explicandum_. Intentional explanations explain a bit of behavior, an action, or a stretch of inaction, by making it reasonable in the light of certain beliefs, intentions, desires ascribed to the agent. (1) to (3) are to be contrasted in this regard with

- (4) He threw himself to the floor because of his belief that the gun was loaded.
- (5) His obsessive desire for revenge led him to follow Jones all the way to Burma.
- (6) He refused to pick up the snake because at that moment he thought of his narrow escape from the rattler.

The man’s heart attack in (1) is not made _reasonable_ in the light of his belief (though we might say we can now understand how it happened), but his perhaps otherwise inexplicable action in (4) is. Sentence (5) conspicuously has “led” where its counterpart has “caused,” and for good reason. Doubts about (5) would not be settled by appeal to inductive evidence of past patterns if constant conjunctions, and the man’s own pronouncements about his trip to Burma have an authority his self-diagnosis in (2) lacks.

The difference in what one is attempting to provide in mechanistic and intentional explanations is especially clear in the case of “psychosomatic” disorders. One can say—in the manner of (1) and (2)—that a desire or belief merely _caused_ a symptom, say, paralysis, or one can say that a desire or belief led a person to _want_ to be paralyzed—to become paralyzed _deliberately_. The latter presumes to be a purely intentional explanation, a case of making the paralysis—as an _intended condition—reasonable_ in the light of certain beliefs and desires, e.g., the desire to be waited on, the belief that relatives must be made to feel guilty.

## III

Intentional explanations have the actions of persons as their primary domain, but there are times when we find intentional explanations (and predictions based on them) not only useful but indispensable for accounting for the behavior of complex machines. Consider the case of the chess-playing computer, and the different stances one can choose to adopt in trying to predict and explain its behavior. First there is the _design stance_. If one knows exactly how the computer’s program has been designed (and we will assume for simplicity that this is not a learning or evolving program but a static one), one can predict the computer’s designed response to any move one makes. One’s prediction will come true provided only that the computer performs as designed, that is, without breakdown. In making a prediction from the design stance, one _assumes_ there will be no malfunction, and predicts, as it were, from the blueprints alone. The essential feature of the design stance is that we make predictions solely from knowledge of or assumptions about the system’s design, often without making any examination of the innards of the particular object.

Second, there is what we may call the _physical stance_. From this stance our predictions are based on the actual state of the particular system, and are worked out by applying whatever knowledge we have of the laws of nature. It is from this stance alone that we can predict the malfunction of systems (unless, as sometimes happens these days, a system is _designed_ to malfunction after a certain time, in which case malfunctioning in one sense becomes a part of its proper functioning). Instances of predictions from the physical stance are common enough: “If you turn on that switch you’ll get a nasty shock,” and, “When the snows come that branch will break right off,” are cases in point. One seldom adopts the physical stance in dealing with a computer just because the number of critical variables in the physical constitution of a computer would overwhelm the most prodigious human calculator. Significantly, the physical stance is generally reserved for instances of breakdown, where the condition preventing normal operation is generalized and easily locatable, e.g., “Nothing will happen when you type in your question, because it isn’t plugged in,” or, “It won’t work with all that flood water in it.” Attempting to give a physical account or prediction of the chess-playing computer would be a pointless and herculean labor, but it would work in principle. One could predict the response it would make in a chess game by tracing out the effects of the input energies all the way through the computer until once more type was pressed against paper and a response was printed.

There is a third stance one can adopt toward a system, and that is the _intentional stance_. This tends to be most appropriate when the system one is dealing with is too complex to be dealt with effectively from the other stances. In the case of the chess-playing computer one adopts this stance when one tries to predict its response to one’s move by figuring out what a good or reasonable response would be, given the information the computer has about the situation. Here one assumes not just the absence of malfunction, but the rationality of design or programming as well. Of course the stance is pointless, in view of its extra assumption, in cases where one has no reason to believe in the system’s rationality. In weather predicting, one is not apt to make progress by wondering what clever move the wise old West Wind will make next. Prediction from the intentional stance assumes rationality in the system, but not necessarily perfect rationality. Rather, our pattern of inference is that we start with the supposition of what we take to be perfect rationality, and then alter our premise in individual cases as we acquire evidence of individual foibles and weaknesses of reason. This bias in favor of rationality is particularly evident in the tactics of chess players, who set out to play a new opponent by assuming that he will make reasonable responses to their moves, and then seeking out weaknesses. The opponent who started from an assumption of irrationality would be foolhardy in the extreme. But notice, in this regard, how the designer of a chess-playing program might himself be able to adopt the design stance, and capitalize from the very beginning on flaws in rationality he knew were built into the program. In the early days of chess-playing programs, this tactic was feasible, but today, with evolving programs capable of self-improvement, designers are no longer capable of maintaining the design stance in playing against their own programs, and must resort, as any outsider would, to the intentional stance in trying to outwit their own machines.

Whenever one can successfully adopt the intentional stance toward an object, I call that object an _intentional system_. The success of the stance is of course a matter settled pragmatically, without reference to whether the object _really_ has beliefs, intentions, and so forth; so whether or not any computer can be conscious, or have thoughts or desires, some computers undeniably _are_ intentional systems, for they are systems whose behavior can be predicted, and most efficiently predicted, by adopting the intentional stance toward them (see Chapter 1).

This tolerant assumption of rationality is the hallmark of the intentional stance with regard to people as well as computers. We start by assuming rationality in our transactions with other adult human beings, and adjust our predictions as we learn more about personalities. We do not _expect_ new acquaintances to react irrationally to particular topics, but when they do, we adjust our strategies accordingly. The presumption that we will be able to communicate with our fellow men is founded on the presumption of their rationality, and this is so strongly entrenched in our inference habits that when our predictions prove false we first cast about for external mitigating factors (he must not have heard, he must not know English, he must not have seen x, been aware that y, etc.) before questioning the rationality of the system as a whole. In extreme cases personalities may prove to be so unpredictable from the intentional stance that we abandon it, and if we have accumulated a lot of evidence in the meanwhile about the nature of response patterns in the individual, we may find that the design stance can be effectively adopted. This is the fundamentally different attitude we occasionally adopt toward the insane. It need hardly be added that in the area of behavior (as opposed to the operation of internal organs, for instance) we hardly ever know enough about the physiology of individuals to adopt the physical stance effectively, except for a few dramatic areas, like the surgical cure of epileptic seizures.

## IV

The distinction of stance I have drawn appears closely related to MacKay’s distinction between the “personal aspect” and the “mechanical aspect” of some systems. Of central importance in MacKay’s account is his remarking that the choice of stance is “up to us,” a matter of _decision_, not discovery.[^4] Having chosen to view our transactions with a system from the intentional stance, certain characterizations of events necessarily arise, but that these arise _rightly_ cannot be a matter of proof. Much the same distinction, I believe, is presented in a different context by Strawson, who contrasts “participation in a human relationship” with “the objective attitude.” “If your attitude toward someone is wholly objective, then though you may fight him, you cannot quarrel with him, and though you may talk to him, even negotiate with him, you cannot reason with him. You can at most pretend to quarrel, or to reason, with him.”[^5] Both MacKay and Strawson say a great deal that is illuminating about the conditions and effects of adopting the personal or participant attitude toward someone (or something), but in their eagerness to establish the implications for ethics of the distinction, they endow it with a premature moral dimension. That is, both seem to hold that adopting the personal attitude toward a system (human or not) involves admitting the system into the moral community. MacKay says, in discussing the effect of our adopting the attitude toward a particular animate human body,

> At the personal level, Joe will have established some personal claims on us, and we on Joe. We shall not be able rightly to tamper with his brain, for example, nor feel free to dismantle his body. … He has become ‘one of us’, a member of the linguistic community—not, be it noted, by virtue of the particular _stuff_ of which his brain is built … but by virtue of the particular kinds of mutual interaction that it can sustain with our own—interaction which at the personal level we describe as that of person-to-person.[^6]

MacKay is, I believe, conflating two choices into one. The first choice, to ascend from the mechanistic to the intentional stance, as portrayed by our chess-playing designer, has no moral dimension. One is guilty of no monstrosities if one dismembers the computer with whom one plays chess, or even the robot with whom one has long conversations. One adopts the intentional stance toward any system one assumes to be (roughly) rational, where the complexities of its operation preclude maintaining the design stance effectively. The second choice, to adopt a truly moral stance toward the system (thus viewing it as a person), might often turn out to be psychologically irresistible given the first choice, but it is logically distinct. Consider in this context the hunter trying to stalk a tiger by thinking what _he_ would do if he were being hunted down. He has adopted the intentional stance toward the tiger, and perhaps very effectively, but though the psychological tug is surely there to disapprove of the hunting of any creature wily enough to deserve the intentional treatment, it would be hard to sustain a charge of either immorality or logical inconsistency against the hunter. We might, then, distinguish a fourth stance, above the intentional stance, called the _personal stance_. The personal stance presupposes the intentional stance (note that the intentional stance presupposes _neither_ lower stance) and seems, to cursory view at least, to be just the annexation of moral commitment to the intentional. (A less obvious relative of my distinctions of stance is Sellars’ distinction between the manifest and scientific _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett of man. Sellars himself draws attention to its kinship to Strawson: “Roughly, the manifest image corresponds to the world as conceived by P. F. Strawson. … The manifest image is, in particular, a framework in which the distinctive features of persons are conceptually irreducible to features of nonpersons, e.g. animals and merely material things.”[^7] A question I will not attempt to answer here is whether Sellars’ manifest image lines up more with the narrower, and essentially moral, personal stance or the broader intentional stance. (See Chapter 14.)

Something like moral commitment can exist in the absence of the intentional stance, as Strawson points out, but it is not the same; the objective attitude—my design or physical stances—“may include pity or even love, though not all kinds of love.” The solicitude of a gardener for his flowers, or for that matter, of a miser for his coins, cannot amount to moral commitment; because of the absence of the intentional. (Parenthetical suggestion: is the central fault in utilitarianism a confusion of gardener-solicitude with person-solicitude?)

Since the second choice (of moral commitment) is like the first in being just a choice, relative to ends and desires and not provably right or wrong, it is easy to see how they can be run together. When they are, important distinctions are lost. Strawson’s union of the two leads him to propose, albeit cautiously, a mistaken contrast: “But what is above all interesting is the tension there is, in us, between the participant attitude and the objective attitude. One is tempted to say: between our humanity and our intelligence. But to say this would be to distort both notions. “[^8] The distortion lies in allying the non-intentional, mechanistic stances with the coldly rational and intelligent, and the intentional stance with the emotional. The intentional stance of one chess player toward another (or the hunter toward his prey) can be as coldly rational as you wish, and alternatively one can administer to one’s automobile in a bath of sentiment.

Distinctions are also obscured if one makes _communicating with_ a system the hallmark of intentionality or rationality. Adopting the intentional stance toward the chess-playing computer is not necessarily viewing one’s moves as _telling_ the computer anything (I do not have to _tell_ my human opponent where I moved—he can _see_ where I moved); it is merely predicting its responses with the assumption that it will respond rationally to its _perceptions_. Similarly, the hunter stalking the tiger will be unlikely to try to _communicate_ with the tiger (although in an extended sense even this might be possible—consider the sort of _entente_ people have on occasion claimed to establish with bears encountered on narrow trails, etc.), but he will plan his strategy on his assessment of what the tiger would be reasonable to _believe_ or _try_, given its perceptions. As Grice has pointed out,[^9] one thing that sets communication as a mode of interaction apart from others is that in attempting a particular bit of communication with _A_, one intends to produce in _A_ some response _and_ one intends _A_ to recognize that one intends to produce in him this response _and_ one intends that _A_ produce this response on the basis of recognition. When one’s assessment of the situation leads to the belief that these intentions are not apt to be fulfilled, one does not try to communicate with _A_, but one does not, on these grounds, necessarily abandon the intentional stance. _A_ may simply not understand any language one can speak, or any language at all (e.g., the tiger). One can still attempt to influence _A_’s behavior by relying on _A_’s rationality. For instance, one can throw rocks at _A_ in an effort to get _A_ to leave, something that is apt to work with Turk or tiger, and in each case what one does is at best marginal communication.*]]

Communication, then, is not a separable and higher _stance_ one may choose to adopt toward something, but a type of interaction one may attempt within the intentional stance. It can be seen at a glance that the set of intentions described by Grice would not be fulfilled with any regularity in any community where there was no _trust_ among the members, and hence communication would be impossible, and no doubt this sort of consideration contributes to the feeling that the intentional community (or at least the smaller _communicating_ community) is co-extensive with the moral community, but of course the only conclusion validly drawn from Grice’s analysis here is a pragmatic one: if one wants to influence _A_’s behavior, and _A_ is capable of communicating, then one will be able to establish a very _effective_ means of influence by establishing one’s trustworthiness in _A_’s eyes (by hook or by crook). It is all too easy, however, to see interpersonal, convention-dependent communication as the mark of the intentional—perhaps just because intentional systems process information—and thus make the crucial distinction out to be that between “poking at” a system (to use MacKay’s vivid phrase) and communicating with it. Not only does this way of putting the matter wrongly confuse the system’s perception of communications with its perception more generally, but it is apt to lead to a moralistic inflation of its own. The notion of communication is apt to be turned into something mystical or semi-divine—synonyms today are “rap,” “groove,” “dig,” “empathize.” The critical sense of communication, though, is one in which the most inane colloquies between parent and teenager (or man and bear) count as communication. (MacKay himself has on occasion suggested that the personal attitude is to be recognized in Buber’s famous I—Thou formula, which is surely inflation.) The ethical implication to be extracted from the distinction of stance is not that the intentional stance is _a_ moral stance, but that it is a precondition of _any_ moral stance, and hence if it is jeopardized by any triumph of mechanism, the notion of moral responsibility is jeopardized in turn.

## V

Reason, not regard, is what sets off the intentional from the mechanistic; we do not just reason about what intentional systems will do, we reason about how they will reason. And so it is that our predictions of what an intentional system will do are formed on the basis of what would be reasonable (for anyone) to do under the circumstances, rather than on what a wealth of experience with this system or similar systems might inductively suggest the system will do. It is the absence from the mechanistic stances of this presupposition of rationality that gives rise to the widespread feeling that there is an antagonism between predictions or explanations from these different stances. The feeling ought to be dissipated at least in part by noting that the absence of a presupposition of rationality is not the same as a presupposition of non-rationality.

Suppose someone asks me whether a particular desk calculator will give 108 as the product of 18 and 6.[^10] I work out the sum on a piece of paper and say, “Yes.” He responds with, “I know that it _should_, but will it? You see, it was designed by my wife, who is no mathematician.” He hands me her blueprints and asks for a prediction (from the design stance). In working on this prediction the assumption of rationality, or good design, is useless, so I abandon it, not as false but as question-begging. Similarly, if in response to his initial question I reply, “It’s an IBM, so yes,” he may reply, “I know it’s _designed_ to give that answer, but I just dropped it, so maybe it’s broken.” In setting out to make this prediction I will be unable to avail myself of the assumption that the machine is designed to behave in a certain way, so I abandon it. My prediction does not depend on any assumptions about rationality or design, but neither does it rescind any.

One reason we are tempted to suppose that mechanistic explanations preclude intentional explanations is no doubt that since mechanistic explanations (in particular, physical explanations) are for the most part attempted, or effective, only in cases of malfunction or breakdown, where the rationality of the system is obviously impaired, we associate the physical explanation with a failure of intentional explanation, and ignore the possibility that a physical explanation will go through (however superfluous, cumbersome, unfathomable) in cases where intentional explanation is proceeding smoothly. But there is a more substantial source of concern than this, raised by MacIntyre.

> Behaviour is rational—in this arbitrarily, defined sense—if, and only if, it can be influenced, or inhibited by the adducing of some logically relevant consideration. … But this means that if a man’s behaviour is rational it cannot be determined by the state of his glands or any other antecedent causal factor. For if giving a man more or better information or suggesting a new argument to him is a both necessary and sufficient condition for, as we say, changing his mind, then we exclude, for this occasion at least, the possibility of other sufficient conditions. … Thus to show that behaviour is rational is enough to show that it is not causally determined in the sense of being the effect of a set of sufficient conditions _operating independently of the agent’s deliberation or possibility of deliberation._ So the discoveries of the physiologist and psychologist may indefinitely increase our knowledge of why men behave irrationally but they could never show that rational behaviour in this sense was causally determined. (_my italics_)[^11]

MacIntyre’s argument offers no license for the introduction of the italicized phrase above, and without it his case is damaged, as we shall see later, when the effect of prediction is discussed. More fundamental, however, is his misleading suggestion that the existence of sufficient conditions for events in a system puts that system in a straitjacket, as it were, and thus denies it the flexibility required of a truly rational system. There is a grain of truth in this, which should be uncovered. In elaborating the distinction between stances, I chose for an example a system of rather limited versatility; the chess-playing system is unequipped even to play checkers or bridge, and input appropriate to these other games would reveal the system to be as non-rational and unresponsive as any stone. There is a fundamental difference between such limited-purpose systems and systems that are supposed to be capable of responding appropriately to input of all sorts. For although it is possible in principle to design a system that can be guaranteed to respond appropriately (relative to some stipulated ends) to any limited number of inputs given fixed, or finitely ambiguous or variable, environmental “significance,” there is no way to design a system that can be guaranteed to react appropriately under _all_ environmental conditions. A detailed argument for this claim would run on too long for this occasion, and I have presented the major steps of it in _Content and Consciousness_ so I will try to establish at least comprehension, if not conviction, for the claim by a little thought-experiment about _tropistic behavior_.

Consider Wooldridge’s account of the tropistic behavior of the _Sphex_ wasp (Chapter 4, pp. 71–72 of this volume).[^12] The interference with the wasp unmasks the behavior as a tropism, rigid within the limits set on the significance of the input, however felicitous its operation under normal circumstances. The wasp’s response lacks that freewheeling flexibility in response to the situation that Descartes so aptly honored as the infinity of the rational mind. For the notion of a perfectly rational, perfectly adaptable system, to which all input compatible with its input organs is significant and comprehensible is the notion of an unrealizable physical system. For let us take the wasp’s tropism and improve on it. That is, suppose we take on the role of wasp designers, and decide to enlarge the subroutine system of the tropism to ensure a more rational fit between behavior and _whatever_ environment the wasp may run into. We think up one stymying environmental condition after another, and in each case design subroutines to detect and surmount the difficulty. There will always be room for yet one more set of conditions in which the rigidly mechanical working out of response will be unmasked, however long we spend improving the system. Long after the wasp’s behavior has become so perspicacious that we would not think of calling it tropistic, the fundamental nature of the system controlling it will not have changed; it will just be more complex. In this sense any behavior controlled by a finite mechanism must be tropistic.

What conclusion should be drawn from this about human behavior? That human beings, as finite mechanical systems, are not rational after all? Or that the demonstrable rationality of man proves that there will always be an inviolable _terra incognita_, an infinite and non-mechanical mind beyond the grasp of physiologists and psychologists? It is hard to see what evidence could be adduced in support of the latter conclusion, however appealing it may be to some people, since for every awe-inspiring stroke of genius cited in its favor (the Einstein-Shakespeare gambit), there are a thousand evidences of lapses, foibles, bumbling and bullheadedness to suggest to the contrary that man is only imperfectly rational. Perfection is hard to prove, and nothing short of perfection sustains the argument. The former alternative also lacks support, for although in the case of the wasp we can say that its behavior has been shown to be _merely_ mechanically controlled, what force would the “merely” have if we were to entertain the notion that the control of man’s more versatile behavior is merely mechanical? The denigration might well be appropriate if in a particular case the mechanical explanation of a bit of behavior was short and sweet (consider explanations of the knee-jerk reflex or our hypothetical man who cannot say “father”), but we must also consider cases in which the physiologist or cybernetician hands us twenty volumes of fine print and says, “Here is the design of this man’s behavioral control system.” Here is a case where the philosopher’s preference for simple examples leads him astray, for of course any _simple_ mechanistic explanation of a bit of behavior will disqualify it for plausible intentional characterization, make it a mere happening and not an action, but we cannot generalize from simple examples to complex, for it is precisely the simplicity of the examples that grounds the crucial conclusion.

The grain of truth in Macintyre’s contention is that _any_ system that can be explained mechanistically—at whatever length—must be in an extended sense tropistic, and this can enhance the illusion that mechanistic and intentional explanations cannot coexist. But the only implication that could be drawn from the _general_ thesis of man’s ultimately mechanistic organization would be that man must, then, be imperfectly rational, in the sense that he cannot be so designed as to _ensure_ rational responses to all contingencies, hardly an alarming or counter-intuitive finding; and from any _particular_ mechanistic explanation of a bit of behavior, it would not follow that that particular bit of behavior was or was not a rational response to the environmental conditions at the time, for the mere fact that the response _had_ to follow, given its causal antecedents, casts no more doubt on its rationality than the fact that the calculator _had_ to answer “108” casts doubt on the arithmetical correctness of its answer.

What, then, can we say about the hegemony of mechanistic explanations over intentional explanations? Not that it does not exist, but that it is misdescribed if we suppose that whenever the former are confirmed, they drive out the latter. It is rather that mechanistic predictions, eschewing any presuppositions of rationality, can put the lie to intentional predictions when a system happens to fall short of rationality in its response, whether because of weakness of “design,” or physically predictable breakdown. It is the presuppositions of intentional explanation that put prediction of _lapses_ in principle beyond its scope, whereas lapses are in principle predictable from the mechanistic standpoint, provided they are not the result of truly random events.*]]

## VI

It was noted earlier that the search for a watershed to divide the things we are responsible for from the things we are not comes to rest usually with a formulation roughly harmonious with the distinction drawn here between the intentional and the mechanistic. Many writers have urged that we are responsible for just those events that are our intentional _actions_ (and for their foreseeable results), and a great deal has been written in an effort to distinguish action from mere happening. The performing of actions is the restricted privilege of rational beings, persons, conscious agents, and one establishes that something is an action not by examining its causal ancestry but by seeing whether certain sorts of talk about _reasons_ for action are appropriate in the context. On this basis we exculpate the insane, with whom one is unable to reason, unable to communicate; we also excuse the results of physical _force majeure_ against which reason cannot prevail, whether the force is external (the chains that bind) or internal (the pain that makes me cry out, revealing our position to the enemy). This fruitful distinction between reason-giving and cause-giving is often, however, the source of yet another misleading intuition about the supposed antagonism between mechanism and responsibility. “Roughly speaking,” Anscombe says, “it establishes something as a reason if one argues against it.”[^13] One is tempted to go on: a reason is the sort of thing one can argue against with some hope of success, but one cannot argue against a causal chain. There is of course a sense in which this is obvious: one cannot argue with what has no ears to hear, for instance. But if one tries to get the point into a form where it will do some work, namely: “The presentation of an argument cannot affect a causal chain,” it is simply false. Presentations of arguments have all sorts of effects on the causal milieu: they set air waves in motion, cause ear drums to vibrate, and have hard to identify but important effects deep in the brain of the audience. So although the presentation of an argument may have no detectable effect on the trajectory of a cannonball, or closer to home, on one’s _autonomic_ nervous system, one’s perceptual system is designed to be sensitive to the sorts of transmissions of energy that must occur for an argument to be communicated. The perceptual system can, of course, be affected in a variety of ways; if I sneak up behind someone and yell “Flinch, please!” in his ear, the effects wrought by my utterance would not constitute an action in obedience to my request, not because they were effects of a cause, but because the intricate sort of causal path that in general would have to have existed for an intentional explanation to be appropriate was short-circuited. An intentional system is precisely the sort of system to be affected by the input of information, so the discovery in such a system of a causal chain culminating in a bit of behavior does not at all license the inference: “Since the behavior was caused we could not have argued him out of it,” for a prior attempt to argue him out of it would have altered the causal ancestry of the behavior, perhaps effectively.

The crucial point when assessing responsibility is whether or not the antecedent inputs achieve their effects as inputs of information, or by short-circuit. The possibility of short-circuiting or otherwise tampering with an intentional system gives rise to an interesting group of perplexities about the extent of responsibility in cases where there has been manipulation. We are generally absolved of responsibility in cases where we have been manipulated by others, but there is no one principle of innocence by reason of manipulation. To analyze the issue we must first separate several distinct excusing conditions that might be lumped together under the heading of manipulation.

First, one may disclaim responsibility for an act if one has been led to commit the act by deliberately false information communicated by another, and one might put this: “He manipulated me, by forging documents.” The principle in such cases has nothing to do with one’s intentional system being tampered with, and in fact the appeal to the deliberate malice of the other party is a red herring.[^14] The principle invoked to determine guilt or innocence in such cases is simply whether the defendant had reasonably good evidence for the beliefs which led to his act (and which, if true, would have justified it, presumably). The plain evidence of one’s senses is normally adequate when what is at issue is the presentation of a legal document, and so normally one is absolved when one has been duped by a forgery, but not, of course, if the forgery is obvious or one has any evidence that would lead a reasonable man to be suspicious. And if the evidence that misled one into a harmful act was produced by mere chance or “act of God” (such as a storm carrying away a “Stop” sign) the principle is just the same. When one is duped in this manner by another, one’s intentional system has not been tampered with, but rather exploited.

The cases of concern to us are those in which one’s behavior is altered by some non-rational, non-intentional interference. Here, cases where a person’s body is merely mechanically interposed in an ultimately harmful result do not concern us either (e.g., one’s arm is bumped, spilling Jones’s beer, or less obviously, one is drugged, and hence is unable to appear in court). One is excused in such cases by an uncomplicated application of the _force majeure_ principle. The only difficult cases are those in which the non-rational, non-intentional interference alters one’s beliefs and desires, and subsequently, one’s actions. Our paradigm here is the idea—still fortunately science fiction—of the neurosurgeon who “rewires” me and in this way inserts a belief or desire that was not there before. The theme has an interesting variation which is not at all fictional: the mad scientist might discover enough about a man’s neural _design_ (or program) to figure out that certain inputs would have the effect of reprogramming the man, quite independent of any apparent sense they might have for the man to react to rationally. For instance, the mad scientist might discover that flashing the letters of the alphabet in the man’s eyes at a certain speed would cause him (in virtue of his imperfectly rational design) to believe that Mao is God. We have, in fact, fortuitously hit upon such ways of “unlocking” a person’s mind in hypnotism and brain-washing, so the question of responsibility in such cases is not academic. Some forms of psychotherapy, especially those involving drugs, also apparently fall under this rubric. Again it should be noted that the introduction of an evil manipulator in the examples is superfluous. If I am led to believe that Mao is God by a brain hemorrhage, or by eating tainted meat, or by being inadvertently hypnotized by the monotony of the railroad tracks, the same puzzling situation prevails.

Philosophers have recognized that something strange is going on in these cases, and have been rightly reluctant to grant that such descriptions as I have just given are fully coherent. Thus Melden says,

> If by introducing an electrode into the brain of a person, I succeed in getting him to believe that he is Napoleon, that surely is not a rational belief that he has, nor is he responsible for what he does in consequence of this belief, however convinced he may be that he is fully justified in acting as he does.[^15]

Why, though, is the man not responsible? Not because of the absurdity of the belief, for if a merely negligent evidence-gatherer came to believe some absurdity, his consequent action would not be excused, and if the electrode-induced belief happened to be true but just previously unrecognized by the man, it seems we would still deny him responsibility. (I do not think this is obvious. Suppose a benevolent neurosurgeon implants the belief that honesty is the best policy in the heads of some hardened criminals; do we, on grounds of non-rational implantation, deny these people status in the society as responsible agents?) The non-rationality, it seems, is not to be ascribed to the _content_ of the belief, but somehow to the manner in which it is believed or acquired. We do, of course, absolve the insane, for they are _in general_ irrational, but in this case we cannot resort to this precedent for the man has, _ex hypothesi,_ only one non-rational belief. Something strange indeed is afoot here, for as was mentioned before, the introduction of the evil manipulator adds nothing to the example, and if we allow that the presence of one non-rationally induced belief absolves from responsibility, and if the absurdity or plausibility of a belief is independent of whether it has been rationally acquired or not, it seems we can never be sure whether a man is responsible for his actions, for it just may be that one of the beliefs (true or false) that is operative in a situation has been produced by non-rational accident, in which case the man would be ineligible for praise or blame. Can it be that there is a tacit assumption that no such accidents have occurred in those cases where we hold men responsible? This line is unattractive, for suppose it were _proved_ in a particular case that Smith had been led to some deed by a long and intricate argument, impeccably formulated by him, with the exception of one joker, a solitary premise non-rationally induced. Our tacit assumption would be shown false; would we deny him responsibility?

A bolder skepticism toward such examples has been defended by MacIntyre: “If I am right the concept of causing people to change their beliefs or to make moral choices, by brain-washing or drugs, for example, is not a possible concept.”[^16] Hampshire, while prepared to countenance causing beliefs in others, finds a conceptual difficulty in the reflexive case: “I must regard my own beliefs as formed in response to free inquiry; I could not otherwise count them as beliefs.”[^17] Flew vehemently attacks MacIntyre’s proposal:

> If it did hold it would presumably rule out as logically impossible all indoctrination by such non-rational techniques. The account of Pavlovian conditionings in Aldous Huxley’s _Brave New World_ would be not a nightmare fantasy but contradictory nonsense. Again if this consequence did hold, one of the criteria for the use of the term _belief_ would have to be essentially backward-looking. Yet this is surely not the case. The actual criteria are concerned with the present and future dispositions of the putative believer; and not at all with how he may have been led, or misled, into his beliefs.[^18]

Flew’s appeal to the reality of brain-washing is misplaced, however, for what is at issue is how the results of brain-washing are to be coherently described, and MacIntyre is right to insist that there is a conceptual incoherency in the suggestion that in brain-washing one causes beliefs, _tout simple_. In _Content and Consciousness_ I have argued that there _is_ an essential backward-looking criterion of belief; here I shall strike a more glancing blow at Flew’s thesis. Suppose for a moment that we put ourselves in the position of a man who wakes up to discover a non-rationally induced belief in his head (he does not know it was non-rationally induced; he merely encounters this new belief in the course of reflection, let us say). What would this be like? We can tell several different stories, and to keep the stories as neutral as possible, let us suppose the belief induced is false, but not wild: the man has been induced to believe that he has an older brother in Cleveland.

In the first story, Tom is at a party and in response to the question, “Are you an only child?” he replies, “I have an older brother in Cleveland.” When he is asked, “What is his name?” Tom is baffled. Perhaps he says something like this: “Wait a minute. Why do I think I have a brother? No name or face or experiences come to mind. Isn’t that strange: _for a moment_ I had this feeling of conviction that I had an older brother in Cleveland, but now that I think back on my childhood, I remember perfectly well I was an only child.” If Tom has come out of his brainwashing still predominantly rational, his induced belief can last only a moment once it is uncovered. For this reason, our earlier example of the impeccable practical reasoning flawed by a lone induced belief is an impossibility.

In the second story, when Tom is asked his brother’s name, he answers, “Sam,” and proceeds to answer a host of other obvious questions, relates incidents from his childhood, and so forth. Not _one_ belief has been induced, but an indefinitely large stock of beliefs, and other beliefs have been wiped out. This is a more stable situation, for it may take a long time before Tom encounters a serious mismatch between this large and interrelated group and his other beliefs. Indeed, the joint, as it were, between this structure of beliefs and his others may be obscured by some selective and hard to detect amnesia, so that Tom never is brought up with any hard-edge contradictions.

In the third story, Tom can answer no questions about his brother in Cleveland, but insists that he believes in him. He refuses to acknowledge that well-attested facts in his background make the existence of such a brother a virtual impossibility. He says bizarre things like, “I know I am an only child and have an older brother living in Cleveland.” Other variations in the story might be interesting, but I think we have touched the important points on the spectrum with these three stories. In each story, the question of Tom’s responsibility can be settled in an intuitively satisfactory way by the invocation of familiar principles. In the first case, while it would be _hubris_ to deny that a neurosurgeon might some day be able to set up Tom in this strange fashion, if he could do it without disturbing Tom’s prevailing rationality the effect of the surgery on Tom’s beliefs would be evanescent. And since we impose a general and flexible obligation on any rational man to inspect his relevant beliefs before undertaking important action, we would hold Tom responsible for any rash deed he committed while under the temporary misapprehension induced in him. Now if it turned out to be physically impossible to insert a single belief without destroying a large measure of Tom’s rationality, as in the third story, we would not hold Tom responsible, on the grounds of insanity—his rationality would have been so seriously impaired as to render him invulnerable to rational communication. In the second story, determining responsibility must wait on answers to several questions. Has Tom’s rationality been seriously impaired? If not, we must ask the further question: did he make a reasonable effort to examine the beliefs on which he acted? If the extent of his brainwashing is so great, if the fabric of falsehoods is so broad and well-knit, that a reasonable man taking normal pains could not be expected to uncover the fraud, then Tom is excused. Otherwise not.

With this in mind we can reconsider the case of the hardened criminals surgically rehabilitated. Are they responsible citizens now, or zombies? If the surgeon has worked so delicately that their rationality is not impaired (perhaps improved!), they are, or can become, responsible. In such a case the surgeon will not so much have implanted a belief as implanted a suggestion and removed barriers of prejudice so that the suggestion _will be_ believed, given the right sort of evidential support. If on the other hand the patients become rigidly obsessive about honesty, while we may feel safe allowing them to run loose in the streets, we will have to admit that they are less than persons, less than responsible agents. A bias in favor of true beliefs can be detected here: since it is hard to bring an evidential challenge to bear against a true belief (for lack of challenging evidence—unless we fabricate or misrepresent), the flexibility, or more specifically rationality, of the man whose beliefs all seem to be true is hard to establish. And so, if the rationality of the hardened criminals’ new belief in honesty is doubted, it can be established, if at all, only by deliberately trying to shake the belief!

The issue between Flew and MacIntyre can be resolved, then, by noting that one cannot directly and simply cause or implant a belief, for a belief is essentially something that has been _endorsed_ (by commission or omission) by the agent on the basis of its conformity with the rest of his beliefs. One may well be able to produce a zombie, either surgically or by brainwashing, and one might even be able to induce a large network of false beliefs in a man, but if so, their persistence _as beliefs_ will depend, not on the strength of any sutures, but on their capacity to win contests against conflicting claims in evidential showdowns. A parallel point can be made about desires and intentions. Whatever might be induced in me is either fixed and obsessive, in which case I am not responsible for where it leads me, or else, in MacIntyre’s phrase, “can be influenced or inhibited by the adducing of some logically relevant consideration,” in which case I am responsible for _maintaining_ it.

## VII

I believe the case is now complete against those who suppose there to be an unavoidable antagonism between the intentional and the mechanistic stance. The intentional stance toward human beings, which is a precondition of any ascriptions of responsibility, _may_ coexist with mechanistic explanations of their motions. The other side of this coin, however, is that we _can_ in principle adopt a mechanistic stance toward human bodies and their motions, so there remains an important question to be answered. _Might_ we abandon the intentional stance altogether (thereby of necessity turning our backs on the conceptual field of morality, agents, and responsibility) in favor of a purely mechanistic world view, or is this an alternative that can be ruled out on logical or conceptual grounds? This question has been approached in a number of different ways in the literature, but there is near unanimity about the general shape of the answer: for Strawson the question is whether considerations (of determinism, mechanism, etc.) could lead us to look on everyone exclusively in the “objective” way, abandoning the “participant” attitude altogether. His decision is that this could not transpire, and he compares the commitment to the participant attitude to our commitment to induction, which is “original, natural, non-rational (not _ir_rational), in no way something we choose or could give up.”[^19] Hampshire puts the point in terms of the mutual dependence of “two kinds of knowledge,” roughly, inductive knowledge and knowledge of one’s intentions. “Knowledge of the natural order derived from observation is inconceivable without a decision to test this knowledge, even if there is only the test that constitutes a change of point of view in observation of external objects.”[^20] In other words, one cannot _have_ a world view of any sort without having beliefs, and one could not have beliefs without having intentions, and having intentions requires that one view _oneself_, at least, intentionally, as a rational agent. Sellars makes much the same point in arguing that “the scientific image cannot replace the manifest without rejecting its own foundation.”[^21] Malcolm says, “The motto of the mechanist ought to be: One cannot speak, therefore one must be silent.[^22] But here Malcolm has dropped the ball on the goal line; how is the mechanist to _follow_ his “motto,” and how _endorse_ the “therefore”? The doctrine that emerges from all these writers is that you can’t get there from here, that to assert that the intentional is eliminable “is to imply pragmatically that there is at least one person, namely the one being addressed, if only oneself, with regard to whom the objective attitude cannot be the only kind of attitude that is appropriate to adopt.”[^23] Recommissioning Neurath’s ship of knowledge, we can say that the consensus is that there is at least one plank in it that cannot be replaced.

Caution is advisable whenever one claims to have proved that something cannot happen. It is important to see what does not follow from the consensus above. It does not follow, though Malcolm thinks it does,[^24] that there are some things in the world, namely human beings, of which mechanism as an embracing theory cannot be true, for there is no incompatibility between mechanistic and intentional explanation. Nor does it follow that we will always characterize some things intentionally, for we may all be turned into zombies next week, or in some other way the human race may be incapacitated for communication and rationality. All that is the case is that we, _as persons_, cannot _adopt_ exclusive mechanism (by eliminating the intentional stance altogether). A corollary to this which has been much discussed in the literature recently is that we, as persons, are curiously immune to certain sorts of predictions. If I cannot help but have a picture of myself as an intentional system, I am bound, as MacKay has pointed out, to have an _underspecified_ description of myself, “not in the sense of leaving any parts unaccounted for, but in the sense of being compatible with more than one state of the parts.”[^25] This is because no information system can carry a complete true representation of itself (whether this representation is in terms of the physical stance or any other). And so I cannot even in principle have all the data from which to predict (from any stance) my own future.[^26] Another person might in principle have the data to make all such predictions, but he could not tell them all to me without of necessity falsifying the antecedents on which the prediction depends by interacting with the system whose future he is predicting, so I can never be put in the position of being obliged to believe them. As an intentional system I have an epistemic horizon that keeps my own future as an intentional system indeterminate. Again, a word of caution: this barrier to prediction is not one we are going to run into in our daily affairs; it is not a barrier preventing or rendering incoherent predictions I might make about my own future decisions, as Pears for one has pointed out.[^27] It is just that since I must view myself as a person, a full-fledged intentional system, there is no complete biography of my future I would be right to accept.

All this says nothing about the impossibility of dire depersonalization in the future. Wholesale abandonment of the intentional is in any case a less pressing concern than partial erosion of the intentional domain, an eventuality against which there are no conceptual guarantees at all. If the growing area of success in mechanistic explanation of human behavior does not in itself rob us of responsibility, it does make it more pragmatic, more effective or efficient, for people on occasion to adopt less than the intentional stance toward others. Until fairly recently the only well-known generally effective method of getting people to do what you wanted them to was to treat them as persons. One might threaten, torture, trick, misinform, bribe them, but at least these were forms of control and coercion that appealed to or exploited man’s rationality. One did not attempt to adopt the design stance or the physical stance, just because it was so unlikely that one could expect useful behavioral results. The advent of brainwashing, subliminal advertising, hypnotism and even psychotherapy (all invoking variations on the design stance), and the more direct physical tampering with drugs and surgical intervention, for the first time make the choice of stance a genuine one. In this area many of the moral issues are easily settled; what dilemmas remain can be grouped, as MacKay has observed, under the heading of treating a person as less than a person _for his own good_. What if mass hypnosis could make people stop wanting to smoke? What if it could make them give up killing? What if a lobotomy will make an anguished man content? I argued earlier that in most instances we must ask for much more precise descriptions of the changes wrought, if we are to determine whether the caused change has impaired rationality and hence destroyed responsibility. But this leaves other questions still unanswered.

## Notes

*]] J. Bennett, in _Rationality_ (London: Routledge & Kegan Paul, 1964), offers an extended argument to the effect that communication and rationality are essentially linked, but his argument is vitiated, I believe, by its reliance on an artificially restrictive sense of rationality—a point it would take too long to argue here. See Chapter 1 for arguments for a more generous notion of rationality.

*]] In practice we predict lapses at the intentional level (“You watch! He’ll forget all about your knight after you move the queen”) on the basis of loose-jointed inductive hypotheses about individual or widespread human frailties. These hypotheses are expressed in intentional terms, but if they were given rigorous support, they would in the process be recast as predictions from the design or physical stance.

1.]] J. Hospers, “What Means This Freedom?” in S. Hook, ed., _Determinism and Freedom in the Age of Modern Science_ (New York: Collier, 1958): 133.

2.]] N. Malcolm, “The Conceivability of Mechanism,” _Philosophical Review_, LXXVII (1968): 51.

3.]] A. I. Melden, _Free Action_ (London: Routledge & Kegan Paul, 1961); D. Davidson, “Actions, Reasons and Causes,” _Journal of Philosophy_, LX (1963): 685–700.

4.]] D. M. MacKay, “The Use of Behavioral Language to Refer to Mechanical Pro­cesses,” _British Journal of Philosophical Science_, XIII (1962): 89–103. See also H. Putnam, “Robots: Machines or Artificially Created Life?,” read at A.P.A. Eastern Division Meeting, 1964, subsequently published in S. Hampshire, ed., _Philosophy of Mind_ (New York: Harper & Row, 1966): 91.

5.]] P. F. Strawson, “Freedom and Resentment,” _Proceedings of the British Academy_ (1962), reprinted in Strawson, ed., _Studies in the Philosophy of Thought and Action_ (Oxford University Press, 1968): 79.

6.]] MacKay, _op. cit_.: 102.

7.]] W. Sellars, “Fatalism and Determinism,” in K. Lehrer, ed., _Freedom and Determinism_ (New York: Random House, 1966): p. 145. A. Flew, “A Rational Animal,” in J. R. Smythies, ed., _Brain and Mind_ (London: Routledge & Kegan Paul, 1968): 111–35, and A. Rorty, “Slaves and Machines,” _Analysis_ (April 1962): 118–20, develop similar distinctions.

8.]] Strawson, _op. cit_., p. 80.

9.]] H. P. Grice, “Meaning,” _Philosophical Review_, LXVI (1957); “Utterer’s Meaning and Intentions,” _Philosophical Review_, LXVI (1969).

10.]] Cf. L. W. Beck, “Agent, Actor, Spectator, and Critic,” _Monist_ (1965): 175–79.

11.]] A. C. MacIntyre, “Determinism,” _Mind_, MMXMLVII: 248ff.

12.]] D. Wooldridge, _The Machinery of the Brain_ (New York: McGraw-Hill, 1963): p. 82.

13.]] G. E. M. Anscombe, _Intention_ (Oxford: Blackwell, 1957): p. 24.

14.]] Cf. D. M. MacKay, “Comments on Flew,” in Smythies, _op. cit_., p. 130.

15.]] Melden, _op. cit_., p. 214.

16.]] Quoted by Flew, _op. cit_., p. 118.

17.]] S. Hampshire, _Freedom of the Individual_ (New York: Harper & Row, 1965), p. 87.

18.]] Flew, _op. cit_., p. 120.

19.]] Strawson, _op. cit_., p. 94.

20.]] This is of course an echo of Strawson’s examination of the conditions of knowledge in a “no-space world” in _Individuals_ (London: Methuen, 1959).

21.]] W. Sellars, _Science, Perception and Reality_ (London: Routledge & Kegan Paul, 1963): p. 21.

22.]] Malcolm, _op. cit_., p. 71.

23.]] J. E. Llewelyn, “The Inconceivability of Pessimistic Determinism,” _Analysis_ (1966): pp. 39–44. Having cited all these authorities, I must acknowledge my own failure to see this point in _Content and Consciousness_, p. 190. This is correctly pointed out by R. L. Franklin in his review in _Australasian Journal of Philosophy_ (September 1970).

24.]] Malcolm, _op. cit_., p. 71.

25.]] D. M. MacKay, “On the Logical Indeterminacy of a Free Choice,” _Mind_ (1960): 31–40; “The Use of Behavioral Language to Refer to Mechanical Processes,” _loc. cit._; “The Bankruptcy of Determinism,” unpublished, read June 1969, at University of California at Santa Barbara.

26.]] Cf. K. Popper, “Indeterminism in Quantum Physics and Classical Physics,” _British Journal of the Philosophy of Science_ (1950).

27.]] D. F. Pears, “Pretending and Deciding,” _Proceedings of the British Academy_ (1964), reprinted in Strawson, ed., _Studies in the Philosophy of Thought and Action_, _loc. cit._, pp. 97–133.

The Abilities of Men and Machines

Mechanism as a theory of mind would be refuted if it could be shown that a human being (or his mind)*]] _can do_ what no machine _can do_, and there is a family of arguments invoking Gödel’s Theorem which purport to prove just that.[^1] I wish to show that all these arguments must fail because at one point or another they must implicitly deny an obvious truth, namely that the constraints of logic exert their force not on the things in the world directly, but rather on what we are to count as defensible descriptions or interpretations of things. The common skeleton of the anti-mechanistic arguments is this: any computing machine at all can be represented as some Turing machine,†]] but a man cannot, for suppose Jones over there were a realization of some Turing machine TMj, then (by Gödel) there would be something _A_ that Jones _could not do_ (namely, prove TMj’s Gödel sentence). But watch!—this is the crucial empirical part of the argument—Jones _can_ do _A_; therefore Jones is not a realization of TMj, and since _it can be seen_ that this will be true whatever Turing machine we choose, Jones transcends, angel-like, the limits of mechanism. The error in this lies, I will argue, in supposing that the determination of Jones’s acts and hence his abilities, and also the determination of the activities of a computing device, can proceed in a neutral way that will not beg the question of the applicability of Gödel’s Theorem.

Gödel’s Theorem says that in any consistent axiom system rich enough to generate the arithmetic of the natural numbers, there are statements we cannot prove in the system, but which can be seen by other means to be true. Gödel’s Theorem, then, is about axiom systems, and the “machines” it governs are as abstract as the axiom systems. A Turing machine can be viewed as nothing more than a finite system of instructions to perform simple operations on strings of symbols which constitute the “input.” The instructions are gathered into “machine states,” each of which is a finite sequence of instructions, and a master instruction, or state-switching function, which prescribes which sequence of instructions is to be followed given the input. Such a specification is obviously entirely neutral about how such operating and switching is to be accomplished, and hence a particular Turing machine can be “realized” in very different ways: by a mechanical tape-reading device, by simulation on a digital computer, or by “hand simulation,” where the operations are performed by a person or persons following written instructions on “state” cards.

The engineer setting out to construct directly a mechanical realization of a Turing machine has the following task: he must exploit the laws of nature in such a way as to achieve the regularities prescribed or presupposed in the Turing machine specification. Thus, for each of the symbols in the alphabet in which the input and output are to be expressed, the engineer must devise some physical feature that can be reliably distinguished mechanically from its brethren (like a hole in a paper tape and unlike a pencil mark or spoken word). These features should also be relatively stable, quickly producible and small scale—for obvious reasons of engineering economy. Paired with the symbol features must be the devices, whatever they are, that react differently to the symbol features, that “read” or “discriminate” them. Then, whatever these different reactions are, they must in turn be so designed to differ from one another that the next rank of effectors, whatever they are, can be caused to react differently to them, and so forth. For each shift of _state_ there must be a corresponding _physical_ shift, which might be a shift of gears, or the sliding of a different cam onto a drive shaft, or the opening and closing of series of electrical or hydraulic relays. The whole machine may exploit only a few simple principles of electronics or it may be a Rube Goldberg contraption, but in either case it must be more or less insulated from the rest of the environment, so that coincidental features of the outside world do not interfere with its operation, e.g., changes in temperature or relative humidity, or sudden accelerations. The better the design, the more immune to interference the machine will be. But what counts as interference, and what counts as a physical change “read” as input by the machine is relative to the designer’s choice of physical laws. A hole in the tape may be a symbol to one machine, a major disruptive event to another (depending usually on whether we are speaking of paper tape or magnetic recording tape). Similarly, what internal changes in the machine are to count as state changes, and what are to count as breakdowns is also relative to the designer’s scheme of realization. If we discover a machine that is drastically affected by accelerations, it may be a computer poorly designed for mobile applications, e.g., in airplanes, but on the other hand it may be an inertial guidance system, and the accelerations may be its input.

Since the choices engineers actually make when designing hardware are a fairly standard and well-known lot, and since the purposes of machines are usually either obvious or suitably announced by the manufacturer, it is easy to overlook this relativity to the designer’s choices and suppose that we can directly observe the input, output, operations and state changes of any device, and hence can settle in an objective fashion which Turing machine, if any, it is. In principle, however, we cannot do this. Suppose Jones and Smith come across a particular bit of machinery churning away on a paper tape. They both study the machine, they each compile a history of its activity, they take it apart and put it back together again, and arrive finally at their pronouncements. What sorts of disagreements might there be between Jones and Smith?

First we might find them disagreeing only on the interpretation of the input and output symbols, and hence on the purpose or function of the Turing machine, so that, for instance, Jones treats the symbol-features as numbers (base two or base ten or what have you) and then “discovers” that he can characterize the Turing machine as determining the prime factors of the input numbers, while Smith interprets the symbol features as the terms and operators of some language, and has the Turing machine proving theorems using the input to generate candidates for proof sequences. This would not be a disagreement over which Turing machine had been realized, for this is purely semantic disagreement; a Turing machine specification is in terms of syntactic relationships and functions only, and _ex hypothesi_ Jones and Smith agree on which features are symbols and on the rules governing the production of the output strings. In principle a particular Turing machine could thus serve many purposes, depending on how its users chose to interpret the symbols.

More interesting and radical disagreements are also possible however. Jones may announce that his device is TMj, that its input and output are expressions of binary arithmetic, and that its function is to extract square roots. However, let us suppose, he proves mathematically (that is, on the basis of the machine table he assigns it and not the details of engineering) that the program is faulty, giving good answers for inputs less than a hundred but failing periodically for larger numbers. He adds that the engineering is not all that sound either, since if you tip the machine on its side the tape reader often misreads the punched holes. Smith disagrees. He says the thing is TMs, designed to detect certain sorts of symmetries in the input sequences of holes, and whose output can be read (in a variation of Morse Code) as a finite vocabulary of English words describing these symmetries. He goes on to say that tipping the machine on its side amounts to a shift in input, to which the machine responds quite properly by adjusting its state-switching function. The only defect he sees is that there is one cog in the works that is supposed to be bent at right angles and is not; this causes the machine to miscompute in certain states, with the result that certain symmetries are misdescribed. Here there is disagreement not only about the purpose of the machine, or the semantics of the language it uses, but also about the syntax and alphabet. There is no one-to-one correspondence between their enumerations of symbols or instructions. The two may still agree on the nature of the mechanism, however, although they disagree on what in the mechanism counts as deliberate design and what is sloppiness. That is, given a description of the physical state of the machine and the environment, and a physical description of the tape to be fed in, they will give the same prediction of its subsequent _motions_, but they will disagree on which features of this biography are to be called malfunctions, and on which parts of the machine’s emissions count as symbols. Other sorts of disagreement over interpretation are possible in principle. For instance, one can treat any feature of the environment as input, even in the absence of any salient and regular reaction to it by any part of the machine, if one is prepared to impute enough stupidity to the designer. There is no clear boundary between entities that count as imperfect or broken or poorly designed realizations of TMx and entities that are not at all realizations of TMx. By the same token, discovering that an entity can be viewed as a highly reliable, well-designed TMa does not preclude its also being viewed as an equally good realization of some other TMb. To give a trivial example, almost any good computer could be construed as a Turing machine yielding as output “_p_” if and only if it receives as input “_q_,” where our symbol “_p_” is realized by a very faint hum, and “_q_” by turning on the power switch.

Faced with the competing interpretations of the tape reader offered by Jones and Smith, if we decide that one interpretation, say Jones’s, is more plausible all things considered, it will only be because these things we consider include our intuitions and assumptions about the likely intentions and beliefs of the designer of the machine. The quick way to settle the dispute, then, is to ask the designer what his intentions were. Of course he may lie. So it seems we may never find out for sure; only the designer knows for sure, but what is it he knows that we do not? Only what his intentions were, what Turing machine he intended to realize—and he may even discover that his own intentions were confused. In any case, what the designer’s intentions were in his heart of hearts does not determine any objective fact about the device before us. If Smith purchases it on the spot and proceeds to use it as a symmetry classifier, then what he has is just as truly a symmetry classifier as any he could build on his own, under his own intentions, inspired by this prototype. If we find something on the beach and can figure out how to use it as a TMb, then it is a TMb in the fullest possible sense.

Now how does this affect the possibility of there being _living_ Turing machines? It is not at all far-fetched and is even quite the vogue to suppose that an animal can profitably be viewed as a computer or finite automaton of a special sort, and since any finite automaton can be simulated by a Turing machine, this amounts in a fashion to the supposition that we might want to treat an animal as a Turing machine. (There are difficulties in this, but let us concede to the anti-mechanists the shortest visible route to their goal; it will only hasten their demise.) The question then is, can we settle whether or not we have chosen the _correct_ Turing machine interpretation of a particular animal? First we have to decide which of the impingements on the animal count as input and which as interference, and it is not at all clear what criteria we should use in deciding this. Suppose we ask ourselves if changes in barometric pressure constitute input or interference for a particular animal. Probably in most animals we will find detectable, salient physical changes associated with changes in barometric pressure, but suggestive as this might seem, what would it show? Suppose we learn that the effect of such pressure changes in cows is to make them all lie down in the field. So what? What advantage, one wants to ask, accrues to the cows from this reaction? Our search for a plausible Turing machine specification is guided here, as it was for the paper tape device, by the assumption that a Turing machine always has some point, some purpose. From a strictly mathematical point of view this assumption is unwarranted; a Turing machine may compute a function of no interest, elegance or utility whatever, of no value to anyone, and still meet the formal requirements for a Turing machine. Of course we would not be interested in the notion of a Turing machine at all were it not the case that we can isolate and study those that can be used to serve interesting purposes. The application of the concept to animals will be fruitful just so long as it leads us to mechanizations of apparently purposeful activity observed in the animal. Thus in some animals changes in barometric pressure can be highly _significant,_ and may be responded to in some _appropriate_ way by the animal—by finding shelter before the impending storm, for instance—and in these cases we will have reason to treat the effects on the animals as the receipt of information, and this will set us searching for an information-processing model of this capacity in the animal, or in other words (to take the short route again) to view the animal as a Turing machine for which barometric pressure is input.

At another level in our examination of the living candidate for machinehood, we would have to decide which features of the animal’s physical constitution are working as they were designed to, as they were supposed to, and which are malfunctioning, misdesigned or merely fortuitous. The “response” of mice to the “stimulus” of being dropped in molten lead is no doubt highly uniform, and no doubt we can give sufficient physiological conditions for this uniformity of reaction, but “burning to a crisp” does not describe a sort of behavior to which mice are prone; they are not _designed_ or _misdesigned_ to behave this way when so stimulated.*]] This does not mean that we cannot treat a mouse as an analogue of the one-state, two-symbol humming Turing machine described above. Of course we can; there just is no point. In one sense an animal—in fact any fairly complicated object—can be a number of different Turing machines at once, depending on our choice of input, state descriptions, and so forth. No one of these can be singled out on purely structural or mechanical grounds as _the_ Turing machine interpretation of the animal. If we want to give sense to that task, we must raise considerations of purpose and design, and then no objectively confirmable answer will be forthcoming, for if Smith and Jones disagree about the ultimate purpose of particular structures or activities of the animal, there is no Designer to interview, no blueprint to consult.

Similar considerations apply _pari passu_ when we ask if a man is a Turing machine. As a complicated chunk of the world he will surely qualify for any number of Turing machine characterizations, and it is even possible that by some marvelous coincidence some of these will match Turing machine interpretations of interest to mathematicians. Thus the wandering patches of light on a baby’s retina, and the subsequent babble and arm-waving, might be given a systematic interpretation as input and output so that the arm-wavings turn out to be proofs of theorems of some non-Euclidean geometry, for example. It is important to recognize that it is solely non-mathematical assumptions that make this suggestion outlandish; it is only because of what we believe about the lack of _understanding_ in babies, the _meaninglessness_ of their babble, the _purposes_, if you will, for which babies are intended, that we would discard such an interpretation were some mathematician clever enough to devise it. By suitable gerrymandering (e.g., incessant shifting of input vocabulary) it ought to be possible to interpret any man as any Turing machine—indeed as all Turing machines at the same time. So construed, every infant and moron would be engaged (among its other activities) in proving theorems and Gödel sentences (any Gödel sentence you choose), but of course the motions that constituted these feats of proving would not _look_ like feats of proving, but like sleeping, eating, talking about the weather. The antimechanist is not interested in Turing machine interpretations of this sort; the activities and abilities he supposes he has crucial information about are those of mature, sane mathematicians in their professional endeavors.

He is interested in those motions of a man the purpose or interpretation of which is natural and manifest—his _actions_ in short—but once we turn to the question of which Turing machine interpretation fits these actions, and hence might deserve to be called _the_ description of the man, we come up against the relativities encountered by Smith and Jones: the ultimate function and design of every part of a man is not in the end to be decided by any objective test. Moreover, since _the_ Turing machine interpretation of a man (if there is one) is picked out as the one best capturing the biological design of a man, and since man the biological entity has more ulterior goals than mere theorem-proving, no plausible candidate for _the_ Turing machine interpretation of any man will be of the right sort to give the anti-mechanist the premise he needs. In addition to whatever computing a man may do (in school, in business, for fun) he also eats, acquires shelter, makes friends, protects himself and so forth; we do not need Gödel to demonstrate that man is not just a computer in _this_ sense—that is, a device whose sole purpose is to compute functions or prove theorems. Suppose, to illustrate this, we have a particular hardware TMk churning out theorems in some system, and a mathematician, Brown, sitting next to the computer churning out the same theorems in the same order. If we tentatively adopt the hypothesis that we have two realizations of TMk, then we can go on to apply the Gödelian limitations to them both, but we have an easy way of disproving the hypothesis with respect to Brown. Once we have fixed, for our hypothesis, the list of Brown-motions that count as the issuing of output symbols (and these will appear to be in one-to-one correspondence with some symbol-printing motions of the hardware model), we merely ask Brown to pause in his calculations for a moment and give forth with a few of these symbols “out of order.” His doing this is enough to establish _not_ that Brown is not a machine, but that Brown is not (just) a (good) TMk. Brown is not a TMk because here we see output symbols being emitted contrary to the hypothesized instructions for TMk, so either our request broke him (is a mathematician to be viewed as a sequence of exquisitely fragile self-repairing mechanical theorem-provers?) or he was not a TMk in the first place.

The fact that Brown was producing the same proofs as the hardware TMk does not imply that if Brown is a mechanism he is a hardware TMk, for producing the same proofs is not a sufficient condition for being, in this sense, the same Turing machine. Perhaps we were fooled into thinking Brown was a TMk because, for a while, he had “hand simulated ” a TMk. Simulating a TMk is in one sense being a TMk (a simulation is a realization), but of course it is not the sense the antimechanist needs, for in the sense in which simulating is being, the anti-mechanist claim is just false: it is not true that if a man were a TMk he could not prove _S_, where _S_ is TMk’s Gödel sentence. If a man’s being a TMk is a matter of simulating a TMk then in all likelihood he can prove _S_; all he has to do is cease for a while following the instructions that amount to simulating a TMk, and Gödel says nothing about this role-changing being impossible. What the man _cannot do_ (and this regardless of whether he is a machine, organism or angel) is prove _S_ while following the instructions of TMk, but this is no more a limitation on his powers than is his inability to square the circle.

Gödel’s Theorem has its application to machines via the notion of a Turing machine specification, but Turing machine specifications say very little about the machines they specify. Characterizing something as a TMk ascribes certain capacities to it, and puts certain limitations on these capacities, but says nothing about other features or capacities of the thing. From the fact that something _A_ is a realization of a TMk we cannot deduce that _A_ is made of steel or has rubber tires, nor can we deduce that it cannot fly, for although the specification of a TMk does not stipulate that it can fly, it does not and cannot rule out this possibility. We also cannot deduce that _A_ cannot speak English. Perhaps _A_ can, and perhaps while speaking English, _A_ may issue forth with a proof of TMk’s Gödel sentence. _A_ could not do this if _A_ were _just_ a TMk, but that is precisely the point: nothing concrete could be _just_ a particular Turing machine, and any concrete realization of any Turing machine can in principle have capacities under one interpretation denied it under another.

The fundamental error behind attempts to apply Gödel’s Theorem to philosophy of mind is supposing that objective and exclusive determinations of the activities and capacities of concrete objects are possible which would determine uniquely which Turing machine specification (if any) is _the_ specification for the object. Once we acknowledge this error, this apparent application of Gödel’s Theorem to the philosophy of mind reveals its vacuity: if a man were (a realization of) a particular theorem-proving Turing machine with Gödel sentence _S_, then _in his role as that Turing machine_ he could not prove _S_, but this says nothing about his capacities in other roles on the one hand, and on the other we surely have no evidence—and could have no evidence—that a man while playing the role of a Turing machine can do what Gödel says he cannot.

## _Postscript_, _1978_

We can put the point of this paper in a form that should be mildly astonishing to the anti-mechanists who hope to use Gödel: a realization of the _Universal Turing machine_ can, in principle, do the one thing Gödel’s theorem says the Universal Turing machine cannot do: prove the Gödel sentence of the Universal Turing machine. How could this be? A thought experiment will explain.

Children can be taught to hand simulate simple Turing machines. There are primers and textbook chapters designed to do just that. Suppose a graduate student in Artificial Intelligence took as his dissertation task, as his “toy problem,” writing a program that could learn to “hand simulate” a Turing machine just as children do—by reading the instruction book, doing the practice exercises, etc. Call this imaginary program _WUNDERKIND._ The rules of this project are that _WUNDERKIND_, after debugging but before being given its “lessons,” should be _unable_ to hand simulate a Turing machine, but after being fed exactly the same instruction (in English) as the school children—no more, no less—should be able to hand simulate a Turing machine as well as, or even better than, the school children. (This proviso has nothing to do with the applicability of Gödel’s theorem to the case; it is added to give the project a non-trivial task, and hence to give _WUNDERKIND_ something like a human set of abilities and interests.)

Imagine that _WUNDERKIND_ was designed, and that it worked—it “learned” to hand simulate a Turing machine, TMk. Suppose we witness a demonstration of the program, actual hardware producing actual symbols. Now just what is this hardware object not supposed to be able to do? Prove “its” Gödel sentence. But which sentence is that? Which Turing machine is in front of us? If the hardware is a standard, commercial, programmable computer, it is—given enough time and storage—a realization of the Universal Turing machine, which has a Gödel number and a Gödel sentence, call it _SU._ But it is currently running the _WUNDERKIND_ program (let’s ignore the fact that probably today it is time-sharing, and running many bits of many programs in quick succession), which is (mathematically equivalent to) a Turing machine with a different Gödel number and a different Gödel sentence, call it _SW._ Then there is the Turing machine, TMk, that _WUNDERKIND_ is hand simulating, and it too has a Gödel sentence, call it _Sk._ Now we know that _WUNDERKIND_, while hand simulating TMk, cannot _as part of that hand simulation_ produce _Sk._ But _WUNDERKIND_ does other things as well; it asks questions, reads, practices, corrects errors in its exercises, and who knows what else. Perhaps it plays chess or writes fairy tales. What it does while not simulating TMk is an independent matter. It might well, for all we know, start offering proofs of sentences, and one of them might be _Sk._ Another might be _SU!_ Or _SW! There is nothing in Gödel’s theorem to prevent this_. The computer, the actual hardware device, is a realization, let us grant, of the Universal Turing machine, and _in that guise_ it cannot offer a proof of _SU_. In that guise what it is doing is _imitating TMW—WUNDERKIND_. When we shift perspective and view the object before us as _WUNDERKIND (WUNDERKIND_ the child-simulation, not _WUNDERKIND_ the algorithm represented by the machine table) we see it not as a theorem-prover-in-the-vocabulary-of-the-Universal-Turing-machine, but as a rather childlike converser in English, who asks questions, says things true and false on a variety of topics, and in that guise might well come up with a string of symbols that constituted a proof of _SU_ or any other sentence.

The idea that _WUNDERKIND_ itself can be viewed from more than one perspective will benefit from further specification. Consider what is wrong with the following argument: all computer programs are algorithms; there is no feasible algorithm for checkmate in chess; therefore checkmate by computer is impossible. The first premise is true, and so is the second. Chess is a finite game, so there is a brute force algorithm that gives the best line of play by simply enumerating all possible games in a tree structure and then working back from the myriad last moves to the line or lines of play that guarantee checkmate or draw for white or black, but this algorithm is impractical to say the least. But of course chess programs are not that bad at achieving checkmate, so the conclusion is false. What is true is that good chess programs are not algorithms for checkmate, but rather just algorithms for playing legal chess. Some are better than others, which means that some terminate in checkmate more often against strong players than others, but they are not _guaranteed_ to end in checkmate, or even in a draw. In addition to the rather unilluminating perspective from which such a program can be viewed as a mere algorithm, there is the perspective from which it can be viewed as _heuristic—_taking chances, jumping to conclusions, deciding to ignore possibilities, searching for solutions to problems. If you want to design a good chess algorithm, you must look at the task from this perspective. Similarly, _WUNDERKIND_ can be viewed as a mere algorithm for taking symbols as input and issuing symbols as output. _That_ it is guaranteed to do, but from the other perspective it is an English-understanding learner of hand simulation, who follows hunches, decides on semantic interpretations of the sentences it reads, ignores possible lines of interpretation or activity, and so on. From the fact that something can be viewed as proceeding heuristically, it does not follow that it is heuristic “all the way down”—whatever that might mean. And similarly, limitations (Gödelian and other) on what can be done by algorithms are not limitations on what can be done—without guarantee but with a high degree of reliability—heuristically by an algorithm.

This is no _refutation_ of anti-mechanism, no _proof_ that a human being and a computer are in the relevant respects alike—for of course it all depends on my asking you to imagine the success of _WUNDERKIND._ Perhaps _WUNDERKIND_ is “impossible”—though it looks modest enough, by current AI standards. If it is “impossible,” this is something Gödel’s theorem is powerless to show. That is, if all the versions of _WUNDERKIND_ that are _clearly_ possible fall far short of our intuitive ideal (by being too narrow, too keyed to the particular wording in the instruction book, etc.) and if no one can seem to devise a satisfactory version, this failure of Artificial Intelligence will be independent of the mathematical limits on algorithms.

## Notes

*]] or _her_ mind. This paper is about human beings, the biological class—not necessarily persons (see Chapter 14)—and certainly not just male human beings. Since I find “his or her” abominable style, I am stuck with either “his” or “her.” I tried rewriting the paper with “her,” but found that this left connotations that would distract or offend.

†]] As Judson Webb points out, this premise can be viewed as a form of Church’s Thesis. (“Metamathematics and the Philosophy of Mind,” _Philosophy of Science_ 1968: 171.) My criticism does not depend on questioning Church’s Thesis.

*]] Cf. Wilfrid Sellars’ remarks on “earthworm-behavioristics” in _Science, Perception, and Reality_ (London: Routledge & Kegan Paul, 1963), pp. 23–4.

1.]] The most widely discussed is J. R. Lucas, “Minds, Machines and Gödel,” _Philosophy_ (1961), reprinted in A. R. Anderson, ed., _Minds and Machines_ (Englewood Cliffs, New Jersey: Prentice-Hall, 1964): 43–59. See also Lucas, _The Freedom of the Will_ (New York: Oxford, 1970), and my review, _Journal of Philosophy_, LXIX (1972): 527–31.

Conditions of Personhood

I am a person, and so are you. That much is beyond doubt. I am a human being, and _probably_ you are too. If you take offense at the “probably” you stand accused of a sort of racism, for what is important about us is not that we are of the same biological species, but that we are both persons, and I have not cast doubt on that. One’s dignity does not depend on one’s parentage even to the extent of having been born of woman or born at all. We normally ignore this and treat humanity as the deciding mark of personhood, no doubt because the terms are locally coextensive or almost coextensive. At this time and place, human beings are the only persons we recognize, and we recognize almost all human beings as persons, but on the one hand we can easily contemplate the existence of biologically very different persons—inhabiting other planets, perhaps—and on the other hand we recognize conditions that exempt human beings from personhood, or at least some very important elements of personhood. For instance, infant human beings, mentally defective human beings, and human beings declared insane by licensed psychiatrists are denied personhood, or at any rate crucial elements of personhood.

One might well hope that such an important concept, applied and denied so confidently, would have clearly formulatable necessary and sufficient conditions for ascription, but if it does, we have not yet discovered them. In the end there may be none to discover. In the end we may come to realize that the concept of a person is incoherent and obsolete. Skinner, for one, has suggested this, but the doctrine has not caught on, no doubt in part because it is difficult or even impossible to conceive of what it would be like if we abandoned the concept of a person. The idea that we might cease to view others and _ourselves_ as persons (if it does not mean merely that we might annihilate ourselves, and hence cease to view anything as anything) is arguably self-contradictory (see Chapter 12). So quite aside from whatever might be right or wrong in Skinner’s grounds for his claim, it is hard to see how it could win out in contest with such an intuitively invulnerable notion. If then the concept of a person is in some way an ineliminable part of our conceptual scheme, it might still be in rather worse shape than we would like. It might turn out, for instance, that the concept of a person is only a free-floating honorific that we are all happy to apply to ourselves, and to others as the spirit moves us, guided by our emotions, aesthetic sensibilities, considerations of policy, and the like—just as those who are _chic_ are all and only those who can get themselves considered _chic_ by others who consider themselves _chic_. Being a person is certainly _something_ like that, and if it were no more, we would have to reconsider if we could the importance with which we now endow the concept.

Supposing there _is_ something more to being a person, the searcher for necessary and sufficient conditions may still have difficulties if there is more than one concept of a person, and there are grounds for suspecting this. Roughly, there seem to be two notions intertwined here, which we may call the moral notion and the metaphysical notion. Locke says that “person”

> is a forensic term, appropriating actions and their merit; and so belongs only to intelligent agents, capable of a law, and happiness, and misery. This personality extends itself beyond present existence to what is past, only by consciousness—whereby it becomes concerned and accountable. (_Essay_, Book II, Chap. XXVII)

Does the metaphysical notion—roughly, the notion of an intelligent, conscious, feeling agent—_coincide_ with the moral notion—roughly, the notion of an agent who is accountable, who has both rights and responsibilities? Or is it merely that being a person in the metaphysical sense is a necessary but not sufficient condition of being a person in the moral sense? Is being an entity to which states of consciousness or self-consciousness are ascribed _the same_ as being an end-in-oneself, or is it merely one precondition? In Rawls’s theory of justice, should the derivation from the original position be viewed as a demonstration of how metaphysical persons _can become_ moral persons, or should it be viewed as a demonstration of why metaphysical persons _must be_ moral persons?*]] In less technical surroundings the distinction stands out as clearly: when we declare a man insane we cease treating him as accountable, and we deny him most rights, but still our interactions with him are virtually indistinguishable from normal personal interactions unless he is very far gone in madness indeed. In one sense of “person,” it seems, we continue to treat and view him as a person. I claimed at the outset that it was indubitable that you and I are persons. I could not plausibly hope—let alone aver—that all readers of this essay will be legally sane and morally accountable. What—if anything—was beyond all doubt may only have been that anything properly addressed by the opening sentence’s personal pronouns, “you” and “I,” was a person in the metaphysical sense. If that was all that was beyond doubt, then the metaphysical notion and the moral notion must be distinct. Still, even if we suppose there are these distinct notions, there seems every reason to believe that metaphysical personhood is a necessary condition of moral personhood.†]]

What I wish to do now is consider six familiar themes, each a claim to identify a necessary condition of personhood, and each, I think, a correct claim on some interpretation. What will be at issue here is first, how (on my interpretation) they are dependent on each other; second, why they are necessary conditions of moral personhood, and third, why it is so hard to say whether they are jointly sufficient conditions for moral personhood. The _first_ and most obvious theme is that persons are _rational beings_. It figures, for example, in the ethical theories of Kant and Rawls, and in the “metaphysical” theories of Aristotle and Hintikka.[^1] The _second_ theme is that persons are beings to which states of consciousness are attributed, or to which psychological or mental or _intentional predicates_, are ascribed. Thus Strawson identifies the concept of a person as “the concept of a type of entity such that _both_ predicates ascribing states of consciousness _and_ predicates ascribing corporeal characteristics” are applicable.*]] The _third_ theme is that whether something counts as a person depends in some way on an _attitude taken_ toward it, a _stance adopted_ with respect to it. This theme suggests that it is not the case that once we have established the objective fact that something is a person, we treat him or her or it in a certain way, but that our treating him or her or it in this certain way is somehow and to some extent constitutive of its being a person. Variations on this theme have been expressed by MacKay, Strawson, Rorty, Putnam, Sellars, Flew, Nagel, Van de Vate, and myself.[^2] The _fourth_ theme is that the object toward which this personal stance is taken must be capable of _reciprocating_ in some way. Very different versions of this are expressed or hinted at by Rawls, MacKay, Strawson, Grice, and others. This reciprocity has sometimes been rather uninformatively expressed by the slogan: to be a person is to treat others as persons, and with this expression has often gone the claim that treating another as a person is treating him morally—perhaps obeying the Golden Rule, but this conflates different sorts of reciprocity. As Nagel says, “extremely hostile behavior toward another is compatible with treating him as a person” (p. 134), and as Van de Vate observes, one of the differences between some forms of manslaughter and murder is that the murderer treats the victim as a person.

The _fifth_ theme is that persons must be capable of _verbal communication_. This condition handily excuses nonhuman animals from full personhood and the attendant moral responsibility, and seems at least implicit in all social contract theories of ethics. It is also a theme that has been stressed or presupposed by many writers in philosophy of mind, including myself, where the moral dimension of personhood has not been at issue. The _sixth_ theme is that persons are distinguishable from other entities by being _conscious_ in some special way: there is a way in which _we_ are conscious in which no other species is conscious. Sometimes this is identified as _self_-consciousness of one sort or another. Three philosophers who claim—in very different ways—that a special sort of consciousness is a precondition of being a moral agent are Anscombe, in _Intention_, Sartre, in _The Transcendence of the Ego_, and Frankfurt, in his recent paper, “Freedom of the Will and the Concept of a Person.”[^3]

I will argue that the order in which I have given these six themes is—with one proviso—the order of their dependence. The proviso is that the first three are mutually interdependent; being rational is being intentional is being the object of a certain stance. These three together are a necessary but not sufficient condition for exhibiting the form of reciprocity that is in turn a necessary but not sufficient condition for having the capacity for verbal communication, which is the necessary*]] condition for having a special sort of consciousness, which is, as Anscombe and Frankfurt in their different ways claim, a necessary condition of moral personhood. (I will not discuss Sartre’s claim here.)

I have previously exploited the first three themes, rationality, intentionality and stance, to define not persons, but the much wider class of what I call _intentional systems_, and since I intend to build on that notion, a brief résumé is in order. An intentional system is a system whose behavior can be (at least sometimes) explained and predicted by relying on ascriptions to the system of _beliefs_ and _desires_ (and other intentionally characterized features—what I will call _intentions_ here, meaning to include hopes, fears, intentions, perceptions, expectations, etc.). There may _in every case_ be other ways of predicting and explaining the behavior of an intentional system—for instance, mechanistic or physical ways—but the intentional stance may be the handiest or most effective or in any case _a_ successful stance to adopt, which suffices for the object to be an intentional system. So defined, intentional systems are obviously not all persons. We ascribe beliefs and desires to dogs and fish and thereby predict their behavior, and we can even use the procedure to predict the behavior of some machines. For instance, it is a good, indeed the only good, strategy to adopt against a good chess-playing computer. By _assuming_ the computer has certain beliefs (or information) and desires (or preference functions) dealing with the chess game in progress, I can calculate—under auspicious circumstances—the computer ‘s most likely next move, _provided I assume that the computer deals rationally with these beliefs and desires_. The computer is an intentional system in these instances not because it has any particular intrinsic features, and not because it really and truly has beliefs and desires (whatever that would be), but just because it succumbs to a certain _stance_ adopted toward it, namely the intentional stance, the stance that proceeds by ascribing intentional predicates under the usual constraints to the computer, the stance that proceeds by considering the computer as a rational practical reasoner.

It is important to recognize how bland this definition of _intentional system_ is, and how correspondingly large the class of intentional systems can be. If, for instance, I predict that a particular plant—say a potted ivy—will grow around a corner and up into the light because it “seeks” the light and “wants” to get out of the shade it now finds itself in, and “expects” or “hopes” there is light around the corner, I have adopted the intentional stance toward the plant, and lo and behold, within very narrow limits it works. Since it works, some plants are very low-grade intentional systems.

The actual utility of adopting the intentional stance toward plants was brought home to me talking with loggers in the Maine woods. These men invariably call a tree not “it” but “he,” and will say of a young spruce, “He wants to spread his limbs, but don’t let him; then he’ll have to stretch up to get his light,” or, “Pines don’t like to get their feet wet the way cedars do.” You can “trick” an apple tree into “thinking it’s spring” by building a small fire under its branches in the late fall; it will blossom. This way of talking is not just picturesque and is not really superstitious at all; it is simply an efficient way of making sense of, controlling, predicting, and explaining the behavior of these plants in a way that nicely circumvents one’s ignorance of the controlling mechanisms. More sophisticated biologists may choose to speak of information transmission from the tree’s periphery to other locations in the tree. This is less picturesque, but still intentional. Complete abstention from intentional talk about trees can become almost as heroic, cumbersome, and pointless as the parallel strict behaviorist taboo when speaking of rats and pigeons. And even when intentional glosses on (e.g.,) tree-activities are of vanishingly small heuristic value, it seems to me wiser to grant that such a tree is a very degenerate, uninteresting, negligible intentional system than to attempt to draw a line above which intentional interpretations are “objectively true.”

It is obvious, then, that being an intentional system is not sufficient condition for being a person, but is surely a necessary condition. Nothing to which we could not successfully adopt the intentional stance, with its presupposition of rationality, could count as a person. Can we then define persons as a subclass of intentional systems? At first glance it might seem profitable to suppose that persons are just that subclass of intentional systems that _really_ have beliefs, desires, and so forth, and are not merely _supposed_ to have them for the sake of a short-cut prediction. But efforts to say what counts as really having a belief (so that no dog or tree or computer could qualify) all seem to end by putting conditions on genuine belief that (1) are too strong for our intuitions, and (2) allude to distinct conditions of personhood farther down my list. For instance, one might claim that genuine beliefs are necessarily _verbally expressible_ by the believer,[^4] or the believer must be _conscious_ that he has them, but people seem to have many beliefs that they cannot put into words, and many that they are unaware of having—and in any case I hope to show that the capacity for verbal expression, and the capacity for consciousness, find different _loci_ in the set of necessary conditions of personhood.

Better progress can be made, I think, if we turn to our fourth theme, reciprocity, to see what kind of definition it could receive in terms of intentional systems. The theme suggests that a person must be able to reciprocate the stance, which suggests that an intentional system that itself adopted the intentional stance toward other objects would meet the test. Let us define a _second-order intentional system_ as one to which we ascribe not only simple beliefs, desires and other intentions, but beliefs, desires, and other intentions _about_ beliefs, desires, and other intentions. An intentional system _S_ would be a second-order intentional system if among the ascriptions we make to it are such as _S believes that T desires that p, S hopes that T fears that q_, and reflexive cases like _S believes that S desires that p_. (The importance of the reflexive cases will loom large, not surprisingly, when we turn to those who interpret our sixth condition as _self_-consciousness. It may seem to some that the reflexive cases make all intentional systems automatically second-order systems, and even _n_-order systems, on the grounds that believing that _p_ implies believing that you believe that _p_ and so forth, but this is a fundamental mistake; the iteration of beliefs and other intentions is never redundant, and hence while some iterations are normal—are to be expected—they are never trivial or automatic.)

Now, are human beings the only second-order intentional systems so far as we know? I take this to be an empirical question. We ascribe beliefs and desires to dogs, cats, lions, birds, and dolphins, for example, and thereby often predict their behavior—when all goes well—but it is hard to think of a case where an animal’s behavior was so sophisticated that we would need to ascribe second-order intentions to it in order to predict or explain its behavior. Of course if some version of mechanistic physicalism is true (as I believe), we will never _need_ absolutely to ascribe any intentions to anything, but supposing that for heuristic and pragmatic reasons we were to ascribe intentions to animals, would we ever feel the pragmatic tug to ascribe second-order intentions to them? Psychologists have often appealed to a principle known as Lloyd Morgan’s Canon of Parsimony, which can be viewed as a special case of Occam’s Razor; it is the principle that one should attribute to an organism as little intelligence or consciousness or rationality or mind as will suffice to account for its behavior. This principle can be, and has been, interpreted as demanding nothing short of radical behaviorism,[^5] but I think this is a mistake, and we can interpret it as the principle requiring us when we adopt the intentional stance toward a thing to ascribe the simplest, least sophisticated, lowest-order beliefs, desires, and so on, that will account for the behavior. Then we will grant, for instance, that Fido _wants_ his supper, and _believes_ his master will give him his supper if he begs in front of his master, but we need not ascribe to Fido the further _belief_ that his begging induces a _belief_ in his master that he, Fido, _wants_ his supper. Similarly, my _expectation_ when I put a dime in the candy machine does not hinge on a further _belief_ that inserting the coin induces the machine to _believe_ I _want_ some candy. That is, while Fido’s begging looks very much like true second-order interacting (with Fido treating his master as an intentional system), if we suppose that to Fido his master is just a supper machine activated by begging, we will have just as good a predictive ascription, more modest but still, of course, intentional.

Are dogs, then, or chimps or other “higher” animals. incapable of rising to the level of second-order intentional systems, and if so why? I used to think the answer was yes, and I thought the reason was that nonhuman animals lack language, and that language was needed to represent second-order intentions. In other words, I thought condition four might rest on condition five. I was tempted by the hypothesis that animals cannot, for instance, have second-order beliefs, beliefs about beliefs, for the same reason they cannot have beliefs about Friday, or poetry. Some beliefs can only be acquired, and hence represented, via language.[^6] But if it is true that some beliefs cannot be acquired without language, it is false that all second-order beliefs are among them, and it is false that non-humans cannot be second-order intentional systems. Once I began asking people for examples of nonhuman second-order intentional systems, I found some very plausible cases. Consider this from Peter Ashley (in a letter):

> One evening I was sitting in a chair at my home, the _only_ chair my dog is allowed to sleep in. The dog was lying in front of me, whimpering. She was getting nowhere in her trying to “convince” me to give up the chair to her. Her next move is the most interesting, nay, the _only_ interesting part of the story. She stood up, and went to the front door where I could still easily see her. She scratched the door, giving me the impression that she had given up trying to get the chair and had decided to go out. However as soon as I reached the door to let her out, she ran back across the room and climbed into her chair, the chair she had “forced” me to leave.

Here it seems we must ascribe to the dog the _intention_ that her master _believe_ she _wants_ to go out—not just a second-order, but a third-order intention. The key to the example, what makes it an example of a higher-order intentional system at work, is that the belief she intends to induce in her master is false. If we want to discover further examples of animals behaving as second-order intentional systems it will help to think of cases of deception, where the animal, believing _p_, tries to get another intentional system to believe _not-p_. Where an animal is trying to induce behavior in another which _true_ beliefs about the other’s environment would not induce, we cannot “divide through” and get an explanation that cites only first-level intentions. We can make this point more general before explaining why it is so: where _x_ is attempting to induce behavior in _y_ which is inappropriate to _y’s true_ environment and needs but appropriate to _y’s perceived_ or _believed_ environment and needs, we are forced to ascribe second-order intentions to _x_. Once in this form the point emerges as a familiar one, often exploited by critics of behaviorism: one can be a behaviorist in explaining and controlling the behavior of laboratory animals only so long as he can rely on there being no serious dislocation between the actual environment of the experiment and the environment perceived by the animals. A tactic for embarrassing behaviorists in the laboratory is to set up experiments that deceive the subjects: if the deception succeeds, their behavior is predictable from their false _beliefs_ about the environment, not from the actual environment. Now a first-order intentional system is a behaviorist; it ascribes no intentions to anything. So if we are to have good evidence that some system _S_ is _not_ a behaviorist—is a second-order intentional system—it will only be in those cases where behaviorist theories are inadequate to the data, only in those cases where behaviorism would not explain system _S_’s success in manipulating another system’s behavior.

This suggests that Ashley’s example is not so convincing after all, that it can be defeated by supposing his dog is a behaviorist of sorts. She need not believe that scratching on the door will induce Ashley to believe she wants to go out; she may simply believe, as a good behaviorist, that she has conditioned Ashley to go to the door when she scratches. So she applies the usual stimulus, gets the usual response, and that’s that. Ashley’s case succumbs if this is a _standard_ way his dog has of getting the door opened, as it probably is, for then the more modest hypothesis is that the dog believes her master is conditioned to go to the door when she scratches. Had the dog done something _novel_ to deceive her master (like running to the window and looking out, growling suspiciously) then we would have to grant that rising from the chair was no mere conditioned response in Ashley, and could not be “viewed” as such by his dog, but then, such virtuosity in a dog would be highly implausible.

Yet what is the difference between the implausible case and the well-attested cases where a low-nesting bird will feign a broken wing to lure a predator away from the nest? The effect achieved is novel, in the sense that the bird in all likelihood has not repeatedly conditioned the predators in the neighborhood with this stimulus, so we seem constrained to explain the ploy as a bit of genuine deception, where the bird _intends_ to induce a false _belief_ in the predator. Forced to this interpretation of the behavior, we would be mightily impressed with the bird’s ingenuity were it not for the fact that we know such behavior is “merely instinctual.” But why does it disparage this trick to call it merely instinctual? To claim it is instinctual is to claim that all birds of the species do it; they do it even when circumstances aren’t entirely appropriate; they do it when there are better reasons for staying on the nest; the behavior pattern is rigid, a tropism of sorts, and presumably the controls are genetically wired in, not learned or invented.

We must be careful not to carry this disparagement too far; it is not that the bird does this trick “unthinkingly,” for while it is no doubt true that she does not in any sense run through an argument or scheme in her head (“Let’s see, if I were to flap my wing as if it were broken, the fox would think …”), a man might do something of similar subtlety, and of genuine intelligence, novelty, and appropriateness, and not run through the “conscious thoughts” either. _Thinking the thoughts_, however that is characterized, is not what makes truly intelligent behavior intelligent. Anscombe says at one point “If [such an expression of reasoning] were supposed to describe actual mental processes, it would in general be quite absurd. The interest of the account is that it described an order which is there whenever actions are done with intentions.”[^7] But the “order is there” in the case of the bird as well as the man. That is, when we ask why birds evolved with this tropism, we explain it by noting the utility of having a means of _deceiving_ predators, or inducing false beliefs in them; what must be explained is the provenance of the bird’s second-order intentions. I would be the last to deny or dismiss the vast difference between instinctual or tropistic behavior and the more versatile, intelligent behavior of humans and others, but what I want to insist on here is that if one is prepared to adopt the intentional stance without qualms as a tool in predicting and explaining behavior, the bird is as much a second-order intentional system as any man. Since this is so, we should be particularly suspicious of the argument I was tempted to use, viz., that _representations_ of second order intentions would depend somehow on language.[^8] For it is far from clear that all or even any of the beliefs and other intentions of an intentional system need be _represented_ “within” the system in any way for us to get a purchase on predicting its behavior by _ascribing_ such intentions to it. (I argue this in more detail in Chapters 2 and 3.) The situation we elucidate by citing the bird’s desire to induce a false belief in the predator seems to have no room or need for·a representation of this sophisticated intention in any entity’s “thoughts” or “mind,” for neither the bird nor evolutionary history nor Mother Nature need think these thoughts for our explanation to be warranted.

Reciprocity, then, provided we understand by it merely the capacity in intentional systems to exhibit higher-order intentions, while it depends on the first three conditions, is independent of the fifth and sixth. Whether this notion does justice to the reciprocity discussed by other writers will begin to come clear only when we see how it meshes with the last two conditions. For the fifth condition, the capacity for verbal communication, we turn to Grice’s theory of meaning. Grice attempts to define what he calls “nonnatural” meaning, an utterer’s meaning something by uttering something, in terms of the _intentions_ of the utterer. His initial definition is as follows:*]]

> “U meant something by uttering x” is true if, for some audience A, U uttered x intending
> 
> - (1) _A_ to produce a particular response _r_.
> - (2) A to think (recognize) that U intends (1).
> - (3) _A_ to fulfill (1) on the basis of his fulfillment of (2).

Notice that intention (2) ascribes to _U_ not only a second- but a third-order Intention: _U_ must _intend_ that _A recognize_ that _U intends_ that _A_ produce _r._ It matters not at all that Grice has been forced by a series of counterexamples to move from this initial definition to much more complicated versions, for they all reproduce the third-order Intention of (2). Two points of great importance to us emerge from Grice’s analysis of nonnatural meaning. First, since nonnatural meaning, meaning something by saying something, must be a feature of any true verbal communication, and since it depends on third-order intentions on the part of the utterer, we have our case that condition five rests on condition four and not vice versa. Second, Grice shows us that mere _second_-order intentions are not enough to provide genuine reciprocity; for that, _third_-order intentions are needed. Grice introduces condition (2) in order to exclude such cases as this: I leave the china my daughter has broken lying around for my wife to see. This is not a case of meaning something by doing what I do, intending what I intend, for though I am attempting thereby to induce my wife to believe something about our daughter (a second-order intention on my part), success does not depend on her recognizing this intention of mine, or recognizing my intervention or existence at all. There has been no real _encounter_, to use Erving Goffman’s apt term, between us, no _mutual recognition_. There must be an encounter between utterer and audience for utterer to mean anything, but encounters can occur in the absence of non-natural meaning (witness Ashley’s dog), and ploys that depend on third-order intentions need not involve encounters (e.g., _A_ can intend that _B_ believe that _C_ desires that _p_). So third-order intentions are a necessary but not sufficient condition for encounters which are a necessary but not sufficient condition for instances of nonnatural meaning, that is, instances of verbal communication.

It is no accident that Grice’s cases of nonnatural meaning fall into a class whose other members are cases of deception or manipulation. Consider, for instance, Searle’s ingenious counterexample to one of Grice’s formulations: the American caught behind enemy lines in World War II Italy who attempts to deceive his Italian captors into concluding he is a German officer by saying the one sentence of German he knows: “_Kennst du das Land, wo die Zitronen blühen?_”[^9] As Grice points out, these cases share with cases of nonnatural meaning a reliance on, or exploitation of, the rationality of the victim. In these cases success hinges on inducing the victim to embark on a chain of reasoning to which one contributes premises directly or indirectly. In deception the premises are disbelieved by the supplier; in normal communication they are believed. Communication, in Gricean guise, appears to be a sort of collaborative manipulation of audience by utterer; it depends, not only on the rationality of the audience who must sort out the utterer’s intentions, but on the audience’s _trust_ in the utterer. Communication, as a sort of manipulation, would not work, given the requisite rationality of the audience, unless the audience’s trust in the utterer were _well-grounded_ or reasonable. Thus the _norm_ for utterance is sincerity; were utterances not normally trustworthy, they would fail of their purpose (see Chapter 1).

Lying, as a form of deception, can only work against a background of truth-telling, but other forms of deception do not depend on the trust of the victim. In these cases success depends on the victim being _quite_ smart, but not quite smart enough. Stupid poker players are the bane of clever poker players, for they fail to see the bluffs and ruses being offered them. Such sophisticated deceptions need not depend on direct encounters. There is a book on how to detect fake antiques (which is also, inevitably, a book on how to _make_ fake antiques) which offers this sly advice to those who want to fool the “expert” buyer: once you have completed your table or whatever (having utilized all the usual means of simulating age and wear) take a modern electric drill and drill a hole right through the piece in some conspicuous but perplexing place. The would-be buyer will argue: no one would drill such a disfiguring hole without a reason (it can’t be supposed to look “authentic” in any way) so it must have served a purpose, which means this table must have been in use in someone’s home; since it was in use in someone’s home, it was not made expressly for sale in this antique shop … therefore it is authentic. Even if this “conclusion” left room for lingering doubts, the buyer will be so preoccupied dreaming up uses for that hole it will be months before the doubts can surface.

What is important about these cases of deception is the fact that just as in the case of the feigning bird, success does not depend on the victim’s _consciously entertaining_ these chains of reasoning. It does not matter if the buyer just notices the hole and “gets a hunch” the piece is genuine. He _might_ later accept the reasoning offered as his “rationale” for finding the piece genuine, but he might deny it, and in denying it, he might be deceiving himself, even though the _thoughts_ never went through his head. The chain of reasoning explains why the hole works as it does (if it does), but as Anscombe says, it need not “describe actual mental processes,” if we suppose actual mental processes are conscious processes or events. The same, of course, is true of Gricean communications; neither the utterer nor the audience need consciously entertain the complicated intentions he outlines, and what is a bit surprising is that no one has ever used this fact as an objection to Grice. Grice’s conditions for meaning have been often criticized for falling short of being sufficient, but there seems to be an argument not yet used to show they are not even necessary. Certainly, few people ever consciously framed those ingenious intentions before Grice pointed them out, and yet people have been communicating for years. Before Grice, were one asked: “Did you intend your audience to recognize your intention to provoke that response in him?” one would most likely have retorted: “I intended nothing so devious. I simply intended to inform them that I wouldn’t be home for supper” (or whatever). So it seems that if these complicated intentions underlay our communicating all along, they must have been unconscious intentions. Indeed, a perfectly natural way of responding to Grice’s papers is to remark that _one was not aware_ of doing these things when one communicated. Now Anscombe has held, very powerfully, that such a response establishes that the action under that description was not _intentional_.*]] Since one is not _aware_ of these _intentions_ in speaking, one cannot be speaking _with_ these _intentions_.

Why has no one used this argument against Grice’s theory? Because, I submit, it is just too plain that Grice is on to something, that Grice is giving us necessary conditions for nonnatural meaning. His analysis illuminates so many questions. Do we communicate with computers in Fortran? Fortran seems to be a language; it has a grammar, a vocabulary, a semantics. The transactions in Fortran between man and machine are often viewed as cases of _man communicating with machine_, but such transactions are pale copies of human verbal communication precisely because the Gricean conditions for nonnatural meaning have been bypassed. There is no room for them to apply. Achieving one’s ends in transmitting a bit of Fortran to the machine does not hinge on getting the machine to recognize one’s _intentions_. This does not mean that all communications with computers in the future will have this shortcoming (or strength, depending on your purposes), but just that we do not now communicate, in the strong (Gricean) sense, with computers.†]]

If we are not about to abandon the Gricean model, yet are aware of no such _intentions_ in our normal conversation, we shall just have to drive these _intentions_ underground, and call them unconscious or preconscious _intentions_. They are _intentions_ that exhibit “an order which is there” when people communicate, _intentions_ of which we are not normally aware, and _intentions_ which are a precondition of verbal communication.

We have come this far without having to invoke any sort of consciousness at all, so if there is a dependence between consciousness or self-consciousness and our other conditions, it will have to be consciousness depending on the others. But to show this I must first show how the first five conditions by themselves might play a role in ethics, as suggested by Rawls’s theory of justice. Central to Rawls’s theory is his setting up of an idealized situation, the “original position,” inhabited by idealized persons, and deriving from this idealization the first principles of justice that generate and illuminate the rest of his theory. What I am concerned with now is neither the content of these principles nor the validity of their derivation, but the nature of Rawls’s tactic. Rawls supposes that a group of idealized persons, defined by him as rational, self-interested entities, make calculations under certain constraints about the likely and possible interactive effects of their individual and antagonistic interests (which will require them to frame higher-order intentions, for example, beliefs about the desires of others, beliefs about the beliefs of others about their own desires, and so forth). Rawls claims these calculations have an optimal “solution” that it would be reasonable for each self-interested person to adopt as an alternative to a Hobbesian state of nature. The solution is to agree with his fellows to abide by the principles of justice Rawls adumbrates. What sort of a proof of the principles of justice would this be? Adopting these principles of justice can be viewed, Rawls claims, as the solution to the “highest order game” or “bargaining problem.” It is analogous to derivations of game theory, and to proofs in Hintikka’s epistemic logic,[^10] and to a “demonstration” that the chess-playing computer will make a certain move because it is the most rational move given its information about the game. All depend on the assumption of ideally rational calculators, and hence their outcomes are intrinsically normative. Thus I see the derivations from Rawls’s original position as continuous with the deductions and extrapolations encountered in more simple uses of the intentional stance to understand and control the behavior of simpler entities. Just as truth and consistency are norms for belief (see Chapter 1 of this volume), and sincerity is the norm for utterance, so, if Rawls is right, justice as he defines it is the norm for interpersonal interactions. But then, just as part of our warrant for considering an entity to have any beliefs or other intentions is our ability to construe the entity as _rational_, so our grounds for considering an entity a person include our ability to view him as abiding by the principles of justice. A way of capturing the peculiar status of the concept of a person as I think it is exploited here would be to say that while Rawls does not at all intend to argue that justice is the inevitable result of _human_ interaction, he does argue in effect that it is the inevitable result of _personal_ interaction. That is, the concept of a person is itself inescapably normative or idealized; to the extent that justice does not reveal itself in the dealings and interactions of creatures, to that extent they are not persons. And once again we can see that there is “an order which is there” in a just society that is independent of any actual episodes of conscious thought. The existence of just practices and the “acknowledgment” implicit in them does not depend on anyone ever consciously or deliberately going through the calculations of the idealized original position, consciously arriving at the reciprocal agreements, consciously adopting a stance toward others.

> To recognize another as a person one must respond to him and act towards him in certain ways; and these ways are intimately connected with the various prima facie duties. Acknowledging these duties in some degree, and so having the elements of morality, is not a matter of choice or of intuiting moral qualities or a matter of the expression of feelings or attitudes … it is simply the pursuance of one of the forms of conduct in which the recognition of others as persons is manifested.[^11]

The importance of Rawls’s attempt to derive principles of justice from the “original position” is, of course, that while the outcome is recognizable as a _moral_ norm, it is not _derived as_ a moral norm. Morality is not presupposed of the parties in the original position. But this means that the derivation of the norm does not in itself give us any answer to the questions of when and why we have the right to hold persons _morally_ responsible for deviations from that norm. Here Anscombe provides help and at the same time introduces our sixth condition. _If I am to be held responsible for an action_ (a bit of behavior of mine under a particular description), I must have been _aware_ of that action under that description.*]] Why? Because only if I was aware of the action can I _say_ what I was about, and participate from a privileged position in the question-and-answer game of giving reasons for my actions. (If I am not in a privileged position to answer questions about the reasons for my actions, there is no special reason to ask _me_.) And what is so important about being able to participate in this game is that only those capable of participating in reason-giving can be argued into, or argued out of, courses of action or attitudes, and if one is incapable of “listening to reason” in some matter, one cannot be held responsible for it. The capacities for verbal communication and for awareness of one’s actions are thus essential in one who is going to be amenable to argument or persuasion, and such persuasion, such reciprocal adjustment of interests achieved by mutual exploitation of rationality, is a feature of the optimal mode of personal interaction.

This capacity for participation in mutual persuasion provides the foundation for yet another condition of personhood recently exposed by Harry Frankfurt.*]] Frankfurt claims that persons are the subclass of intentional systems capable of what he calls “_second-order volitions_.” Now at first this looks just like the class of second-order intentional systems, but it is not, as we shall see.

> Besides wanting and choosing and being moved _to do_ this or that, men may also want to have (or not to have) certain desires and motives. They are capable of wanting to be different, in their preferences and purposes, from what they are. … No animal other than man, however, appears to have the capacity for reflective self-evaluation that is manifested in the formation of second-order desires. (p. 7)

Frankfurt points out that there are cases in which a person might be said to want to have a particular desire even though he would not want that desire to be effective for him, to be “his will.” (One might, for instance, want to desire heroin just to know what it felt like to desire heroin, without at all wanting this desire to become one’s effective desire.) In more serious cases one wants to have a desire one currently does not have, and wants this desire to become one’s will. These cases Frankfurt calls second-order volitions, and it is having these, he claims, that is “essential to being a person” (p. 10). His argument for this claim, which I will not try to do justice to here, proceeds from an analysis of the distinction between having freedom of action and having freedom of the will. One has freedom of the will, on his analysis, only when one can have the will one wants, when one’s second-order volitions can be satisfied. Persons do not always have free will, and under some circumstances can be responsible for actions done in the absence of freedom of the will, but a person always must be an “entity for whom the freedom of its will may be a problem” (p. 14)—that is, one capable of framing second-order volitions, satisfiable or not. Frankfurt introduces the marvelous term “wanton” for those “who have first-order desires but … no second-order volitions.” (Second-order volitions for Frankfurt are all, of course, _reflexive_ second-order desires.) He claims that our intuitions support the opinion that all nonhuman animals, as well as small children and some mentally defective people, are _wantons,_ and I for one can think of no plausible counterexamples. Indeed, it seems a strength of his theory, as he claims, that human beings—the only persons we recognize—are distinguished from animals in this regard. But what should be so special about second-order volitions? Why are they, among higher-order intentions, the peculiar province of persons? Because, I believe, the “reflective self-evaluation” Frankfurt speaks of is, and must be, genuine self-consciousness, which is achieved only by adopting toward _oneself_ the stance not simply of communicator but of Anscombian reason-asker and persuader. As Frankfurt points out, second-order desires are an empty notion unless one can _act_ on them, and acting on a second-order desire must be logically distinct from acting on its first-order component. Acting on a second-order desire, doing something to bring it about that one acquires a first-order desire, is acting upon oneself just as one would act upon another person: one _schools_ oneself, one offers oneself persuasions, arguments, threats, bribes, in the hopes of inducing oneself to acquire the first-order desire.*]] One’s stance toward oneself _and access to oneself_ in these cases is essentially the same as one’s stand toward and access to another. One must _ask oneself_ what one’s desires, motives, reasons really are, and only if one can say, can become aware of one’s desires, can one be in a position to induce oneself to change.[^12] Only here, I think, is it the case that the “order which is there” cannot be there unless it is there in episodes of conscious thought, in a dialogue with oneself.*]]

Now finally, why are we not in a position to claim that these necessary conditions of moral personhood are also sufficient? Simply because the concept of a person is, I have tried to show, inescapably normative. Human beings or other entities can only aspire to being approximations of the ideal, and there can be no way to set a “passing grade” that is not arbitrary. Were the six conditions (strictly interpreted) considered sufficient they would not ensure that any actual entity was a person, for nothing would ever fulfill them. The moral notion of a person and the metaphysical notion of a person are not separate and distinct concepts but just two different and unstable resting points on the same continuum. This relativity infects the satisfaction of conditions of personhood at every level. There is no objectively satisfiable sufficient condition for an entity’s _really_ having beliefs, and as we uncover apparent irrationality under an intentional interpretation of an entity, our grounds for ascribing any beliefs at all wanes, especially when we have (what we always _can_ have in principle) a non-intentional, mechanistic account of the entity. In just the same way our assumption that an entity is a person is shaken precisely in those cases where it matters: when wrong has been done and the question of responsibility arises. For in these cases the grounds for saying that the person is culpable (the evidence that he did wrong, was aware he was doing wrong, and did wrong of his own free will) are in themselves grounds for doubting that it is a person we are dealing with at all. And if it is asked what could _settle_ our doubts, the answer is: nothing. When such problems arise we cannot even tell in our own cases if we are persons.

## Notes

*]] In “Justice as Reciprocity,” a revision of “Justice as Fairness” printed in S. Gorovitz, ed., _Utilitarianism_ (Indianapolis: Bobbs Merrill, 1971), Rawls allows that the persons in the original position may include “nations, provinces, business firms, churches, teams, and so on. The principles of justice apply to conflicting claims made by persons of all these separate kinds. There is, perhaps, a certain logical priority to the case of human individuals” (p. 245). In _A Theory of Justice_ (Cambridge, Mass.: Harvard University Press, 1971), he acknowledges that parties in the original position may include associations and other entities not human individuals (e.g., p. 146), and the apparent interchangeability of “parties in the original position” and “persons in the original position” suggests that Rawls is claiming that for some moral concept of a person, the moral person is _composed_ of metaphysical persons who may or may not themselves be moral persons.

†]] Setting aside Rawls’s possible compound moral persons. For more on compound persons see Amelie Rorty, “Persons, Policies, and Bodies,” _International Philosophical Quarterly_, XIII, 1 (March, 1973).

*]] P. F. Strawson, _Individuals_ (London: Methuen, 1959), pp. 101–102. It has often been pointed out that Strawson’s definition is obviously much too broad, capturing all sentient, active creatures. See, e.g., H. Frankfurt, “Freedom of the Will and the Concept of a Person,” _Journal of Philosophy_, LXVIII (January 14, 1971). It can also be argued (and I would argue) that states of consciousness are only a proper subset of psychological or intentionally characterized states, but I think it is clear that Strawson here means to cast his net wide enough to include psychological states generally.

*]] And sufficient, but I will not argue it here. I argue for this in _Content and Consciousness_ and more recently and explicitly in Chapters 2 and 9 of this volume.

*]] The key papers are “Meaning,” _Philosophical Review_ (July, 1957), and “Utterer’s Meaning and Intentions,” _Philosophical Review_ (April, 1969). His initial formulation, developed in the first paper, is subjected to a series of revisions in the second paper, from which this formulation is drawn (p. 151).

*]] See G. E. M. Anscombe, _Intention_ (Oxford: Blackwell, 1957), p. 11. Here, and in the next few paragraphs, I am using “intentions” and “intentional” in their _ordinary_ sense—and putting them in italics for emphasis.

†]] It has been pointed out to me by Howard Friedman that many current Fortran compilers which “correct ” operator input by inserting “plus” signs and parentheses, etc., to produce well-formed expressions arguably meet Grice’s criteria, since within a very limited sphere, they diagnose the “utterer’s” _intentions_ and proceed on the basis of this diagnosis. But first it should be noted that the machines to date can diagnose only what might be called the operator’s syntactical intentions, and second, these machines do not seem to meet Grice’s subsequent and more elaborate definitions, not that I wish to claim that no computer could.

*]] I can be held responsible for events and states of affairs that I was not aware of and ought to have been aware of, but these are not _intentional_ actions. In these cases I am responsible for these further matters in virtue of being responsible for the foreseeable consequences of actions—including acts of omission—that I was aware of.

*]] H. Frankfurt, “Freedom of the will and the concept of a person,” _loc. cit._ Frankfurt does not say whether he conceives his condition to be merely a necessary or also a sufficient condition of moral personhood.

*]] It has been brought to my attention that dogs at stud will often engage in masturbation, in order, apparently, to _increase their desire_ to copulate. What makes these cases negligible is that even supposing the dog can be said to act on a desire to strengthen a desire, the effect is achieved in a nonintentional (“purely physiological”) way; the dog does not appeal to or exploit his own rationality in achieving his end. (As if the only way a person could act on a second-order volition were by taking a pill or standing on his head, etc.)

*]] Marx, in _The German Ideology_, says: “Language, like consciousness, only arises from the need, the necessity, of intercourse with other men. … Language is as old as consciousness, language is practical consciousness.” And Nietzsche, in _The Joyful Wisdom_, says: “For we could in fact think, feel, will, and recollect, we could likewise ‘act’ in every sense of the term, and nevertheless nothing of it at all need necessarily ‘come into consciousness’ (as one says metaphorically; … _What_ then is the _purpose_ of consciousness generally, when it is in the main _superfluous?_—Now it seems to me, if you will hear my answer and its perhaps extravagant supposition, that the subtlety and strength of consciousness are always in proportion to the _capacity for communication_ of a man (or an animal), the capacity for communication in its turn being in proportion to the _necessity for communication_. … In short, the development of speech and the development of consciousness (not of reason, but of reason becoming self-conscious) go hand in hand.”

1.]] J. Hintikka, _Knowledge and Belief_ (Ithaca: Cornell University Press, 1962).

2.]] D. M. MacKay, “The use of behavioral language to refer to mechanical processes,” _British Journal of Philosophy of Science_ (1962): 89–103; P. F. Strawson, “Freedom and Resentment,” _Proceedings of the British Academy_ (1962), reprinted in Strawson, ed., _Studies in the Philosophy of Thought and Action_ (Oxford, 1968); A. Rorty, “Slaves and machines,” _Analysis_ (1962); H. Putnam, “Robots: Machines or Artificially Created Life?,” _Journal of Philosophy_, LXI (November 12, 1964); W. Sellars, “Fatalism and Determinism,” in K. Lehrer, ed., _Freedom and Determinism_ (New York: Random House, 1966); A. Flew, “A Rational Animal,” in J. R. Smythies, ed., _Brain and Mind_ (London: Routledge & Kegan Paul, 1968); T. Nagel, “War and Massacre,” _Philosophy and Public Affairs_ (Winter 1972); D. Van de Vate, “The Problem of Robot Consciousness,” _Philosophy and Phenomenological Research_ (December 1971); Chapter 1 of this volume.

3.]] H. Frankfurt, “Freedom of the Will and the Concept of a Person,” _Journal of Philosophy_, LXVIII (January 14, 1971).

4.]] Cf. Bernard Williams, “Deciding to Believe,” in H. E. Kiefer and M. K. Munitz, eds., _Language, Belief and Metaphysics_ (New York: New York University Press, 1970).

5.]] E.g., B. F. Skinner, “Behaviorism at Fifty,” in T. W. Wann, ed., _Behaviorism and Phenomenology_ (Chicago: University of Chicago Press, 1964).

6.]] For illuminating suggestions on the relation of language to belief and rationality, see Ronald de Sousa, “How to Give a Piece of Your Mind; or the Logic of Belief and Assent,” _Review of Metaphysics_ (September 1971).

7.]] G. E. M. Anscombe, _Intention_ (Oxford: Blackwell, 1957): p. 80.

8.]] Cf. Ronald de Sousa, “Self-Deception,” _Inquiry_, XIII (1970), especially p. 317.

9.]] John Searle, “What is a Speech Act?” in Max Black, ed., _Philosophy in America_ (London: Allen & Unwin, 1965), discussed by Grice in “Utterer’s Meaning and Intentions,” p. 160.

10.]] See Hintikka, _Knowledge and Belief_, p. 38.

11.]] J. Rawls, “Justice as Reciprocity,” in S. Gorovitz, ed., _Utilitarianism_ (Indianapolis: Bobbs Merrill, 1971): 259.

12.]] Margaret Gilbert, in “Vices and Self-Knowledge,” _Journal of Philosophy_, LXVIII (August 5, 1971): 452, examines the implications of the fact that “when, and only when, one believes that one has a given trait can one decide to change out of it.”

On Giving Libertarians What They Say They Want

Why is the free will problem so persistent? Partly, I suspect, because it is called _the_ free will problem. Hilliard, the great card magician, used to fool even his professional colleagues with a trick he called the tuned deck. Twenty times in a row he’d confound the quidnuncs, as he put it, with the same trick, a bit of prestidigitation that resisted all the diagnostic hypotheses of his fellow magicians. The trick, as he eventually revealed, was a masterpiece of subtle misdirection; it consisted entirely of the _name_, “the tuned deck,” plus a peculiar but obviously non-functional bit of ritual. It was, you see, _many_ tricks, however many different but familiar tricks Hilliard had to perform in order to stay one jump ahead of the solvers. As soon as their experiments and subtle arguments had conclusively eliminated one way of doing the trick, that was the way he would do the trick on future trials. This would have been obvious to his sophisticated onlookers had they not been so intent on finding _the_ solution to _the_ trick.

The so called free will problem is in fact many not very closely related problems tied together by a name and lots of attendant anxiety. Most people can be brought by reflection to care very much what the truth is on these matters, for each problem poses a threat: to our self-esteem, to our conviction that we are not living deluded lives, to our conviction that we may justifiably trust our grasp of such utterly familiar notions as possibility, opportunity and ability.*]] There is no very good reason to suppose that an acceptable solution to _one_ of the problems will be, or even point to, an acceptable solution to the others, and we may be misled by residual unallayed worries into rejecting or undervaluing partial solutions, in the misguided hope that we might allay all the doubts with one overarching doctrine or theory. But we don’t have any good theories. Since the case for determinism is persuasive and since we all want to believe we have free will, _compatibilism_ is the strategic favorite, but we must admit that no compatibilism free of problems while full of the traditional flavors of responsibility has yet been devised.

The alternatives to compatibilism are anything but popular. Both the libertarian and the hard determinist believe that free will and determinism are incompatible. The hard determinist says: “So much of the worse for free will.” The libertarian says: “So much the worse for determinism,” at least with regard to human action. Both alternatives have been roundly and routinely dismissed as at best obscure, at worst incoherent. But alas for the compatibilist, neither view will oblige us by fading away. Their persistence, like Hilliard’s success, probably has many explanations. I hope to diagnose just one of them.

In a recent paper, David Wiggins has urged us to look with more sympathy at the program of libertarianism.[^1] Wiggins first points out that a familiar argument often presumed to demolish libertarianism begs the question. The first premise of this argument is that every event is either causally determined or random. Then since the libertarian insists that human actions cannot be both free and determined, the libertarian must be supposing that any and all free actions are random. But one would hardly hold oneself responsible for an action that merely happened at random, so libertarianism, far from securing a necessary condition for responsible action, has unwittingly secured a condition that would defeat responsibility altogether. Wiggins points out that the first premise, that every event is either causally determined or random, is not the innocent logical truth it appears to be. The innocent logical truth is that every event is either causally determined or not causally determined. There may be an established sense of the word “random” that is unproblematically synonymous with “not causally determined,” but the word “random” in common parlance has further connotations of pointlessness or arbitrariness, and it is these very connotations that ground our acquiescence in the further premise that one would not hold oneself responsible for one’s random actions. It may be the case that whatever is random in the sense of being causally undetermined, is random in the sense connoting utter meaninglessness, but that is just what the libertarian wishes to deny. This standard objection to libertarianism, then, assumes what it must prove, it fails to show that undetermined action would be random action, and hence action for which we could not be held responsible.

But is there in fact any reasonable hope that the libertarian can find some defensible ground between the absurdity of “blind chance” on the one hand and on the other what Wiggins calls the cosmic unfairness of the determinist’s view of these matters? Wiggins thinks there is. He draws our attention to a speculation of Russell’s: “It might be that without infringing the laws of physics, intelligence could make improbable things happen, as Maxwell’s demon would have defeated the second law of thermo-dynamics by opening the trap door to fast-moving particles and closing it to slow-moving particles.”[^2] Wiggins sees many problems with the speculation, but he does, nevertheless, draw a glimmer of an idea from it.

> For indeterminism maybe all we really need to imagine or conceive is a world in which (a) there is some macroscopic indeterminacy founded in microscopic indeterminacy, and (b) an appreciable number of the free actions or policies or deliberations of individual agents, although they are not even in principle hypothetico-deductively derivable from antecedent conditions, can be such as to persuade us to fit them into meaningful sequences. We need not trace free actions back to volitions construed as little pushes aimed from outside the physical world. What we must find instead are patterns which are coherent and intelligible in the low level terms of practical deliberation, even though they are not amenable to the kind of generalization or necessity which is the stuff of rigorous theory. (p. 52)

The “low level terms of practical deliberation” are, I take it, the familiar terms of intentional or reason-giving explanation. We typically render actions intelligible by citing their reasons, the beliefs and desires of the agent that render the actions at least marginally reasonable under the circumstances. Wiggins is suggesting then that if we could somehow _make sense_ of human actions at the level of intentional explanation, then in spite of the fact that those actions might be physically undetermined, they would not be random. Wiggins invites us to take this possibility seriously, but he has little further to say in elaboration or defense of this. He has said enough, however, to suggest to me a number of ways in which we could give libertarians what they seem to want.

Wiggins asks only that human actions be seen to be _intelligible_ in the low-level terms of practical deliberation. Surely if human actions were _predictable_ in the low-level terms of practical deliberation, they would be intelligible in those terms. So I propose first to demonstrate that there is a way in which human behavior could be strictly undetermined from the physicist’s point of view while at the same time accurately predictable from the intentional level. This demonstration, alas, will be very disappointing, for it relies on a cheap trick and what it establishes can be immediately seen to be quite extraneous to the libertarian’s interests. But it is a necessary preamble to what I hope will be a more welcome contribution to the libertarian’s cause. So let us get the disappointing preamble behind us.

Here is how a bit of human behavior could be undetermined from the physicist’s point of view, but quite clearly predictable by the intentionalist. Suppose we were to build an electronic gadget that I will call an answer box. The answer box is designed to record a person’s answers to simple questions. It has two buttons, a Yes button, and a No button, and two foot pedals, a Yes pedal, and a No pedal, all clearly marked. It also has a little display screen divided in half, and on one side it says “use the buttons” and on the other side it says “use the pedals.” We design this bit of apparatus so that only one half of this display screen is illuminated at any one time. Once a minute, a radium randomizer determines, in an entirely undetermined way of course, whether the display screen says “use the buttons” or “use the pedals.” I now propose the following experiment. First, we draw up a list of ten very simple questions that have Yes or No answers, questions of the order of difficulty of “Do fish swim?” and “Is Texas bigger than Rhode Island?” We seat a subject at the answer box and announce that a handsome reward will be given to those who correctly follow all the experimental instructions, and a bonus will be given to those who answer all our questions correctly.

Now, can the physicist in principle predict the subject’s behavior? Let us suppose the subject is in fact a physically deterministic system, and let us suppose further that the physicist has perfect knowledge of the subject’s initial state, all the relevant deterministic laws, and all the interactions within the closed situation of the experimental situation. Still, the unpredictable behavior of the answer box will infect the subject on a macroscopic scale with its own indeterminacy on at least ten occasions during the period the physicist must predict. So the best the physicist can do is issue a multiple disjunctive or multiple conditional prediction. Can the intentionalist do any better? Yes, of course. The intentionalist, having read the instructions given to the subject and having sized up the subject as a person of roughly normal intelligence and motivation, and having seen that all the odd numbered questions have Yes answers and the even numbered questions have No answers, confidently predicts that the subject will behave as follows: “The subject will give Yes answers to questions _1_, _3_, _5_, 7, and _9_, and the subject will answer the rest of the questions in the negative.” There are no _if’s_, _or’s_ or _maybe’s_ in those predictions. They are categorical and precise—precise enough for instance to appear in a binding contract or satisfy a court of law.

This is, of course, the cheap trick I warned you about. There is no real difference in the predictive power of the two predictors. The intentionalist for instance is no more in a position to predict whether the subject will move finger or foot than the physicist is, and the physicist may well be able to give predictions that are tantamount to the intentionalist’s. The physicist may for instance be able to make this prediction: “When question _6_ is presented, if the illuminated sign on the box reads use the pedals, the subject’s right foot will move at velocity _k_ until it depresses the No pedal _n_ inches, and if the illuminated sign says use the buttons, the subject’s right index finger will trace a trajectory terminating on the No button.” Such a prediction is if anything more detailed than the intentionalist’s simple prediction of the negative answer to question _6_, and it might in fact be more reliable and better grounded as well. But so what? What we are normally interested in, what we are normally interested in _predicting_, moreover, is not the skeletal motion of human beings but their actions, and the intentionalist can predict the actions of the subject (at least insofar as most of us would take any interest in them) without the elaborate rigmarole and calculations of the physicist. The possibility of indeterminacy in the environment of the kind introduced here, and hence the possibility of indeterminacy in the subject’s reaction to that environment, is something with regard to which the intentionalistic predictive power is quite neutral. Still, we could not expect the libertarian to be interested in this variety of undetermined human behavior, behavior that is undetermined simply because the behavior of the answer box, something entirely external to the agent, is undetermined.

Suppose then we move something like the answer box inside the agent. It is a commonplace of action theory that virtually all human actions can be accomplished or realized in a wide variety of ways. There are, for instance, indefinitely many ways of insulting your neighbor, or even of asserting that snow is white. And we are often not much interested, nor should we be, in exactly which particular physical motion accomplishes the act we intend. So let us suppose that our nervous system is so constructed and designed that whenever in the implementation of an intention, our control system is faced with two or more options with regard to which we are non-partisan, a purely undetermined tie-breaking “choice” is made. There you are at the supermarket, wanting a can of Campbell’s Tomato Soup, and faced with an array of several hundred identical cans of Campbell’s Tomato Soup, all roughly equidistant from your hands. What to do? Before you even waste time and energy pondering this trivial problem, let us suppose, a perfectly random factor determines which can your hand reaches out for. This is of course simply a variation on the ancient theme of Buridan’s ass, that unfortunate beast who, finding himself hungry, thirsty and equidistant between food and water, perished for lack of the divine nudge that in a human being accomplishes a truly free choice. This has never been a promising vision of the free choice of responsible agents, if only because it seems to secure freedom for such a small and trivial class of our choices. What does it avail me if I am free to choose _this_ can of soup, but not free to choose between buying and stealing it? But however unpromising the idea is as a centerpiece for an account of free will, we must not underestimate its possible scope of application. Such trivial choice points seldom obtrude in our conscious deliberation, no doubt, but they are quite possibly ubiquitous nonetheless at an unconscious level. Whenever we choose to perform an action of a certain sort, there are no doubt slight variations in timing, style and skeletal implementation of those actions that are within our power but beneath our concern. For all we know, which variation occurs is _undetermined._ That is, the implementation of any one of our intentional actions may encounter undetermined _choice points_ in many places in the causal chain. The resulting behavior would not be distinguishable to our everyday eyes, or from the point of view of our everyday interests, from behavior that was rigidly determined. What we are mainly interested in, as I said before, are actions, not motions, and what we are normally interested in predicting are actions.

It is worth noting that not only can we typically predict actions from the intentional stance without paying heed to possibly undetermined variations of implementation of these actions, but we can even put together chains of intentional predictions that are relatively immune to such variation. In the summer of 1974 many people were confidently predicting that Nixon would resign. As the day and hour approached, the prediction grew more certain and more specific as to time and place; Nixon would resign not just in the near future, but in the next hour, and in the White House and in the presence of television cameramen and so forth. Still, it was not plausible to claim to know just how he would resign, whether he would resign with grace, or dignity, or with an attack on his critics, whether he would enunciate clearly or mumble or tremble. These details were not readily predictable, but most of the further dependent predictions we were interested in making did not hinge on these subtle variations. However Nixon resigned, we could predict that Goldwater would publicly approve of it, Cronkite would report that Goldwater had so approved of it, Sevareid would comment on it, Rodino would terminate the proceedings of the Judiciary Committee, and Gerald Ford would be sworn in as Nixon’s successor. Of course some predictions we might have made at the time would have hinged crucially on particular details of the precise manner of Nixon’s resignation, and if these details happened to be undetermined both by Nixon’s intentions and by any other feature of the moment, then some human actions of perhaps great importance would be infected by the indeterminacy of Nixon’s manner at the moment just as our exemplary subject’s behavior was infected by the indeterminacy of the answer box. That would not, however, make these actions any the less intelligible to us as actions.

This result is not just what the libertarian is looking for, but it is a useful result nevertheless. It shows that we can indeed install indeterminism in the internal causal chains affecting human behavior _at the macroscopic level_ while preserving the intelligibility of practical deliberation that the libertarian requires. We may have good reasons from other quarters for embracing determinism, but we need not fear that macroscopic indeterminism in human behavior would of necessity rob our lives of intelligibility by producing chaos. Thus, philosophers such as Ayer and Hobart,[^3] who argue that free will requires determinism, must be wrong. There are _some_ ways our world could be macroscopically indeterministic, without that fact remotely threatening the coherence of the intentionalistic conceptual scheme of action description presupposed by claims of moral responsibility.

Still, it seems that all we have done is install indeterminism in a _harmless_ place by installing it in an _irrelevant_ place. The libertarian would not be relieved to learn that although his decision to murder his neighbor was quite determined, the style and trajectory of the death blow was not. Clearly, what the libertarian has in mind is indeterminism at some earlier point, prior to the ultimate decision or formation of intention, and unless we can provide that, we will not aid the libertarian’s cause. But perhaps we can provide that as well.

Let us return then, to Russell’s speculation that intelligence might make improbable things happen. Is there any way that something like this could be accomplished? The idea of intelligence exploiting randomness is not unfamiliar. The poet, Paul Valéry, nicely captures the basic idea:

> It takes two to invent anything. The one makes up combinations; the other one chooses, recognizes what he wishes and what is important to him in the mass of the things which the former has imparted to him. What we call genius is much less the work of the first one than the readiness of the second one to grasp the value of what has been laid before him and to choose it.*]]

Here we have the suggestion of an intelligent _selection_ from what may be a partially arbitrary or chaotic or random _production,_ and what we need is the outline of a model for such a process in human decision-making.

An interesting feature of most important human decision-making is that it is made under time pressure. Even if there are, on occasion, algorithmic decision procedures giving guaranteed optimal solutions to our problems, and even if these decision procedures are in principle available to us, we may not have time or energy to utilize them. We are rushed, but moreover, we are all more or less lazy, even about terribly critical decisions that will affect our lives—our own lives, to say nothing of the lives of others. We invariably settle for a _heuristic_ decision procedure; we _satisfice_;*]] we poke around hoping for inspiration; we do our best to think about the problem in a more or less directed way until we must finally stop mulling, summarize our results as best we can, and act. A realistic model of such decision-making just _might_ have the following feature: When someone is faced with an important decision, something in him generates a variety of more or less relevant considerations bearing on the decision. Some of these considerations, we may suppose, are determined to be generated, but others may be non-deterministically generated. For instance, Jones, who is finishing her dissertation on Aristotle and the practical syllogism, must decide within a week whether to accept the assistant professorship at the University of Chicago, or the assistant professorship at Swarthmore. She considers the difference in salaries, the probable quality of the students, the quality of her colleagues, the teaching load, the location of the schools, and so forth. Let us suppose that considerations _A_, _B_, C, _D_, _E_, and _F_ occur to her and that those are the only considerations that occur to her, and that on the basis of those, she decides to accept the job at Swarthmore. She does this _knowing_ of course that she could devote more time and energy to this deliberation, could cast about for other relevant considerations, could perhaps dismiss some of _A_–_F_ as being relatively unimportant and so forth, but being no more meticulous, no more obsessive, than the rest of us about such matters, she settles for the considerations that have occurred to her and makes her decision.

Let us suppose though, that after sealing her fate with a phone call, consideration _G_ occurs to her, and she says to herself: “If only _G_ had occurred to me before, I would certainly have chosen the University of Chicago instead, but _G_ didn’t occur to me.” Now it just might be the case that _exactly_ which considerations occur to one in such circumstances is to some degree strictly undetermined. If that were the case, then even the intentionalist, knowing everything knowable about Jones’ settled beliefs and preferences and desires, might nevertheless be unable to predict her decision, except perhaps conditionally. The intentionalist might be able to argue as follows: “If considerations _A_–_F_ occur to Jones, then she will go Swarthmore,” and this would be a prediction that would be grounded on a rational argument based on considerations _A_–_F_ according to which Swarthmore was the best place to go. The intentionalist might go on to add, however, that if consideration _G_ also occurs to Jones (which is strictly unpredictable unless we interfere and draw Jones’ attention to G), Jones will choose the University of Chicago instead. Notice that although we are supposing that the decision is in this way strictly unpredictable except conditionally by the intentionalist, whichever choice Jones makes is retrospectively intelligible. There will be a rationale for the decision in either case; in the former case a rational argument in favor of Swarthmore based on _A_–_F_, and in the latter case, a rational argument in favor of Chicago, based on _A_–_G_. (There may, of course be yet another rational argument based on _A_–_H_, or _I_, or _J_, in favor of Swarthmore, or in favor of going on welfare, or in favor of suicide.) Even if _in principle_ we couldn’t predict which of many rationales could ultimately be correctly cited in justification or retrospective explanation of the choice made by Jones, we could be confident that there would be some sincere, authentic, and not unintelligible rationale to discover.

The model of decision making I am proposing has the following feature: when we are faced with an important decision, a consideration-generator whose output is to some degree undetermined produces a series of considerations, some of which may of course be immediately rejected as irrelevant by the agent (consciously or unconsciously). Those considerations that are selected by the agent as having a more than negligible bearing on the decision then figure in a reasoning process, and if the agent is in the main reasonable, those considerations ultimately serve as predictors and explicators of the agent’s final decision. What can be said in favor of such a model, bearing in mind that there are many possible substantive variations on the basic theme?

First, I think it captures what Russell was looking for. The intelligent selection, rejection and weighting of the considerations that do occur to the subject is a matter of intelligence making the difference. Intelligence makes the difference here because an intelligent selection and assessment procedure determines which microscopic indeterminacies get amplified, as it were, into important macroscopic determiners of ultimate behavior.

Second, I think it installs indeterminism in the right place for the libertarian, if there is a right place at all. The libertarian could not have wanted to place the indeterminism _at the end_ of the agent’s assessment and deliberation. It would be insane to hope that after all rational deliberation had terminated with an assessment of the best available course of action, indeterminism would then intervene to flip the coin before action. It is a familiar theme in discussions of free will that the important claim that one could have done otherwise under the circumstances is not plausibly construed as the claim that one could have done otherwise given _exactly_ the set of convictions and desires that prevailed at the end of rational deliberation. So if there is to be a crucial undetermined nexus, it had better be prior to the final assessment of the considerations on the stage, which is right where we have located it.

Third, I think that the model is recommended by considerations that have little or nothing to do with the free will problem. It may well tum out to be that from the point of view of biological engineering, it is just more efficient and in the end more rational that decision-making should occur in this way. Time rushes on, and people must act, and there may not be time for a person to canvass all his beliefs, conduct all the investigations and experiments that he would see were relevant, assess every preference in his stock before acting, and it may be that the best way to prevent the inertia of Hamlet from overtaking us is for our decision-making processes to be expedited by a process of partially random generation and test. Even in the rare circumstances where we know there is, say, a decision procedure for determining the optimal solution to a decision problem, it is often more reasonable to proceed swiftly and by heuristic methods, and this strategic principle may in fact be incorporated as a design principle at a fairly fundamental level of cognitive-conative organization.

A fourth observation in favor of the model is that it permits moral education to make a difference, without making all of the difference. A familiar argument against the libertarian is that if our moral decisions were not in fact determined by our moral upbringing, or our moral education, there would be no point in providing such an education for the young. The libertarian who adopted our model could answer that a moral education, while not completely determining the generation of considerations and moral decision-making, can nevertheless have a prior selective effect on the sorts of considerations that will occur. A moral education, like mutual discussion and persuasion generally, could adjust the boundaries and probabilities of the generator without rendering it deterministic.

Fifth—and I think this is perhaps the most important thing to be said in favor of this model—it provides some account of our important intuition that we are the authors of our moral decisions. The unreflective compatibilist is apt to view decision-making on the model of a simple balance or scale on which the pros and cons of action are piled. What gets put on the scale is determined by one’s nature and one’s nurture, and once all the weights are placed, gravity as it were determines which way the scale will tip, and hence determines which way we will act. On such a view, the agent does not seem in any sense to be the author of the decisions, but at best merely the locus at which the environmental and genetic factors bearing on him interact to produce a decision. It all looks terribly mechanical and inevitable, and seems to leave no room for creativity or genius. The model proposed, however, holds out the promise of a distinction between authorship and mere implication in a causal chain.*]]

Consider in this light the difference between completing a lengthy exercise in long division and constructing a proof in, say, Euclidian geometry. There is a sense in which I can be the author of a particular bit of long division, and can take credit if it turns out to be correct, and can take pride in it as well, but there is a stronger sense in which I can claim authorship of a proof in geometry, even if thousands of school children before me have produced the very same proof. There is a sense in which this is something original that I have created. To take pride in one’s _computational accuracy_ is one thing, and to take pride in one’s _inventiveness_ is another, and as Valéry claimed, the essence of invention is the intelligent selection from among randomly generated candidates. I think that the sense in which we wish to claim authorship of our moral decisions, and hence claim responsibility for them, requires that we view them as products of intelligent invention, and not merely the results of an assiduous application of formulae. I don’t want to overstate this case; certainly many of the decisions we make are so obvious, so black and white, that no one would dream of claiming any special creativity in having made them and yet would still claim complete responsibility for the decisions thus rendered. But if we viewed all our decision-making on those lines, I think our sense of our dignity as moral agents would be considerably impoverished.

Finally, the model I propose points to the multiplicity of decisions that encircle our moral decisions and suggests that in many cases our ultimate decision as to which way to act is less important phenomenologically as a contributor to our sense of free will than the prior decisions affecting our deliberation process itself: the decision, for instance, not to consider any further, to terminate deliberation; or the decision to ignore certain lines of inquiry.

These prior and subsidiary decisions contribute, I think, to our sense of ourselves as responsible free agents, roughly in the following way: I am faced with an important decision to make, and after a certain amount of deliberation, I say to myself: “That’s enough. I’ve considered this matter enough and now I’m going to act,” in the full knowledge that I could have considered further, in the full knowledge that the eventualities may prove that I decided in error, but with the acceptance of responsibility in any case.

I have recounted six recommendations for the suggestion that human decision-making involves a non-deterministic generate-and-test procedure. First, it captures whatever is compelling in Russell’s hunch. Second, it installs determinism in the only plausible locus for libertarianism (something we have established by a process of elimination). Third, it makes sense from the point of view of strategies of biological engineering. Fourth, it provides a flexible justification of moral education. Fifth, it accounts at least in part for our sense of authorship of our decisions. Sixth, it acknowledges and explains the importance of decisions internal to the deliberation process. It is embarrassing to note, however, that the very feature of the model that inspired its promulgation is _apparently_ either gratuitous or misdescribed or both, and that is the causal indeterminacy of the generator. We have been supposing, for the sake of the libertarian, that the process that generates considerations for our assessment generates them at least in part by a physically or causally undetermined or random process. But here we seem to be trading on yet another imprecision or ambiguity in the word “random.” When a system designer or programmer relies on a “random” generation process, it is not a _physically undetermined_ process that is required, but simply a _patternless_ process. Computers are typically equipped with a random number generator, but the process that generates the sequence is a perfectly deterministic and determinate process. If it is a good random number generator (and designing one is extraordinarily difficult, it turns out) the sequence will be locally and globally patternless. There will be a complete absence of regularities on which to base predictions about unexamined portions of the sequence.

Isn’t it the case that the new improved proposed model for human deliberation can do as well with a random-but-deterministic generation process as with a causally undetermined process? Suppose that to the extent that the considerations that occur to me are unpredictable, they are unpredictable simply because they are fortuitously determined by some arbitrary and irrelevant factors, such as the location of the planets or what I had for breakfast. It appears that this alternative supposition diminishes not one whit the plausibility or utility of the model that I have proposed. Have we in fact given the libertarians what they really want without giving them indeterminism? Perhaps. We have given the libertarians the materials out of which to construct an account of personal authorship of moral decisions, and this is something that the compatibilistic views have never handled well. But something else has emerged as well. Just as the presence or absence of macroscopic indeterminism in the implementation style of intentional actions turned out to be something essentially undetectable from the vantage point of our _Lebenswelt_, a feature with no significant repercussions in the “manifest image,” to use Sellars’ term, so the rival descriptions of the consideration generator, as random-but-causally-deterministic _versus_ random-and-causally-_in_deterministic, will have no clearly testable and contrary implications at the level of micro-neurophysiology, even if we succeed beyond our most optimistic fantasies in mapping deliberation processes onto neural activity.

That fact does not refute libertarianism, or even discredit the motivation behind it, for what it shows once again is that we need not fear that causal indeterminism would make our lives unintelligible. There may not be compelling grounds from _this_ quarter for favoring an indeterministic vision of the springs of our action, but if considerations from other quarters favor indeterminism, we can at least be fairly sanguine about the prospects of incorporating indeterminism into our picture of deliberation, even if we cannot yet see what point such an incorporation would have. Wiggins speaks of the cosmic unfairness of determinism, and I do not think the considerations raised here do much to allay our worries about _that._ Even if one embraces the sort of view I have outlined, the deterministic view of the unbranching and inexorable history of the universe can inspire terror or despair, and perhaps the libertarian is right that there is no way to allay these feelings short of a brute denial of determinism. Perhaps such a denial, and only such a denial, would permit us to make sense of the notion that our actual lives are created by us over time out of possibilities that exist in virtue of our earlier decisions; that we trace a path through a branching maze that both defines who we are, and why, to some extent (if we are fortunate enough to maintain against all vicissitudes the integrity of our deliberational machinery) we are _responsible_ for being who we are. That prospect deserves an investigation of its own. All I hope to have shown here is that it is a prospect we can and should take seriously.

## Notes

*]] An incomplete list of the very different questions composing the free will problem: (1) How can a material thing (a mechanism?) be correctly said to reason, to have reasons, to act on reasons? (a question I attempt to answer in Chapter 12). (2) How can the unique four dimensional non-branching world-worm that comprises all that has happened and will happen admit of a notion of possibilities that are not actualities? What does an _opportunity_ look like when the world is viewed _sub specie aeternitatis_? (3) How can a person be an author of decisions, and not merely the locus of causal summation for external influences? (4) How can we make sense of the intuition that an agent can only be responsible if he could have done otherwise? (5) How can we intelligibly describe the relevant mental history of the truly culpable agent—the villain or rational cheat with no excuses? As Socrates asked, can a person knowingly commit evil?

*]] Quoted by Jacques Hadamard, in _The Psychology of Invention in the Mathematical Field,_ Princeton University Press, 1949, p. 30. I discuss the implications of Valéry’s claim in Chapter 5.

*]] The term is Herbert Simon’s. See his _The Sciences of the Artificial_ (1969) for a review of the concept.

*]] Cf. the suggestive discussion of genius in Kant’s _Critique of Judgment_, Sections 46, 47.

1.]] D. Wiggins, “Towards a Reasonable Libertarianism” in T. Honderich, ed., _Essays on Freedom of Action_ (London: Routledge & Kegan Paul, 1973).

2.]] Bertrand Russell, _Human Knowledge:_ _Its Scope and Limits_ (New York: Simon and Schuster, 1948), Chapter 15, “The Physiology of Sensation and Volition,” p. 54.

3.]] A. J. Ayer, “Freedom and Necessity,” in _Philosophical Essays_ (London: MacMillan, 1954); R. B. Hobart, “Free Will as Involving Determination and Inconceivable Without It,” _Mind_ (1934).

How to Change Your Mind

Annette Baier claims*]] that we can discover something important about the mind, something overlooked or denied in recent accounts, by examining a particular sort of episode in the natural history of minds, the sort of episode ordinarily called a _change of mind._ We can, she says, see more clearly what does and does not count as a _mind_ by seeing what does and does not count as a change of mind. We can understand thought by understanding second thoughts. I propose to extend her analysis of change of mind, to do some very impressionistic theory sketching and speculating, and in the process try to provide at least a partial explanation for some of her observations about the nature of change of mind.

Baier’s contention is that there is an important distinction to be drawn between those of us who can change our minds and other intelligent creatures. The conviction that _some_ such distinction is to be made is shared by several philosophers whose claims have long tempted me, without converting me until now. Norman Malcolm, in his APA Presidential Address of 1972, “Thoughtless Brutes,” claimed that there was an oft-neglected distinction between _thinking that p_ and _having the thought that p_; a dog may think a squirrel is up the tree, Malcolm allows, and this may explain its barking where it does, but it would be wrong, Malcolm thinks, to suppose the dog has the thought that the squirrel is up the tree. Thoughts _occur_ to people, but not to dogs. I think this is on the right track, or almost on the right track, and with help and inspiration from Baier, I will try to locate and travel down the _better_ track.

Ronald de Sousa, in a fascinating paper, “How to give a piece of your mind: or, the logic of belief and assent,” argues that we should distinguish sharply between what he calls _belief_ and _assent._*]] Belief, on his view, is a sort of lower, less intellectual phenomenon; it forms the dispositional foundation for the fancier phenomenon, assent, which is restricted to human beings. I think he is closer to the truth than Malcolm. Putting de Sousa’s ideas together with Baier’s seems to me to produce one of those mutual illuminations that gives off more than twice the light of the parts taken separately.

First we must recognize, as Baier says, that not every alteration in “cognitive state” is a change of mind. You were not born knowing that turtles can’t fly, it almost certainly never _occurred_ to you before this moment, but it is hardly news to you. You already knew it, but were not born knowing it, so there was some cognitive change in you in the interim; it was not a case of changing your mind, that’s for sure. Many other cognitive alterations are harder to distinguish from our targets, changes of mind, but Baier suggests that it will help if we look at the clearest cases: where one changes one’s mind or makes up one’s mind about _what to do_—the practical cases. I think she is right to point to these cases, for more reasons than she gives. I want to get at this rather indirectly by looking at a different distinction between types of cognitive transition. Consider for a moment the transition between what tradition would call _de dicto_ or _notional_ desire and _de re_ or _relational_ desire. As Quine memorably puts it, when one _wants a sloop_ in the former, notional sense, one’s desire does not link one to any particular boat; what one wants is mere _relief from slooplessness._ This is to be contrasted with wanting _de re that_ sloop, _Courageous_, the sloop tied up at the slip.

Now suppose I am in the former state, as in fact I often have been. Suppose I am _in the market_ for a sloop. Now let us see what happens if I pass from that state into the presumed state of desire _de re._ I start out wanting something much more specific than mere relief from slooplessness, of course. I want relief from 32-to-36-foot-wooden-diesel-auxilliary-spinnaker-rigged-slooplessness. And this is what I tell the boat broker. I give him a list of requirements. I want a sloop that is _F_, _G_, _H_, _I_, _J_, etc. My list is finite, but if I am wise I don’t declare it to be unrevisably complete when I give it to him. For suppose he then says: “I know just the boat _you want_; you want the _Molly B_, here.” The _Molly B_ is a sloop that is _F_, _G_, _H_, _I_, _J_, etc. Of course the salesman is pushing when he says this. He must mean I _will_ want the Molly B when I see it. Certainly it doesn’t follow from the fit between my requirement and the boat that I am already in a state of relational desire for it. I may have neglected to mention or even reflect upon the color of the boat I want, and if the _Molly B_ is painted purple, this may suffice to keep me out of the _de re_ state. On seeing the purple _Molly B_, I add, like Tigger: “Well, _not being purple_ was really one of my tacit requirements.” The boat broker sets out again with a clearer sense of what I desire _de dicto._ But even if the yacht he then presents me with is exactly what I had in mind, even if when I examine it I can find no disqualifying features, there is still a motion of the mind that must happen, and this is just what the broker tries so resourcefully to evoke in me. He’s got to get me to plump for it, to declare my love, to sign on the line. What he wants to stimulate in me though is not correctly viewed, I submit, as a change of desire or wanting. What he wants to stimulate in me is a _choosing_, a decision to opt for something. Once I have opted, in fact, I may get cold feet about the whole deal. Desire may drain out of me. Having made up my mind, not to desire the boat but to _buy_ it, I may begin having second thoughts. But having second thoughts in this sense is not changing my mind. That happens only if I back out of the deal and renege on a contract. Now several features of this special case seem to me to bear importantly on the general issue of change of mind.

First, in such a case my decision to buy the boat or not is not logically tied to the grounds on which it is made in the way that, arguably, other cognitive states and events can be logically tied. That is, there may be a rule requiring me to attribute to you a disbelief that _not-p_, if I attribute to you a belief that _p._ But there is nothing incoherent or logically odd about the description of me as desiring _de dicto_ a sloop that is _F_, _G_, _H_, _I_, _J_, and not desiring _de re_, the _Molly B_, which I even _believe_ to be _F_, _G_, _H_, _I_, _J_. Moreover it is least not obvious that in such a state I always warrant criticism. It is not obvious that I am clearly irrational. My bullheadedness or caution may in fact be a highly useful and adaptive feature of my cognitive make-up. More important to our concerns, I think, is the fact that although opting for the boat or not is _my_ decision, it is something _I do_, I don’t know in the end why I do it, what causes me to do it. I am not in a privileged position to tell someone else _exactly_ what prevented me from opting if l refrain or what tipped the balance if I buy. In this matter, my decision is the occasion of the discovery I make about myself. It is in this domain that E. M. Forster’s marvelous remark draws our attention to a truth. He once asked: “How can I tell what I think until I see what I say?”

The distinction between _de re_ and _de dicto_ propositional attitudes is a vexed one. I am in fact quite inclined to believe that nothing real or worth talking about deserves to be called either a _de re_ belief or a _de re_ desire. But that is a topic for another time. What interests me here is just that the case I’ve described of moving from _de dicto_ desire to an opting, a choice, has certain features in common with other nonconative cases of making up one’s mind. This decision is first of all an act, an exemplary case of doing something that has consequences for which one may be held responsible. Moreover, it bears the marks of freedom, such as they are. (I discuss these in Chapters 5 and 15.) Here, what is important is just that it is a choice point that terminates a process of deliberation or consideration that is not apparently algorithmic, but rather at best heuristic. At some point, we just stop deliberating. We take one last look at the pros and cons, and leap. As Baier says, the changes of belief that are cut and dried, the mere corrections having no taint of uncertainty, are not changes of mind.

Another important feature to note in our boat-buying example is that the result or product of this episode of choice is not the same sort of thing as the “raw material.” Although my opting arises from and is ultimately explained by my desires, it is not a desire, but a choice, and the state it initiates is not a state of desire, but of _commitment to acquire_ or something like that. This point sets the stage for de Sousa’s claims, for the parallel remark to make regarding all cases of making up or changing one’s mind is that changes of mind are a species of judgment, and while such judgments arise from beliefs and are ultimately to be explained by one’s beliefs, such judgments themselves are _not_ beliefs—when such judgments are called occurrent or episodic beliefs, this is a serious misnomer—but _acts_, and these acts initiate states that are also not states of belief, but of something rather like commitment, rather like ownership. I trust it sounds at least faintly paradoxical to claim that when I change my mind or make up my mind, the result is not a new belief at all, but this is just what I want to maintain, and so does de Sousa. He calls such judgings “assenting,” but is then left with no good term for the products of assent, the states one enters into as a result of such judging. I suggest that we would do quite well by ordinary usage if we called these states _opinions_, and hence sharply distinguished opinions from beliefs. Then we can immediately extend Malcolm’s train of thought in this way: animals may have _beliefs_ about this and that, but they don’t have _opinions._ They don’t have opinions because they don’t _assent._ Making up your mind is coming to have an opinion, I am proposing, and changing one’s mind is changing one’s opinion. Here is what de Sousa says (roughly) about the relationship between belief and assent (or now: opinion). On some theories of belief, he notes, the belief that _p_ is considered to admit of degree. One believes 0.9 that _p_ and so believes 0.1 that _not-p._ Bayesian accounts have this feature, for instance. Other accounts of belief—he calls them classical accounts—treat belief as all or nothing. One believes flat out that _p_ and hence disbelieves flat out that _not-p._ Now de Sousa’s interesting suggestion is that there ought to be no quarrel to adjudicate here, that the intuitions that support the latter variety of theory are not to be dismissed or overridden by intuitions that support the former. We should simply view the latter intuitions as about a different category of mental state, the state of assent, i.e., opinion, not belief. So de Sousa proposes a two-level theory for human beings (and other persons if such there be). We human beings are believers, as are the beasts. But moreover (and here he is echoing Hume, as we learn from Baier) we harbor epistemic desires. We are collectors, he says, of true sentences. We have a hunger for such items, which we add to our collections by what he calls “_a bet on truth alone_, solely determined by epistemic desirabilities.” He is careful to say that there is an analogy only, but a strong one, between betting and assenting. Now when a gambler bets, his wagers, if he is rational, are a function of the subjective probability of the outcome for him and the desirability of the payoff, or at least that’s the Bayesian line. This Bayesian line is applied or exploited by de Sousa to explain (or at least predict statistically) the acts of assent we will make given our animal-level beliefs and desires. We are equipped first with animal-type belief and desire, which behave in Bayesian fashion, and which explain our proclivity to make these leaps of assent, to act, to bet on the truth of various sentences.

Now of course subjective probabilities and degrees of desirability are not in any sense introspectable features of our beliefs or desires. That’s why we can’t calculate, on the basis of introspected data, a Bayesian prediction about _what we will decide._ We must wait and see what we decide. Some observant Bayesian psychologist might attribute weighted beliefs and desires to us, and on this basis predict our decisions; we might endorse those predictions, but not from any _privileged_ position. (By the way, it seems to me that this fact about non-introspectable subjective probabilities has an important and damaging implication for epistemological doctrines that pretend to enunciate principles about _what one ought to believe_ when one believes such and such with subjective probability _k_, as if they were giving us rules for the regulation of the understanding that we could actually attempt to follow.)

What is the role of language in the difference between belief and opinion? I'll suggest a few sketchy points. Belief, the lower brutish state, is best considered divorced from language. Robert Stalnaker suggests in “Propositions”[^1] that for the purposes of cognitive psychology, the task of which is to explain the behavior of both beast and man in terms of beliefs and desires, the objects of belief should be viewed as propositions, because the identity of propositions can be fixed, not by their being tied closely or even indirectly to sentences of a particular language, but by reference to the selective function of the state of belief in determining behavior. We want a way of speaking of this selective function that abstracts from particular languages. Fortunately, a manner of speaking is available: we say that a particular belief is a function taking possible worlds into truth values.

> First, the functional account, as a theory of rational action, already contains implicitly an intuitive notion of alternative possible courses of events. The picture of a rational agent deliberating is a picture of a man who considers various alternative possible futures, knowing that the one to become actual depends in part on his choice of action. The function of desire is simply to divide these alternative courses of events into the ones to be sought and the ones to be avoided, or in more sophisticated versions of the theory, to provide an ordering or measure of the alternative possibilities with respect to their desirability. The function of belief is simply to determine which are the relevant alternative possible situations, or in more sophisticated versions of the theory, to rank them with respect to their probability under various conditions of becoming actual.
> 
> If this is right, then the identity conditions for the objects for desire and belief are correctly determined by the possible-world account of propositions. That is, two sentences P and Q express the same proposition from the point of view of the possible-world theory if and only if a belief or desire that P necessarily functions exactly like a belief or desire that Q in the determination of any rational action. (p. 81)

Propositions thus viewed, Stalnaker observes, have no syntax, or structure, and this fits our intuitions about _belief_ so long as we keep belief firmly distinguished from opinion. Philosophers are forever taking as examples of belief such things as:

Tom believes that Don Larson pitched the only perfect game in World Series history

an example par excellence of a _sentence collected as true_ by Tom, not a belief in the basic sense at all.

Now why do we have opinions at all? And why do we have them while animals don’t? Because we have language. I think the way to look at it is this: once you have a language, there are all these sentences lying around, and you have to do something with them. You have to put them in boxes labeled “True” and “False” for one thing. In Chapter 3 (p. 52) I discuss an example from Chekhov’s _Three Sisters._ Tchebutykin is reading a newspaper and he mutters (_a propos_ of nothing, apparently), “Balzac was married in Berditchev,” and repeats it, saying he must make a note of it. Irina repeats it. Now did Tchebutykin believe it? Did Irina? One thing I know is that I have never forgotten the sentence. Without much conviction, I’d bet on its truth if the stakes were right, if I were on a quiz show for instance. Now my state with regard to this sentence is radically unlike my current state of perceptual belief, a state utterly unformulated into sentences or sentence-like things so far as common sense or introspection or casual analysis can tell. (That is, what the ultimate cognitive psychology might discover about the machinery of my information processing system is only very indirectly tied to this.)

Now then, what are the problems with the view that I have been sketching here, in my headlong rush to theory? Well, one might claim that de Sousa and I have got the whole matter completely backward. I agree with de Sousa that the realm of opinion is where the classical, all-or-nothing notion of belief really belongs, but isn’t it in fact our “intellectual” _opinions_ that are _most_ amenable to treatment in terms of degrees of confidence? I think this objection springs from an illusion. Remember my opting for the _Molly B._ My desire for this craft may subsequently wane following my decision to buy it. Similarly, my happiness with my own opinions may increase and diminish, but they are nevertheless the opinions I am _committed_ to unless I change my mind. I may express or exhibit less confidence in them, while not relinquishing them. Most importantly, I may fail to act on them as I would were my conviction unflagging. It is my beliefs and desires that predict my behavior _directly._ My opinions can be relied on to predict my behavior only to the degree, normally large, that my opinions and beliefs are in rational correspondence, i.e., roughly as Bayes would have them. It is just this feature of the distinction between opinion and belief that gives us, I think, the first steps of an acceptable account of those twin puzzles, self-deception and _akrasia_ or weakness of will. Animals, I submit, whatever their cognitive and conative frailties, are immune to both self-deception and akrasia. Why? Because they have only beliefs, not opinions, and _part_ of what is true when one exhibits either of these normal pathologies, self-deception or weakness of will (I think they may be just one affliction in the end), is that one _behaves_ one way while _judging_ another. One’s behavior is consonant with one’s beliefs “automatically,” for that is how in the end we individuate beliefs and actions. It is only because we have among our acts _acts of deeming true_, _acts of assent_, that these afflictions have a domain in which to operate.

There seems to be another problem with this account. I believe that Don Larson pitched the only perfect game in World Series history. My account would require that since this is one of those sentences latched onto and deemed true, it should count as an _opinion_, and not a belief, yet it doesn’t ring in the ear as ordinary usage to say that it is one of my opinions. It is not something, as Baier would point out, that I’ve _made up my mind about._ I think that’s right. It isn’t something I’ve made up my mind about. I think the way to handle such cases is dictated by considerations raised by de Sousa and also by Baier. Many sentences that are candidates for acquisition into my collection of truths are not at all dubious under the circumstances in which they first occur as candidates. This sentence about Don Larsen is surely one of them. No heuristic, informal deliberation or consideration or tealeaf gazing is required as a prelude to their acceptance. But that is just to say that there are _sure bets._ The outcome is so expectable and the stakes are so favorable, that they hardly count as bets at all. Recall the professional card player who says: “Oh I bet, but I never gamble.”

There are in any case many ways of adding to one’s collection of opinions, just as there are many ways of acquiring paintings or overcoats.*]] One can inherit them, fall into possession of them without noticing, fail to discard them after deciding to discard them, take them on temporary loan and forget that this is what one has done. For instance, one’s verbal indoctrination as a child—as an adult too—certainly has among its effects the inculcation of many ill-considered _dicta_ one will be willing to parade as true though one has never examined them. Habits of thought tied to well-turned phrases may persist long after one has denied the relevant assertions. One may suspend disbelief in a few enabling assumptions “for the sake of argument,” and become so engrossed in the argument that one forgets one has done this. Years later one may still be suspending disbelief in those assumptions—for no reason at all, save reasons that have lapsed in the interim. Losing is not discarding, and forgetting is not changing one’s mind, but it is a way of divesting oneself of an opinion. Why is Saul’s conversion on the road to Damascus not a change of mind? Ordinary language gives us a hint. His mind changes all right, but _he_ doesn’t change it; it is changed for him. It is not an act of his.

Baier points to the role of _other critics_ in changing one’s mind, and claims it is important to recognize this role. Why are critics important? Because one changes one’s own mind the way one changes somebody else’s: by an actual colloquy or soliloquy of persuasion (see Chapter 14). Note that in such an enterprise there can be success, or failure, or an intermediate result between success and failure. Understanding these intermediate results is important to understanding self-deception and _akrasia._ Surely the following has happened to you—it has happened to me many times: somebody corners me and proceeds to present me with an argument of great persuasiveness, of _irresistible_ logic, step by step by step. I can think of nothing to say against any of the steps. I get to the conclusion and can think of _no reasons_ to deny the conclusion, _but I don’t believe it_! This can be a social problem. It is worse than unsatisfying to say: “Sorry, I don’t believe it, but I can’t tell you why. I don’t know.” You might, depending on the circumstances, lie a little bit, nod and assent publicly while keeping your private disbelief to yourself, and it might not always be a craven or vicious thing to do. But I suggest that there is another thing that could happen. Genuine (de Sousian) assent, _inner_ assent if you like, can be wrung from you by such an argument so that the conclusion does become one of your _opinions—but you don’t believe it._ This is what happens, I think, when you follow an argument whose conclusion is that all things considered cigarette smoking is harmful, acquiesce in the conclusion (“Yes indeed, that conclusion falls in the set of true sentences.”), and then light up another cigarette. The _gap_ that must be located one place or another is any account of weakness of will is between what one has been provoked or goaded or enticed into _judging_ (quite sincerely) by such an act of persuasion (which might be self-persuasion), and one’s deeper behavior-disposing states—one’s beliefs. As many accounts would have it, weakness of will is exhibited when one acts intentionally against one’s better _judgment_, a claim that can be saved from incoherence if we distinguish sharply between such judgment and belief.

To return to an issue raised at the outset, is the ordinary distinction between changes of mind and other cognitive shifts an important distinction? Yes. It is very important, for only a theory of mind that makes change of mind and the resulting opinions a salient category can begin to account for self-deception and _akrasia_, two phenomena that are not just cognitive pathologies, but moral pathologies as well. If we are to explain how a person can be a moral agent, we must first grant that it is of the essence of personhood that a person can change his mind.

## Notes

*]] This chapter was prepared as a reply to an early draft of Annette Baier’s “Change of Mind,” delivered at the Chapel Hill Colloquium, October 16, 1977. The references in it to Baier’s essay do not prevent it from being understood by itself, or better yet it can be read as an introduction to Baier’s subsequent version of “Change of Mind” (titled “Mind and Change of Mind,” _Midwest Studies in Philosophy_, IV [1979]: 157–176).

*]] _Review of Metaphysics_, XXV (1971): 52–79. There is a terminological problem in de Sousa’s paper that I am going to pretend is not there. To evade exegetical problems I will claim that de Sousa says things he doesn’t quite say—but might agree with. Forewarned that my account of his paper is distorted, everyone should read it and see for themselves. Everyone should read it anyway, since it is an important paper, containing many more insights than I discuss here.

*]] Amelie Rorty provided many of the ideas in this paragraph.

1.]] In A. MacKay and D. Merrill, eds., _Issues in the Philosophy of Language_ (New Haven: Yale, 1976).

Where Am I?

Now that I’ve won my suit under the Freedom of Information Act, I am at liberty to reveal for the first time a curious episode in my life that may be of interest not only to those engaged in research in the philosophy of mind, artificial intelligence and neuroscience but also to the general public.

Several years ago I was approached by Pentagon officials who asked me to volunteer for a highly dangerous and secret mission. In collaboration with NASA and Howard Hughes, the Department of Defense was spending billions to develop a Supersonic Tunneling Underground Device, or STUD. It was supposed to tunnel through the earth’s core at great speed and deliver a specially designed atomic warhead “right up the Red’s missile silos,” as one of the Pentagon brass put it.

The problem was that in an early test they had succeeded in lodging a warhead about a mile deep under Tulsa, Oklahoma, and they wanted me to retrieve it for them. “Why me?” I asked. Well, the mission involved some pioneering applications of current brain research, and they had heard of my interest in brains and of course my Faustian curiosity and great courage and so forth … Well, how could I refuse? The difficulty that brought the Pentagon to my door was that the device I’d been asked to recover was fiercely radioactive, in a new way. According to monitoring instruments, something about the nature of the device and its complex interactions with pockets of material deep in the earth had produced radiation that could cause severe abnormalities in certain tissues of the brain. No way had been found to shield the brain from these deadly rays, which were apparently harmless to other tissues and organs of the body. So it had been decided that the person sent to recover the device should _leave his brain behind._ It would be kept in a safe place where it could execute its normal control functions by elaborate radio links. Would I submit to a surgical procedure that would completely remove my brain, which would then be placed in a life-support system at the Manned Spacecraft Center in Houston? Each input and output pathway, as it was severed, would be restored by a pair of microminiaturized radio transceivers, one attached precisely to the brain, the other to the nerve stumps in the empty cranium. No information would be lost, all the connectivity would be preserved. At first I was a bit reluctant. Would it really work? The Houston brain surgeons encouraged me. “Think of it,” they said, “as a mere _stretching_ of the nerves. If your brain were just moved over an _inch_ in your skull, that would not alter or impair your mind. We’re simply going to make the nerves indefinitely elastic by splicing radio links into them.”

I was shown around the life-support lab in Houston and saw the sparkling new vat in which my brain would be placed, were I to agree. I met the large and brilliant support team of neurologists, hematologists, biophysicists, and electrical engineers, and after several days of discussions and demonstrations, I agreed to give it a try. I was subjected to an enormous array of blood tests, brain scans, experiments, interviews, and the like. They took down my autobiography at great length, recorded tedious lists of my beliefs, hopes, fears, and tastes. They even listed my favorite stereo recordings and gave me a crash session of psychoanalysis.

The day for surgery arrived at last and of course I was anesthetized and remember nothing of the operation itself. When I came out of anesthesia, I opened my eyes, looked around, and asked the inevitable, the traditional, the lamentably hackneyed post-operative question: “Where am I?” The nurse smiled down at me. “You’re in Houston,” she said, and I reflected that this still had a good chance of being the truth one way or another. She handed me a mirror. Sure enough, there were the tiny antennae poking up through their titanium ports cemented into my skull.

“I gather the operation was a success,” I said, “I want to go see my brain.” They led me (I was a bit dizzy and unsteady) down a long corridor and into the life-support lab. A cheer went up from the assembled support team, and I responded with what I hoped was a jaunty salute. Still feeling lightheaded, I was helped over to the life-support vat. I peered through the glass. There, floating in what looked like ginger-ale, was undeniably a human brain, though it was almost covered with printed circuit chips, plastic tubules, electrodes, and other paraphernalia. “Is that mine?” I asked. “Hit the output transmitter switch there on the side of the vat and see for yourself,” the project director replied. I moved the switch to OFF, and immediately slumped, groggy and nauseated, into the arms of the technicians, one of whom kindly restored the switch to its ON position. While I recovered my equilibrium and composure, I thought to myself: “Well, here I am, sitting on a folding chair, staring through a piece of plate glass at my own brain … But wait,” I said to myself, “shouldn’t I have thought, ‘Here I am, suspended in a bubbling fluid, being stared at by my own eyes’?” I tried to think this latter thought. I tried to project it into the tank, offering it hopefully to my brain, but I failed to carry off the exercise with any conviction. I tried again. “Here am _I_, Daniel Dennett, suspended in a bubbling fluid, being stared at by my own eyes.” No, it just didn’t work. Most puzzling and confusing. Being a philosopher of firm physicalist conviction, I believed unswervingly that the tokening of my thoughts was occurring somewhere in my brain: yet, when I thought “Here I am,” where the thought occurred to me was _here,_ outside the vat, where I, Dennett, was standing staring at my brain.

I tried and tried to think myself into the vat, but to no avail. I tried to build up to the task by doing mental exercises. I thought to myself, “The sun is shining _over there_,” five times in rapid succession, each time mentally ostending a different place: in order, the sun-lit corner of the lab, the visible front lawn of the hospital, Houston, Mars, and Jupiter. I found I had little difficulty in getting my “there’s” to hop all over the celestial map with their proper references. I could loft a “there” in an instant through the farthest reaches of space, and then aim the next “there” with pinpoint accuracy at the upper left quadrant of a freckle on my arm. Why was I having such trouble with “here”? “Here in Houston” worked well enough, and so did “here in the lab,” and even “here in this part of the lab,” but “here in the vat” always seemed merely an unmeant mental mouthing. I tried closing my eyes while thinking it. This seemed to help, but still I couldn’t manage to pull it off, except perhaps for a fleeting instant. I couldn’t be sure. The discovery that I couldn’t be sure was also unsettling. How did I know _where_ I meant by “here” when I thought “here”? Could I _think_ I meant one place when in fact I meant another? I didn’t see how that could be admitted without untying the few bonds of intimacy between a person and his own mental life that had survived the onslaught of the brain scientists and philosophers, the physicalists and behaviorists. Perhaps I was incorrigible about where I _meant_ when I said “here.” But in my present circumstances it seemed that either I was doomed by sheer force of mental habit to thinking systematically false indexical thoughts, or where a person is (and hence where his thoughts are tokened for purposes of semantic analysis) is not necessarily where his brain, the physical seat of his soul, resides. Nagged by confusion, I attempted to orient myself by falling back on a favorite philosopher’s ploy. I began naming things.

“Yorick,” I said aloud to my brain, “you are my brain. The rest of my body, seated in this chair, I dub ‘Hamlet.’” So here we all are: Yorick’s my brain, Hamlet’s my body, and I am Dennett. _Now,_ where am I? And when I think “where am I?” where’s that thought tokened? Is it tokened in my brain, lounging about in the vat, or right here between my ears where it _seems_ to be tokened? Or nowhere? Its _temporal_ coordinates give me no trouble; must it not have spatial coordinates as well? I began making a list of the alternatives.

(1) _Where Hamlet goes, there goes Dennett._ This principle was easily refuted by appeal to the familiar brain transplant thought-experiments so enjoyed by philosophers. If Tom and Dick switch brains, Tom is the fellow with Dick’s former body—just ask him; he’ll claim to be Tom, and tell you the most intimate details of Tom’s autobiography. It was clear enough, then, that my current body and I could part company, but not likely that I could be separated from my brain. The rule of thumb that emerged so plainly from the thought experiments was that in a brain-transplant operation, one wanted to be the _donor_, not the recipient. Better to call such an operation a _body_-transplant, in fact. So perhaps the truth was,

(2) _Where Yorick goes, there goes Dennett._ This was not at all appealing, however. How could I be in the vat and not about to go anywhere, when I was so obviously outside the vat looking in and beginning to make guilty plans to return to my room for a substantial lunch? This begged the question I realized, but it still seemed to be getting at something important. Casting about for some support for my intuition, I hit upon a legalistic sort of argument that might have appealed to Locke.

Suppose, I argued to myself, I were now to fly to California, rob a bank, and be apprehended. In which state would I be tried: In California, where the robbery took place, or in Texas, where the brains of the outfit were located? Would I be a California felon with an out-of-state brain, or a Texas felon remotely controlling an accomplice of sorts in California? It seemed possible that I might beat such a rap just on the undecidability of that jurisdictional question, though perhaps it would be deemed an inter-state, and hence Federal, offense. In any event, suppose I were convicted. Was it likely that California would be satisfied to throw Hamlet into the brig, knowing that Yorick was living the good life and luxuriously taking the waters in Texas? Would Texas incarcerate Yorick, leaving Hamlet free to take the next boat to Rio? This alternative appealed to me. Barring capital punishment or other cruel and unusual punishment, the state would be obliged to maintain the life-support system for Yorick though they might move him from Houston to Leavenworth, and aside from the unpleasantness of the opprobrium, I, for one, would not mind at all and would consider myself a free man under those circumstances. If the state has an interest in forcibly relocating persons in institutions, it would fail to relocate me in any institution by locating Yorick there. If this were true, it suggested a third alternative.

(3) _Dennett is wherever he thinks he is._ Generalized, the claim was as follows: At any given time a person has a _point of view_, and the location of the point of view (which is determined internally by the content of the point of view) is also the location of the person.

Such a proposition is not without its perplexities, but to me it seemed a step in the right direction. The only trouble was that it seemed to place one in a heads-I-win/tails-you-lose situation of unlikely infallibility as regards location. Hadn’t I myself often been wrong about where I was, and at least as often uncertain? Couldn’t one get lost? Of course, but getting lost _geographically_ is not the only way one might get lost. If one were lost in the woods one could attempt to reassure oneself with the consolation that at least one knew where one was: one was right _here_ in the familiar surroundings of one’s own body. Perhaps in this case one would not have drawn one’s attention to much to be thankful for. Still, there were worse plights imaginable, and I wasn’t sure I wasn’t in such a plight right now.

Point of view clearly had something to do with personal location, but it was itself an unclear notion. It was obvious that the content of one’s point of view was not the same as or determined by the content of one’s beliefs or thoughts. For example, what should we say about the point of view of the Cinerama viewer who shrieks and twists in his seat as the roller-coaster footage overcomes his psychic distancing? Has he forgotten that he is safely seated in the theater? Here I was inclined to say that the person is experiencing an illusory shift in point of view. In other cases, my inclination to call such shifts illusory was less strong. The workers in laboratories and plants who handle dangerous materials by operating feedback-controlled mechanical arms and hands undergo a shift in point of view that is crisper and more pronounced than anything Cinerama can provoke. They can feel the heft and slipperiness of the containers they manipulate with their metal fingers. They know perfectly well where they are and are not fooled into false beliefs by the experience, yet it is as if they were inside the isolation chamber they are peering into. With mental effort, they can manage to shift their point of view back and forth, rather like making a transparent Neckar cube or an Escher drawing change orientation before one’s eyes. It does seem extravagant to suppose that in performing this bit of mental gymnastics, they are transporting _themselves_ back and forth.

Still their example gave me hope. If I was in fact in the vat in spite of my intuitions, I might be able to train myself to adopt that point of view even as a matter of habit. I should dwell on _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett of myself comfortably floating in my vat, beaming volitions to that familiar body _out there._ I reflected that the ease or difficulty of this task was presumably independent of the truth about the location of one’s brain. Had I been practicing before the operation, I might now be finding it second nature. You might now yourself try such a _tromp l’oeil_. Imagine you have written an inflammatory letter which has been published in the _Times_, the result of which is that the Government has chosen to impound your brain for a probationary period of three years in its Dangerous Brain Clinic in Bethesda, Maryland. Your body of course is allowed freedom to earn a salary and thus to continue its function of laying up income to be taxed. At this moment, however, your body is seated in an auditorium listening to a peculiar account by Daniel Dennett of his own similar experience. Try it. Think yourself to Bethesda, and then hark back longingly to your body, far away, and yet _seeming_ so near. It is only with long-distance restraint (yours? the Government’s?) that you can control your impulse to get those hands clapping in polite applause before navigating the old body to the rest room and a well-deserved glass of evening sherry in the lounge. The task of imagination is certainly difficult, but if you achieve your goal the results might be consoling.

Anyway, there I was in Houston, lost in thought as one might say, but not for long. My speculations were soon interrupted by the Houston doctors, who wished to test out my new prosthetic nervous system before sending me off on my hazardous mission. As I mentioned before, I was a bit dizzy at first, and not surprisingly, although I soon habituated myself to my new circumstances (which were, after all, well nigh indistinguishable from my old circumstances). My accommodation was not perfect, however, and to this day I continue to be plagued by minor coordination difficulties. The speed of light is fast, but finite, and as my brain and body move farther and farther apart, the delicate interaction of my feedback systems is thrown into disarray by the time lags. Just as one is rendered close to speechless by a delayed or echoic hearing of one’s speaking voice so, for instance, I am virtually unable to track a moving object with my eyes whenever my brain and my body are more than a few miles apart. In most matters my impairment is scarcely detectable, though I can no longer hit a slow curve ball with the authority of yore. There are some compensations of course. Though liquor tastes as good as ever, and warms my gullet while corroding my liver, I can drink it in any quantity I please, without becoming the slightest bit inebriated, a curiosity some of my close friends may have noticed (though I occasionally have _feigned_ inebriation, so as not to draw attention to my unusual circumstances). For similar reasons, I take aspirin orally for a sprained wrist, but if the pain persists I ask Houston to administer codeine to me _in vitro._ In times of illness the phone bill can be staggering.

But to return to my adventure. At length, both the doctors and I were satisfied that I was ready to undertake my subterranean mission. And so I left my brain in Houston and headed by helicopter for Tulsa. Well, in any case, that’s the way it seemed to me. That’s how I would put it, just off the top of my head as it were. On the trip I reflected further about my earlier anxieties and decided that my first postoperative speculations had been tinged with panic. The matter was not nearly as strange or metaphysical as I had been supposing. Where was I? In two places, clearly: both inside the vat and outside it. Just as one can stand with one foot in Connecticut and the other in Rhode Island, I was in two places at once. I had become one of those scattered individuals we used to hear so much about. The more I considered this answer, the more obviously true it appeared. But, strange to say, the more true it appeared, the less important the question to which it could be the true answer seemed. A sad, but not unprecedented, fate for a philosophical question to suffer. This answer did not completely satisfy me, of course. There lingered some question to which I should have liked an answer, which was neither “Where are all my various and sundry parts?” nor “What is my current point of view?” Or at least there seemed to be such a question. For it did seem undeniable that in some sense _I_ and not merely _most of me_ was descending into the earth under Tulsa in search of an atomic warhead.

When I found the warhead, I was certainly glad I had left my brain behind, for the pointer on the specially built Geiger counter I had brought with me was off the dial. I called Houston on my ordinary radio and told the operation control center of my position and my progress. In return, they gave me instructions for dismantling the vehicle, based upon my on-site observations. I had set to work with my cutting torch when all of a sudden a terrible thing happened. I went stone deaf. At first I thought it was only my radio earphones that had broken, but when I tapped on my helmet, I heard nothing. Apparently the auditory transceivers had gone on the fritz. I could no longer hear Houston or my own voice, but I could speak, so I started telling them what had happened. In mid-sentence, I knew something else had gone wrong. My vocal apparatus had become paralyzed. Then my right hand went limp—another transceiver had gone. I was truly in deep trouble. But worse was to follow. After a few more minutes, I went blind. I cursed my luck, and then I cursed the scientists who had led me into this grave peril. There I was, deaf, dumb, and blind, in a radioactive hole more than a mile under Tulsa. Then the last of my cerebral radio links broke, and suddenly I was faced with a new and even more shocking problem: whereas an instant before I had been buried alive in Oklahoma, now I was disembodied in Houston. My recognition of my new status was not immediate. It took me several very anxious minutes before it dawned on me that my poor body lay several hundred miles away, with heart pulsing and lungs respirating, but otherwise as dead as the body of any heart transplant donor, its skull packed with useless, broken electronic gear. The shift in perspective I had earlier found well nigh impossible now seemed quite natural. Though I could think myself back into my body in the tunnel under Tulsa, it took some effort to sustain the illusion. For surely it was an illusion to suppose I was still in Oklahoma: I had lost all contact with that body.

It occurred to me then, with one of those rushes of revelation of which we should be suspicious, that I had stumbled upon an impressive demonstration of the immateriality of the soul based upon physicalist principles and premises. For as the last radio signal between Tulsa and Houston died away, had I not changed location from Tulsa to Houston at the speed of light? And had I not accomplished this without any increase in mass? What moved from A to B at such speed was surely myself, or at any rate my soul or mind—the massless center of my being and home of my consciousness. My _point of view_ had lagged somewhat behind, but I had already noted the indirect bearing of point of view on personal location. I could not see how a physicalist philosopher could quarrel with this except by taking the dire and counter-intuitive route of banishing all talk of persons. Yet the notion of personhood was so well entrenched in everyone’s world view, or so it seemed to me, that any denial would be as curiously unconvincing, as systematically disingenuous, as the Cartesian negation, “non sum.”[^1]

The joy of philosophic discovery thus tided me over some very bad minutes or perhaps hours as the helplessness and hopelessness of my situation became more apparent to me. Waves of panic and even nausea swept over me, made all the more horrible by the absence of their normal body-dependent phenomenology. No adrenaline rush of tingles in the arms, no pounding heart, no premonitory salivation. I did feel a dread sinking feeling in my bowels at one point, and this tricked me momentarily into the false hope that I was undergoing a reversal of the process that landed me in this fix—a gradual undisembodiment. But the isolation and uniqueness of that twinge soon convinced me that it was simply the first of a plague of phantom body hallucinations that I, like any other amputee, would be all too likely to suffer.

My mood then was chaotic. On the one hand, I was fired up with elation at my philosophic discovery and was wracking my brain (one of the few familiar things I could still do), trying to figure out how to communicate my discovery to the journals; while on the other, I was bitter, lonely, and filled with dread and uncertainty. Fortunately, this did not last long, for my technical support team sedated me into a dreamless sleep from which I awoke, hearing with magnificent fidelity the familiar opening strains of my favorite Brahms piano trio. So that was why they had wanted a list of my favorite recordings! It did not take me long to realize that I was hearing the music without ears. The output from the stereo stylus was being fed through some fancy rectification circuitry directly into my auditory nerve. I was mainlining Brahms, an unforgettable experience for any stereo buff. At the end of the record it did not surprise me to hear the reassuring voice of the project director speaking into a microphone that was now my prosthetic ear. He confirmed my analysis of what had gone wrong and assured me that steps were being taken to re-embody me. He did not elaborate, and after a few more recordings, I found myself drifting off to sleep. My sleep lasted, I later learned, for the better part of a year, and when I awoke, it was to find myself fully restored to my senses. When I looked into the mirror, though, I was a bit startled to see an unfamiliar face. Bearded and a bit heavier, bearing no doubt a family resemblance to my former face, and with the same look of spritely intelligence and resolute character, but definitely a new face. Further self-explorations of an intimate nature left me no doubt that this was a new body and the project director confirmed my conclusions. He did not volunteer any information on the past history of my new body and I decided (wisely, I think in retrospect) not to pry. As many philosophers unfamiliar with my ordeal have more recently speculated, the acquisition of a new body leaves one’s _person_ intact. And after a period of adjustment to a new voice, new muscular strengths and weaknesses, and so forth, one’s _personality_ is by and large also preserved. More dramatic changes in personality have been routinely observed in people who have undergone extensive plastic surgery, to say nothing of sex change operations, and I think no one contests the survival of the person in such cases. In any event I soon accommodated to my new body, to the point of being unable to recover any of its novelties to my consciousness or even memory. The view in the mirror soon became utterly familiar. That view, by the way, still revealed antennae, and so I was not surprised to learn that my brain had not been moved from its haven in the life-support lab.

I decided that good old Yorick deserved a visit. I and my new body, whom we might as well call Fortinbras, strode into the familiar lab to another round of applause from the technicians, who were of course congratulating themselves, not me. Once more I stood before the vat and contemplated poor Yorick, and on a whim I once again cavalierly flicked off the output transmitter switch. Imagine my surprise when nothing unusual happened. No fainting spell, no nausea, no noticeable change. A technician hurried to restore the switch to ON, but still I felt nothing. I demanded an explanation, which the project director hastened to provide. It seems that before they had even operated on the first occasion, they had constructed a computer duplicate of my brain, reproducing both the complete information processing structure and the computational speed of my brain in a giant computer program. After the operation, but before they had dared to send me off on my mission to Oklahoma, they had run this computer system and Yorick side by side. The incoming signals from Hamlet were sent simultaneously to Yorick’s transceivers and to the computer’s array of inputs. And the outputs from Yorick were not only beamed back to Hamlet, my body; they were recorded and checked against the simultaneous output of the computer program, which was called “Hubert” for reasons obscure to me. Over days and even weeks, the outputs were identical and synchronous, which of course did not _prove_ that they had succeeded in copying the brain’s functional structure, but the empirical support was greatly encouraging.

Hubert’s input, and hence activity, had been kept parallel with Yorick’s during my disembodied days. And now, to demonstrate this, they had actually thrown the master switch that put Hubert for the first time in on-line control of my body—not Hamlet, of course, but Fortinbras. (Hamlet, I learned, had never been recovered from its underground tomb and could be assumed by this time to have largely returned to the dust. At the head of my grave still lay the magnificent bulk of the abandoned device, with the word STUD emblazoned on its side in large letters—a circumstance which may provide archeologists of the next century with a curious insight into the burial rites of their ancestors.)

The laboratory technicians now showed me the master switch, which had two positions, labeled _B_, for Brain (they didn’t know my brain ‘s name was Yorick) and _H_, for Hubert. The switch did indeed point to _H_, and they explained to me that if I wished, I could switch it back to _B._ With my heart in my mouth (and my brain in its vat), I did this. Nothing happened. A click, that was all. To test their claim, and with the master switch now set at _B_, I hit Yorick’s output transmitter switch on the vat and sure enough, I began to faint. Once the output switch was turned back on and I had recovered my wits, so to speak, I continued to play with the master switch, flipping it back and forth. I found that with the exception of the transitional click, I could detect no trace of a difference. I could switch in mid-utterance, and the sentence I had begun speaking under the control of Yorick was finished without a pause or hitch of any kind under the control of Hubert. I had a spare brain, a prosthetic device which might someday stand me in very good stead, were some mishap to befall Yorick. Or alternatively, I could keep Yorick as a spare and use Hubert. It didn’t seem to make any difference which I chose, for the wear and tear and fatigue on my body did not have any debilitating effect on either brain, whether or not it was actually causing the motions of my body, or merely spilling its output into thin air.

The one truly unsettling aspect of this new development was the prospect, which was not long in dawning on me, of someone detaching the spare—Hubert or Yorick, as the case might be—from Fortinbras and hitching it to yet another body—some Johnny-come-lately Rosencrantz or Guildenstern. Then (if not before) there would be _two_ people, that much was clear. One would be me, and the other would be a sort of super-twin brother. If there were two bodies, one under the control of Hubert and the other being controlled by Yorick, then which would the world recognize as the true Dennett? And whatever the rest of the world decided, which one would be _me?_ Would I be the Yorick-brained one, in virtue of Yorick’s causal priority and former intimate relationship with the original Dennett body, Hamlet? That seemed a bit legalistic, a bit too redolent of the arbitrariness of consanguinity and legal possession, to be convincing at the metaphysical level. For, suppose that before the arrival of the second body on the scene, I had been keeping Yorick as the spare for years, and letting Hubert’s output drive my body—that is, Fortinbras—all that time. The Hubert-Fortinbras couple would seem then by squatter’s rights (to combat one legal intuition with another) to be the true Dennett and the lawful inheritor of everything that was Dennett’s. This was an interesting question, certainly, but not nearly so pressing as another question that bothered me. My strongest intuition was that in such an eventuality _I_ would survive so long as _either_ brain-body couple remained intact, but I had mixed emotions about whether I should want both to survive.

I discussed my worries with the technicians and the project director. The prospect of two Dennetts was abhorrent to me, I explained, largely for social reasons. I didn’t want to be my own rival for the affections of my wife, nor did I like the prospect of the two Dennetts sharing my modest professor’s salary. Still more vertiginous and distasteful, though, was the idea of knowing _that much_ about another person, while he had the very same goods on me. How could we ever face each other? My colleagues in the lab argued that I was ignoring the bright side of the matter. Weren’t there many things I wanted to do but, being only one person, had been unable to do? Now one Dennett could stay at home and be the professor and family man, while the other could strike out on a life of travel and adventure—missing the family of course, but happy in the knowledge that the other Dennett was keeping the home fires burning. I could be faithful and adulterous at the same time. I could even cuckold myself—to say nothing of other more lurid possibilities my colleagues were all too ready to force upon my overtaxed imagination. But my ordeal in Oklahoma (or was it Houston?) had made me less adventurous, and I shrank from this opportunity that was being offered (though of course I was never quite sure it was being offered to _me_ in the first place).

There was another prospect even more disagreeable—that the spare, Hubert or Yorick as the case might be, would be detached from any input from Fortinbras and just left detached. Then, as in the other case, there would be two Dennetts, or at least two claimants to my name and possessions, one embodied in Fortinbras, and the other sadly, miserably disembodied. Both selfishness and altruism bade me take steps to prevent this from happening. So I asked that measures be taken to ensure that no one could ever tamper with the transceiver connections or the master switch without my (our? no, _my_) knowledge and consent. Since I had no desire to spend my life guarding the equipment in Houston, it was mutually decided that all the electronic connections in the lab would be carefully locked: both those that controlled the life-support system for Yorick and those that controlled the power supply for Hubert would be guarded with fail-safe devices, and I would take the only master switch, outfitted for radio remote control, with me wherever I went. I carry it strapped around my waist and—wait a moment—_here it is._ Every few months I reconnoiter the situation by switching channels. I do this only in the presence of friends of course, for if the other channel were, heaven forbid, either dead or otherwise occupied, there would have to be somebody who had my interests at heart to switch it back, to bring me back from the void. For while I could feel, see, hear and otherwise sense whatever befell my body, subsequent to such a switch, I’d be unable to control it. By the way, the two positions on the switch are intentionally unmarked, so I never have the faintest idea whether I am switching from Hubert to Yorick or _vice versa._ (Some of you may think that in this case I really don’t know _who_ I am, let alone where I am. But such reflections no longer make much of a dent on my essential Dennettness, on my own sense of who I am. If it is true that in one sense I don’t know who l am then that’s another one of your philosophical truths of underwhelming significance.)

In any case, every time I’ve flipped the switch so far, nothing has happened. _So let’s give it a try_ …

“THANK GOD! I THOUGHT YOU’D NEVER FLIP THAT SWITCH! You can’t imagine how horrible it’s been these last two weeks—but now you know, it’s your tum in purgatory. How I’ve longed for this moment! You see, about two weeks ago—excuse me, ladies and gentlemen, but I’ve got to explain this to my … um, brother, I guess you could say, but he’s just told you the facts, so you’ll understand—about two weeks ago our two brains drifted just a bit out of synch. I don’t know whether _my_ brain is now Hubert or Yorick, any more than you do, but in any case, the two brains drifted apart, and of course once the process started, it snowballed, for I was in a slightly different receptive state for the input we both received, a difference that was soon magnified. In no time at all the illusion that I was in control of my body—our body—was completely dissipated. There was nothing I could do—no way to call you. YOU DIDN’T EVEN KNOW I EXISTED! It’s been like being carried around in a cage, or better, like being possessed—hearing my own voice say things I didn’t mean to say, watching in frustration as my own hands performed deeds I hadn’t intended. You’d scratch our itches, but not the way I would have, and you kept me awake, with your tossing and turning. I’ve been totally exhausted, on the verge of a nervous breakdown, carried around helplessly by your frantic round of activities, sustained only by the knowledge that someday you’d throw the switch.

“Now it’s your turn, but at least you’ll have the comfort of knowing _I_ know you’re in there. Like an expectant mother, I’m eating—or at any rate tasting, smelling, seeing—for _two_ now, and I’ll try to make it easy for you. Don’t worry. Just as soon as this colloquium is over, you and I will fly to Houston, and we’ll see what can be done to get one of us another body. You can have a female body—your body could be any color you like. But let’s think it over. I tell you what—to be fair, if we both want this body, I promise I’ll let the project director flip a coin to settle which of us gets to keep it and which then gets to choose a new body. That should guarantee justice, shouldn’t it? In any case, I’ll take care of you, I promise. These people are my witnesses.

“Ladies and gentlemen, this talk we have just heard is not exactly the talk _I_ would have given, but I assure you that everything he said was perfectly true. And now if you’ll excuse me, I think I’d—we’d—better sit down.”[^2]

## Notes

1.]] C.f., Jaakko Hintikka, “Cogito ergo sum: Inference or Performance?” _Philosophical Review_, LXXI, 1962, pp. 3–32.

2.]] Anyone familiar with the literature on this topic will recognize that my remarks owe a great deal to the explorations of Sydney Shoemaker, John Perry, David Lewis and Derek Parfit, and in particular to their papers in Amelie Rorty, ed., _The Identities of Persons_, 1976.Acknowledgments

1. “Intentional Systems” is reprinted from _Journal of Philosophy_, LXVIII, 4 (1971): 87–106 with permission of the editors. Earlier drafts were presented at Princeton and the University of Massachusetts at Amherst, December, 1970.
2. “Reply to Arbib and Gunderson” has not been previously published, but an abstract appears in _Journal of Philosophy_, LXIX, 18 (1972): 604, following the papers by Arbib and Gunderson. It was presented at a symposium on _Content and Consciousness_ at the American Philosophical Association Eastern Division Meeting, December, 1972.
3. “Brain Writing and Mind Reading” is reprinted with permission of the University of Minnesota Press from Keith Gunderson, ed., _Language, Mind, and Knowledge_, _Minnesota Studies in the Philosophy of Science_ VII (1975). Earlier drafts were presented at Tufts, the University of Maine, and the Cincinnati Colloquium on Mind and Brain, November, 1971.
4. “Skinner Skinned” has not been previously published. Earlier versions were presented at Tuskegee Institute, March, 1972; University of Tennessee, November, 1973; Western Michigan University, December, 1974; Lycoming College, November, 1975; and as one of my Taft Lectures at the University of Cincinnati, February, 1978.
5. “Why the Law of Effect Will Not Go Away” is reprinted with permission of the editors from the _Journal of the Theory of Social Behavior_, V, 2 (1975): 169–87. It was first presented at the first meeting of the Society for Philosophy and Psychology, at M.I.T. in October, 1974. Subsequent drafts were presented at UCLA, Brigham Young University, Lycoming College, and the Harvard Society for Fellows in Law and Humanities.
6. “A Cure for the Common Code?” is reprinted with the permission of the editors from _Mind_ (April, 1977), where it was titled simply “Critical Notice: _The Language of Thought_ by Jerry Fodor.” Under its present title it circulated in typescript during 1976.
7. “Artificial Intelligence as Philosophy and as Psychology” is reprinted with the permission of Humanities Press and Harvester Press from Martin Ringle, ed., _Philosophical Perspectives on Artificial Intelligence_ (New York: Humanities Press and Harvester Press, 1978). Earlier drafts were presented at Carnegie Mellon University; SUNY New Paltz Conference on Artificial Intelligence and Philosophy (organized by the Society for the Interdisciplinary Study of the Mind), March, 1977; Hampshire College, March, 1977; the University of North Carolina at Greensboro, April, 1977; University of Alberta, November, 1977; Simon Fraser University, November, 1977; University of Calgary, November, 1977; Rutgers University, March, 1978.
8. “Are Dreams Experiences?” is reprinted with permission of the editors from _Philosophical Review_, LXXIII (April, 1976): 151–171. Earlier drafts were presented at Harvard, February, 1975; U.C. Irvine, March, 1975; Brigham Young University, March, 1975; University of Cincinnati, May, 1975; Vassar College, November, 1975; Lycoming College, November, 1975. It is also reprinted in Charles Dunlop, ed., _Philosophical Essays on Dreaming_, Cornell Univ. Press, 1977.
9. “Toward a Cognitive Theory of Consciousness” is reprinted with permission of the University of Minnesota Press from C. Wade Savage, ed., _Perception and Cognition: Issues in the Foundations of Psychology, Minnesota Studies in the Philosophy of Science_, IX (1978). It was first presented at the Conference on Perception, Cognition, Knowledge and Mind at the University of Minnesota, June, 1975. Subsequent drafts were presented at Cornell University, December, 1975; and at SUNY Binghamton, February, 1976.
10. “Two Approaches to Mental Images” has not been previously published. It was first presented at the Western Canadian Philosophical Association at the University of Manitoba, Winnipeg, October, 1977, and subsequently at the University of Victoria, November, 1977; University of British Columbia, November, 1977; University of Washington, November, 1977, and Brown, December, 1977. A revised version was presented as one of my Taft Lectures at the University of Cincinnati, February, 1978; Sloan Foundation Workshop on Mental Representation, M.I.T., January, 1978; University of Virginia, February, 1978; Columbia University, March, 1978, and the University of Alabama, April, 1978.
11. “Why You Can’t Make a Computer That Feels Pain” is reprinted with permission of D. Reidel Publishing Company from _Synthese_, XXXVIII, 3 (July, 1978). Early drafts were presented at the University of Chicago, February, 1974; Western Michigan University, December, 1974, and Boston University, December, 1976.
12. “Mechanism and Responsibility” is reprinted with permission from Ted Honderich, ed., _Essays on Freedom of Action_ (Routledge & Kegan Paul, 1973). A draft was presented at Yale University, December, 1971.
13. “The Abilities of Men and Machines” has not been previously published. An Abstract appeared in _Journal of Philosophy_, LXVII, 20 (1970): 835. It was presented at the American Philosophical Association Eastern Division Meeting, December, 1970.
14. “Conditions of Personhood” is reprinted with permission from Amelie O. Rorty, ed., _The Identities of Persons_ (University of California Press, 1976). Earlier drafts were presented at the University of Pittsburgh, April, 1972; University of Connecticut, April, 1972; Wellesley College, November, 1972; Harvard University, December, 1972; Rockefeller University, March, 1973, and Temple University, March, 1973.
15. “On Giving Libertarians What They Say They Want” has not been previously published. Earlier drafts were presented at Princeton University, March, 1976, and Williams College, April, 1976.
16. “How to Change Your Mind” has not been previously published. It was presented as a reply to Annette Baier at the Chapel Hill Colloquium, October, 1977.
17. “Where Am I?” has not been previously published. It was presented first at the Chapel Hill Colloquium, October, 1976, and subsequently at M.I.T., in December, 1976, and at the University of Alabama, April, 1978.
18. The passage on pp. 71–72 from _The Machinery of the Brain_, by Dean Wooldridge, Copyright 1963, McGraw Hill, used with permission of the McGraw Hill Book Company.
19. The illustrations on pp. 130–132 from _Artificial Paranoia_, by Kenneth Colby, used with permission of Pergamon Press Ltd.
20. The illustration on p. 182 used with permission of Roger Shepard.Index

- Absurd world, 81
- Access, 146. _See also_ Personal consciousness, access of
    - computational, 164–168, 173
    - introspective, 174
    - to oneself, 305
    - to pain, 245
    - privileged, 32, 33, 38, 40, 246n
    - public, 165, 166
- Action(s), 9, 15, 32, 69, 79–80, 281–282, 313
    - awareness of and responsibility for, 303–305, 308, 309, 314
    - considered, 107, 108
    - free will vs. determinism and, 308–310, 314
    - intelligible, 309–310
    - libertarianism and, 308, 309
    - vs. mere happening, 264, 265
    - plans of, 88
    - predictable, 310
    - random, 309
    - reasons for, 265, 303–304
- Action organizing unit, 222
- Action-potentials, 161n
- Acupuncture, 224n
- Adaptive potential behavior control elements, 88
- Adaptive potential behavior controls
- After-_resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, 21
- Agent, 80, 107–108, 316. _See also_ Morally responsible agents
    - exempt, 111–112
    - free, xx, xxxiv
- AI. _See_ Artificial intelligence
- Akrasia, 330–332
- Algorithm, 102, 285–286, 314, 326
- Alien entities, desire attributed to, 7–10, 292
- Alien intelligence, 214
- Amnesia, 270
    - analgesia and, 228n, 230, 233, 240
    - anesthesia and, 230, 233
    - dream, 144 (_see also_ Dreams)
    - lexical, 240
- Analgesia, 229–234
    - and amnesia, 228n, 230, 233, 240
    - non-drug, 224
- Analgesics, 226, 240. _See also_ Morphine
    - defined, 240
- Analogue computer, 26
- Anesthesia, 224n. _See also_ Analgesia
    - and amnesia, 230, 233
    - barbiturate, 227, 229, 230, 232
    - feigning, 164
    - general, 227–236, 247, 334
    - local, 225–227
- Animals, 290, 295. _See also_ Wasps
    - desires of, 10–11, 16, 292, 294, 297, 305n, 327
    - intentional systems and, 11, 294–295
- Anscombe, G. E. M., 44, 47, 185, 239, 266, 297, 300, 301
    - H. Frankfurt and, 291
    - _Intention_, 31n, 43, 291
- Anthropology, 197–200
- Anthropomorphizing, 9, 89
- Aphasia, Wernicke’s, 127
- Apprehension, 191–193
- Arbib, Michael, 25, 28, 33–36, 40, 111, 177
- Argument presented to someone, 266, 331–332
- Aristotle, 31n, 141, 290, 314
- Armstrong, D. M., 192–193, 204–205
- Artificial intelligence (AI), xxxiv. _See also_ Face-recognizers
    - AI research, 27, 88
    - behaviorism and, 88–91, 95, 123, 129
    - Church’s Thesis and, 91, 92, 122
    - cognitive psychology and, 88, 122, 124
    - computers and, 89, 92, 120, 124–129
    - computer simulation and, 27, 90n
    - criticism and skepticism regarding, 112
    - data structures and, 133–135
    - design and, 88–93, 122–123, 128
    - epistemology and, 122–124, 127, 136
    - Fodor and, 100
    - generate-and-test, 90–93, 95, 98n
    - genuine intelligence and, 214
    - goals, 124
    - homunculus and, 134–135, 178
    - information and, 128, 129, 135
    - introspection and, 120, 129
    - and the Law of Effect, 90, 95
    - learning and, 90–95
    - meanings and connotations of the term, 214
    - mechanism and, 89, 90n, 91–92, 122
    - natural language and, 126, 127
    - nature of, 214
    - as philosophy and psychology, 119–137
    - problem-solving and, 89, 90, 129, 133, 134n, 136
    - recognition and, 71, 89, 128
    - robots and, 115
    - top-down strategy and, 122, 124, 358n3
    - triumphs, failures, and false starts, 207, 284–286
- Ashley, Peter, 295, 296, 299
- Aspirin, 226, 237, 238, 339
- Assent, 324, 331, 332
    - acts of, 330
- Associationism, 111, 133
- Attention
    - allocation of cognitive resources and, 171
    - conscious vs. unconscious, 171
    - disattracting vs. attracting, 34
    - distracted, 220
    - focusing attention on one’s pains
    - notions of, 171, 185
    - objects of, 34, 235, 236
- Augustine of Hippo, Saint, 98
- Automaton, 98, 279
    - non-deterministic, 94, 98
- Autophenomenology, 201
- Awareness, 33, 34, 96, 141, 173, 180, 182, 184n, 301, 303–306. _See also_ Action(s), awareness of and responsibility for; Consciousness
    - aware1, 33, 35, 37–39
    - aware1.5, 33, 34, 36–40
    - aware2, 33, 34
    - defined, 33
- Ayer, A. J., 314

- Baier, Annette, xx, 186, 323–324, 326, 327, 330, 331
- Barbiturates, 227, 229, 230, 232
- Barnette, R., xx
- Bayes, Thomas, 117, 330
- Beecher, Henry K., 241n
- Begging the question, 63, 64, 81–83, 91–92, 95, 122
- Behavior, theories of, 10, 13, 16, 252
- Behavioral controls, unuitilized, 85
- Behavioral control systems, 33, 34. _See also_ Behavior-controlling system; Behavior control systems
- Behavior control elements, adaptive potential, 88
- Behavior-controlling system, 159. _See also_ Behavioral control systems; Behavior control systems
- Behavior control(s), 33–34, 73, 83, 87
- Behavior control systems, 68. _See also_ Behavioral control systems; Behavior-controlling system; Control systems
    - potential, 87
- Behaviorism, xxxiv, 13, 95, 120, 293, 294, 296. _See also_ Reinforcement; Skinner, B. F.
    - artificial intelligence (AI) and, 88–91, 95, 123, 129
    - criticisms of and resistance/opposition to, 59, 83, 97, 98n, 107, 296
    - curiosity and, 63
    - Fodor and, 101, 103–105, 107
    - intentionality and, 16, 296
    - Koestler and, 96n, 98n
    - Law of Effect and, 79, 82–83, 86, 90, 97
    - logical, xxiv, 101, 103, 105, 228n
    - logical internal, 204
    - mentalism and, 61, 63, 80, 104–105 (_see also_ Mentalism)
    - neo-cognitivism and, 101
    - Ryle and, 101, 103–105
- “Behaviorism at Fifty” (Skinner), 64–65, 72, 356n36. _See also_ Skinner, B. F.
- Behaviorists, 123, 129, 164, 238, 296, 335. _See also_ Behaviorism
    - representations and, 45
    - Skinner and, 61 (_see also_ Skinner, B. F.)
- Belief-analogues, 7, 10
- Belief(s), xxxi, 324–332. _See also specific topics_
    - of computers, 7, 65–66, 292
    - epistemic credentials of, 195, 196
    - intentional systems and, 7, 9–12, 15–23, 30
    - intuitions and, 293, 329
    - judgment and, 51, 53–54, 326, 332
    - language and, xvi, 53, 295, 328, 329
    - language of thought and, 112
    - manifest, 9, 44, 46–48
    - vs. opinion, 328, 329
    - phenomenology and, 35
    - point of view and, 337
    - prediction and, 9–11, 294, 327–328, 330
    - rationality and, 19–22, 267–271
    - reasoning and, 136–137
    - reinforcement and, 69
    - Stalnaker on propositions and, 328–329
    - systematically illusory, 202–203
    - theories of, 22, 327
- Bender, John, 186
- Bennett, J., 260n
- Blackburn, Thomas, 186
- Block, N., xx
- Boden, Margaret, 137n
- Bottom-up strategy, 120
- Bradykinin, 226, 237
- Brain. _See_ Neurobiology; Neurophysiology
- Brain surgery, 270, 271, 274
- Brainwashing, 268–271, 274
- Brain writing, xxxiii, 44–55, 103, 116
    - grammar of, 46, 49, 53
    - language and, 46, 51
- Brazier, M., 233
- Brentano, Franz, 3, 23, 103, 196, 253
- Brentano’s Thesis, xxx
- Broadbent, B., 79–81, 173
- Buber, Martin, 261
- Bullock, T. H., 47n
- Buridan’s ass, 312

- Canon of Parsimony, 294, 356n36
- Carr, C. J., 230n
- Carroll, Lewis, 11n, 50
- Cartesian interactionism, 61, 191n, 341
- Cassette theory of dreams, 148–154, 156, 157
- Category mistake, 238
- Causal chains, 266, 312–313, 318
- Causal determinism/causal determinacy, 262, 308–309
- Causal explanation, 104, 252, 253
    - vs. conceptual explanation, 104, 105, 116
- Causal indeterminism/causal indeterminacy, 319, 320
- Causally undetermined processes, 262, 309, 319, 320
- Causal sequences, 29
- Causal theories, 195–196
- Causation, 95, 181, 253–254
    - Fodor and, 104, 105, 108–109, 116
    - Hume on, 181, 253, 254
    - mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett as, 190–202, 204
    - Skinner and, 64
- Causes and effects of pain, 209, 213, 235–239, 241, 243, 244
- C-fibers, 167, 209–210, 217, 224, 237
- Change of mind, 323–332
- Chekhov, Anton, 52, 329
- Chess, 22–23, 285
- Chess algorithms, 285–286
- Chess-playing computers, 20, 34
    - artificial intelligence and, 16
    - design of, 117, 165, 258
    - intentional systems, intentionality, and, 3, 6, 7, 74, 255, 258, 259, 292, 302
    - learning as they play, 75
    - mentalistic terms and, 65–66, 70–71
    - nature of, 7, 8
    - predicting, 5, 6, 23, 255–256
    - rationality and, 5–6, 8, 10, 256, 258–260, 292, 302
    - Skinner and, 65, 71, 74–75
    - stances and, 3–8, 255, 256, 258, 263
    - strategies against, 292
- Chess-playing programs, 75, 117, 165, 285
- Chisholm, Roderick, 67, 253
- Choice, 107, 108, 312, 326. _See also_ Free will; Volition
- Choice points, 197, 312–313, 326
- Chomsky, Noam, 62, 117, 123
- Christian Science, 234
- Church’s Thesis (constructive mathematics), xxx, 91–92, 122, 275n
- Clairvoyance, xxiii, 48
- Clone, 216
- _Cogito_, 98
- Cognitive dissonance, 160
- Cognitive pathology, 332
- Cognitive processes, 27, 101, 104, 107, 125, 129
- Cognitive psychology, xxv
    - artificial intelligence (AI) and, 88, 122, 124
    - cognitive science and, xxxiv
    - computational access and, 165
    - consciousness and, 163
    - as dualistic, 103
    - feelings and, 357n41
    - Fodor and, 99, 100, 105–107, 114, 115
    - _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett and, 175
    - Law of Effect and, 88
    - mentalism and, 103, 105–106
    - neo-cognitivism and, 101
    - philosophers and, 163
    - propositions and, 55, 175, 328
    - Ryle and, 105–106
    - task of, 328
    - top-down strategy and, 122, 124, 358n3
- Cognitive science, xxxiv
- Cognitive simulation (CS), 90n, 91. _See also_ Computer simulation
- Cognitivism, 188. _See also_ Neo-cognitivism
- Cognitivist-dualist equation, 354n3
- Cognitivistic horrors, 355n21
- Cognitivistic hypotheses, 104
- Cognitivistic models, 168, 188
- Cognitivistic theory(ies), 95, 115, 163, 168, 187, 188
- Cognitivists, 76–77, 95, 164
- Colby, Kenneth, 90n, 129
- Combinatorial explosion, 136
- Communication, 195, 265–267
    - capacity for, 17, 18, 304, 306n
    - capacity for verbal, 290–291, 298, 299, 304
    - Fodor and, 117–118
    - Grice and, 117, 135, 260–261, 298–301
    - homuncular, 135
    - intentionality and, 259–261, 298–301
    - nature of, 260, 261
    - necessity for, 306n
    - rationality and, 259–260, 271, 273, 299
- Compatibilism, xvi, 308, 318, 320
- Computational access, 164–168, 173
- Computational processes, 108–110, 112, 113, 115, 117
- Computational speed, 342. _See also_ Computer speed
- Computer chess-playing. _See_ Chess-playing computers
- Computer design, 66, 89, 215, 255, 262. _See also_ Design stance
- Computer languages, 91, 128, 301
- Computer programs, xxviii, 29, 92, 115, 164–165, 285, 342–343. _See also_ Chess-playing computers
- Computers, xxviii, 7, 29, 342–343
    - artificial intelligence (AI) and, 89, 92, 120, 124–129
    - beliefs and desires, 7, 65–66, 292
    - desires of, 7–10, 292
    - Grice and, 301
    - Hume’s problem and, 134n, 136
    - intentionality and, 256–257
    - intentional stance and, 6–8, 292
    - intentional systems and, 6, 7, 256–257, 292, 295
    - nature of, 285
    - philosophers and, 119
    - rationality and, 257, 292
    - “reasoning,” 66, 71, 292
    - representations and, 111–112
    - Skinner and, 71, 74, 75
    - Turing machine and, 92n, 208, 276, 279, 282, 284, 285
- Computer science, 111, 134n, 205
- Computer simulation, 32, 90, 125, 128. _See also_ Computers; Generate-and-test procedure
    - artificial intelligence (AI) and, 27, 90n, 91
    - of cognitive processes, 27, 114
    - of a hurricane, 208–209
    - mental features recalcitrant to, 207
    - of pain, 207, 209–213 (_see also_ Pain)
    - philosophers and, 208
- Computer speed, 128, 129
- Concept learning, 109, 125
- Concepts. _See also specific topics_
    - ordinary, xxxi, 147, 152–153, 158, 184 (_see also_ Pain, ordinary concept of)
    - parochial, 215, 248
- Conceptual analysis, 120, 151, 153, 157, 161, 184, 224, 226, 239
- Conceptual confusion, 142, 147, 207, 214, 235, 240
- Conceptual vs. causal explanation, 104, 105
- Conditioning. _See_ Behaviorism; Operant conditioning; Reinforcement; Skinner, B. F.
- Consciousness, xv, xxiv, 33–35, 163, 184, 185, 291, 302, 306n. _See also Content and Consciousness_; Personal consciousness; Self-consciousness; Unconscious
    - anesthesia and, 229
    - attention and, 171
    - cognitivistic model of, 188
    - information and, 146
    - language and, 306n
    - pain and, 231, 233, 244, 245
    - purpose of, 306n
    - Skinner on, 355n21
    - states of, 229, 244, 288, 290
    - stream of, 40–41, 133, 157
    - theories of, 17, 141, 161, 164, 169
- Content. _See also specific topics_
    - of beliefs, 199
    - of hallucinations, 232
    - of judgments, 53
    - of mental states and events, 16, 22, 39n, 88, 110–111, 166, 170, 177–178, 184–185
- Arbib on, 34–37
- _Content and Consciousness_ and, 33, 186 (_see also Content and Consciousness_)
- Fodor and, 110, 112, 116
- non-rationality and, 268
- _Content and Consciousness_ (Dennett), 36n, 89n, 186, 205n, 263, 269, 291n, 361n7, 365n23
    - iconophobia and, 205n
    - overview and outline of, xix, xx, xxxiv, 3, 17
    - papers related to, 25
    - and the word “aware
- Content-relativity, 142–143, 150
- Content-sensitivity, 234
- Control, 34, 170–175
    - privileged access and, 33
    - Skinner on, 357n41
- Control centers, 93, 222, 226–227, 229, 237, 340
- Controlling stimuli, 14, 15
- Control programs, 88
- Control systems, 34, 211, 220, 225, 264, 312. _See also_ Behavior-controlling system
- Copernicus, 149
- Cortex, 48, 161, 218, 225–227, 232–234
- Creationism, xxiii
- Creative self, 98
- Criterial, 116, 152–153, 155, 157
- Curare, 227–228, 230, 234, 236
- Cybernetics, 164, 177, 264

- Dahlbom, B., xx
- Darwinism, xxiii, 81, 82. _See also_ Evolution; Natural selection
- Data-driven process, 177
- Data structures, 171, 175, 205. _See also_ Information-bearing structures
    - artificial intelligence (AI), 133–135
    - of computer science, 111, 205
    - propositional, 55
- Davidson, D., 30, 54, 253
- Davis, L., 167, 242n
- Deception, 295–300
    - self-deception, 32, 36, 40, 54
- Decision-making. _See also_ Deliberation
    - model of, 314–320
    - under time pressure, 314
- Decisions, 34, 108, 257, 325–326
    - Bayesian prediction of, 328
    - determinism vs. indeterminism and, 314, 317
    - related to pain, 245, 247
    - subconscious, 32
- Decision theory, 116
    - normative, 107–108
- Decision unit of face-recognizers, 28
- Deep structure, 53, 170
- _Déjà vu_, 149, 196
- Deliberation, 262, 312, 315, 316, 319–321, 326, 328, 330. _See also_ Decision-making
    - practical, 309, 313
    - rational, 316–317
- _De re_ and _de dicto_, 324–326
- Descartes, René, 129, 141, 238, 263
    - Cartesian interactionism, 61, 191n, 341
- Design, 8, 25, 26, 93–94, 265. _See also_ Chess-playing computers
    - artificial intelligence (AI) and, 88–93, 122–123, 128
    - biological, 82
    - efficient evolution of, 88n
    - of humans, 266–268, 282
    - and misdesign, 280–281
    - neural, 266, 267
    - optimal, 5, 6, 8, 9, 16, 18, 21, 88
    - rationality and, 256, 262, 268 (_see also_ Rationality)
    - systems designed to malfunction
    - Turing machine and, 276–282
    - well-designed systems, 106
- Design breaks, 89
- Design elements, hierarchical organization of, 88n
- Design principles, 317
- Design stance, 6, 8, 10, 16, 23, 259, 262, 274
    - chess-playing computers and, 5–8, 16, 255, 256
    - essential features of, 4, 255
    - intentional stance and, 10, 256–258, 265n, 274
    - overview, 4
    - rationality and, 12, 256, 262
- Design-stance predictions, 4–6, 8, 12, 23, 255, 262, 265n. _See also_ Prediction, design stance and
- Desire(s), 44, 267, 271–272, 327–330
    - of animals, 10–11, 16, 292, 294, 297, 305n, 327
    - attributed to alien entities, 7–10, 292
    - _de re_ and _de dicto_, 324–326
    - epistemic, 327
    - first- and second-order, 304
    - Fodor and, 107, 112, 115
    - H. Frankfurt on, 304, 305
    - intentional explanations and, 253, 254
    - intentional systems and, 7–12, 17, 20, 256, 291–294
    - pain and, 221–222, 232, 235
    - prediction and, 292, 328, 330
    - propositions and, 328, 329
    - representations and, 46, 47
    - Skinner and, 68, 70
- de Sousa, Ronald, 324, 327, 329–332
- Determinism, 94, 313–314, 319
    - free will and, 308–310, 314
    - hard, 308, 356n34
    - libertarians and, 308, 309, 313–314, 319, 320
    - and moral responsibility, 251
    - physics and, 310–311
    - Skinner and, 356n34
- Deutscher, M., 157, 158
- Dichotic listening experiment, 173
- Dickens, Charles, 189, 196
- Digital and analogue devices, 124
- Digital computer, xxviii, 26, 276
- Dignity, 287, 313, 318
    - Skinner and, 64, 70, 73, 76, 77, 356n34
- Dispositions, 83, 105, 114–115, 193
- Dissociation. _See_ Reactive disassociation
- Dissociative anesthetics, 234
- DNA, 112, 113
- Dream recall/dream memory, 143–144, 146–147, 150
    - prediction of, 143, 147, 150
- Dreams, xxix, xxxi, xxxiv, 141–161, 172–173
    - cassette theory of, 148–154, 156, 157
    - composition of, 146–151, 160
    - as experience, 141–161
    - lucid, 150–151
    - Malcolm and, 141–143, 147–149, 151–153, 155–156, 159n, 160
    - precognitive, 149, 151
- Dreyfus, Hubert, 90n, 102–103
- _d_-tubocurarine, 227
- Dualism, 101, 104, 122, 202n, 354n3
    - Fodor and, 101, 103, 104
    - mentalism and, 62, 103

- Economics, 80
    - as metaphor for intentionality, 12, 13, 17
- Effect, Law of, 81–83, 85, 97
    - artificial intelligence (AI) and, 90, 95
    - behaviorism and, 79, 82–83, 86, 90
    - Broadbent on, 79–81
    - cognitive psychology and, 88
    - learning and, 84–85, 90
    - natural selection and, 81–83, 87, 95
    - overview, 79–80
    - Skinner and, 82, 86
    - Skinnerian creatures and, 84–85
- Ego, 123
- Einstein, Albert, 96, 149, 264
- Electroencephalography (EEG), 142, 144, 150
- Eliminative materialism, xxxii, xxxiii
- Emergent properties, 117
- Empiricism, 109–110
    - British, 129
- Encounters, 299
- Engineering, biological, 112, 317, 319
- Engineering questions, 121, 175, 178, 187, 276–278
- Engrams, 130
- Environment, inner, 85–88, 90
- Epiphenomenalism, xxix, 163, 191, 202–204, 209, 238
- Episodes
    - of conscious experience, 32, 40, 41, 141, 142, 149, 179, 180, 183, 303, 306, 333
    - propositional, 179, 180, 183, 361n7
- Episodic belief as a misnomer, 326
- Epistemic credentials of belief, 195, 196
- Epistemic desires, 327
- Epistemic horizon, 273
- Epistemic logic, 21n, 26, 30, 302
- Epistemic possession, 7
- Epistemic situation, 246
- Epistemic state, 244
- Epistemology, 101, 163, 175, 176, 181, 327, 328. _See also_ Propositions
    - artificial intelligence (AI) and, 122–124, 127, 136
    - psychology and, 101, 121–124, 163, 175, 176
- _Epoché_, 201
- Escher, M. C., 338
- Essential traits, 199, 214–216, 245–247, 255
- Ether, 229
- Ethical dimension of pain, 215
- Ethical intuitions, 215
- Ethics, 290–291, 302. _See also_ Justice; Morality; Moral responsibility
- Evolution, xxiii, 14, 16–18, 21, 36, 81, 83, 86, 87, 93, 297. _See also_ Darwinism; Natural selection
- Expectations, 72, 80, 88, 171, 292, 294
- Experience, 242, 245–247. _See also_ Dreams; Pain
    - perceptual, 172–176, 183–185
    - person as subject of, 168
    - transcendental, 201n
- Experience-generator, simulation as an, 208
- Explanation(s). _See also_ Intentional explanation; Mechanistic explanation (and prediction); Prediction
    - causal vs. conceptual, 104, 105, 116
    - explaining away vs. explaining, 71, 72, 98
    - purposive, 251–253
    - simplicity of, 72–73, 253, 264
- Extrapolator-deducer mechanism, 50, 51

- Face-recognizers, 25–28, 30, 31
- Fatigues, xxxii
- Feature detectors, 28, 171
- Feenoman and Feenomanology, 197–201
- Flew, A., 269, 271, 290
- Flow charts, 169, 169f, 179
    - artificial intelligence (AI) and, 89, 129, 130f–132f, 134
    - for a pain program, 216–217, 218f, 219, 219f, 235, 237–239, 241, 243
- Fodor, Jerry, 99–118
    - behaviorism and, 101, 103–105, 107
    - causation and, 104, 105, 108–109, 116
    - cognitive psychology and, 99, 100, 105–107, 114, 115
    - communication and, 117–118
    - on concept learning, 109
    - and the content of mental states and events, 110, 112, 116
    - desire and, 107, 112, 115
    - dualism and, 101, 103, 104
    - information and, 101, 107, 116
    - intelligence and, 100, 101, 105, 108
    - intentional explanation and, 72, 107
    - language and, 108–110
    - language learning and, 102, 113
    - _The Language of Thought_, 99, 107
    - language of thought and, 109, 112
    - mentalism and, 103–107
    - natural language and, 101, 102, 113, 118
    - perception and, 109–110
    - prediction and, 115, 116
    - on representation and learning, 113–114
    - Ryle and, 104, 105, 114, 116
    - Skinner and, 103, 105
    - sub-personal level and, 107, 112, 173
    - three demonstrations, 107–110
    - three parts to his message, 100–102
- Form, Platonic, 26
- Forster, E. M., 326
- Fortran, 301
- Foulkes, David, 161n
- Frame problem, 136, 137
- Frames, 137, 171
- Frankfurt, Harry, 291, 304–305
- Franklin, R. L., 365n23
- Free-floating rationalists, xv
- Free will, 317. _See also_ Choice; Volition
    - deliberation and, 319
    - determinism and, 308–310, 314 (_see also_ Determinism)
    - vs. freedom of action, 304
    - and responsibility, 305, 306, 308, 312
- Freud, Sigmund, 123, 141, 160, 185n
- Friedman, H., 301n
- Function, 4. _See also specific topics_
- Functionalism, xxiv, xxxiii, 106, 167–169, 174
    - homuncular, xxxiii
    - token, xxix
- Functionalist theories and sub-personal level, 167–168
- Functionalist theory of mind, 167
- Functional organization, 186
- Functional role, 55, 88, 112

- Game theory, 16–17, 23, 302, 352n5
- Gap of ignorance, 124
- Garrett, M., 173
- Generate-and-test procedure, 90–93, 95, 98n, 319. _See also_ Decision-making, model of
- Generative grammar, 46, 49, 53
- Generativity. _See_ Productivity
- Genes and genetics, 68, 112
- Genetic endowment, 84, 96, 297, 357n41
- Genetic inheritance, problem of, 76, 112
- Genius, 95–98, 314
- Gibson, J. J., 123
- Gilbert, Margaret, 366n12
- Gilman, A., 228n, 230–232
- Goals, 5–8, 171, 221, 235, 237, 239, 243
- Gödel sentences, 275, 281, 283, 284
- Gödel’s Theorem, xxxv, 275, 276, 283–286
- Goffman, Erving, 160, 299
- Goldberg, Rube, 129, 276
- Goodman, L. S., 228n, 230–232
- Gorovitz, S., 289n
- Grain problem, 124
- Grammar of brain writing, 46, 49, 53
- Gregory, R. L., 15n
- Gresham’s Law, 106
- Grice, H. Paul
    - communication and, 117, 135, 260–261, 298–301
    - Fodor and, 117
    - intentions and, 298–301
    - reciprocity and, 290, 298
    - theory of non-natural meaning, 298–301
- Gunderson, Keith, 25, 30, 32–34, 40, 186, 207
- Guthrie, E. R., 96n

- Hadamard, J., 98
- Hahnemann, F., 210n
- Hallucination, 146, 159, 341
    - anesthetic drugs and, 228n, 229, 232, 234–235
- Hallucinogens, 190. _See also_ Hallucination, anesthetic drugs and
- Halothane, 230n
- Hampshire, S., 269, 272
- Hand simulation, 129, 276, 282–286
- Hard-wiring, 84, 87
- Hardy-Wolff-Goodell dolorimeter, 241n
- Harman, Gilbert, 44–46, 53, 94n, 246n
- Harris, MacDonald, 18
- Herrnstein, R., 198
- Heterophenomenology, xv
- Heuristic procedures, 92n, 315, 326
- Hilliard, J., 307, 308
- Hintikka, J., 21n, 22, 290, 302
- Hippocampus, 218, 233
- Hobart, R. E., 314
- Hobbes, Thomas, 302
- Hologram, 130
- Homuncular functionalism, xxxiii
- Homunculus, 81, 89, 95, 96, 98, 111, 130, 178
    - artificial intelligence (AI) and, 134–135, 178
    - Fodor and, 112
    - pain and, 238
    - psychology and, 92, 111, 133
    - Skinner and, 63–65
- Hooker, M., xx
- Hospers, J., 252
- Hubel, David H., 143n
- Hull, Clark L., 80–82
- Hume, David, 111, 181–182, 253, 254, 327
    - on causation, 181, 253, 254
    - on ideas, 111, 113
- Hume’s problem, 111, 133, 134n, 136
- Hurricane, computer simulation of a, 208–209
- Husserl, Edmund, 199, 201n, 361n6
- Huxley, Aldous, 269
- Hyle (hyletic phase), 361n6
- Hypnosis, 224, 232, 236–237, 241, 268, 274
- Hypnotics, 227n, 231–232. _See also_ Barbiturates
- Hypothalamus, 217, 238n
- Hypothesis formation, 109–110, 114, 171

- Iconic memory, 169f, 171, 172, 360n7
- Iconophiles, 189–194, 202–205
- Iconophobes, 189, 190, 193, 194, 202–205
- Iconophobia, scientific vs. metaphysical, 205n
- Identity theory, xxvii, xxxii, 30, 100, 209–210, 243
    - type, xxvi, 351n2
- Idioms, xxxi–xxxii
    - intentional, 3, 13, 67–70, 103, 105, 253, 355n26
- Illusions, 71, 203, 340, 346
    - of having had a dream, 149
    - optical, 15, 183
    - regarding human behavior, 73
- Illusory beliefs. _See also_ Illusions
    - systematically, 202–203
- Illusory shift in point of view, 337–338
- Images. _See also_ Mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett; _specific topics_
    - meanings and uses of the term, 361n5
    - vs. propositions as vehicles of information, 55, 175, 177, 178n, 184
- Imagination, xx, 52, 63, 128–129, 159, 161n, 215, 239, 338. _See also_ Face-recognizers
    - vs. a critical eye, 96
    - Skinner and, 86
- Imagined society, xxxii—xxxiii
- Imaging, 202
- Imagistic data structures, 178n, 183–184. _See also_ Images
- Imagistic problem-solving, 172. _See also_ Problem-solving, _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett and
- Impressions, 111, 129, 133, 141
- Incorrigibility, 20–21, 32, 35, 36, 40, 41, 185, 186, 201, 335. _See also_ Infallibility
    - with regard to pain, 240, 245–247
- Indeterminacy, 7, 309, 311–313, 316, 319
    - intentionality and, 52, 273–274
    - quantum-level, 5
    - representations and, 52, 55
    - scientific vs. phenomenological approach and, 203
- Indeterminism, 313–314, 319, 320
    - libertarians and, 312–314, 316–317, 320
    - and moral responsibility, 251
- Infallibility, 253, 337
    - papal, 198
- Inference, 136, 170, 181, 238n, 256
- Information, 6–10, 43, 93–95, 222
    - access to (_see_ Computational access)
    - artificial intelligence (AI) and, 128, 129, 135
    - carried by unlearnable languages and systems, 54
    - coded, 47, 50, 54
    - consciousness and, 146
    - false, 267
    - Fodor and, 101, 107, 116
    - forms of, 175, 178
    - input of, 266–267
    - metalinguistic, 50
    - procedural, 135
    - propositions vs. _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett as vehicles of, 175, 177, 178n, 184
    - Skinner and, 76–77
    - transmission, 16, 292
- Information-bearing structures, 202, 204–205. _See also_ Data structures
- Information-flow, 101, 107
- Information-processing, 123, 177
- Information-processing models, 280
- Information processing systems, 261, 273, 329, 342
- Information storage, 50, 51, 170, 175, 178, 235. _See also_ Propositions, vs. _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett as vehicles of information
- Information theory, 93, 177
- Innate behavioral dispositions, 83
- Innate knowledge, 102, 130, 150
- Innate selectivity, 95
- Inner environment, 85–88, 90
- Insanity, 10, 257, 265, 268, 271, 287, 289
- Instinct, 83, 296–297
- Intellectualist theory, 101, 103, 104
- Intelligence, 12–13, 214, 259
    - artificial intelligence (AI) and, 214 (_see also_ Artificial intelligence)
    - begging the question of, 64, 81, 82, 92, 98
    - Canon of Parsimony and, 294
    - Church’s Thesis and, 92
    - components, 66, 98
    - computer simulation and, 208
    - dreams and, 146
    - emergent form of, 17
    - factors that influence, 65, 81
    - Fodor, 100, 101, 105, 108
    - homunculus and, 65, 89, 95, 134
    - inner environment and, 87–88
    - intentionality and, 13
    - Law of Effect and, 79–80, 83, 86
    - learning and, 79–80, 83, 86, 87, 91
    - modes of, 122
    - natural selection and, 83
    - psychology and, 64, 80–81, 92
    - randomness and, 314
    - representation and, 108
    - Russell and, 309, 314, 316
    - Skinner and, 65, 75
    - Turing on, 208
    - Wiggins and, 309, 310
- Intelligence loans, 12–13, 16, 89n
- Intelligent agents, 288
- Intelligent behavior, 72, 83, 99, 105, 134, 297
    - unmasking, 72, 73, 76
- _Intelligo_, 98
- _Intention_ (Anscombe), 31n, 43, 291
- Intention (in the ordinary sense), 301n
- Intention(s). _See also specific topics_
    - higher-order, 298, 302, 305
    - meanings and uses of the term, 301n
    - semantic, 169, 175, 181, 185–186
- Intentional action, 265, 301, 303n, 312–313, 320
    - against one’s better judgment, 332
- Intentional discourse, 13, 29–30
- Intentional explanation, 13, 21, 253, 254, 266
    - and behavior of complex machines, 255
    - chess-playing computers and, 7
    - defined, 3
    - desire and, 253, 254
    - Fodor and, 72, 107
    - mechanistic explanation and, 254, 262, 264–265, 272–274
    - prediction and, 7, 8, 12, 17, 21, 67–68, 265
    - Skinner and, 13, 67–68, 70–72
    - Wiggins and, 309–310
- Intentional idioms, 3, 13, 67–70, 103, 105, 253, 355n26
- Intentional inexistence, 196
- Intentionality, xxx, 3. _See also_ Intention(s)
    - behaviorism and, 16, 296
    - communication and, 259–261, 298–301
    - computers and, 256–257
    - economics as metaphor for, 12, 13, 17
    - indeterminacy and, 52, 273–274
- Intentional labels, 26, 28–30
- Intentional language, xxx, 28, 29, 39, 89
    - and non-intentional language, 14, 87
- Intentional objects, 196–203, 361n4
- Intentional prediction, 6–8, 10–15
    - intentionalists and, 75, 310–313
    - mechanistic prediction and, 265
- Intentional stance, 21, 273, 274, 294, 302
    - abandoning/eliminating the, 260, 272, 273
    - communication and, 259–260
    - computers and, 6–8, 292
    - design stance and, 10, 256–258, 265n, 274
    - intentional system and, 256–257, 291–293, 297
    - mechanistic stance and, 258–259, 272
    - moral stance and, 258–259, 261
    - overview and nature of, 6, 256
    - payoff, 8
    - personal stance and, 257–259
    - prediction and, 6, 10, 25, 256–257, 297, 313
    - rationality and, 6, 20, 256–260, 292, 293
- Intentional story, 28–31
- Intentional systems, xxix–xxxi, 3–23, 256–257
    - animals and, 11, 294–295
    - artificial intelligence (AI) and, 134
    - behaviorists and, 296
    - belief and, 7, 9–12, 15–23, 30
    - computers and, 6, 7, 256–257, 292, 295
    - defined, 3, 7, 134, 256, 291, 292
    - desire and, 7–12, 17, 20, 256, 291–294
    - first-order, 296
    - functionalist theory of mind and, 167
    - higher-order, xxxv, 295
    - intentional stance and, 256–257, 291–293, 297
    - language and, 14, 17, 18, 295
    - people as, 17, 20–21, 33, 266–267, 273–274
    - plants and, 292
    - prediction and, 3–17, 261, 273–274, 291–292, 297
    - processing information, 261
    - rationality and, 5–6, 8–13, 16–22, 291
    - reason and, 261
    - reciprocity and, 298
    - responsibility and, 266–267
    - second-order, 293–297, 304
    - theory of, 26
    - use of the term, 252n4, 352n4
- Interactionism, xxiv, 61, 191n, 230, 341
- Introspection, 64, 164, 170, 179, 184, 185, 188, 200–201, 205
    - artificial intelligence (AI) and, 120, 129
    - causal, 120
    - Hume and, 181
    - James and, 174
    - nature of, 97
    - pain and, 242–244
    - Poincaré and, 97
    - public access and, 166
- Introspective access, 174
- Introspective consciousness, 163
- Introspective data, 21, 27, 328
- Introspective evidence, 95, 155, 174, 328, 329
- Introspective point of view, 27
- Intuitions, 184, 185, 203, 279, 286, 303, 344
    - belief and, 293, 329
    - categories of, 161
    - Church’s Thesis and, 92
    - concepts and, 175, 184
    - ethical, 215
    - Kantian, 175
    - mentalism and, 60
    - morality and, 303, 308n, 317–318
    - pain and, 214, 215, 228, 231, 241, 243–245, 247–248
    - responsibility and, 254, 266, 270, 308n
    - second-order volitions and, 305
    - Skinner and, 60, 70, 85, 288
    - sub-personal theory and, 168
    - theory of consciousness and, 164
- Intuitive epistemic logic, 26, 30

- James, William, 174
- Judgment, 332
    - assenting and, 327
    - behavior and, 330
    - belief and, 51, 53–54, 326, 332
    - de Sousa and, 326, 327
    - Gunderson and, 40
    - perceptual, 183
    - “tip-of-the-tongue
- Julesz, B., 165
- Justice, 288, 289n, 302, 303, 346

- Kalish, Donald, 157
- Kant, Immanuel, 122, 141, 175
    - on epistemology, 121, 176
    - on ethics, 290
    - rationality and, 289–290
- Kant’s problem. _See_ Frame problem
- Ketamine, 234
- Kivy, Peter, 98n
- Klein, Barbara, 121n
- Knowledge. _See also specific topics_
    - factual, and language, 51
    - innate, 102, 130, 150
    - non-inferential, 253, 254
- Koestler, Arthur, 96n, 98n
- Krantz, J. C., 230n
- Kripke, Saul, 246n

- Lackner, J. R., 173
- Lamaze method, 233–234, 236
- Language. _See also_ Computer languages; Linguistics; Natural language; Semantics; Syntax
    - animals and, 295
    - artificial intelligence (AI) and, 89
    - belief and, xvi, 53, 295, 328, 329
    - brain writing and, 46, 51
    - capacity for, 18
    - and communication, 260
    - consciousness and, 306n
    - evolution of, 36
    - factual knowledge and, 51
    - Fodor and, 108–110
    - functional, xxviii
    - Harman and, 45
    - inner environment and, 87
    - intentional, xxx, 29, 89
    - intentional systems and, 14, 17, 18, 295
    - learnable, 46, 54 (_see also_ Language learning)
    - Marx on, 306n
    - mentalistic, xxx
    - opinion and, 328, 329
    - Quine, logical connectives, and, 9n
    - representation and inner, 44–46, 51, 112–113 (_see also_ Language of thought)
    - second-order intentions and, 295
    - sentences and, 328, 329
    - technical, 128
    - thinking and, 40
    - Turing machine and, xxviii–xxx, 277–278
- Language learning. _See also_ Language, learnable
    - Fodor and, 102, 113
- Language of thought, 99–128
    - belief and, 112
    - concept learning and, 109
    - Fodor and, 109, 112
    - neo-cognitivism and, 107
- _Language of Thought, The_ (Fodor), 99, 107. _See also_ Fodor, Jerry
- Language understanding programs, 125–126, 211n
- Language use and consciousness, 163
- Lashley, K., 181
- Learnable representational systems, 46, 54
- Learning, 86, 90, 93–95, 196. _See also_ Concept learning; Effect, Law of; Language learning
    - artificial intelligence (AI) and, 75, 90–95
    - capacity for, 84
    - defined, 84
    - Fodor on representation and, 113–114
    - “genuine” vs. “apparent
    - to “hand simulate” a Turing machine, 284–286
    - latent, 83
    - and mislearning and unlearning pain locations, 225n
    - philosophy and, xxv
    - as a process of self-design, 93
    - Skinner and, 84–87
- Learning problem, 87
- Learning-while-you-sleep, 151–154
- Leibniz, Gottfried Wilhelm, 63
- Lettvin, Jerome, 101
- Lewis, David, 367n2
- Libertarianism, xxxv, 319, 320
    - objections to, 308, 309
    - randomness, free action, and, 308, 309
    - Wiggins on, 308–310
- Libertarians
    - determinism and, 308, 309, 313–314, 319, 320
    - indeterminism and, 312–314, 316–317, 320
    - moral education and, 317
- Limbic system (“old low path”), 217–218, 219n, 238n
- Linguistics, xxxiv, 99–102. _See also_ Language
- Lobotomy, 227, 240, 242, 274
- Locke, John, 181, 288, 336
- Logical coherence, 49
- Logical construct, 102, 196–198, 200–204
- Logical space, xxiv
- Logical state, xxviii, xxix, 28
- Logical truth, 9, 11–12, 308–309
- Lucid dreams, 150–151
- Lycan, W., xxxiii
- Lying, 299–300

- MacIntyre, Alastair C., 262–264, 269, 271, 272
- MacKay, D. M., 190, 257, 258, 261, 273, 274
- Malcolm, Norman, 252, 273, 323, 327
    - dreaming and, 141–143, 147–149, 151–153, 155–156, 159n, 160
    - on neural systems of representation, 102–103
- Malfunction, 4–6, 38, 75, 106, 185, 255, 262, 278, 280
- Manifest beliefs, 9, 44, 46–48
- Manifest image, 259, 272, 320
- Manipulation, 133, 266–268, 299
- Martians, 124, 252
- Martin, C. B., 157, 158
- Marx, Karl, 306n
- Masochists, 239, 247
- Mass term, 243
- Materialism, 81, 82, 104, 106, 167, 202n, 207, 211, 213
    - eliminative, xxxii, xxxiii
- Maxwell, J. C., 309
- McConnell, J. V., 103
- Meaning, 88
    - Grice’s theory of non-natural, 298–301
- Meaninglessness, 281, 309
- Mechanical aspect vs. personal aspect of a system, 257
- Mechanism, xxxiv–xxxv, 68, 273, 275, 278
    - antimechanism, 122, 275, 279, 281–284, 286, 307n
    - artificial intelligence (AI) and, 89, 90n, 91–92, 122
    - Church’s Thesis and, 92
    - finite, 263–264
    - reason and, 307n
    - and responsibility, 251–274
    - as a theory of mind, 275
    - Turing machine and, 275, 276, 281–284
- Mechanistic explanation (and prediction), 65, 68, 70–72, 81, 251, 252, 254, 264. _See also_ Mechanism
    - intentional explanation and, 262, 264–265, 272–274
- Mechanistic physicalism, 294
- Mechanistic stance, 257–259, 261, 272
- Medullary paralysis, 229, 230
- Melden, A. I., 253, 268
- Melzack, Ronald, 217, 219n, 224, 232, 238n
- Melzack-Wall gate, 217, 220, 223, 224
- Melzack-Wall theory, 224n, 234, 362n8
- Memory, 28, 31, 121, 123, 154–155, 157–158. _See also_ Amnesia; Cassette theory of dreams; Dreams
    - buffer, 170, 172n, 184
    - experience and, 141
    - normal entry into, 158
    - pain and, 209, 220, 228, 233, 242, 243, 245
    - short-term, 156–158, 170, 172n
    - “storehouse” model of, 157–158
- Memory-loading process, 144–146, 151, 156–157, 159, 173
- Memory location “M,” 170–175, 177, 178, 183–185
- Memory mechanisms, 143–144, 148, 149
- Memory trace, 157
- Mendelian genetics, 68
- Mentalese, 102, 115, 118
- Mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett, xxv, xxxi–xxxv. _See also_ Images; Mental representations; _specific topics_
    - two approaches to, 189–205
- Mentalism, xxv, 80
    - behaviorism and, 61, 63, 80, 104–105
    - cognitive psychology and, 103, 105–106
    - dualism and, 62, 103
    - Fodor and, 103–107
    - intentionalism and, xxxiii
    - Quine and, 67
    - Ryle and, 104–105
    - Skinner and, 60–70, 76, 77, 86, 103, 354n1, 354n3, 355n21, 357n41
- Mentalistic concepts, 27
- Mentalistic explanations, 63
- Mentalistic terms, xxvii, xxix–xxxi, 60, 64–67, 70, 80, 86, 103, 105–106
- Mental representations, xxxiii. _See also_ Mental _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett; Representational systems; _specific topics_
- Mind, theory of
    - functionalist, 167
    - mechanism as a, 275
    - philosophical, xxiii–xxx
- Mind–body problem, 32
- Minsky, Marvin L., 49–51, 137, 165
- Model features, intended, 125
- _Modus ponens_, 11
- Molière, 13, 63
- Monism, anomalous, xxiv, 30
- Moore, G. E., 141
- Moral commitment, 259
- Moral community, 7, 215, 258, 260
- Moral decisions, 317–318, 320. _See also_ Decisions
- Moral education, 317, 319
- Morality. _See also_ Ethics; Justice
    - intuitions and, 303, 308n, 317–318
- Morally responsible agents (moral agents), 7, 17, 60, 64, 70, 291, 308n, 312, 318, 319, 332. _See also_ Responsible agents
- Moral norms, 303. _See also_ Norms
- Moral pathology, 332
- Moral personhood, conditions of, 289–291, 293, 304, 306
    - conditions that exempt humans from, 287
- Moral persons, metaphysical persons becoming, 288–289
- Moral responsibility, 288, 314. _See also_ Insanity; Responsibility
    - determinism (and indeterminism) and, 251
    - norms and, 303
    - Skinner and, 64, 70
    - verbal communication and, 290–291
- Moral stance, 258–261
- Morgan, Lloyd, Canon of Parsimony, 294, 356n36
- Morphine, 216n, 242, 246
    - administered before vs. after the onset of pain, 226
    - and amnesia, 228n, 233, 240
    - pain intensity and dosage of, 213, 227n
    - phenomenology and pharmacology, 226–227, 240
    - reactive disassociation effect, 226, 236
    - time-dependence of its analgesic effect, 233, 234
    - used in anesthesia, 228n, 229
- Morphine patients experiencing pain but not minding it, 226, 240, 247
- Morphine poisoning, 213, 240–241
- Mozart, Wolfgang Amadeus, 96–98
- Müller-Lyer illusion, 15

- Nagel, Thomas, 32, 34, 151, 154n, 156, 163, 166, 290
- Natural kinds, xxxi, 106, 247
- Natural language, xxxv, 177
    - acquisition of, 121, 176
    - artificial intelligence (AI) and, 126, 127
    - Fodor and, 101, 102, 113, 118
    - input and output, 126
    - language of thought and, 101
    - representations and, 51, 53
- Natural language programs, 126, 211n
- Natural selection, xv, xxiii, 8, 9, 12, 81–87, 90, 95. _See also_ Darwinism; Evolution; Selection
    - Law of Effect and, 81–83, 87, 95
- Neisser, Ulric “Dick,” 171, 360n7
- Neo-cognitivism, 101, 103, 107, 111, 116
- Neocortex, 218, 219f, 229, 232, 234
- Neurath, O., 273
- Neurobiology. _See also specific organs_
    - of perception, 225, 238n
    - of recognition, 27
    - and responsibility, 251, 252, 270, 271, 274
- Neurophysiology, xxiv, 45, 142–143. _See also_ Electroencephalography
- Neurosurgery, 270, 271, 274
- New Critics, 200
- Nietzsche, Friedrich Wilhelm, 306n
- Nitrous oxide, 226, 229–230, 232, 240
- Nociceptors, 217, 226, 231, 237
- Noise, 222
- Non-inferential knowledge, 253, 254
- Non-physical events, 32, 61, 66, 103, 191n, 204, 354n1
- Normative decision theory, 107–108
- Norms, 19–21, 23, 30, 299, 302, 303
- Novocain, 224, 225, 236
- Nozick, Robert, 160n
- Number crunching, 125

- Occam’s Razor, 294, 356n36
- Ontology, xxxii, 30, 41, 203
- “Operant Behavior” (Skinner), 354n11
- Operant conditioning, 73, 76, 84–88, 354n11. _See also_ Behaviorism; Reinforcement; Skinner, B. F.
    - law of, 82 (_see also_ Effect, Law of)
- Opiates. _See also_ Morphine
    - lobotomy and, 227
- Opinion, 327, 329–332
    - vs. belief, 328, 329
    - de Sousa on, 329
- “Order which is there,” 31n, 297, 301, 303, 306
- Other minds, 188

- Pain, xxvii, xxix, xxxi–xxxiv, 32, 41, 86, 265–266
    - causes and effects of, 209, 213, 235–239, 241, 243, 244
    - central, 210n, 218n, 227n
    - concentrating attention on one’s, 224
    - consciousness and, 231, 233, 244, 245
    - as intentional object, 244
    - intuitions and, 214, 215, 228, 231, 241, 243–245, 247–248
    - ordinary concept of, 213, 240, 243–245, 248
    - recognition of, 233–235
    - theories of
- functionalist, 167
- perceptual, 245n
    - why you can’t make a computer that feels, 207–248
- Pain perception, 218, 244
- “Pain Perception” (Pitcher), 245n, 247n
- Pain threshold, 241, 242
- Paleocortex, 218
- Parallel processing, 171
- Para-mechanical hypotheses, 104, 184n
- Paranoia, 129
- Parfit, Derek, 367n2
- Parochial concepts, 215, 248
- Pavlov, Ivan Petrovich, 269
- Pears, D., 274
- Penfield, Wilder, 146n, 218n, 252
- Pentagon, The, 333
- Perception, 38, 52, 69, 165, 175, 176, 181, 244, 260, 261
    - defined, 9
    - events in the nervous system subserving, 110–111
    - fine-grained, 218, 226
    - Fodor and, 109–110
    - nature of, 245
    - neurobiology of, 225, 238n
- Perceptual analysis, 174, 177
    - Fodor and, 107
    - nature of, 184
- Perceptual analysis, pain and, 225–227, 233, 234, 237, 243, 245
- Perceptual analysis areas of cortex, 225
- Perceptual analysis levels, tier of, 170–172
- Perceptual analysis processes, 179, 233
- Perceptual environments, 179, 180
- Perceptual experience, 172–176, 183–185
- Perceptual judgments, 183
- Perceptual revision, 175
- Perceptual set, 173, 174, 223, 226
- Perceptual system, 266
- Peripheralism, 83, 84n, 86
- Perkel, D. H., 47n
- Perry, John, 367n2
- “Person.” _See also_ Persons
    - Locke on the term, 288
    - as moral agent, 332
- Personal aspect vs. mechanical aspect of a system, 257
- Personal consciousness, access of, 164–166, 180, 185
- Personality, 342
- Personal level, 107, 112, 168n, 243, 258
- Personal location, 333–346
- Personal stance, 257–259, 290
- Personhood, 341. _See also_ Moral personhood; _specific topics_
    - ability to change one’s mind as the essence of, 332
    - animals and, 290
    - metaphysical, 289
    - notion of, 341
- Persons, 341, 342. _See also_ “Person”; _specific topics_
- Persuasion, 304, 305, 331, 332
- Phantom body hallucinations, 341
- Phantom limb, 225
- Phencycladine (PCP), 234
- Phenomenal figures, 362n8
- Phenomenal geometry, 362n8
- Phenomenal space, 182, 185, 201–203
- Phenomenal world/subjective world, 85n
- Phenomenography, 361n4
- Phenomenological approach, 199, 200, 203, 204
- Phenomenological consciousness, 163, 166
- Phenomenological data, 21–22
- Phenomenological effects, 210
- Phenomenological program, 199
- Phenomenological reduction, 201n
- Phenomenologists, 199–201, 203
- Phenomenology, 21, 163, 201n, 203, 210, 224, 318, 341
    - belief and, 35
    - pain and, 210n, 224
    - vs. phenomenography, 361n4
    - of systems, 35
- Phillips Griffiths, A., 19
- Philosophical theory of the mind, xxiii–xxx
- Physicalism, xxiii, xxvi, xxviii, xxxi, 30, 32, 103, 158, 294, 335, 340
    - token, 106
- Physicalists, 81, 158, 335, 341
- Physicalist theory of consciousness, 141
- Physical stance, 4–5, 23, 255–257, 259, 265n, 273, 274
- Physics, 106, 309–311
- Physiology, xxxii, 62, 64, 120, 153, 155, 248, 264. _See also_ Neurophysiology
- Pigeons, 72–74, 76, 123, 198, 293, 356n37
- Pineal gland, 238
- Pitcher, George, 244n, 247n, 362n8
- Placebo, 236
- Plato, 166
- Play back mechanism, 150
- Poincaré, Henri, 97
- Point of view, 337–341
- Popper, Karl, 86, 88
- Possible worlds, 328–329
- Potential behavior control elements, adaptive, 88
- Potential behavior control systems, 87
- Precognition, 148
- Preconscious, 185n
- Preconscious intentions, 301
- Prediction, 9–10, 29. _See also_ Explanation(s); Intentional prediction; Mechanistic explanation (and prediction)
    - assumptions and, 4–6, 255
    - barriers to, 273–274
    - Bayesian, 327–328, 330
    - beliefs, desires, and, 9–11, 294, 327–328, 330
    - by chess-playing computers, 5, 6, 23, 255–256
    - deception and, 296
    - design stance and, 4–6, 8, 12, 23, 255, 262, 265n
    - of dream recollections, 143, 147, 150
    - Fodor and, 115, 116
    - intentional explanation and, 7, 8, 12, 17, 21, 67–68, 265
    - intentional labels and, 29
    - intentional stance and, 6, 10, 25, 256–257, 297, 313
    - intentional systems and, 3–17, 257, 261, 273–274, 291–292, 297
    - physical stance and, 4–5, 255–256
    - rationality and, 5–6, 10, 12, 256, 257, 262
    - Skinner and, 14–16, 67–68, 70, 74–75, 357n41
    - weather, 208–209
    - Wiggins on, 310
- Premonition, 179
- Presentation, mode of, 209
- Presentiment, 179, 180
- Privacy, 20, 43, 61, 64
- Private events, 64
- Private states, 156n
- Privileged access, 32, 33, 38, 40, 246n
- Privileged position, 35, 53, 155–156, 231, 303, 326, 328
- Probability, subjective, 327–328
- Problem-solving, 79, 87n, 95, 172, 181
    - artificial intelligence (AI) and, 89, 90, 129, 133, 134n, 136 (_see also_ Artificial intelligence)
    - attention and, 171
    - generate-and-test procedure and, 90, 92n, 93, 95 (_see also_ Generate-and-test procedure)
    - _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett and, 136, 164, 172, 181–182, 190, 191
    - nature of, 79
    - Shepard phenomenon and, 183
    - Simon on, 79
- Productivity (generativity), 108, 114
- Program-receptive vs. program-resistant, 32–35, 41, 207
- Propositional attitude, 102, 114, 117, 326
- Propositional data structures, 55
- Propositional episodes, 179, 180, 183, 361n7
- Propositions, 10, 22, 26, 50, 51, 53, 136
    - cognitive psychology and, 55, 175, 328
    - vs. _resources/books//Brainstorms, Fortieth Anniversary Edition_ - Daniel C. Dennett as vehicles of information, 55, 175, 177, 178n, 184
    - mental acts as tokens expressing, 45n
    - Stalnaker on beliefs and, 328–329
- Psychoanalysis, 252, 334
- Psycholinguistics, 100, 102, 170
- Psychological reality, 102, 109, 110, 121, 122, 124, 125
- Psychosomatic disorders, 160, 254
- Public access, 165, 166
- Purpose, 69, 81–82, 280, 281
- Purposive explanations, 251–253
- Putnam, Hilary, xx, 30, 36, 106, 141n, 142n, 149n, 152n, 290
- Pylyshyn, Zenon, xx, 128

- Qualia, 110
- Quality of experience, 242
- Question-begging, 13, 63, 64, 81–83, 91–92, 95, 122
- Quine, W. V. O., 9n, 13, 324, 356n28
    - Chisholm and, 67, 356n30
    - intentional idioms and, 13, 67
    - intentionality and, 20
    - naturalism and, xii
    - on radical translation, 48
    - Skinner and, 13, 67, 68
    - on translation of intentional into non-intentional terms, 67, 68, 356n30

- “Random,” meanings and uses of the term, 309
- Random contribution, 94
- “Random” generation, 54, 165, 314, 317–320
- Randomizer, radium, 5, 310
- Randomness, 96, 165, 251, 252, 309, 312, 314
    - intelligence exploiting, 314
    - vs. uncaused, 308–310
- Rapid eye movement (REM) sleep, 142–144, 147, 150–153, 160
- Rational action, 329
    - theory of, 328
- Rational beings, persons as, 289
- Rationality, 9, 265, 267–268, 272, 315–317
    - animals and, 305n
    - assumption/presupposition of, 10–13, 16, 18, 21, 64, 65, 67, 68, 73–74, 80, 256, 257, 260–262, 265, 302, 356n36
    - belief and, 19–22, 267–271
    - brain surgery and, 270, 271, 274
    - communication and, 259–260, 271, 273, 299
    - computers and, 5–6, 8, 10, 256–260, 292, 302
    - demanding standards of, 21
    - design and, 256, 262, 268
    - design stance and, 12, 256, 262
    - game theory and, 16–17, 352n5
    - Grice and, 299, 300
    - impairment of a person’s, 270, 271, 273, 274
    - intentional stance and, 6, 20, 256–260, 292, 293
    - intentional systems and, 5–6, 8–13, 16–22, 291
    - intentional theory and, 16
    - Kant and, 289–290
    - MacIntyre and, 262–264
    - mechanistic stance and, 259, 261
    - perfect (vs. imperfect), 12, 21, 256, 263, 264
    - and prediction, 5–6, 10, 12, 256, 257, 262
    - psychology and, 64
    - Quine and, 67
    - Rawls and, 302
    - and responsibility, 265, 274
    - Skinner and, 64, 65, 67, 68, 73–74, 356n36
- Raw feels, 32
- Rawls, John, 288–290, 302–303
- Reactive disassociation, 226, 236
- Reality, psychological, 102, 109, 110, 121, 122, 124, 125
- Reason, 307n
    - capacity to listen to, 304
    - consciousness and, 306n
    - intentional systems and, 261
- Reason-giving, 304, 309
    - vs. cause-giving, 265–266
- Reasoning, 145, 265–266
    - beliefs and, 136–137
    - chains of, 299, 300
    - computers and, 66, 71, 292
    - practical, 270, 292
- Reciprocity
    - capacity for, 290
    - Grice and, 290, 298
    - intentional systems and, 293, 298
- Recognition, 32, 66, 211, 260, 287, 298–301, 303. _See also_ Face-recognizers
    - artificial intelligence (AI) and, 71, 89, 128
    - as an intentional concept, 26
    - neural basis, 27
    - of pain, 233–235
- Reduction, xxviii–xxxi
    - of intentional to extensional, 67, 91, 101–103, 106
    - phenomenological, 201n
- Reductionism, xxx, xxxi, 101–103, 106
- Referred pain, 225
- Reinforcement, 73–76, 81, 84–88. _See also_ Behaviorism
    - belief and, 69
    - Law of Effect and, 82, 83, 85
    - law of primary, 82
    - negative reinforcement is not punishment, 84n
    - Skinner on, 69, 73, 74, 84n, 85, 86, 354n6
- Reinforcers, 84, 354n6
    - types of, 84
- REM (rapid eye movement) sleep, 142–144, 147, 150–153, 160
- Representational systems, 55, 101, 108. _See also_ Mental representations
- Repression, 149
- Responsibility, 96, 97, 318, 319. _See also_ Insanity; Moral responsibility
    - for action(s), 303
    - for acts based on false information, 267
    - awareness and, 303
    - for being who we are, 321
    - brain surgery and, 252, 270, 271
    - free will and, 305, 306, 308, 312
    - intelligence and, 81
    - intentional and mechanistic stances and, 272
    - intentional systems and, 266–267
    - intuitions and, 254, 266, 270, 308n
    - machines and, 17
    - mechanism and, 251–274
- antagonism between, 266
- first step in reconciling, 252–253
    - neurobiology and, 251, 252, 270, 271, 274
    - of people who have been manipulated, 266–267
    - rationality and, 265, 274
    - reason and, 304
- Responsible agents. _See also_ Insanity; Morally responsible agents
    - random action, libertarianism, and, 308–309
    - vision of the free choice of, 312
- Reticular formation, 217–218, 232–234, 238n
- Reticular system, 153, 167, 219n, 220
- Retrospection, 161, 164, 166, 228
- Rey, G., xx
- Rights, 288, 289
- Rorty, Amelie, xxi, 290, 331n, 367n2
- Rotation of image, 182–183
- Russell, Bertrand, 75, 141, 309, 314, 316, 319
- Ryle, Gilbert, xii, 32, 104, 129, 180
    - attack on intellectualist theorizing, 103, 104
    - behaviorism and, 101, 103–105
    - cognitive psychology and, 105–106
    - dualism and, 101, 104
    - Fodor and, 101, 103–105, 114, 116
    - on log-keeping, 184–185n
    - mentalism and, 104–105
    - personal level and, 168n
    - Skinner and, 103–105
    - on unstudied utterances, 180n
    - on vanity, 105

- Santayana, G., 198
- Sartre, Jean-Paul, 53, 199, 291
- Satisficing, 315
- Schacht, R., 201n
- Schizophrenia, 194
- Science, law of special, 106
- Sciences, reduced vs. unreduced, 106
- Scopalomine, 228n
- Scotophobin, 103
- Searle, J., 299
- Selection, 84, 86, 88, 94. _See also_ Natural selection
    - intelligent, 314, 316, 318
    - principles of, 87, 90, 95
- Selective function, 328
- Self, 98, 133
    - conscious, 98
    - unconscious, 97–98
- Self-consciousness, 7, 34, 180n, 288, 294, 305. _See also_ Lucid dreams
- Self-deception, 32, 36, 40, 54, 330–332
- Self-design, 93
- “Self-evident
- Self-intimation, 204
- Self-understanding, 111, 112, 133–135
- Sellars, Wilfrid, 45n, 180n, 259, 272–273, 280n, 290
- Semantic analysis, 173, 336
- Semantic intentions, 169, 175, 181, 185–186
- Semantic interpretations, 195, 277, 278, 286
- Semantic markerese, 170
- Semantic primitives, 117
- Semantics, 123
- Sensation, 141, 142, 181
- Sentences, 328, 329
    - Gödel sentences, 275, 281, 283, 284
- Sequential processes, 171
- Serial processing. _See_ Sequential processes
- Set, perceptual, 173, 174, 223, 226
- Shakey the robot, 211
- Shepard, Roger, 182–183
- Simon, Herbert, 315n
    - on artificial vs. synthetic, 214n
    - on the design process, 88n, 93
    - on generate-and-test procedure, 90
    - on inner vs. outer environment, 85n
    - on problem-solving, 79, 90
    - on selection, 90
- Simple mechanistic explanation, 264. _See also_ Mechanistic explanation (and prediction)
- Simulation of mind, 8. _See also_ Chess-playing computers; Cognitive simulation; Computer simulation
- Skepticism, 122n
- Skinner, B. F., 13, 14, 76–77, 86, 95, 354n3, 354n6, 354–355nn11–12, 355n21, 356n36, 357n41
    - on autonomy, 64
    - behaviorism and, 59, 63
    - chess-playing computers and, 65, 71, 74–75
    - on cognitivistic horrors, 355n21
    - computers and, 71, 74, 75
    - desires and, 68, 70
    - dignity and, 64, 70, 73, 76, 77, 356n34
    - dualism and, 62, 103
    - explaining, explaining away, and, 71
    - Fodor and, 103, 105
    - and his critics, 59–60
    - homunculus and, 63–65
    - information and, 76–77
    - intelligence and, 65, 75
    - intentional explanation and, 13, 67–68, 70–72
    - intentional idioms and, 67–69
    - intentional systems and, 13–16
    - intuitions and, 60, 70, 85, 288
    - learning and, 84–87
    - mentalism and, 60–70, 76, 77, 86, 103, 354n1, 354n3, 355n21, 357n41
    - moral responsibility and, 64, 70
    - neo-Skinnerians, 86
    - on neural systems of representation, 102–103
    - on personhood, 287
    - on physics, 355n12
    - post-Skinnerian mutations, 90
    - prediction and, 14–16, 67–68, 70, 74–75, 357n41
    - principle of operant conditioning, 82 (_see also_ Operant conditioning)
    - on privacy of the mental, 61
    - psychology and, 16, 67–69, 73, 77, 105
    - Quine and, 13, 67, 68
    - rationality and, 64, 65, 67, 68, 73–74, 356n36
    - reinforcement and, 69, 73, 74, 76, 84–86, 354n6
    - Ryle and, 103–105
    - writings
- “Behaviorism at Fifty,” 64–65, 72, 356n36
- “Operant Behavior,” 354n11
- Skinner box, 14, 15
- Skinnerian creatures, 84–87, 95
- Sleep, REM, 142–144, 147, 150–153, 160. _See also_ Dreams
- Sleep learning, 151–154
- Smart, J. J. C., 110n
- Smith, George, 121n, 124n
- Social contract, 291
- Socrates, xxvii, 308n
- Sodium pentothal, 229
- Somnambulism, 171
- Sortal term, 213, 243
- Special sciences, 106
- Speech. _See also_ Language
    - development of consciousness and, 306n
- Speech act, 135, 169–170, 174, 186
- Spinal reflexes, 220, 230–231, 237
- Stalnaker, Robert, 328, 329
- Stance, 3–8, 257, 290–292, 305. _See also_ Design stance; Intentional stance; Moral stance; Physical stance
    - personal, 258–259, 290
- Stearns, T., 213
- Stimulus that fails to stimulate, 191
- Straumanis, Joan, 49
- Strawson, P. F., 257–259, 290, 362n8, 364n20
- Stream of consciousness, 40–41, 133, 157
- Structure
    - content as a function of, 116, 177–178, 184
    - deep and surface, 170
- Subconscious choices/decisions, 32
- Subconscious processes, 40, 150
- Subjective probability, 328
- Subliminal perception (subception), 160, 184
- Subliminal peripheral recollection-production theory, 156, 158–160
- Subliminal self, 98
- Sub-personal level, 168, 187
    - cognitivist theories and, 115, 168
    - Fodor and, 107, 112, 173
    - functionalist theories and, 167–168
    - personal level and, 205, 238
    - Ryle and, 180nw
- Sub-personal theory(ies), 167–168
    - of pain, 235–238, 243, 248
- Subroutines, 165, 166, 170, 235, 243, 263. _See also_ Generate-and-test procedure
- Substantia gelatinosa, 217, 220, 224, 225, 232
- Supersonic Tunneling Underground Device (STUD), 333
- Superstitious behavior, 198
- Supply and demand, law of, 80
- Syntactic intention, 301n
- Syntax, 116, 123, 127, 278, 329
- Synthetic vs. artificial, 214
- System-designing, 122–123. _See also_ Design

- Tachistoscope, 172, 180
- Talbott, William, 186
- Taylor, Charles, 352n4
- Teleological law, 81
- Thalamus, 218, 232, 238n
- Theory, xxiii. _See also specific topics_
- Thinking, two notions of, 40. _See also specific topics_
- Thorndike, Edward. L., 82
- Threshold of pain, 241, 242
- Token identity theory, xxviii
- Tolman, E. C., 20
- Top-down strategy, 120
    - artificial intelligence (AI) and, 122, 124, 358n3
    - cognitive psychology and, 122, 124, 358n3
- Topic-neutral report, 110n
- Transcendental experience, 201n
- Trial and error, 79, 85, 86, 90
- Trigg, Roger, 239n, 240n
- Tropism, 83–85, 92, 94, 263, 264, 297
- Trust, 260, 299
- Truth rule, 113
- Turing, A., 124n, 208
- Turing machine, xxviii–xxx, 28, 36, 91, 92n
    - computers and, 92n, 208, 276, 279, 282, 284, 285
    - design and, 276–282
    - language and, xxviii–xxx, 277–278
    - learning to “hand simulate” a, 284–286
    - mechanism and, 275, 276, 281–284
- Turing machine functionalism, xxiv, xxviii
- Twilight sleep, 228n
- Type and token, xxvi–xxviii. _See also_ Identity theory
- Type intentionalism, xxxi–xxxiii

- Unconscious, 97, 150–151, 156–157, 160, 184, 192, 252, 312. _See also_ Amnesia
- Unconscious attention, 171
- Unconscious intentions, 300–301
- Unconsciousness, drug-induced state of, 229, 231–233. _See also_ Anesthesia
- Unconscious problem-solving processes, 181
- Understanding, 12, 21, 89, 124, 127, 133–134, 176, 246n, 281. _See also_ Self-understanding
- Ungar, G., 103
- Unmasking intelligent behavior, 72, 73, 76
- Urea, 213, 214
- Utilitarianism, 259

- Valéry, Paul, 79, 90, 96, 98n, 314, 318
- Van de Vate, D., 290
- Verificationism, 142n, 152, 166, 188
- _Virtus dormitiva_, 13, 63–64, 74, 76
- Vitalism, 122, 216
- Volition, 308. _See also_ Choice; Free will
    - second-order, 304–305
- Von Neumann, John, 124n

- Wall, Patrick David, 217, 219n, 238n. _See also_ Melzack-Wall gate; Melzack-Wall theory
    - on pain, 219n, 224n, 232, 238n
- Wanton, 305
- Wasps, 71–73, 263, 264
- Watson, John B., 129
- Weather prediction and simulation, 208–209
- Webb, Judson, 275n
- Weizenbaum, Joseph, xxi, 124n, 127, 211n
- Wernicke’s aphasia, 127
- Wiesel, T. N., 143n
- Wiggins, David, 308–310, 320, 367n2
- Winograd, Terry, 125–127, 211n
- Wittgenstein, Ludwig, 155n, 168n, 228n, 241–242
- Wöhler, F., 213
- Woodruff, Peter, 351n1
- Wooldridge, D., 71–72, 263
- Writing as a dependent form of information storage, 50

- Yost, R. M., 157

- Zen Buddhism, 224, 234
- Zombie, 168, 231, 271, 273