---
title: The Future of Ideas_ The Fate of the Commo - Lawrence Lessig
author: Lawrence Lessig
tags:
  - book
cover: _resources/books/The Future of Ideas_ The Fate of the Commo - Lawrence Lessig/calibre_cover.jpg
---
   

![[calibre_cover.jpg]]"Free."

So deep is the rhetoric of control within our culture that whenever one says a resource is "free," most believe that a price is being quoted-free, that is, as in zero cost. But "free" has a much more fundamental meaning-in French, libre rather than gratis, or for us non-French speakers, and as the philosopher of our age and founder of the Free Software Foundation Richard Stallman puts it, "free, not in the sense of free beer, but free in the sense of free speech."12 A resource is "free" if (1) one can use it without the permission of anyone else; or (2) the permission one needs is granted neutrally. So understood, the question for our generation will be not whether the market or the state should control a resource, but whether that resource should remain free.13

This is not a new question, though we've been well trained to ignore it. Free resources have always been central to innovation, creativity, and democracy. The roads are free in the sense I mean; they give value to the businesses around them. Central Park is free in the sense I mean; it gives value to the city that it centers. A jazz musician draws freely upon the chord sequence of a popular song to create a new improvisation, which, if popular, will itself be used by others. Scientists plotting an orbit of a spacecraft draw freely upon the equations developed by Kepler and Newton and modified by Einstein. Inventor Mitch Kapor drew freely upon the idea of a spreadsheet-VisiCalc-to build the first killer application for the IBM PC-Lotus 1-2-3. In all of these cases, the availability of a resource that remains outside the exclusive control of someone else-whether a government or a private individual-has been central to progress in science and the arts. It will also remain central to progress in the future.

Yet lurking in the background of our collective thought is a hunch that free resources are somehow inferior. That nothing is valuable that isn't restricted. That we shouldn't want, as Groucho Marx might put it, any resource that would willingly have us. As Yale professor Carol Rose writes, our view is that "the whole world is best managed when divided among private owners,"14 so we proceed as quickly as we can to divide all resources among private owners so as to better manage the world.

This is the taken-for-granted idea that I spoke of at the start: that control is good, and hence more control is better; that progress always comes from dividing resources among private owners; that the more dividing we do, the better off we will be; that the free is an exception, or an imperfection, which depends upon altruism, or carelessness, or a commitment to communism.

Free resources, however, have nothing to do with communism. (The Soviet Union was not a place with either free speech or free beer.) Neither are the resources that I am talking about the product of altruism. I am not arguing that there is such a thing as a "free lunch." There is no manna from heaven. Resources cost money to produce. They must be paid for if they are to be produced.

But how a resource is produced says nothing about how access to that resource is granted. Production is different from consumption. And while the ordinary and sensible rule for most goods is the "pay me this for that" model of the local convenience store, a second's reflection reveals that there is a wide range of resources that we make available in a completely different way.

Think of music on the radio, which you consume without paying anything. Or the roads that you drive upon, which are paid for independently of their use. Or the history that we hear about without ever paying the researcher. These too are resources. They too cost money to produce. But we organize access to these resources differently from the way we organize access to chewing gum. To get access to these, you don't have to pay up front. Sometimes you don't have to pay at all. And when you do have to pay, the price is set neutrally or without regard to the user, inside or outside the company. And for good reason, too. Access to chewing gum may rightly be controlled all the way down; but access to roads, and history, and control of our government must always, and sensibly, remain "free."

THE ARGUMENT of this book is that always and everywhere, free resources have been crucial to innovation and creativity; that without them, creativity is crippled. Thus, and especially in the digital age, the central question becomes not whether government or the market should control a resource, but whether a resource should be controlled at all. Just because control is possible, it doesn't follow that it is justified. Instead, in a free society, the burden of justification should fall on him who would defend systems of control.

No simple answer will satisfy this demand. The choice is not between all or none. Obviously many resources must be controlled if they are to be produced or sustained. I should have the right to control access to my house and my car. You shouldn't be allowed to rifle through my desk. Microsoft should have the right to control access to its source code. Hollywood should have the right to charge admission to its movies. If one couldn't control access to these resources, or resources called "mine," one would have little incentive to work to produce these resources, including those called mine.

But likewise, and obviously, many resources should be free. The right to criticize a government official is a resource that is not, and should not be, controlled. I shouldn't need the permission of the Einstein estate before I test his theory against newly discovered data. These resources and others gain value by being kept free rather than controlled. A mature society realizes that value by protecting such resources from both private and public control.

We need to learn this lesson again. The opportunity for this learning is the Internet. No modern phenomenon better demonstrates the importance of free resources to innovation and creativity than the Internet. To those who argue that control is necessary if innovation is to occur, and that more control will yield more innovation, the Internet is the simplest and most direct reply. For as I will show in the chapters that follow, the defining feature of the Internet is that it leaves resources free. The Internet has provided for much of the world the greatest demonstration of the power of freedom-and its lesson is one we must learn if its benefits are to be preserved.

Yet at just the time that the Internet is reminding us about the extraordinary value of freedom, the Internet is being changed to take that freedom away. Just as we are beginning to see the power that free resources produce, changes in the architecture of the Internet-both legal and technical-are sapping the Internet of this power. Fueled by a bias in favor of control, pushed by those whose financial interests favor control, our social and political institutions are ratifying changes in the Internet that will reestablish control and, in turn, reduce innovation on the Internet and in society generally.

I am dead against the changes we are seeing, but it is too much to believe I could convince you that the full range is wrong. My aim is much more limited. My hope is to show you the other side of what has become a taken-for-granted idea-the view that control of some sort is always better. If you stay with me to the end, then I want you to leave this book simply with a question about whether control is best. I don't have the data to prove anything more than this limited hope. But we do have a history to show that there is something important here to understand.

THIS SHOWING moves in three steps. In the part that follows, I introduce more formally what I mean by "free." I relate that concept to the notion of "the commons" and then introduce three contexts where resources in the Internet are held in common. These commons are related to the innovation the Internet has produced. My aim in this first part is to show just how.

I then consider in part II a parallel environment for innovation and creativity in "real space"-the space not tied directly to the Internet, though increasingly affected by it. This is the space where records are now made, books are still written, and film is primarily shot. This space does not present the commons the Internet is-and for good reason, too. The character of production in real space does not permit the freedom that the Internet does. The constraint on creativity it yields there is a necessary, if unfortunate, feature of that space.

This context of creativity has been changed by the Internet. In the balance of part II, I offer examples of how. These examples will show how many of the constraints that affected real-space creativity have been removed by the architecture, and original legal context, of the Internet. These limitations, perhaps justified before, are justified no more.

Or at least, were justified no more. For the argument of the third and final part of this book is that the environment of the Internet is now changing. Features of the architecture-both legal and technical-that originally created this environment of free creativity are now being changed. They are being changed in ways that will reintroduce the very barriers that the Internet originally removed.

These barriers, however, don't have the neutral justification that the constraints of real-space economics do.15 If there are constraints here, it is simply because we are building them in. And as I will argue, there are strong reasons why many are trying to rebuild these constraints: they will enable these existing and powerful interests to protect themselves from the competitive threat the Internet represents. The old, in other words, is bending the Net to protect itself against the new.PART I. DOT.COMMONS

2. Building Blocks: "Commons" and "Layers"

THIS BOOK is fundamentally about the Internet and its effect on innovation, both commercial and non-. "Internet" and "society" are familiar enough notions. But at the core of my argument are two fairly obscure ideas that we must begin by making a bit more clear. The first of these is the idea of a "commons"; the second is the notion of "layers." The commons is an old idea; layers, in the sense made familiar by network theorists, are relatively new. But the two together organize the argument that follows. They are building blocks to an end that will help reveal the Internet's effect on society.

IF YOU'VE used the word commons before, you're likely to think of a park, as in the Boston Common. If you've studied economics or political science, your mind will race to tragedy (as in "the tragedy of the commons"). Both senses are related to what I mean, but neither alone is enough.1

The Oxford English Dictionary (mankind's first large-scale collaborative open source text project)2 equates the "commons" to a resource held "in common." That it defines as "in joint use or possession; to be held or enjoyed equally by a number of persons."3 In this sense, a resource held "in common" is "free" (as I've defined that term) to those "persons." In most cases, the commons is a resource to which anyone within the relevant community has a right without obtaining the permission of anyone else. In some cases, permission is needed but is granted in a neutral way.

Think about some examples:

The public streets are commons. Anyone is free to access the streets without first getting the permission of someone else. We don't auction rights of access, selling the right to use a particular bit of highway during a particular bit of time. (Of course there are exceptions.) Nor do we insist on particular licenses before we allow people to use the streets or highways. Instead the highways are open and free-in the sense I mean a commons to be free.

Parks and beaches are increasingly commons. Anyone is free to access these spaces without getting the permission of someone else. Access is not auctioned off to the highest bidder, and the right to control access is not handed off to some private or governmental entity. The resource-as Carol Rose calls it, "the recreational resource"-is made available to anyone.

Einstein's theory of relativity is a commons. It is a resource-a way of understanding the nature of the universe-that is open and free for anyone to take. Access to this resource is not auctioned off to the highest bidder; the right to use the theory is not allocated to a single organization.

Writings in the public domain are a commons. They are a resource that is open and free for anyone to take without the permission of anyone else. An 1890 edition of Shakespeare is free for anyone to take and copy. Your right to use and redistribute that 1890 text is without restraint.

Each of these resources is held in common. Each is "free" for others to take. Some are free in the sense that no price is paid (you can use most roads without paying a toll; as we will see, it would be unconstitutional in the United States to require anyone to pay to use Einstein's theory of relativity). Some are free even though a price must be paid (a park is "free" in the sense that I mean even if an access fee is required-as long as the fee is neutrally and consistently applied).4 In both cases, the essential feature is reasonable, and that access to the resource is not conditioned upon the permission of someone else. The essence, in other words, is that no one exercises the core of a property right with respect to these resources-the exclusive right to choose whether the resource is made available to others.5

Economists will object, however, that my list conflates two very different cases. Einstein's theory of relativity is different from the streets or public beaches. Einstein's theory is fully "nonrivalrous"; the streets and beaches are not. If you use the theory of relativity, there is as much left over afterward as there was before. Your consumption, in other words, does not rival my own. But roads and beaches are very different. If everyone tries to use the roads at the very same time (something that apparently happens out here in California often), then their use certainly rivals my own. Traffic jams; public beaches crowd. Your SUV, or your loud radio, reduces my ability to enjoy the roads or beach.

The economists are right. This list of resources held in "the commons" does conflate rivalrous with nonrivalrous resources. But our tradition is not as tidy as the economists' analytics. We have always described as "commons" both rivalrous and nonrivalrous resources. The Boston Common is a commons, though its resource is rivalrous (my use of it competes with your use of it). Language is a commons, though its resource is nonrivalrous (my use of it does not inhibit yours).6 What has determined "the commons," then, is not the simple test of rivalrousness. What has determined the commons is the character of the resource and how it relates to a community. In theory, any resource might be held in common (whether it would survive is another question). But in practice, the question a society must ask is which resources should be, and for those resources, how.

Here the distinction that the economists draw begins to help. Economists distinguish rivalrous and nonrivalrous resources because the issues or problems raised by each kind are different.

If a resource is nonrivalrous, then the problem is whether there is enough incentive to produce it, not whether there is too much demand to consume it. A nonrivalrous resource can't be exhausted. Once it is produced, it can't be undone. Thus the issue for nonrivalrous resources is whether the Edith Whartons of the world have enough incentive to create. The problem with nonrivalrous resources is to assure that I reap enough benefit to induce me to sow.

A rivalrous resource presents more problems. If a resource is rivalrous, then we must worry both about whether there is sufficient incentive to create it (if it is the sort of resource that humans produce) and about whether consumption by some will leave enough to others. With a rivalrous resource, I must still worry that I will reap enough benefit to make it worth it to sow. But I must worry as well that others not deplete the resource that I've produced. If a rivalrous resource is open to all, there is a risk that it will be depleted by the consumption of all.

This depletion of a rivalrous resource is the dynamic that biologist Garrett Hardin famously termed "the tragedy of the commons."7 "Picture a pasture open to all," Hardin writes, and consider the expected behavior of "herdsmen" who roam that pasture. Each herdsman must decide whether to add one more animal to his herd. In making a decision to do so, Hardin writes, the herdsman reaps a benefit, while everyone else suffers. The herds-man gets the benefit of one more animal, yet everyone suffers the cost, because the pasture has one more consuming cow. And this defines the problem: Whatever costs there are in adding another animal are costs that others bear. The benefits, however, are enjoyed by a single herdsman. Therefore each herdsman has an incentive to add more cattle than the pasture as a whole can bear. As Hardin describes the consequence:

Therein is the tragedy. Each man is locked into a system that compels him to increase his herd without limit-in a world that is limited. Ruin is the destination toward which all men rush, each pursuing his own best interest in a society that believes in the freedom of the commons. Freedom in a commons brings ruin to all.8

This "tragedy" consumes talk about "the commons."

"Ruin" is taken for granted as the destiny of those who believe in the "freedom of the commons." Hardheaded sorts thus scorn the rhetoric of undivided resources. Only the romantic wastes time wondering about anything different from the perfect control of property.

But obviously Hardin was not describing a law of nature that must apply to every good left in the commons. There is, for example, no tragedy for nonrivalrous goods left in the commons-no matter how many times you read a poem, there's as much left over as there was when you started. Nor is there always a tragedy even for rivalrous goods. As researchers have shown, in many different contexts, norms adequately limit the problem of over-consumption.9 Communities work out how to regulate overconsumption. How and why are certainly complex questions. But that some do is undeniable. 10

We therefore can't just jump from the observation that a resource is held "in common" to the conclusion that "freedom in a commons brings ruin to all." Instead, we must think empirically and look at what works. Where there is a benefit from leaving a resource free, we should see whether there is a way to avoid overconsumption, or inadequate incentives, without its falling under either state or private (market) control.

My central claim throughout is that there is a benefit to resources held in common and that the Internet is the best evidence of that benefit. As we will see, the Internet forms an innovation commons. It forms this commons not just through norms, but also through a specific technical architecture. The Net of these norms and this architecture is a space where creativity can flourish. Yet so blind are we to the possible value of a commons that we don't even notice the commons that the Internet is. And, in turn, this blindness leads us to ignore changes to the norms and architecture of the Net that weaken this commons. There is a tragedy of the commons that we will identify here; it is the tragedy of losing the innovation commons that the Internet is, through the changes that are being rendered on top.11LAYERS

THE IDEA of the commons may be obscure, but the notion of "layers" is more easily recognized. The layers that I mean here are the different layers within a communications system that together make communications possible. The idea is taken from perhaps the best communications theorist of our generation, NYU law professor Yochai Benkler.12 As he uses the idea, it helps organize our thought about how any communications system functions. But in organizing our thought, his work helps show something we might otherwise miss.

Following the technique of network architects, Benkler suggests that we understand a communications system by dividing it into three distinct "layers."13 At the bottom is a "physical" layer, across which communication travels. This is the computer, or wires, that link computers on the Internet. In the middle is a "logical" or "code" layer-the code that makes the hardware run. Here we might include the protocols that define the Internet and the software upon which those protocols run. At the top is a "content" layer-the actual stuff that gets said or transmitted across these wires. Here we include digital _resources/books//The Future of Ideas_ The Fate of the Commo - Lawrence Lessig, texts, on-line movies, and the like. These three layers function together to define any particular communications system.

Each of these layers in principle could be controlled or could be free. Each, that is, could be owned or each could be organized in a commons. We could imagine a world where the physical layer was free but the logical and content layers were not. Or we could imagine a world where the physical and code layers were controlled but the content layer was not. And so on.

Consider some examples to make the possibilities real.

Speakers' Corner: Speakers' Corner is a place in London 's Hyde Park where people who want to speak publicly gather on Sundays to deliver their speeches. It is a wonderfully English spectacle, ordinarily filled with both orators and loons. But the system of communication is distinctive: the physical layer (the park) is a commons; the code layer (the language used) is a commons, too; and the content layer is ordinarily unowned-what these nuts say is their own creation. All three layers in this context are free; no one can exercise control over the kinds of communications that might happen here.

Madison Square Garden: Madison Square Garden is another place where people give speeches or, more likely, play games. It is a huge stadium/ auditorium near the center of Manhattan, owned by Madison Square Garden, L.P. Only those who pay get to use the auditorium; and the Garden is not obligated to take all comers. The physical layer is therefore controlled. But as with Speakers' Corner, both the code layer (the language) and the content layer (what gets uttered) are at least sometimes not controlled. They too can remain free.

The telephone system: The telephone system before its breakup was a single unitary system. The physical infrastructure of this system was owned by AT&T and its affiliates; so too was its logical infrastructure-determining how and who you could connect-controlled by AT&T. But what you said on an AT&T phone (within limits, at least)14 was free: the content of the telephone conversations was not controlled, even if the physical and code layers underneath were.

Cable TV: Finally, think of cable TV. Here the physical layer is owned-the wires that run the content into your house. The code layer is owned-only the cable companies get to decide what runs into your house. And the content layer is owned-the shows that get broadcast are copyrighted shows. All three layers are within the control of the cable TV company; no communications layer, in Benkler's sense, remains free.

These examples suggest the range of ways of organizing systems of communications. No single mix is best, though the differences among the four are important. To the extent that we want a decentralized system of communications, unowned layers will help. To the extent that we want controlled systems of communications, owned layers will help. But the point of the scheme so far is not to make predictions. The point is simply to make clear the range, and that trade-offs within this range exist.

Speakers' Corner|Madison Square Garden|Telephone System|Cable TV

Content

Free|Free|Free|Controlled

Code

Free|Free|Controlled|Controlled

Physical

Free|Controlled|Controlled|Controlled

Now, from the language I've used so far, you might think that the Internet is a communications system free all the way down-free, that is, at every one of Benkler's layers. It is not. What is special about the Internet is the way it mixes freedom with control at different layers. The physical layer of the Internet is fundamentally controlled. The wires and the computers across which the network runs are the property of either government or individuals. Similarly, at the content layer, much in the existing Internet is controlled. Not everything served across the Net is free for the taking. Much is properly and importantly protected by property law.

At the code layer, however, in ways that will become clearer below, the Internet was free. So too was much of the content served across the network free. The Internet thus mixed both free and controlled layers, not just layers that were free.

Our aim is to understand how this mix produced the innovation that we have seen so far and why the changes to this mix will kill what we have seen so far.3. Commons on the Wires

THE INTERNET is a network of networks. In the main, these networks connect over wires. All of these wires, and the machines linked by them, are controlled by someone. The vast majority are owned by private parties-owned, that is, by individuals and corporations that have chosen to link to the Net. Some are owned by the government.

Yet this vast network of privately owned technology has built one of the most important innovation commons that we have ever known. Built on a platform that is controlled, the protocols of the Internet have erected a free space of innovation. These private networks have created an open resource that any can draw upon and that many have. Understanding how, and in what sense, is the aim of this chapter.

PAUL BARAN was a researcher at the Rand Corporation from 1959 to 1968. His project in the early 1960s was communications reliability. The fear slowly dawning upon the leaders of the world's largest nuclear arsenal was that the communications system controlling that arsenal was vulnerable to the smallest of attacks. An accident, or a single nuclear explosion, could disable the ability of the commander in chief to command. Chaos-or worse-would be unavoidable.

Baran's task was to explore a more secure telecommunications system. His first step was to understand the system then in place. So he asked the then provider of telecommunications in America, American Telephone & Telegraph, to see the plans for the AT&T network to determine whether the communications system was secure.

AT&T balked. Though Baran had the proper security clearance, and though the Defense Department supported his request, AT&T refused Baran's inquiry. They had studied the matter, AT&T reported. The system was secure.

This was "the Bell system." It is hard for us today to appreciate the power of such a company. This was not just a large company, or even a large company with a very large market share. This was a partner with the government, ruling telecommunications in America. It was therefore, in its own view of itself, the governor of communications. States and the Federal Communications Commission (FCC) might regulate it, but the information and cooperation to make that regulation possible came from AT&T. It had been, since interconnection began in earnest in 1912, America 's telecommunications master.1

Things with telephones were not always this way. Indeed, the early history of telecommunications is essentially unrecognizable to us. Though the Bell companies held the first patents on telephone technology, once those patents expired, a vigorous competition emerged to bring telephone service to Americans. AT&T concentrated on businesses. "Independents" focused on residences. The competition produced a rapid expansion of coverage. "From 1900 to 1915, at least 45% of the U.S. cities with populations over 5,000 had competing, non-interconnected telephone exchanges. During the peak of the independent movement's strength, between 1902 and 1910, that percentage was more than 55%."2

Today we would not recognize the phone system that this early competition produced. Though the reach of the telephone network was great-in 1920, 38.7 percent of farms and 30 percent of residences had a telephone3-the networks did not interconnect. There was no guarantee that if your grandmother across town had a telephone, you, using yours, could call her. Thus when you purchased telephone service, your decision in part depended upon whom you wanted to call and what service they were likely to have.

The world was then with telephones as the world was with personal computers ten years ago,4 or as the world with instant messaging is today. Though there was a dominant system (AT&T for phones; Microsoft/Intel for computers; AOL's AIM for instant messaging), there was vigorous competition among other systems (the "independents" for phones; Apple's Macintosh or IBM's OS/2 for computers; Yahoo! or MSN for messaging). This competition effectively pushed the dominant system to become better and different. Just as the windows of Macintosh pushed Microsoft to Windows, so too the rural service of the "independents" pushed AT&T to extend its reach to farmers.

After a while, however, AT&T grew weary of this competition. The view grew within the company that security would come only by merging with the competitors. From 1908 to 1913, the Bell system adopted a number of strategies to destroy the independents, including selective interconnection and acquisition of competitors. If it could not gain customers through direct competition, it would gain customers by purchasing competitors.5

Initially, this consolidation inspired skepticism among regulators and the public. AT&T was attacked as a monster seeking monopoly. But by the early 1920s, antitrust enforcement in the United States was waning. The spirit of the time favored consolidation and rationalization; competition was viewed as "ruinous." Thus AT&T was slowly able to secure agreements with the government that essentially permitted it to extend its reach while protecting it against antitrust review.

Paradoxically, AT&T's most effective weapon in this expansion was to offer competitors the ability to interconnect. Though our intuition is likely to tell us that it was the failure to interconnect that hampered competition, in fact, as economist Milton Mueller has effectively argued, it was a lack of interconnection that spurred competition. 6 As each independent interconnected to the AT&T system, any distinctive advantage it could offer as an independent disappeared. Consumers had no further interest in subscribing to it over AT hence the drive to AT&T as universal provider was only increased. The network advantage of AT&T would grow relative to other independents; hence the power of AT&T's increasing monopoly was enhanced.

Independents at the time understood this dynamic. Associations of independents vigorously attacked the "traitors" among them that chose to interconnect with AT&T.7 But these competitors were increasingly seen as inconveniences, by both regulators and the public. The idea of a world of "universal service"-meaning not a telephone in every house, but a system where every phone could reach every other phone-was too seductive.8 So in 1913 the government entered into an agreement with AT&T that would secure its monopoly in telecommunications in America, even though it was sold as a solution to telecommunications monopoly.

Named after Bell vice president Nicholas C. Kingsbury, the "Kingsbury Commitment" required that Bell "stop acquiring independent phone companies and [] connect the remaining independents to Bell 's long-distance network."9 Bell also had to divest its telegraph arm, Western Union. This stopped the company from its increasingly ravenous practice of acquisition, but it "did nothing to promote competition in either telephony or telegraphy."10 The commitment did not force local exchanges to be more competitive. It did not require interconnection with other long-distance carriers. The solution, "in short, was not the steamy unsettling cohabitation that marks competition, but rather a sort of competitive apartheid, characterized by segregation and quarantine."11 As a major treatise on telecommunications describes it:

The Kingsbury Commitment could be viewed as a solution only by a government bookkeeper who counted several separate monopolies as an advance over a single monopoly, even absent any trace of competition among them.12

Monopolies are not all bad, and no doubt this monopoly did lots of good. AT&T produced an extraordinary telephone system, linking 85 percent of American homes at the peak of its monopoly power in 1965. 13 It spent billions of dollars to support telecommunications research. Bell Labs invented fiber optic technology, the transistor, and scads of other major technological advances. Its scientists earned at least half a dozen Nobel Prizes in physics.

And it attracted a certain kind of person. As Paul Baran described it:

They were not motivated by making a lot of money. They were in the business to provide a service: Loyalty to the organization and help to the country providing the world's best communication. And that was their motivation and their belief. It was a religion, a pure religion In their mind, they were doing the right thing.14

These were not fat monopolists seeking to rob the nation of a quick buck. These were "soldiers of communications," for whom control and hierarchy were key. As one publication in 1941 put it:

Because each of them has a part in this speeding of the spoken word, the thousands of men and women who are engaged in the telephone service in America are ever conscious of the fact that theirs is a high calling.15

AT&T in turn succeeded during its monopoly reign in attracting the very best telecommunications researchers. Baran attributes its success to "an absolutely brilliant compensation system,"16 but the reason may well be that AT&T was the only show in town. As Baran describes, "[F]or years and years, that was the only place in the country that was doing work in telecommunications."17 One could research different telecommunications systems, and one could in principle even develop other telecommunications systems. But there was nothing one could do with one's innovation unless AT&T bought it.

For much of the twentieth century, it was essentially illegal even to experiment with the telephone system. It was a crime to attach a device to the telephone system that AT&T didn't build or expressly authorize. In 1956, for example, a company built a device called a "Hush-a-Phone." The Hush-a-Phone was a simple piece of plastic that attached to the mouthpiece of a telephone. Its design was to block noise in a room so that someone on the other end of the line could better hear what was being said. The device had no connection to the technology of the phone, save the technology of the plastic receiver. All it did was block noise, the way a user might block noise by cupping his hand over the phone.18

When the Hush-a-Phone was released on the market, AT&T objected. This was a "foreign attachment." Regulations forbade any foreign attachments without AT&T's permission. AT&T had not given Hush-a-Phone any such permission. The FCC agreed with AT&T. Hush-a-Phone was history.

Hush-a-Phone is an extreme case.19 The real purpose of the foreign attachments rule was, at least as AT&T saw it, to protect the system from dirty technology. A bad telephone or a misbehaving computer attached to the telephone system could, AT&T warned, bring down the system for the whole region. Telephones were lifelines, and they had to be protected from the experiments of an inquisitive nation. Rules such as the foreign attachments rules were intended to achieve this protection.

Whatever their intent, however, these rules had an effect on innovation in telecommunications. Their effect was to channel innovation through Bell Labs. Progress would be as Bell Labs determined it. Experiments would be pursued as Bell Labs thought best. Thus telecommunications would evolve as Bell Labs thought best.

BARAN UNDERSTOOD this. As a researcher at a Defense Department-supported lab, he knew how the "military" thought, and AT&T was military. Thus he had reason to be skeptical about the claims that the existing system would withstand a nuclear attack. He didn't believe AT&T really understood the threat. And if it did, he believed it simply didn't want anyone else understanding its weakness.

So he pushed AT&T to let him examine the system. It pushed back. And so, from sources unnamed, Baran secured a copy of AT&T's plans-the blueprints for the telecommunications system of the United States.

When he saw the plans, Baran knew AT&T was wrong. He was certain that the system it had built would not withstand a nuclear attack. The network was too concentrated; it had no effective redundancy. So he continued to press his idea for a different telecommunications system. He had a different design for telecommunications, and he wanted AT&T to help him build it.

This different model was not the Internet, but it was close to the Internet. Baran proposed a kind of packet-switching technology to replace the persistent circuits around which the telephone system was built. Under AT&T's design, when you called someone in Paris, a circuit was opened between you and Paris. In principle, you could trace the line of copper that linked you to Paris; along that line of copper, all your conversation would travel.

Baran's idea was fundamentally different. If you digitized a conversation-translating it from waves to bits-and then chopped the resulting stream into packets, these packets could flow independently across a network and create the impression of a real-time connection on the other end. As long as they flowed fast enough, and the computers at both ends were quick, the conversation encoded in this packet form would seem just like a conversation along a single virtual wire across the ocean.

Baran was probably not the first person to come up with this idea-MIT loyalists insist that that was Leonard Kleinrock.20 And he was also not the only person working on the idea in the early 1960s. Independently, in England, Donald Davies was developing something very similar.21 But whether the first, or the only, doesn't really matter for our purposes here. What is important is that Baran outlined a telecommunications system fundamentally different from the dominant design, and that different telecommunications system would have effected a radically different evolution of telecommunications.

BARAN pushed to get AT&T to help build this alternative design. AT&T said he didn't understand telephones. Over the course of many months, he attended classes sponsored by AT&T so that he would get with its program. But the more Baran saw, the more convinced he was. And in a final push, the Defense Department offered simply to pay AT&T to build the system. The government promised no risk; it wanted only cooperation. But even here, AT&T balked. As recounted in John Naughton's A Brief History of the Future:22

[AT&T's] views were once memorably summarised in an exasperated outburst from AT&T's Jack Osterman after a long discussion with Baran.

'First,' he said, 'it can't possibly work, and if it did, damned if we are going to allow the creation of a competitor to ourselves.' 23

"Allow." Here is the essence of the AT&T design, supported by the state-sanctioned monopoly. In "defend[ing] the monopoly,"24 it reserved to itself the right to decide what telecommunications would be "allowed." As Baran put it, AT&T "didn't want anybody in their vicarage."25 It controlled the wires; nothing but its technology could be attached, and no other system of telecommunications would be permitted. One company, through one research lab, with its vision of how communications should occur, decided. Innovation here, for this crucial aspect of modern economic life, was as this single organization would decide.

Now again, the point is not that AT&T was evil. Indeed, quite the contrary. We get nowhere in understanding how systems of innovation work when we personify organizations and imagine them responsible for social goals. AT&T had an obligation to its stockholders; it had an obligation to the government to assure consistent quality service. It was simply acting to assure that it met both of these obligations-maximizing its profits for its shareholders while meeting its obligations to the government.

But what's good for AT&T is not necessarily good for America. What AT&T was doing may well have made sense for it; its vision of telecommunications may well have made sense for the interests it understood itself to be serving. But AT&T's vision of what a telecommunications service should be is not necessarily what a telecommunications service should be. There is a possible-and in this case actual-conflict between the interests of a centralized controller of innovation and the interest in innovation generally.

Here the conflict was plain. If the Defense Department built a telecommunications system based on packets rather than circuits, then the efficiency of that system could in theory be much greater. When you're on a circuit-switched system, listening to your lover in Paris tell you about someone new, there's lots of downtime on the line-silence-that is just wasted bandwidth. If instead the system were packets, then the data from the downtime would be silence; it's easier to send the information necessary to reproduce silence than it is to hold open a line while silence happens. The system could better utilize the wires if the architecture enabled the sharing of the wires.

The owner of a legacy system built on a different model could well decide that this challenge was too dangerous. If a more efficient system came on-line, there would be strong pressure from the government to allow the exception; that exception would not be easy to limit; the corrosion of the existing model could be great. Monopoly control would be lost.

Thus it is completely understandable that a company like AT&T would not want to give birth to this new competitor, even if this new competitor would be better for communications as a whole. The natural desire of any company is to find ways to protect its market. And the chosen desire of a competitive market is to limit the ways in which a company can protect its market-but for most of the century, this chosen desire was not telecommunications policy. For most of the century, in this context and others that we will consider later on, the chosen desire of policy makers was to back up the desire of companies to architect and support systems that protected them against competition in the market. Competition was a bother; the vision of a telecommunications system was limited; and our telecommunications architecture-including, as we will see, broadcasting and radio-was architected to maximize the power and control of the few.26

AT A CERTAIN point, Baran understood. When the project was pushed into the Defense Communications Agency (DCA), Baran realized the project would be bungled. As he told author John Naughton:

I felt that [DCA] could be almost guaranteed to botch the job since they had no understanding of digital technology, nor for leading edge high technology development. Further, they lacked enthusiasm. Sometimes, if a manager doesn't have the staff but has the drive and smarts to assemble the right team, one could justify taking a chance. But lacking skills, competence and motivation meant backing a sure loser.27

So Baran had the project pulled. There were not "the people at the time who could successfully undertake this project, [and they] would likely screw up the program. An expensive failure would make it difficult for a more competent agency to later undertake the project." 28 Thus, this architecture of control-centralizing innovation and protecting an existing model of doing business-would not be questioned by Baran's work. At least not then.

THE INTERNET is not the telephone network. It is a network of networks that sometimes run on the telephone lines. These networks and the wires that link them are privately owned, like the wires of the old AT&T. Yet at the core of this network is a different principle from the principle that guided AT&T. Like the principle Baran confronted, this principle affects what is allowed and what is not. And like the principle that Baran confronted, this principle has an effect on innovation.

First described by network architects Jerome Saltzer, David Clark, and David P. Reed in 1981, this principle-called the "end-to-end argument" (e2e)-guides network designers in developing protocols and applications for the network.29 End-to-end says to keep intelligence in a network at the ends, or in the applications, leaving the network itself to be relatively simple.

There are many principles in the Internet's design. This one is key. But it will take some explaining to show why.

Network designers commonly distinguish computers at the "end" or "edge" of a network from computers within that network. The computers at the end of a network are the machines you use to access the network. (The machine you use to dial into the Internet, or your cell phone connecting to a wireless Web, is a computer at the edge of the network.) The computers "within" the network are the machines that establish the links to other computers-and thereby form the network itself. (The machines run by your Internet service provider, for example, could be computers within the network.)

The end-to-end argument says that rather than locating intelligence within the network, intelligence should be placed at the ends: computers within the network should perform only very simple functions that are needed by lots of different applications, while functions that are needed by only some applications should be performed at the edge. Thus, complexity and intelligence in the network are pushed away from the network itself. Simple networks, smart applications. As a recent National Research Council (NRC) report describes it:

Aimed at simplicity and flexibility, [the end-to-end] argument says that the network should provide a very basic level of service-data transport-and that the intelligence-the information processing needed to provide applications-should be located in or close to the devices attached to the edge [or ends] of the network.30

The reason for this design was flexibility, inspired by a certain humility. As Reed describes it, "we wanted to make sure that we didn't somehow build in a feature of the underlying network technology that would restrict our using some new underlying transport technology that turned out to be good in the future That was really the key to why we picked this very, very simple thing called the Internet protocol."31

It might be a bit hard to see how a principle of network design could matter much to issues of public policy. Lawyers and policy types don't spend much time understanding such principles; network architects don't waste their time thinking about the confusions of public policy.

But architecture matters.32 And arguably no principle of network architecture has been more important to the success of the Internet than this single principle of network design-e2e. How a system is designed will affect the freedoms and control the system enables. And how the Internet was designed intimately affected the freedoms and controls that it has enabled. The code of cyberspace-its architecture and the software and hardware that implement that architecture-regulates life in cyberspace generally. Its code is its law. Or, in the words of Electronic Frontier Foundation (EFF) cofounder Mitch Kapor, "Architecture is politics."33

To the extent that people have thought about Kapor's slogan, they've done so in the context of individual rights and network architecture. Most think about how "architecture" or "software" or, more simply, "code" enables or restricts the things we think of as human rights-speech, or privacy, or the rights of access.

That was my purpose in Code and Other Laws of Cyberspace. There I argued that it was the architecture of cyberspace that constituted its freedom, and that, as this architecture was changed, that freedom was erased. Code, in other words, is a law of cyberspace and, as the title suggests, in my view, its most significant law.

But in this book, my focus is different. The question I want to press here is the relationship between architecture and innovation-both commercial innovation and cultural innovation. My claim is that here, too, code matters. That to understand the source of the flourishing of innovation on the Internet, one must understand something about its original design. And then, even more important, to understand as well that changes to this original architecture are likely to affect the reach of innovation here.

SO WHICH code matters? Which parts of the architecture?34

The Internet is not a novel or a symphony. No one authored a beginning, middle, and end. At any particular point in its history, it certainly has a structure, or architecture, that is implemented through a set of protocols and conventions. But this architecture was never fully planned; no one designed it from the bottom up. It is more like the architecture of an old European city, with a central section that is clear and well worn, but with additions that are many and sometimes confused.

At various points in the history of the Net's development, there have been efforts at restating its principles. Something called "RFC 1958," published in 1996, is perhaps the best formal effort. The Internet was built upon "re-quests for comments," or RFCs. Researchers-essentially grad students-charged with the task of developing the protocols that would eventually build the Internet developed these protocols through these humble requests for comments. RFC 1 was written by Steve Crocker and outlined an understanding about the protocols for host ("IMP") software. Some RFCs specify particular Internet protocols; some wax philosophical. RFC 1958 is clearly in the latter camp-an "informational" document about the "Architectural Principles of the Internet."35

According to RFC 1958, though "[m]any members of the Internet community would argue that there is no architecture," this document reports that "the community" generally "believes" this about the Internet: "that the goal is connectivity, the tool is the Internet protocol and the intelligence is end-to-end rather than hidden in the network."36 "The network's job is to transmit datagrams as efficiently and flexibly as possible. Everything else should be done at the fringes."37

This design has important consequences for innovation-indeed, we can count three:

o First, because applications run on computers at the edge of the network, innovators with new applications need only connect their computers to the network to let their applications run. No change to the computers within the network is required. If you are a developer, for example, who wants to use the Internet to make telephone calls, you need only develop that application and get users to adopt it for the Internet to be capable of making "telephone" calls. You can write the application and send it to the person on the other end of the network. Both of you install it and start talking. That's it.

o Second, because the design is not optimized for any particular existing application, the network is open to innovation not originally imagined. All the Internet protocol (IP) does is figure a way to package and route data; it doesn't route or process certain kinds of data better than others. That creates a problem for some applications (as we'll see below), but it creates an opportunity for a wide range of other applications too. It means that the network is open to adopting applications not originally foreseen by the designers.

o Third, because the design effects a neutral platform-neutral in the sense that the network owner can't discriminate against some packets while favoring others-the network can't discriminate against a new innovator's design. If a new application threatens a dominant application, there's nothing the network can do about that. The network will remain neutral regardless of the application.

The significance of each of these consequences to innovation generally will become apparent as we work through the particulars that follow. For now, all that's important is that you see this design as a choice. Whether or not the framers of the network understood what would grow from what they built, they built it with a certain philosophy in mind. The network itself would not control how it would grow. Applications would. That was the key to end-to-end design. As the inventor of the World Wide Web, Tim Berners-Lee, describes it:

Philosophically, if the Web was to be a universal resource, it had to be able to grow in an unlimited way. Technically, if there was any centralized point of control, it would rapidly become a bottleneck that restricted the Web's growth, and the Web would never scale up. Its being "out of control" was very important.38

NETWORK ARCHITECTS Saltzer, Clark, and Reed were not the only people to notice the value of an end-to-end design. Quite independently, if later, the idea became apparent within AT&T itself. In the early 1990s, while trying to implement an improvement in the voice quality of the AT&T network (competition was beginning to have an effect: the effort was in response to the claim by Sprint that on its network you could hear a pin drop), Bell Labs researcher David Isenberg became increasingly frustrated with the "smart" network that AT&T was: at every layer in the distributional chain, the AT&T network had been optimized for voice telephony. But this optimization meant that any effort to change a layer in the AT&T distributional chain would disable other layers. Tweaking one part threw other parts into disarray. The system was in no sense "modularized," so change became impossibly difficult.

This led Isenberg to a treasonous thought: what if the problem was in the fundamental design of the network itself? What if the whole idea of a smart network was a mistake? What if a better design would be a "stupid network," with intelligence built into the devices, and the network itself kept as simple as possible?39

Isenberg had arrived through frustration at Saltzer, Clark, and Reed's fundamental insight: A simple, or, as Isenberg described it, stupid network would facilitate the greatest degree of innovation. A smart, or intelligent, network would perhaps be optimized for certain users, but its own sophistication would inhibit different or new uses not initially understood. By "build[ing] in assumptions about what the business proposition of the network is, you constrain what's possible." 40 The AT&T network was burdened by the intelligence built into it. A simpler design could beat the sophisticated design, at least along the dimension of innovation and change.

When Isenberg started to discuss his seditious thoughts, his employer, AT&T, was not happy. In the early summer of 1997, he was permitted to post a reply to an article that sang the virtues of smart networks. But soon after his article was posted, it was republished in many different places on the Net. Finally, in August 1997, Harry Newton published the article in his Computer Telephony magazine-without AT&T's permission. Isenberg became the enemy from within the AT&T network. He was told not to accept invitations from others to discuss his ideas. This control, understandably, became intolerable. As he told me, "[T]he AT&T pension became portable on January 1, 1998. I quit on January 2, 1998."

However disliked high up within the fortress, Isenberg's ideas began to catch on both outside and inside. The virtues of "stupid networks" became increasingly obvious, as the power of this simple network, the Internet, became undeniable. Isenberg's idea echoed the end-to-end principle: the two were the same, and both showed why the Internet would flourish.

* * *

THE INTERNET isn't the only network to follow an end-to-end design, though it is the first large-scale computer network to choose that principle at its birth. The electricity grid is an end-to-end grid; as long as my equipment complies with the rules for the grid, I get to plug it in.41 Conceivably, things could be different. In principle, we might imagine that every device you plug into a grid would register itself with the network before it would run. Before you connected, you would have to get permission for that device. The owner of the network could then choose which devices to prohibit.

Likewise, the roads are end-to-end systems. Any car gets to enter the highway grid (put tolls to one side). As long as the car is properly inspected, and the driver properly licensed, whether and when to use the highway is no business of the highway. Again, we could imagine a different architecture: each car might first register with the grid before it got on the highway (the way airlines file flight plans before they fly).

But these systems don't require this sort of registration, likely because, when they were built, such registration was simply impracticable. The electronics of a power grid couldn't handle the registration of different devices; roads were built stupid because smart roads were impossible. Things are different now; smart grids, and smart roads, are certainly possible. Control is now feasible. So we should ask, would control be better?

In at least some cases, it certainly would be better. But from the perspective of innovation, in some cases it would not. In particular, when the future is uncertain-or more precisely, when future uses of a technology cannot be predicted-then leaving the technology uncontrolled is a better way of helping it find the right sort of innovation. Plasticity-the ability of a system to evolve easily in a number of ways-is optimal in a world of uncertainty.

This strategy is an attitude. It says to the world, I don't know what functions this system, or network, will perform. It is based in the idea of uncertainty. When we don't know which way a system will develop, we build the system to allow the broadest range of development.

This was a key motivation of the original Internet architects. They were extremely talented; no one was more expert. But with talent comes humility. And the original network architects knew more than anything that they didn't know what this network would be used for.

As David Reed describes, "[T]here were a lot of experiments in those days," and "we realized that [there] was very little in common [other] than the way they used the network. There were sort of interesting ways that they used the network differently from application to application. So we felt that we couldn't presume anything about how networks would be used by applications. Or we wanted to presume as little as possible We basically said, 'Stop. You're all right' as opposed to running a bake-off."42 These designers knew only that they wanted to assure that it could develop however users wanted.

Thus, end-to-end disables central control over how the network develops. As Berners-Lee puts it, "There's a freedom about the Internet: as long as we accept the rules of sending packets around, we can send packets containing anything to anywhere."43 New applications "can be brought to the Internet without the need for any changes to the underlying network."44 The "architecture" of the network is designed to be "neutral with respect to applications and content."45 By placing intelligence in the ends, the network has no intelligence to tell which functions or content are permitted or not. As RFC 1958 puts it, the job of the network is simply to "transmit datagrams." As the NRC has recently concluded:

Underlying the end-to-end argument is the idea that it is the system or application, not the network itself, that is in the best position to implement appropriate protection.46

In chapter 2, I introduced the idea of a commons. We can now see how the end-to-end principle renders the Internet an innovation commons, where innovators can develop and deploy new applications or content without the permission of anyone else. Because of e2e, no one need register an application with "the Internet" before it will run; no permission to use the bandwidth is required. Instead, e2e means the network is designed to assure that the network cannot decide which innovations will run. The system is built-constituted-to remain open to whatever innovation comes along.

This design has a critical effect on innovation. It has been, in the words of the NRC, a "key to the explosion of new services and software applications" on the Net.47 Because of e2e, innovators know that they need not get the permission of anyone-neither AT&T nor the Internet itself-before they build a new application for the Internet. If an innovator has what he or she believes is a great idea for an application, he or she can build it without authorization from the network itself and with the assurance that the network can't discriminate against it.

At this point, you may be wondering, So what? It may be interesting (at least I hope you think this) to learn that the Internet has this feature; it is at least plausible that this feature induces a certain kind of innovation. But why do we need to worry about this feature of the Internet? If this is what makes the Internet run, then as long as we have the Internet, won't we have this feature? If e2e is in the Internet's nature, why do we need to worry about e2e?

But this raises the fundamental point: The design the Internet has now need not be its design tomorrow. Or more precisely, any design it has just now can be supplemented with other controls or other technology. And if that is true, then this feature of e2e that I am suggesting is central to the network now can be removed from the network as the network is changed. The code that defines the network at one time need not be the code that defines it later on. And as that code changes, the values the network protects will change as well.

THE CONSEQUENCES of this commitment to e2e are many. The birth of the World Wide Web is just one. If you're free from geekhood, you are likely not to distinguish the WWW from the Internet. But in fact, they are quite distinct. The World Wide Web is a set of protocols for displaying hyper-linked documents linked across the Internet. These protocols were developed in the late 1980s by researchers at the European particle physics lab CERN-in particular by Tim Berners-Lee. These protocols specify how a "Web server" serves content on the WWW. They also specify how "browsers"-such as Netscape Navigator or Microsoft's Internet Explorer-retrieve content on the World Wide Web. But these protocols themselves simply run on top of the protocols that define the Internet. These Internet protocols, referred to as TCP/IP, are the foundation upon which the protocols that make the World Wide Web function-HTTP (hypertext transfer protocol) and HTML (hypertext markup language)-run.48

The emergence of the World Wide Web is a perfect illustration of how innovation works on the Internet and of how important a neutral network is to that innovation. Tim Berners-Lee came up with the idea of the World Wide Web after increasing frustration over the fact that computers at CERN couldn't easily talk to each other. Documents built on one system were not easily shared with other systems; content stored on individual computers was not easily published to the networks generally. As Berners-Lee writes:

Incompatibility between computers had always been a huge pain in every-one's side, at CERN and anywhere else The real world of high-energy physics was one of incompatible networks, disk formats, and character-encoding schemes, which made any attempt to transfer information between computers generally impossible. The computers simply could not communicate with each other.49

Berners-Lee thus began to think about a system to enable linking among documents-through a process called "hypertext"-and to build this linking on top of the protocols of the Internet. His ideal was a space where any document in principle could be linked to any other and where any document published was available to anyone.

The components of this vision were nothing new. Hypertext-links from one document to another-had been born with Vannevar Bush,50 and made famous by Bill Atkinson's HyperCard on the Apple Macintosh. The world where documents could all link to each other was the vision of Robert Fano in an early article in the Proceedings of the IEEE. 51 But Berners-Lee put these ideas together using the underlying protocol of the Internet. Hyperlinked documents would thus be available to anyone with access to the Internet, and any document published according to the protocols of the World Wide Web would be available to all.

The idea strikes us today as genius. Its success makes us believe the idea must have been obvious. But what is amazing about the story of the birth of the World Wide Web is how hard it was for Tim Berners-Lee to convince anyone of the merit in the plan. When Berners-Lee tried to sell the plan at CERN, management was unimpressed. As Berners-Lee writes:

What we hoped for was that someone would say, "Wow! This is going to be the cornerstone of high-energy physics communications! It will bind the entire community together in the next ten years. Here are four programmers to work on the project and here's your liaison with Management Information Systems. Anything else you need, you just tell us." But it didn't happen.52

When he went to a meeting of hypertext fans, he could get few to understand the "ah-ha" of hypertext on the Net. For years he wandered from expert to expert, finding none who understood the potential here. And it was only after he started building the Web out, and started informing ordinary people on a hypertext mailing list about the protocols he was developing, that the Net started to grow.

The experts didn't get it. Someone should put that on a bumper sticker and spread it around. Those controlling the resources of the CERN computer lab wouldn't support the technology that would give the world the Web. Only those innovators outside of the control of these managers saw something of the potential for the Web's growth.

Berners-Lee feared that competing protocols for using the Internet would wipe away interest in the WWW. One protocol built about the same time was called Gopher. Gopher enabled the easy display of a menu of options from a site. When you went to a Gopher-enabled site, you would see a list of links that you could then click on to perform some function. Gopher was extremely popular as an Internet application-running on the Internet protocols-and use of Gopher took off in the early 1990s.53

But for the purposes that Berners-Lee imagined, Gopher was extremely limited. It would not enable the easy construction of interlinked documents. It was closer to a universal menuing system than a system for linking ideas. Berners-Lee was afraid that this inferior standard would nonetheless stick before the new and better WWW became well known.

His fear, however, was not realized, both because of something Berners-Lee did and because of something the creators of Gopher did-and both are lessons for us.

Berners-Lee was no bully. He was not building a protocol that everyone had to follow. He had a protocol for displaying content on the World Wide Web-the HTML language that Web pages are built in. But he decided not to limit the content that one could get through a WWW browser to just Web pages. Instead he designed the transfer protocol-HTTP-so that a wide range of protocols could be accessed through the WWW-including the Gopher protocol, a protocol for transferring files (FTP), and a protocol for accessing newsgroups on the Internet (NNTP). The Web would be neutral among these different protocols-it would in this sense interconnect.54

That made it easy to use the Web, even if one wanted to get access to Gopher content. But the second doing was much more important to the death of Gopher as a standard.

As Berners-Lee describes it, high off its success in populating the world with Gopher, the University of Minnesota-owner of the right to Gopher-suggested it might exercise its rights to charge for the use of the Gopher protocol.55 Even the suggestion of this terrified developers across the world. (It was, Berners-Lee writes, "an act of treason."56) Would developers be hijacked by the university once they depended upon their system? How much would they lose if the platform eventually turned against the developers?

Berners-Lee responded to this by convincing CERN to release the right to the Web to the public. At first he wanted to release the protocol under the GPL, or General Public License (the "GNU General Public License," which we will see much more of in chapter 4). But when negotiations over that bogged down, he convinced CERN simply to release the rights into the public domain. Anyone had the right to take and use the protocols of the WWW and build anything upon them that they wanted. 57

The birth of the Web is an example of the innovation that the end-to-end architecture of the original Internet enabled. Though no one quite got it-this the most dramatic aspect of the Internet's power-a few people were able to develop and deploy the protocols of the World Wide Web. They could deploy it because they didn't need to convince the owners of the network that this was a good idea or the owners of computer operating systems that this was a good idea. As Berners-Lee put it, "I had designed the Web so there should be no centralized place where someone would have to 'register' a new server, or get approval of its contents."58 It would be a "good idea" if people used it, and people were free to use it because the Internet's design made it free.

THUS TWO NETWORKS-the network built by AT&T and the network we call the Internet-create two different environments for innovation. One network centralizes creativity; the other decentralizes it. One network is built to keep control of innovation; the other constitutionally renounces the right to control. One network closes itself except where permission is granted; the other dedicates itself to a commons.

How did we get from the one to the other? What moved the world governing our telecommunications system from the centralized to the decentralized?

This is one of the great forgotten stories of the Internet's birth. Everyone knows that the government funded the research that led to the protocols that govern the Internet.59 It is part of the Internet's lore that it was the government that pushed network designers to design machines that could talk to each other.60 The government in general, and the Defense Department in particular, had grown tired of spending millions for "autistic computing machines." 61 It therefore wanted some system for linking the systems.

Yet we are practically trained to ignore another form of governmental intervention that also made the Internet possible. This is the regulation that assured that the platform upon which the Internet was built would not turn against it.

The physical platform on which the Internet took off came prewired. It was the telephone wires that linked homes to homes. But the legal right to use the telephone wires to link to the Internet did not come preordained. That right had to be earned, and it was regulation that earned it. Nothing guaranteed that modems would be permitted on telephone lines. Even today, countries in Asia regulate the use of modems on telephone lines.62 What was needed before the revolution could begin was permission to connect the Net to this net.

And what made that permission possible? What made it possible for a different use to be made of the telephone wires from that which AT&T had originally imagined?

Here a second kind of regulation enters the story. Beginning in force in 1968, when it permitted foreign attachments to telephone wires, continuing through the 1970s, when it increasingly forced the Bells to lease lines to competitors, regardless of their purpose, and ending in the early 1980s with the breakup of AT&T, the government increasingly intervened to assure that this most powerful telecommunications company would not interfere with the emergence of competing data-communications companies.

This intervention took many forms. In part it was a set of restrictions on AT&T's permissible businesses.63 In part it was a requirement that it keep its lines open to competitors.64 In part it was the general fear that any effort to bias communications more in its favor would result in a strong reaction from the government.65

But whatever the mix, and whichever factor was most significant, the consequence of this strategy was to leave open the field for innovation in telecommunications. AT&T did not control how its wires would be used, because the government restricted that control. By restricting that control, the government in effect created a commons on AT&T's wires.

In a way analogous to the technical requirements of end-to-end, then, these regulations had the effect of leaving the network open and hence of keeping the use of the network neutral. Once the telephone system was used to establish a circuit, the system was kept free for that circuit to send whatever data across it the user wished. The network thus functioned as a resource left open for others to use.

This is end-to-end operating at a different layer in the network design. It is end-to-end not at the layer determining the connection between two phones on the telephone system. That connection may well be formed by a system that does not comply with the end-to-end rule.

But once the circuit is connected, then the environment created by the mix of technical principles and legal rules operating upon the telecommunications system paralleled an end-to-end design at the network layer. This mix of design and control kept the telephone system open for innovation; that innovation enabled the Internet.

ARE THERE costs to the e2e design? Do we lose something by failing to control access to the resources-the bandwidth-of the network?

Certainly the Internet is not without its weaknesses. The capacity of the Net at any one moment is not infinite, and though it grows more quickly than the demand, it does at times get congested. It deals with this congestion equally-packets get transported on a first-come, first-served basis. Once packets leave one end, the network relays them on a best-efforts basis. If nodes on the network become overwhelmed, then packets passing across those nodes slow down.66

For certain applications, "best efforts" is not enough. Internet telephony, for example, doesn't do well when packets carrying voice get delayed. Any delay greater than 250 milliseconds essentially makes the system unusable.67 And as content on the Net moves to real-time, bandwidth-demanding technology, this inability to guarantee quality of service becomes increasingly costly.

To deal with this problem, technologists have begun to propose changes to the architecture of the Net that might better enable some form of guaranteed service. These solutions generally pass under the title "Quality of Service" (QoS) solutions. These modifications would enable the network to treat different "classes" of data differently-video, for example, would get different treatment from e-mail; voice would get different treatment from the Web.

To enable this capacity to discriminate, the network would require more functionality than the original design allowed. At a minimum, the network would need to be able to decide what class of service a particular application should get and then treat the service accordingly. This in turn would make developing a new application more complex, as the programmer would need to consider the behavior of the network and enable the application to deal with that behavior.

The real danger, however, comes from the unintended consequences of these additional features-the ability of the network to then sell the feature that it will discriminate in favor of (and hence also against) certain kinds of content. As the marketing documents from major router manufacturers evince, a critical feature of QoS solutions will be their ability to enable the network owner to slow down a competitor's offerings while speeding up its own-like a television set with built-in static for ABC but a clear channel for CBS.

These dangers could be minimized depending upon the particular QoS technology chosen. Some QoS technologies, in other words, are more consistent with the principle of end-to-end than are others.68 But proponents of these changes often overlook another relatively obvious solution-increasing capacity.69 That is, while these technologies will certainly add QoS to the Internet, if QoS technologies like the "RSVP" technology do so only at a significant cost, then perhaps increased capacity would be a cheaper social cost solution.70

Put differently, a pricing system for allocating bandwidth solves certain problems, but if it is implemented contrary to end-to-end, it may well do more harm than good.

That is not to argue that it will do more harm than good. We don't know enough yet to know that. But it raises a fundamental issue that the scarcity mentality is likely to overlook: The best response to scarcity may not be a system of control. The best response may simply be to remove the scarcity.

This is the promise that conservative commentator George Gilder reports. The future, Gilder argues, is a world with "infinite" bandwidth.71 Our picture of the Net now-of slow connections and fast machines-will soon flip. As copper is replaced with glass (as in fiber optics) and, more important, as electronic switches are replaced by optical switches, the speed of the network will approach the speed of light. The constraints that we know from the wires we now use will end, Gilder argues. And the end of scarcity, he argues, will transform all that we do.72

There is skepticism about Gilder's claims about technology.73 So, too, about his economics. The economist in all of us can't quite believe that any resource would fail to be constrained; the realist in all of us refuses to believe in Eden. But I'm willing to believe in the potential of essentially infinite bandwidth. And I am happy to imagine the scarcity-centric economist proven wrong.

The part I'm skeptical about is the happy progress toward a world where network owners simply provide neutral fat (or glass) pipe. This is not the trend now, and there is little to suggest it will be the trend later. As law professor Tim Wu wrote to me about Gilder's book:

I think it is a "delta dollar sign" problem as we used to say in chemistry (to describe reactions that were possible, but not profitable). Private actors seem to only make money from infrastructure projects if built with the ability to exclude [H]ere in the industry, all the projects that are "hot" are networks with built-in techniques of exclusion and prioritization.74

Here is a tragedy of the commons. If the commons is the innovation commons that the protocols of the Net embrace, e2e most important among them, then the tragedy of that commons is the tendency of industry to add technologies to the network that undermine it. But this is an issue for the dark part of this book. For now, my aim is only brightness: to get you to see the commons that has been built through a set of protocols that defined the Internet that was.

THE INTERNET was born on a controlled physical layer; the code layer, constituted by the TCP/IP, was nonetheless free. These protocols expressed an end-to-end principle, and that principle effectively opened the space created by the computers linked to the Net for innovation and change. This open space was an important freedom, built upon a platform that was controlled. The freedom built an innovation commons. That commons, as do other commons, makes the controlled space more valuable.75

Freedom thus enhanced the social value of the controlled: this is a lesson that will recur.4. Commons Among the Wired

WIRED IS A magazine that was first published in early 1993. Its title is undefined, but it aspires to signal those who are connected or, as one online dictionary puts it, "with it" with respect to all things digital. To those outside the world of "things digital," the "wired" are those caffeine-chugging techheads staring at C code as the clock chimes 0100 (military time). But to those inside digital culture, "the wired" are those who understand the potential of this place called cyberspace and who are making that potential real.

The character of this group has changed. In the early 1990s, they were more intrigued by fast code than fast cash. Today, it is more the opposite. Yet if there is a group that can still be called "connected"-those who have built and are building the Internet that we have come to know-then this chapter is about them, about the commons among them, and about the innovation this commons built.

This commons had three aspects. One is a commons of code-a commons of software that built the Net and many of the applications that run on the Net. A second is a commons of knowledge-a free exchange of ideas and information about how the Net, and code that runs on the Net, runs. And a third is the resulting commons of innovation built by the first two together-the opportunity, kept open to anyone, to innovate and build upon the platform of the network.

A certain culture made each of these commons possible, as did a certain feature about the stuff these coders built-code. Something, that is, about the norms that first defined this world, as well as something about the nature of the code. My aim in this chapter is to explore both the character of this culture and the nature of this code, and how the two interact to produce a layer of freedom at the content layer.

For the content layer is the layer at which the commons in this chapter lives. The commons of the last chapter, built by end-to-end, is a commons at the code layer of the network. The commons here lies on top, even though built, like the code layer below it, in software. Code here is content, and at the birth of the Net, much of this content was free.

As will become clearer in chapter 11, however, the content of code is not fundamentally different from the content we are more familiar with-music, or film, or (at least digital) texts. As I will argue, in the digital world, all the stuff protected by copyright law is in one sense the same: It all depends fundamentally upon a rich and diverse public domain. Free content, in other words, is crucial to building and supporting new content. The free content among the "wired" is just a particular example of a more general point.

TO INTRODUCE these commons, however, we need to think a bit more about code. Our world is increasingly constituted by environments built in code-in the instructions inscribed in either software or hardware. Yet our intuitions about "code" are likely to be incomplete.

"Code" is written (primarily) by humans, though the code that humans write is quite unlike the code that computers run. Humans write "source code"; computers run "object code." Source code is a fairly understandable collection of logical languages designed to instruct the computer what it should do. Object code is a string of ones and zeros impenetrable to the ordinary human. Source code, however, is too cumbersome for a computer to run; it is therefore "compiled" before it is run, meaning translated from human-readable to machine-understandable code.

Object code is therefore the lifeblood of the computer, but it is the source code that links computers and humans. To understand how a program runs; to be able to tinker with it and change it; to extend a program or link it to another-to do any of these things with a program requires some access to the source.1

Things were not always this way. When computers were first built, they didn't have "software." Their functions were literally wired into the machines. This way of coding was obviously cumbersome. By the early 1960s, it was essentially replaced.2 While some computer functions are still performed by "hard-wired" code (for example, the code in the ROM chip that is executed when you boot up your computer), the meat of computers today is software.

At first, no one much cared about controlling this code. In the beginning of commercial computing, computer companies wrote software, but that software was peculiar to each company's machine. Each company had its own operating system (OS, the underlying program upon which all other programs are run). These operating systems were not compatible. A program written for an IBM machine would not run on a Data General machine. Thus, the companies had very little reason to worry about a program being "stolen." Computer companies were in the business of selling computers. If someone "stole" a program meant for a particular computer, they could run it only if they had that computer.

This was a world of incompatible machines, and that troubled those who depended upon many different kinds of machines to do their work. The government, for example, spent millions on computers but grew frustrated that these machines could not talk with one another. The same was true of the company that would build perhaps the most important operating system in the history of computing: AT&T.

For in this chapter, for at least this part of this chapter, AT&T is the hero. AT&T purchased many computers to run its national network of phones. Because of a consent decree with the government in 1956, however, it was not permitted to build and sell these computers itself. It was therefore dependent upon the computers that others built and frustrated, like the government, by the fact that these other computers couldn't talk to each other.3

Researchers at Bell Labs, however, decided to do something about this. In 1969, Ken Thompson and Dennis Ritchie began an operating system that could be "ported" (read: translated) to every machine. 4 This operating system would therefore be a common platform upon which programs could run. And because this platform would be common among many different machines, a program written once could-with tiny changes-be run on many different machines.

In the history of computing, this urge for a cross-platform-compatible language was long-standing. ALGOL was an early example.5 So too was COBOL, when the government announced that it would not purchase or lease any computer equipment that could not run COBOL.6 But the birth of Unix-the name given to AT&T's ur-operating system-was the most important. For not only did AT&T develop this foundational operating system, it also gave it away. Because of the restrictions imposed by the 1956 consent decree, AT&T was not allowed to sell a computer operating system. Thus, Thompson and Ritchie succeeded in convincing the company to simply give the OS to anyone who wanted it.

The first takers of this free OS were universities.7 Computer science departments could use the source code to teach their students about how operating systems were written. The system could be critiqued, just as English grad students can critique Shakespeare because they have the text of the Shakespeare plays to read. And as this system became understood, fixes to bugs in this system were contributed back to AT&T. The process produced a vast and powerful network of people coming to speak the language of Unix and of a generation growing up tutored by Unix.

In this way, for this period, Unix was a commons. The code of Unix was a commons; the knowledge that this code generated was a commons; and the opportunity to innovate with and on top of this code was a commons. No one needed permission from AT&T to learn how its file system worked or how the OS handled printing. Unix was a trove of knowledge that was made available to many. Upon this treasure, many built.

OVER TIME, however, the openness of commercial code began to change. As products became more numerous and users became more diverse, and as the cross-platform compatibility of programs grew, the companies producing these products exercised more and more control over how the products might be used. The code thus "forked"-developing in different and incompatible ways, increasingly proprietary. Users became less partners in the process of developing and using computer systems and more consumers. And suppliers of code were less eager to permit their code to be copied by others.

One instance of this increase in control turned out to be quite important in the history of computing. Richard Stallman was a researcher at MIT. He was an early disciple of the norms of openness (as in "the open society") or, more generally, freedom. The whole world, of course, was not open. But throughout much of the 1970s, the norm in computing was. Exceptions were scorned.

The lab where Stallman worked had a printer connected to the network. The clever coders in this lab had written a program to notify them when the printer malfunctioned. A jam, for example, would generate a message to users on the network, and someone close to the printer could then go correct the problem.

In 1984, the lab updated the software (a driver) that ran the printer. Stall-man then asked the company supplying the printer for a copy of the source code so that he could replicate the notification function in this new version of the printer driver. The company refused. The code to the printer driver was now closed, Stallman was informed. No one was allowed to tinker with it.8

To Stallman, this was a moral offense. The knowledge built into that driver had been produced by many people, not all of whom had been employed by the company. There was something wrong, then, with the company locking up that knowledge. And this wrong sowed the seed in Stallman's mind of a movement to resist this closing.9

In 1985, that movement was born. Stallman founded the Free Software Foundation. Its aim was to encourage the development of software that carried its source with it. The aim was to assure that the knowledge built into software was not captured and kept from others. The objective was to support a commons for code.10

AT&T gave the movement an important boost, quite unintentionally. In 1984, after AT&T was broken up, the company was freed of the restrictions on computing that it had been living under since 1956. Once freed of these restrictions, AT&T decided to enter the computing business. One of its most important assets in this business was Unix. Hence, AT&T decided to exercise control over Unix. After 1984, Unix would no longer be free. Companies, universities, and individuals wishing to use Unix would have to license the right from AT&T.

To many, this too was betrayal. A generation had devoted its professional career to learning and building upon the Unix system. Now AT&T claimed the exclusive right to the product of this learning. Although AT&T had taken the suggestions that had been made, although Unix had been improved in response, the company now wanted to trade on these improvements by making the code exclusive and unfree.

The reactions against AT&T's take-back were sharp. Berkeley had a version of Unix that it had been distributing; after AT&T's change, the Berkeley release had to undergo a massive retooling to extract the AT&T code so that it could release a version of Unix (BSD Unix) that was free of AT&T's restrictions.

But Stallman responded in a more productive and ambitious way. He wanted to build a free version of Unix that would, by design, always be free. So the Free Software Foundation launched project GNU-a recursive acronym meaning "GNU's not Unix." The GNU project was first to develop the suite of tools necessary to build an operating system and then to use those tools to build the GNU OS.

Throughout the 1980s, Stallman worked to do just that. Slowly he added tools to the project. Beginning with an extraordinary editor, Emacs, and then, even more important, with a compiler, the GNU C Compiler (GCC),11 the project slowly pulled together the tools an operating system would need.

But as the 1980s came to an end, Stallman's project began to slow down. Stallman had developed a problem with his hands. For a while he lost the ability to type. As he turned to the final step in the project-building a kernel (the heart of an OS) for the GNU operating system-his pace had been cut dramatically. He had mixed all of the ingredients needed for an operating system to function, but he was missing the core.12

IN FINLAND, a young student studying computer science wanted the chance to experiment with an operating system. Unix was the gold standard; Linus Torvalds had little gold.13

Instead, Torvalds started playing with Minix, an educational version of an OS released by computer science professor Andrew Tannenbaum. Minix ran on a PC but was designed as a teaching tool. It was therefore incomplete. So in 1990, Torvalds began building an alternative to Minix, which he released to the Internet in 1991. That code was released subject to a license called the General Public License (GPL). (We'll see more of this later.) It was therefore free for anyone to take and use, as long as they didn't bottle up what they took.

People rapidly realized, however, that with a little bit of work linking the parts of an OS that Stallman had built to the core of the OS that Torvalds had released, Stallman's objective of an open and free Unix-flavored OS could be realized. Quite quickly, then, Linux-or GNU/Linux for those who want to keep the contributions in view-was born.14 GNU/Linux was a platform that came with its source; anyone could take and build upon this platform. Because it came with its source, anyone could tinker with it to make it better. Many did, and in a very short period of time, GNU/Linux became quite good.155. Commons, Wire-less

"The radio spectrum" refers to that swath of electromagnetic radio frequencies that are used today for everything from the transmitting of AM radio to the broadcasting of television and cellular phones. Technically it refers to the use of radio waves, for any purpose, between 3 kilohertz and 300 gigahertz.

This spectrum is regulated. The Titanic gave us that regulation. In the aftermath of her sinking, navy analysts argued that had the radio spectrum been better regulated, a ship less than twenty miles from the wreck could have saved hundreds of passengers.1 The chaos in the spectrum confused the ship, however, so it missed the calls of help from the sinking luxury liner. The government used this confusion as a reason to begin to regulate access and use of the spectrum.

By the fall of 1912, the push to extend this regulation was great. Congress enacted the Radio Act of 1912, vesting in the Secretary of Commerce the right to license the operation of a radio apparatus.2 In 1926, after a series of court decisions limiting the power of the Department of Commerce, then Secretary of Commerce Herbert C. Hoover said the authority was insufficient. Congress responded with the Radio Act of 1927, vesting in the Federal Radio Commission (FRC) control over the radio spectrum.3 The FRC thus established a process by which the right to use a certain spectrum was licensed. Any use without a license was a criminal offense.

Thus spectrum, after 1927, at least, was not a commons. To use the spectrum required the permission of someone else-the government. That permission was granted according to the government's view of what uses were best. There was no neutrality in the government's decisions about who got to use this "public" resource. This was a resource that was fundamentally controlled, with the government as the controller.

This control had an increasingly profound effect upon radio programming. Early radio programming was different from today's. The spectrum was not filled with commercial broadcasters and Rush Limbaugh. Indeed, there was no such thing as a radio commercial. Radio at its start looked a lot like the Internet at its start. Broadcasters on early radio included a wide range of noncommercial, religious, and educational services. Commercial radio was just a tiny fraction of the total.4

But once the government got involved, all this quickly changed. It is an iron law of modern democracy that when you create a regulator, you create a target for influence, and when you create a target for influence, those in the best position to influence will train their efforts upon that target. Thus, commercial broadcasters-NBC and CBS in particular-were effective in getting the government to allocate spectrum according to their view of how spectrum should be used.5 (This was helped by the broadcasters' practice of offering free airtime to members of Congress.)6 The period from 1927 to 1934 saw an extraordinary shift in the nature of radio use-from a diverse collection of uses, some commercial, most not, to a single dominant use of the radio spectrum-namely, commercial radio. As Thomas Hazlett writes, "[B]y the mid-1930s, [NBC and CBS] would be responsible for an astounding 97% of night-time broadcasting."7

This transition was not without opposition. When radio stations started advertising, they incited a massive and continuous campaign of opposition. Herbert Hoover said of the trend, "It is inconceivable that we should allow so great a possibility for service to be drowned in advertising chatter."8 Poll after poll indicated that the people hated the emerging commercial system.9

Over time, however, people got used to the commercials, and the opposition died. By the mid-1930s, Congress was ready for a new statute, the Communications Act of 1934. The act charged a renamed agency (the Federal Communications Commission) with the duty to regulate "as public interest, convenience or necessity" requires within certain spectrum-defining areas.10 And it empowered the FCC to make decisions about how best to use the spectrum in the public interest.

This extensive regulation of what before 1912 had been a purely unregulated practice of wireless communication was upheld by the Supreme Court in 1946. Regulation of the radio spectrum was necessary, Justice Felix Frankfurter argued, because "[t]here is a fixed natural limitation upon the number of stations that can operate without interfering with one another."11 Justice Frank Murphy, though dissenting from the Court's opinion, agreed with the Court at least this far:

Owing to its physical characteristics[,] radio, unlike the other methods of conveying information, must be regulated and rationed by the government. Otherwise there would be chaos, and radio's usefulness would be largely destroyed.12

It was in the nature of things, the government argued and the Court agreed, that only if spectrum were controlled by the government would spectrum be usable. Spectrum could not be free.

ABOUT THE TIME the Supreme Court came to this conclusion, an English economist was concluding just the opposite. In a review of the FCC's regulation of spectrum, economist Ronald Coase concluded that there was no justification for political regulation of access to spectrum.13 Spectrum was no more "scarce" than land or trees were scarce. Scarcity is the nature of all valuable resources; but not all valuable resources are allocated by the government-at least, not in a free society. 14

Rather than a regime of licensing, Coase argued, spectrum should be allocated into property rights and sold to the highest bidder.15 A market for spectrum would better and more efficiently allocate spectrum than a system of government-granted licenses.

History has been kinder to Coase than to the regulators of the early FCC. In 1991, he won a Nobel Prize for his work on transaction cost economics. And long before the Nobel committee recognized his genius, many policy makers in the United States came to believe that Coase's system was better than the FCC's. A market in spectrum would more efficiently allocate spectrum than any system controlled by the government.

This is the debate I described at the start of the book. It is a debate between two regimes for controlling access to a resource-in this case, spectrum. One regime (the FCC's) relies upon the government; the other (Coase's) relies upon the market. Both presume that spectrum must be controlled. They differ only in the controller. Both thus reject a model of spectrum as a commons.

Among these proponents of a market for spectrum, none is more vocal and persuasive than American Enterprise Institute Fellow Thomas Hazlett.16 A system of government licenses, Hazlett argues, chills innovation. A world where holders of rights in spectrum cannot sell those rights chills the process by which new uses of spectrum develop. Far better, Hazlett argues, if the holders of spectrum rights had the freedom to sell those rights to the highest bidder. Then, Hazlett argues, more creative and innovative uses of spectrum would be enabled.17

Hazlett has done an extraordinary service demonstrating the harm of government-managed spectrum. He is certainly right that the current regime stifles innovation in spectrum use. If the innovator must first get permission from the government, then the innovator is much less likely to try. Permission from the government is an expensive commodity. New ideas rarely have this kind of support. Old ideas often have deep legislative connections to defend them against the new.

But to demonstrate the harm in government control of a resource is not yet to demonstrate the need for private control. Hazlett is right if control is necessary. But is control necessary? Even if the market is a better system for allocating control than the state, is the market in spectrum better than free spectrum, if no ex ante allocation is required?

THE ANSWER IS: Maybe not.18 Increasingly, there are strong technical arguments for a different way of allocating spectrum-or, better, arguments for a different way of not allocating spectrum. These "different ways" we can abbreviate as "wideband technologies." These technologies include "spread spectrum" technologies as well as technologies that allow some spectrum uses to be "overlayed" on top of others.19 Wideband technologies would allow many different users to "share" spectrum without the government or the market handing out rights to use the spectrum up front. Just as users of the Internet "share" the resources of the Internet through protocols that coordinate multiple, unplanned use, so too users of spectrum could "share" the resources of spectrum through protocols that coordinate multiple, unplanned use. Rather than controlled, spectrum would be, in this model, "free." Rather than permission to use it, the right to use it would be granted to anyone who wanted it. Rather than property, spectrum would be a commons.

This would not mean, as I will explain more fully below, that use of the spectrum would not be regulated. The regulation would simply be different. We speak of the "freeway" system to refer to highways. Highways are "free" in the sense that I mean: they are a commons open to anyone to use. But the devices that use the highway system are highly regulated. You can't take a go-cart onto Route 66; you can't drive a tank down your local street. Regulations control the devices that can be used on a highway. But regulation does not control who gets to go where. Use remains, in our terms, free.

The same could exist for spectrum. But to see how, we need to think a bit differently about what spectrum is and how it is used. As David Reed says of policy makers, so is it for most of us: We are "grounded in theory or common sense [about spectrum] that does not match the phenomena we are seeing every day."20

TO UNDERSTAND the possibility of free spectrum, consider for a moment the way old versions of Ethernet worked. Ethernet is the protocol you most likely use to connect your computer to your company's local area network. If you have a cable modem at home, it is the protocol used to connect your computer to the cable modem. It is essentially a way for many devices on a single network to "share" the resources of that network. But the critical feature of this sharing is that it occurs without any central authority deciding who does what when.

How?

When a machine on an Ethernet network wants to talk with another machine-when it wants, for example, to send content to a printer, or to send an e-mail across the Internet through an e-mail server-the machine requests from the network the right to transmit. It asks, in other words, to reserve a period of time on the network when it can transmit. It makes this reservation only if it hears that the network at that moment is quiet. It behaves like a (good) neighbor sharing a telephone party line: first the neighbor listens to make sure no one is on the line, and only then does she proceed to call. Likewise with the old versions of Ethernet: the machine would first determine that the network was not being used; if it wasn't, it would send a request to reserve the network.21

What if two machines sent that request at the very same time? If that happened, the network would record a "collision" on the network, and each machine would register that its request had failed. Each machine would need to request access to the network again. But rather than each machine requesting access at the same time, each waits for a random amount of time until it sends its request again. Ethernet technologies demonstrate that this protocol for dealing with collisions is quite good at facilitating coordinated use of a common network.

In this story, the Ethernet network is functioning as a commons. It is a resource that is made available generally to everyone connected to the network. Of course, everyone on the network must request permission to use the resource. But this permission can be content neutral. The network does not have to ask what your application is before it reserves space on the network.

More important, these protocols are a way for many different machines to share this common resource, without the coordination of any top-down controller. No one licenses the use of one machine over another; no system for selling rights to use the Ethernet network is needed. Instead, many different machines share access to this common resource and coordinate its use without top-down control.

Ethernet is not radio spectrum, though it is "spectrum in a tube." 22 And wideband technologies work differently from Ethernet protocols, though the Ethernet protocols do at least show how bottom-up coordination is possible. This bottom-up coordination of spectrum in a tube should in turn suggest the possibility of a different way of controlling spectrum in the air. It should suggest, that is, the possibility that radio spectrum might be allocated in this shared bottom-up way, rather than in the traditional top-down model of coordination advocated by the licensors-of-property types.

HOW WOULD such a system work?

The existing paradigm of radio spectrum broadcasting embraces the opposite of end-to-end principles. The ends in the broadcast medium-receivers-are stupid, not smart. All the intelligence is in the broadcaster itself. A receiver just listens for the strong signal separated by silence. When another strong signal comes close to the signal it's listening to, existing receivers get confused. They can't decide which signal to focus upon, so they wander in and out among them all.

A different paradigm for broadcasting imagines smart radios (smart receivers and transmitters) replacing the dumb. These receivers can distinguish the transmissions they are to focus on from background noise. They distinguish the good from the bad either because each transmission, coming as a packet of data, tells the system where to listen next or because there is a fixed pattern of listening that the receivers are programmed to follow. In either case, smart receivers make it possible for many receivers to effectively share the same spectrum range. And through technologies that facilitate coordination-again, analogous to the technologies of Ethernet-this system would permit many receivers, and hence many broadcasters, to coordinate use of the same radio spectrum.23

The idea for this way of allocating spectrum reaches back to World War II, and to the work of actress Hedy Lamarr.24 Lamarr and her partner, George Antheil, were exploring ways for submarines to communicate without detection. They invented a system where a transmitter would hop along the radio spectrum-transmitting for a moment at one frequency, and then jumping at the next moment to another-while the receiver, knowing the pattern the transmitter would take, would tune to the different frequencies at precisely the right moment in time.

Lamarr's technology was taken up by the Defense Department, though her invention was never deployed. Instead, work on the technology was classified. In the mid-1980s, however, information about this research was declassified, and interest in this mode of using spectrum increased.25 The deployment of the idea, of course, was now different. Digital processors made it possible to jump across the spectrum much more quickly and efficiently. And researchers increasingly saw that not only would this be a more efficient way to use the radio spectrum, but communications using this technology would be more secure. Rather than simply tuning in to a conversation on a cell phone (as many "scanners" do now), the conversation on the cell phone would be spewed across many different channels. The receiver would be unable to keep up unless it was clued in to the pattern of the transmission.

This is the Internet sans wires. The data being transmitted-for instance, a song or a TV show-are carved up into packets of data; those packets are sent across the radio spectrum along a broad swath of spectrum. They are then collected at the other end and reassembled by the smart receiver. Collisions or mistransmissions are retransmitted, as on the Internet. A vast array of spectrum is in turn effectively shared, in just the way "spectrum in a tube" (the wires of the Internet) is shared. No central controller is needed, just as no controller on the Internet is needed. Anyone with an idea, and a device that obeyed simple spectrum rules, could deploy that idea, just as anyone with an idea for the Internet, and a computer that obeyed TCP/IP, could deploy that idea to the whole of the Net.26

AROUND THE EARLY 1980s, the rules governing spectrum became an obsession with a retired West Point officer, David Hughes. Hughes had begun online community life in Colorado by setting up one of the first on-line bulletin boards in the nation. His aim was to find a cheap way for communities to connect, and he was located in rural America, where the thought of wires being used to connect was neither obvious nor useful.

So Hughes began exploring radios and soon came across the exploding technologies of spread spectrum radio. Using cheap (and increasingly cheaper) radio devices, Hughes began setting up spread spectrum experiments-demonstrating the power of a technology that did not depend upon spectrum being owned.

Though this work was technical, Hughes's motivation was "community-not politics, not business, not technology, not government-community in all of its parts."27 As he explained:

My work with radio has been based upon how you reach the smallest communities, and across community. Not just to it, but within it It's always been to the end of the highest level of connectivity at the lowest cost for every community on the face of the globe.28

Hughes began to push free access to spectrum. His work was designed to demonstrate how open spectrum could connect communities much more cheaply. At the core of his plan was a technology for sharing spectrum rather than allocating it-in other words, a plan for making the physical layer of spectrum free by treating the physical layer as if it were in a commons.

Hughes worked for a time with FCC technical adviser Dewayne Hendricks. Hendricks too was eager to exploit this new technology. In the early 1980s, the FCC announced its plan to explore using spectrum as a commons. Hendricks was eager to develop technologies to do just this. While Hendricks was at the FCC, he pushed Chairman William Kennard's program to experiment with these alternative uses of spectrum. But when the FCC slowed its progress, Hendricks decided to follow the path of Hughes, leaving the government to build what many in Washington said could not be built.

The problem was again the FCC. While the FCC had allocated a range of spectrum to be "unlicensed"-meaning people could use this spectrum without receiving a license-it was not encouraging this alternative use. So Hendricks had the idea to go elsewhere to explore new ways to use the spectrum. The Kingdom of Tonga was receptive to this alternative model for regulating spectrum use. Hendricks packed his bags.

In Tonga, Hendricks built a system to deliver high-speed Internet access to all citizens in Tonga. This access would use the radio spectrum; the speed was two to five times faster than the fastest cable modem in the United States.29 Once built, the system would deliver this content at just about zero marginal cost.

Hendricks could build this system in Tonga because he was free of FCC regulations. Tonga has its own rules for allocating spectrum; it chose to make a sufficient amount free to enable this free Internet use. Rather than fight with the skeptics over whether the system would work in theory, Hendricks decided to prove it would work simply by building it.

Hendricks has not stopped with Tonga. Encouraged by the FCC's push to develop Internet infrastructure in Native American tribal lands, he has now begun a program to give Native American tribes access to free spectrum.30 Within eight tribal lands, he is building a similar system to that in Tonga. Native Americans on those reservations will have access to superfast, supercheap wireless Internet technologies-long before the rest of America does.

How can Hendricks do this, given the rules of the FCC? Hendricks's plan starts within the rules the FCC has set; when he runs against the rules, he will shift to plan B: The Native American tribes argue that they are sovereign nations. The Supreme Court has agreed. Their claim is that they are free to regulate spectrum on their lands as long as they don't interfere with spectrum off their lands. Hendricks's system won't interfere. And by the time the lawyers resolve the battle, these Native American tribes will be connected at a higher speed than the fastest cable modems in AT&T's labs. This is regulatory activism in its finest form.

HUGHES AND HENDRICKS are just two of a gaggle of innovators experimenting with these alternatives to allocated spectrum. Some of the most famous innovations are the "Bluetooth" protocols, which enable low-power connections between mobile devices and PCs. Millions of devices now embed the protocol, which uses one of the few "unlicensed" bands that the FCC has allowed.31 Another example is Apple Computer's AirPort technology, enabling wireless links to computer networks using a protocol called 802.11b.32 (Real Madison Avenue whizzes, these protocol namers!) This technology enables extremely fast wireless connections between computers and a network.

But these are just the beginning.33 Consider the work of Charmed Technologies. Founded by MIT Media Lab alumnus Alex Lightman, Charmed Technologies aims to develop wearable computing systems. These wearable devices will link to the Internet and feed information in real time back to the user or wearer. Think of Robocop, with the wearer as the robot-able to see a person and have the computer identify him or look up an address merely by pointing the viewer at a building.

Who knows whether such a system would catch on? Who at this point can tell whether being perpetually connected is what people really want? But the fact that we can't tell means the opportunity to experiment is important. And the opportunity to experiment here depends upon access to the resources needed to experiment-spectrum, in other words. Ideas like Lightman's require space to develop, without first having to prove to existing AT&Ts why each new idea is a good idea.

If Lightman's idea depended solely on the Internet-if the last leap were not across the air but simply a link to a wired computer-then he would have this right to experiment. The right to connect is guaranteed by a broadly competitive market for Internet service providers (ISPs). He could make his service available on the Net, and anyone on the Net could get access to it. But because his service depends upon a leap from a person to a server across the air, he must depend on the right to access wireless spectrum. And that right is threatened.

For if there were a broad swath of unlicensed spectrum-spectrum that anyone could use and many could link to-then many Alex Lightmans could experiment with ways to link the Net to people and the Net to things. 34 These experiments would generally fail, but a few no doubt would succeed. And it is these successes that could transform the Internet as it is. If the same opportunity for innovation and creativity existed around wireless technologies as existed initially around the Net, then the changes we should expect are the same as the extraordinary changes the Net has built. Free access to this free resource should produce the same sort of innovation that free access to the controlled resource of telephone lines produced.

This free resource would thus enable wireless access for a wide range of new services-some still unimagined, others the dream of Internet innovators. And this free resource would compete with other providers of access to the Internet, keeping competition strong in this critical part of our information infrastructure.

BY NOW YOU will have noticed something different about the argument of this chapter. Unlike the commons I've described so far, a broad commons in radio spectrum does not yet (generally) exist. And unlike the commons I've described so far, with wireless there is not yet a wide range of innovation to point to and ponder. Instead, in this chapter my argument is about a commons that could be, not one that already exists. My claim is that there is enough evidence of a different way to order spectrum that we should be exploring whether spectrum could be ordered as a commons.

How exactly would such a regime work? Well, again, to say that spectrum should be in a commons is not to say that the government would leave spectrum "unregulated." There would be a role for regulation even if spectrum were "free." But this regulation would look very different from the regulation that now controls spectrum. The government (or the market) would not be deciding who gets to use the spectrum. The government would simply be assuring that the technologies that use the spectrum are properly certified technologies.35 The FCC would need to certify that the devices were properly configured. Just as the FCC does now with computers (to make sure they don't interfere with radio transmissions), it would do with radios (to make sure of the same).

Thus, the spectrum-as-commons model does not assume no role for the government. The role of the government, however, would be much less invasive than under the current regulatory regime. The government does decide who gets to drive on the highways; it doesn't sell off a right to drive on the highways; it simply makes sure that the devices that are used on the highway are certified as safe.

AS THE TECHNOLOGICAL potential to share spectrum becomes increasingly clear, a wide range of scholars and technicians is now pushing the FCC to adopt a very different mode for allocating spectrum.36 These advocates cover a broad political spectrum. As I've suggested over and over in this book, this diversity makes perfect sense. The advocates for free or open spectrum want to enable an extensive range of new technologies. They resist the efforts by entrenched interests to use government-granted rights over spectrum as a way to protect their own interests. They resist, that is, both government-granted and market-regulated licenses. Thus, when the government proposed auctioning off more of the radio spectrum, conservative economist George Gilder responded not by praising markets, but by attacking the political corruption implicit in these deals. Says Gilder:

Still more subversive of good policy, the very auction process entrenches obsolescent technology and promotes the false idea that spectrum is the basis of a natural monopoly.37

Gilder favors innovation and change over state-supported monopolies. His aim is to push policies that would open up the resources of spectrum to the widest range of innovators. A spectrum commons would do just this. Just as the Internet did, it would open up a resource for the common use of a wide range of innovators. These many innovators would experiment with ways of using the network that none of us could now imagine. They would fuel a second and possibly far more important wave of innovation than the initial wave of the Internet that we have seen so far.

LIBERATING SPECTRUM from the control of government is an important first step to innovation in spectrum use. On this point there is broad agreement, from those who push for a spectrum commons to those, like Hazlett, who push for a fully propertized spectrum market. All agree that the only thing that government-controlled spectrum has produced is an easy opportunity for the old to protect themselves against the new. Innovation moves too slowly when it must constantly ask permission from politically controlled agencies. The solution is to eliminate the need to ask permission, by removing these controllers at least.

Liberating spectrum from the control of the market is a second and much more controversial step. Hazlett and others insist that the rationing of a market is necessary, both to avoid overuse and to provide a sufficient incentive to improve spectrum efficiency. A spectrum commons will invite tragedy too quickly.

For the moment, we can defer resolving the differences between these two positions, to emphasize their common view: Both want a world where the power of controllers to stifle innovation has been eliminated. Both agree that government control over spectrum is simply a way for the old to protect themselves against the new. Both therefore push for a radical change in spectrum management policies, to free innovators from the need to please politicians before they have the right to innovate.6. Commons Lessons

COMMONS MAY be rare. They may evoke tragedies. They may be hard to sustain. And at times, they certainly may interfere with the efficient use of important resources.

But commons also produce something of value. They are a resource for decentralized innovation. They create the opportunity for individuals to draw upon resources without connections, permission, or access granted by others. They are environments that commit themselves to being open. Individuals and corporations draw upon the value created by this openness. They transform that value into other value, which they then consume privately.

The Internet has been built on two kinds of commons; it has the potential to move to a third. The protocols of the Net embedded principles in the Net that constructed an innovation commons at the code layer. Though running on other people's property, this commons invited anyone to innovate and provide content for this space. It was a common market of innovation, protected by an architecture that forbade discrimination.

Free or open source software provided a second commons at the content layer. The open code components of the Net were perpetual options for innovation. The ideas and implementation of code that would build the Internet were made freely available both technically and legally. Legally, to the extent that licenses that protected open code required that it remain in the commons. Technically, in the sense that the code that built core and peripheral functions-including, importantly, the World Wide Web-was made available to all.

Finally, free spectrum was the promise to produce a new commons at the physical layer. Here again, access would be uncontrolled and the use of this access would be determined by a wide range of innovators. Not solely by the handful of innovators owning these essential facilities, but by a wide range of innovators who might have a different view of how the facilities might be used.

These three commons work together. They increase the value of controlled resources by connecting them with free resources. The strands of fiber being laid across the world are all controlled by individuals and corporations. They are, for the most part, private. But the value they have is a function of the use to which they will be put. And that use is this commons called the Internet. The commons contributes to its value, and it makes the control that contributes to it possible.

NO DOUBT my account is incomplete. I have spoken of how these commons induce innovation; I have not pretended to measure how much or how significantly. I have not surveyed the full range of factors that might be said to affect innovation. My focus has been narrow and selective.

My excuse, however, is that the debate right now is not about the degree to which free or common resources help. The attitude of the most influential in public policy is that the free, or common, resources provide little or no benefit. There is for us a cultural blindness-an unwillingness to even account for the role of the commons. As Yale law professor Carol Rose argues, and as I indicated at the start, though "our legal doctrine has strongly suggested that some kinds of property should not be held exclusively in private hands, but should be open to the public,"1 we live in a time when the dominant view is that "the whole world is best managed when divided among private owners."2 The very idea that nonexclusive rights might be more efficient than exclusive rights rarely enters the debate. The assumption is control, and public policy is dedicated to maximizing control.

But there is another view: not that property is evil, or that markets are corrupt, or that the government is the best regime for allocating resources, but that free resources, or resources held in common, sometimes create more wealth and opportunity for society than those same resources held privately. Against the background of the commons we've seen in the context of the Internet, my aim in this chapter is to explore some reasons why. What do we gain by keeping resources free? What is lost when we allow certain resources to be controlled?

In this chapter, I draw together a few clues to answer this question. Drawing upon a wide range of writing, I want to pull together different accounts that suggest the value in keeping resources free. My aim is not proof; it is instead simply to connect ideas that are often left apart.

We can begin with our own legal tradition and with the resources that our tradition has left in the commons and the reasons why. Professor Rose has identified two reasons why our tradition has kept a particular resource-such as a public road, a right-of-way, a navigable waterway, or a town square-in common. First, these resources are "physically capable of monopolization by private persons."3 Monopoly means power, and the monopolist would be capable of exerting power over the community. Second, the public has a superior claim to these resources because "the properties themselves [are] most valuable when used by indefinite and unlimited numbers of persons."

The easiest example here is the case of a road. A road is kept in the commons because the opportunity for "holdouts" would be too great if the road were private. If a road became the common path along which all commerce passed, if along that path other businesses were built and other services were provided, then there would be a great value secured by this common road. And selling that road might then risk a hijacking by the owner of the road. The public gets great value out of the road, and the road has value because of the "publicness" of the road. The risk this value creates is that a private actor might take advantage. The property is thus "affected with a public interest" in the sense that the road's value comes from the public's dependence on it.4

Likewise with a town square. No doubt in any town there are many different places that might be a town square. But over time, one place is the town square, and it may well become valuable just because it is associated with custom and history within a given community. Keeping this resource in the hands of a community is a way to assure that no single actor takes advantage of the value the community has created. The value of this particular square comes not from the actions of its owner, but from a tradition that invests it with significance above others.5

In both these cases, the resource is kept in the commons because of the risk of an unfair capture if the resource were private. But why "unfair"? Why isn't it completely fair that the "owner" of the property be able to extract all of its value?

Here is the great insight in Carol Rose's analysis. Where the resource has a value because of its openness-where its value increases just because more use it; where "the more the merrier"-then it makes sense to attribute much of the value of this resource to the "publicness" of the resource. Indeed, as Rose argues, "the usual rationing function of pricing could be counterproductive [in these cases]: participants need encouragement to join these activities, where their participation produces beneficial 'externalities' for other participants."6

These are cases where "increasing participation enhances the value of the activity rather than diminishing it." Or, we might say more precisely, these are cases where the value from increased participation outweighs any cost from increased utilization. The value, in these cases, comes from the convergence of many upon a common use, or standard, or practice. And in these cases, keeping the resource in the commons is a way to assure that that value is preserved for all. 7

These arguments from tradition are thus grounded in both fairness and efficiency, and economists have extended the arguments from efficiency. 8 One extension in particular links back directly to the end-to-end argument.

The linking goes like this: Some resources have an understood purpose. We know what we will do with a certain resource, or at least the range of possible uses for that resource is small. But other resources don't come with their purpose preset.

Take telephone wires in the 1910s. Communications wires had been strung in America since the early 1800s. When they were first strung, their use was simple: telegraph. Given the technology at the time, there was little more that the wire could be used for; it was single-purpose. When telephones came along, there was a second possible use for the wire. That led to a new shake-up in business models. But here again, given the technology, the range of possible uses for these wires was not great.

Contrast this with computer networks. The most striking feature of the early history of the Internet is the repeated assertion by those at its founding that they simply didn't know what the network would be used for. Here they were building this large-scale computer network, with a large number of resources devoted to it, but none of them had a clear idea of the uses to which this network would be put. Many in the 1980s believed the Internet would be a fair substitute for telephones (they of course were wrong); none had any idea of the potential for many-to-many publishing that the World Wide Web would produce.9

Where we have little understanding about how a resource will be used, we have more reason to keep that resource in the commons.10 And where we have a clear vision of how a resource will be used, we have more reason to shift that resource to a system of control.

The reason is straightforward. Where a resource has a clear use, then, from a social perspective, our objective is simply to assure that that resource is available for this highest and best use. We can use property systems to achieve this end. By assigning a strong property right to the owners of such resources, we can then rely upon them to maximize their own return from this resource by seeking out those who can best use the resource at issue. But if there is no clear option for using the resource-if we can't tell up front how best to use it-then there is more reason to leave it in common, so that many can experiment with different uses.11 Not knowing how a resource will be used is a good reason for making it widely available.12

Scott Bradner and Mark Gaynor have captured this insight in a paper that uses "real options theory" to value different network designs. Their conclusion is that where uncertainty is highest, network designs that embrace endto-end maximize the value of the network; and where uncertainty is low, then end-to-end is not a particular value.13

In this case, end-to-end is a stand-in for a commons. Here too is a resource that can be used in any number of unpredictable ways. As David Reed describes the founding of the network design, "[T]he idea was we didn't want to decide We felt that we couldn't presume anything about how networks would be used by applications."14 And given the unpredictable character of the ways it might be used, there is something gained by keeping the resource open.

There is a second line of work that suggests another efficiency-based reason why open resources may have more value than closed resources. This work derives from the theory of management, and it helps explain why control can sometimes systematically fail.

The idea here has been made familiar by Professor Clay Christensen of the Harvard Business School in his book The Innovator's Dilemma. 15 The dilemma describes a perfectly understandable series of decisions that leads well-managed companies to miss the opportunities of disruptive technological change. Leading companies within a particular market will outperform others in perfecting the technology that defines their existing market. They will consistently develop superior products for continuing the development of their product line.

What these companies can't do is identify and develop disruptive technologies. (As David Isenberg puts it, "[T]he milk of disruptive innovation doesn't flow from cash-cows.")16 And this is not because a company is irrational or because it doesn't understand the nature of the market. The blindness that keeps the company fixed in a dying path is actually its clear understanding of probable returns. It sees real revenue from existing customers who need marginally better technology. It doesn't see the revenue from radically new technologies that depend upon unidentified or undeveloped markets. From its perspective, given its customers and reasonable expectations, these successful companies rationally fail.

Christensen offers the disk drive industry as an example. Disk drives have increased in capacity while falling in physical size at a dramatic rate.17 Overall, we can see that this shrinking created an extraordinary new market for computing power. In hindsight, it is clear that victory would go to the company that developed the smallest, most powerful drive.

But at each stage of that development, this obvious truth was missed by the very best disk drive manufacturers. The progress that led to the market we see now was not continuous; it was punctuated by disruptive changes in disk drive size. At each of these moments of disruption, the change occurred not because some genius had discovered a new technology that permitted the drive to shrink in size. The technology of each smaller drive was familiar and available to all. Instead, the disruptive changes occurred when an outside firm saw a new market and was willing to bet the firm on the success of this market. This new market was always more competitive than the old. The size of this market was uncertain. So from the perspective of the dominant player, moving into this new market seemed like a bad move. Its customers wanted nothing like the technology of the new drives; and it didn't have a vision that showed it the potential of a radically different market.

This blindness of successful companies comes not from management's failing. This pattern of failure can be seen in the very best firms. This is not the market's acting irrationally; it is the product of a rational strategy, given the market as it appears at any one time.

As David Reed says about AT&T: It was not willing to bet on data given that "the known applications couldn't justify it and they weren't willing to bet on the unknown applications."18

Others have described a similar blindness. Jim Carlton tells the story of Apple Computer's failing to see the potential of a market where its OS was licensed to Microsoft. Apple looked at the margins it was getting from its relatively small but rich market for PCs, and it compared those margins to those of other computer manufacturers. Apple's position looked far superior, so, rather than licensing the OS, Apple kept it closed.

Carlton describes this as pathology.19 It was the product, he suggests, of committee decision making. And trading upon what happened since 1985, the reader is left with the view that mismanagement has accounted for Apple's failure.

But the Christensen story suggests how it was Apple's success that caused Apple's failure. Its inability to see was not a function of its blindness. Its inability to recognize the value in a radically different model of doing business may well have been a rational decision, given the information available. What Christensen teaches is why, systematically, the view of what is rational from the perspective of a single actor may well prove irrational from the perspective of the market as a whole.

The Innovator's Dilemma offers its own strategy for dealing with this blindness. But we can see in the Internet a strategy for dealing with the very same blindness. If firms will be focused on continuing progress, if they will ignore new markets that fail to promise the same level of supracompetitive returns, if they will miss disruptive technologies that in fact produce radical new industries, then we have another reason, in theory, to keep at least some critical resources for innovation within a commons. If the platform remains neutral, then the rational company may continue to eke out profit from the path it has chosen, but the competitor will always have the opportunity to use the platform to bet on a radically different business model.

This again is the core insight about the importance of end-to-end. It is a reason why concentrating control will not produce disruptive technology. Not necessarily because of evil monopolies, or bad management, but rather because good business is focused on improving its lot, and disruptive technologists haven't a lot to improve. The disrupters are hungry to build a different market; the incumbent is happy to keep the markets as they are.

This last point suggests a third line of work suggesting an efficiency-based reason for preferring open rather than controlled resources. If the Christensen story is of the blundering giant, then this is the story of the malevolent giant. Here the actor-a company or an individual holding some monopoly privilege-fully understands how a new technology might increase social value. But the giant also realizes that there is no way it can capture this increase in social value. Unable to capture the gain, and certain to lose its own rents, the malevolent giant acts to resist the technological change, as a way of preserving its own power.

Such cases are easy to describe in the abstract; proving they exist in reality is much harder. Whatever its intent, the malevolent giant rarely has the power to control a technology completely; and even where it does have the power, other market forces may be adequate in checking the exercise of the power.

We as a society should favor the disrupters. They will produce movement toward a more efficient, prosperous economy. Christensen argues for management structures that would facilitate that; the Internet is an architectural structure that does the same.

This link between innovation and architecture is the focus of the work of two other Harvard Business School professors as well. Professors Carliss Baldwin and Kim Clark have demonstrated the importance of modular design in facilitating design evolution and hence industry innovation. In the first volume of an intended two-volume work, they demonstrate the fundamental shift in the design of the computer industry, as IBM increasingly modularized the design of its systems, and as regulators increasingly forced IBM to permit the modules to be provided by others. This change reduced the market value of IBM, but that reduction was overwhelmed by the increase in value in the rest of the industry. As they describe it, a "multiplication and decentralization of design options led to the emergence of a new industry structure for the computer industry," and this in turn radically increased the value of the industry.20

Modularity liberates control resources, as the multiplication of interfaces frees innovators to develop new and competing designs. It is another example of how free resources enable innovation.

Efficiency is not the end of the reasons why free resources might prove valuable. Instead, one final set of values also indicates the value in keeping a resource in common. These are democratic values.

The democratic tradition is our strongest ground for resisting the system of control. Why don't we simply sell the right to govern to the highest bidder? (The cynical will say we already have in effect. Maybe, but I'm talking formally.) Why don't we have a system where we auction off the rights to control the government as a permanent property right?

This is clearly not how we arrange governance today. The right to participate in a democracy is kept in common. We don't permit people to sell their right to vote. We permit neither the government to control how that resource is used nor the market to control how that resource is used. Instead, we keep that resource in common hands, whether perpetually (in democracies that can be recalled at any moment) or periodically (in democracies like that in the United States, where elections are held every few years).

Democracies thus forbid propertizing the right to control government. Why? This is not a hard question to answer, though raising it as a question will help us think through this problem elsewhere. We don't sell the right to vote because the currency-cash-is not the only or most important dimension of value in our society. There are people who devote themselves to careers that don't make them wealthy-schoolteachers and civil servants. We don't think they, by virtue of that choice, should have less power to control how their government is run. They've made choices that result in their having less power in the marketplace; but the marketplace is not a proxy for every domain of social power. As the philosopher Michael Walzer properly observes, there are many spheres of social influence in our lives.21 And we permit power in one sphere to dominate power in another in very few contexts. We don't in the United States permit sex to be purchased; we don't sell wives for dowries; we don't sell babies; and we don't sell votes.22 These transactions are blocked because allowing the market to control them would be to allow one sphere total power over all others. This we have chosen not to do.

A similar insight gives more reason for certain resources to remain in common. Access to locations where protest happens-town halls, or town squares, or, in the language of First Amendment law, public fora-remains open to all if open to any, or remains open on equal terms. Here the market is not permitted control.

And likewise, one might well argue, when the resource becomes foundational to participation in a society, then we assure that it remains in the commons. The right to vote is a foundational resource in our society; we don't allow it to be bought or sold. Access to the roads or highways is central to social freedom; we don't auction off such access and thereby restrict the right to travel. And some have argued that basic infrastructure-like phones or emergency services-should be considered common resources that must be made available to all. The specifics we can argue about, but the general point should be clear: There are values that a commons could serve that are lost if the resource is privatized.

IN ADVOCATING the commons, I have not argued for a world with only a commons. Not all resources can or should be organized in a commons. Not all resources must be organized as a commons just because some are. There are public streets as well as private drives, freeways as well as toll roads. The Internet links seamlessly with networks that are completely private. A world with open wires radio spectrum is perfectly consistent with a world where exclusive cable lines are reserved to those who pay. The open and the closed always coexist and depend upon each other in this coexistence.

But there are reasons why some resources need to be controlled and others do not. We've seen these reasons before, but we are in a better position now to understand them. While some resources must be controlled, others can be provided much more freely. The difference is in the nature of the resource, and therefore in the nature of how the resource is supplied.

This was the insight of many in the Enlightenment and, within our tradition, Thomas Jefferson most forcefully. Listen to Jefferson writing to Isaac McPherson in 1813 about the character of the patent power:

[1] If nature has made any one thing less susceptible than all others of exclusive property, it is the action of the thinking power called an idea, which an individual may exclusively possess as long as he keeps it to himself; but the moment it is divulged, it forces itself into the possession of everyone, and the receiver cannot dispossess himself of it. [2] Its peculiar character, too, is that no one possesses the less, because every other possesses the whole of it. He who receives an idea from me, receives instruction himself without lessening mine; as he who lites his taper at mine, receives light without darkening me. [3] That ideas should freely spread from one to another over the globe, for the moral and mutual instruction of man, and improvement of his condition, seems to have been peculiarly and benevolently designed by nature, when she made them, like fire, expansible over all space, without lessening their density at any point, and like the air in which we breathe, move, and have our physical being, incapable of confinement, or exclusive appropriation. [4] Inventions then cannot, in nature, be a subject of property.23

I've added numbers in brackets to Jefferson 's text to make clear the distinct points he is making:

First, Jefferson is describing the nature of an "idea." An idea is, in the terms of the economist, imperfectly excludable. I can keep a secret from you (and therefore exclude you from the secret), but once I tell you the secret, I can't take it back. We can't (yet) erase what has entered our heads.

Second, he is describing the nonrivalrous character of resources like ideas. Your consumption does not lessen mine, as your lighting a candle at mine does not darken me.

These two points then suggest a third: that "nature" has made this world to guarantee that "ideas should freely spread from one to another over the globe." Enlightenment was in her plan.

Thus it follows that without government, in the state of nature, there would be no such thing as a "patent" since patents are granted for "inventions" and inventions, "in nature," cannot be "a subject of property."

What is striking about this passage is the glee with which Jefferson reports this fact of nature. Here is the first patent commissioner showing just why nature is against the work of the U.S. Patent Office. But the motive of his glee is the betterment of man. This fact about nature means that of all the resources, information can be the freest.

Yet obviously, Jefferson 's story is not true of all resources, or even all resources in the commons. His is an account of a nonrivalrous resource. A rivalrous resource would not permit your consumption without lessening mine. And his argument cannot be taken to mean that there should be no control that governs nonrivalrous resources. Nature may not protect them, but neither does nature erect governments. Jefferson was not arguing against patent protection; he was instead arguing against the idea that patent protection was in some sense a natural right.

This distinction between resources helps us isolate the different reasons why a resource might need to be controlled.

1. If the resource is rivalrous, then a system of control is needed to assure that the resource is not depleted-which means the system must assure the resource is both produced and not overused.

2. If the resource is nonrivalrous, then a system of control is needed simply to assure the resource is created-a provisioning problem, as Professor Elinor Ostrom describes it. Once it is created, there is no danger that the resource will be depleted. By definition, a nonrivalrous resource cannot be used up.

What follows then is critical: The system of control that we erect for rivalrous resources (land, cars, computers) is not necessarily appropriate for nonrivalrous resources (ideas, music, expression). Indeed, the same system for both kinds of resources may do real harm. Thus a legal system, or a society generally, must be careful to tailor the kind of control to the kind of resource. One size won't fit all.

A second point also follows and is equally important: Even for resources that are nonrivalrous, some form of control will often be required. For these resources, there is still the need to assure an adequate incentive to supply or to provision the resource. Thus, even here, some control will often be needed.

In both cases, the necessary control could be provided through a number of techniques-through law, norms, the market, or, importantly, technology. Laws against theft can protect the property interest of rivalrous resources; norms against overuse can protect some shared resources; prices imposed by the market can induce provisioning and reduce consumption; and technology can make it easier to control.

This range of techniques means that there are many different ways to provide the degree of control that any particular resource might need. The commons that Carol Rose describes are governed not by the market or by law imposed by state actors. They are instead governed by "custom" or norms within the relevant community.

Custom thus suggests a means by which a "commons" may be managed-a means different from exclusive ownership by either individuals or governments. The intriguing aspect of customary rights is that they vest property rights in groups that are indefinite and informal, yet nevertheless capable of self-management. Custom might be the medium through which such an informal group acts generally; thus, the community claiming customary rights was really not an "unorganized" public at all.24

Commons in the Internet are regulated differently. "Custom" is not the typical controller anymore. It was at a certain time-USENET, for example, which facilitated a worldwide messaging board organized into separate topics, was governed by a custom that forbade commercial advertising; when that custom died, much of the value of USENET died. But in the contexts we have considered, custom is not the ruler. Controls imposed through technology instead govern many of these resources.

The cases we've seen so far are a mix of rivalrous and nonrivalrous resources, and the techniques of control within each are mixed as well. The wires that supported the network that was the original Internet are clearly rivalrous; so too may be the radio spectrum that Hendricks and Hughes want to share (though maybe not).25 But digital copies of operating systems are not rivalrous. One copy is as good as the next, and once we have a single copy, there is no limit to the copies we might make. Likewise with music, or video that is made available in digital form.

With the rivalrous resources we've seen, technology guards against depletion. Protocols for sharing the resource assure that many can use it without anyone depleting the rest for others.

With the nonrivalrous resources, technology can't itself solve the problem of incentives. Here one kind of law (contract law, through self-imposed licenses) serves to solve some of the provisioning problems, at least with open source or free software. This is the function of the GPL and other open source licenses: relying upon the particular character of code, they create a strong incentive for coders to contribute back to the commons.

But where these incentives are not enough, the law (through the odd device of what we have come to call "intellectual property")26 adds more. Intellectual property does this by giving the producers a limited exclusive right over their intellectual property, so they can recover the costs of producing that property and receive a sufficient return to give them the incentives to produce that property. A "sufficient return," however, is not perfect control, and intellectual property law does not, therefore, give authors or inventors perfect control. The basic premise, found in our Constitution, is that "neither the creator of a new work of authorship nor the general public ought to be able to appropriate all the benefits that flow from the creation of a new, original work."27 Instead, some of that benefit ought to be reserved to the public, in common.

SOCIETY BENEFITS from resources that are free; but unless some system of control is implemented for resources that must be created, or for resources, once created, whose use is rivalrous, then no benefit will be received. The key is to balance the free against control, so that the benefits of each can be achieved.

Yet this balance is not automatic. There is no guarantee that the control will be enough and no promise that it won't be too much or too little. The aim of society must always be to draw the optimal balance, and our obligation over time is to assure that that drawing not become skewed. The level of control at one time might be insufficient at a different time. And the level of freedom assured at one time might become threatened as the technologies of control change.

THIS POINT should be obvious, but let's make sure.

Let's imagine a fishing village that for generations has managed to fish in equilibrium with the stocks. The fish, in this example, are held in a commons; the community doesn't allocate right of control. Fishermen have an understanding about when a catch is too much; they have boats designed with this understanding in mind.

Along comes a new technology for fishing, which if used by each fisherman would radically deplete the existing stocks. Now the community faces a decision-how best to regulate the use of this technology to assure the resource is not depleted. If the community does nothing, the norms of the community might still be sufficient to keep the catch in line. But if the norms are not enough, then the community must deploy a new technology of control.

This "new technology," however, is not determined. The solutions could be many. The community might issue a regulation that says how much each fisherman can catch; it might create a property right in the resource and allow individual fishermen to trade it. Or it might deploy some technology that would limit the catch of each fisherman over a given period of time. All of these are possible responses to the threat posed to the common resource by the new technology. Each responds to a change that undermines the old equilibrium.

The same story can happen the other way around. Consider the problem of copyright on the Internet. As I've already explained, the aim of copyright is to give an author an exclusive right sufficient to create an incentive to produce, but not so great a right as to undermine the public domain. The Constitution limits this exclusive right-Congress may not, for example, give copyright to ideas, nor may it deny a right to fair use. These limits are in addition to the express constitutional limits imposed by the clause granting Congress the power to create these "exclusive rights"-namely, that the rights be for a "limited term" and that they "promote the progress of Science."

When the Internet first became popular, there was great fear that the technology for digital copying would render useless the rights granted by law. If I could make perfect copies for free and distribute them for free, then the legal restriction would become much less useful.

This led Congress to expand the rights protected by the Copyright Act, to balance the change in technology that the Internet produced. But as many have argued, this change may have been premature. For there are technologies that can be deployed to protect copyrighted work. And if deployed successfully, these technologies may actually give copyright holders more control than they would have had absent the Internet, thereby lessening the need for law.28

Here the technology is expanding control beyond the balance originally set, where, as in the previous example, technology was expanding free use beyond the balance originally set. In both cases, the point is the same: the balance must reflect the technologies as they exist. And changes in technologies can significantly change this balance.

The point is more than theoretical. In essence, the changes in the environment of the Internet that we are observing now alter the balance between control and freedom on the Net. The tilt of these changes is pronounced: control is increasing. And while one cannot say in the abstract that increased control is a mistake, it is clear that we are expanding this control with no sense of what is lost. The shift is not occurring with the idea of a balance in mind. Instead, the shift proceeds as if control were the only value.

The aim in the balance of this book is to make this transformation plain. We are remaking cyberspace, and these remakings will undermine the innovation we have seen so far.PART II. Dot.Contrast

7. Creativity in Real Space

THERE WAS a time before the Internet. Innovation and creativity were different then. I don't mean that creators were different then or that the process of creativity has changed. But the constraints on creativity and innovation were different. This difference can be expressed at each layer of Yochai Benkler's system. Because the physical, and code, and content layers were controlled differently, the opportunities for innovation were different.

We all know about these differences in the constraints among these layers. They are all obvious, if a bit in the background. They flow directly from the nature of real constraints within a scarcity-based economy. They are not the product of conspiracy or the will of evil minds. They are importantly unavoidable, at least in real space.

My aim in this chapter is to remind you of these things that we all know. I will rehearse the constraints on innovation that flow from the character of these different layers of communication in real space, so that we can better see how they have changed.

In real space. It is this qualification about which we must become self-conscious. Our intuitions about property, and about how best to order society, are intuitions built in a particular physical world. We have learned a great deal about how best to order that world, given the physics, as it were, of that particular world.

But the physics of cyberspace is different. The character of the constraints is different. So while there may be good reason to carry structures that define real space into cyberspace, we should not assume that those structures will automatically map. The different physics of cyberspace means that the rules that govern that space may be different as well.1

A different physics. I'm not talking about science fiction or about ideas that you've never considered before. Indeed, we've already seen a careful translation of real-space constraints into the physics of a very different world-the world of ideas. Jefferson made that translation in his writing about the nature of patent. My argument is nothing more (and certainly much less) than Jefferson 's. The world we must consider is partway between the world of ideas that he describes and the world of things that colors our intuitions. Cyberspace is between these two worlds. It offers not quite the freedom of the world of ideas, though it offers much more of that freedom than the world of things.

In the balance of this chapter, I want to make explicit constraints in the world of things, so that we can better see how these constraints have affected our thought about the world of ideas, and hence also about cyberspace.

One final note. My argument is not that all constraints are corrupting of something called "creativity." Certain constraints obviously enable creativity. The constraints of the classical form gave us Mozart and Beethoven. The aim is therefore not to find a world without constraint; it is to remove the constraints that might otherwise inhibit innovation. Just because it is good that sonnets forbid rambling paragraphs, it doesn't follow that a tax on books would inspire better writing.

CREATIVITY IN THE DARK AGES

PUT YOURSELF back in the dark ages, the time before the Internet took off-say, the 1970s-and ask: What was the environment for creativity then? What was required of a creator or innovator to bring his or her creativity to market? What limits were imposed? I want to consider this question in two contexts-first the arts and then commerce.

The Arts

WE CAN understand the environment for creativity in the arts with the same three layers that Benkler describes when talking of a communications system. Like a communications system, creativity in the arts is affected by constraints at the physical, code, and content layers. To author, or to create, requires some amount of content to begin with, to which the author adds a creative component, which, for a few, is then published and distributed.

Content

THE CONTENT an author must draw upon varies with the "writing." Some part is new-this is the part we think of as "creative." But as many have argued, we've come to exaggerate the new and forget that a great deal in the "creative" is actually old.2 The new builds on the old, and hence depends, to a degree, on access to the old. Academics writing textbooks about poetry need to be able to criticize and hence, to some degree, use the poetry they write about. Playwrights often base their plays upon novels by others. Novelists use familiar plots to tell their story. Historians use facts about the history they retell. Filmmakers retell stories from our culture. Musicians write within a genre that itself determines how much of the past content it needs to be within that genre. (There is no such thing as jazz that does not take from the past.) All of this creativity depends in part on access to, and use of, the already created.

In our present legal regime, some of this content is free; some is controlled. A poet has a copyright on his or her poetry. Others cannot simply take and reproduce it without the copyright holder's permission. The same with plays and novels: A play that is close enough to the plot of a novel is a derivative work. Copyright law gives the copyright holder control over these derivative works. Musical chords cannot be controlled; the design of public buildings cannot be copyrighted. These bits of content in these traditions are free, even if the control created by copyright is strong.

But this control is still limited-indeed, it is constitutionally limited. While a poet or author has the right to control copies of his or her work, that right is limited by the rights of "fair use." Regardless of the will of the owners of a copyright, others have a defense against copyright infringement if their use of the copyrighted work is within the bounds of "fair use." Quoting a bit of a poem to demonstrate how it scans, or making a copy of a chapter of a novel for one's own critical use-these are paradigmatic examples of use that is "fair" even if the copyright owner forbids it.

A similar limitation protects the historian. For content to be controlled, it must be "creative." Facts on their own are not "creative." As the Supreme Court has said, "[T]he sine qua non of copyright is originality. To qualify for copyright protection, a work must be original to the author [But] facts do not owe their origin to an act of authorship. The distinction is one between creation and discovery."3 Thus, facts remain in the commons for anyone to draw upon-even if these facts were discovered only because of the hard work of some investigator. Hard work does not entitle someone to a copyright. Only "creativity" does. Thus facts remain a resource that-constitutionally-cannot be subject to a system of legal control.

So too with all creative works-eventually. Disney, for example, did not license the right to make The Hunchback of Notre Dame or Pocahontas. These works, though originally copyrighted, are no longer subject to copy-right's control. Copyright is, in the United States, at least, constitutionally required to be for a "limited time." After that limited time, the work falls into the public domain-free of restraint, so that "second comers," as Judge Learned Hand described them, "might do a much better job than the originator" with the original idea.4

Or at least that's the theory, though Congress has done its best in recent years to ignore this theory. The distinctive feature of modern American copyright law is its almost limitless bloating-its expansion both in scope and in duration. The framers of the original Copyright Act would not begin to recognize what the act has become.

Scope: The first Copyright Act gave authors of "maps, charts, and books" an exclusive right to control the publishing and vending of these works, but only if their works had been "published," only after the works were registered with a copyright registry, and only if the authors were Americans. (Our outrage at China notwithstanding, we should remember that before 1891, the copyrights of foreigners were not protected in the United States. We were born a pirate nation.)5

This initial protection did not restrict "derivative" works: one was free to translate an original work into a foreign language,6 and one was free to make a play out of a novel without the original author's permission. And because of the burdens of registering, most works were not copyrighted. Between 1790 and 1799, 13,000 titles were published in America, but only 556 copyright registrations were filed. 7 The vast majority of creative work was free for others to use; and the work that was protected was protected only for limited purposes.

Time, with a little help from lobbyists, works changes. After two centuries of copyright statutes, the scope of copyright has exploded, and the reach of copyright is now universal. There is no registration requirement-every creative act reduced to a tangible medium is now subject to copyright protection. Your e-mail to your child or your child's finger painting: both are automatically protected.

This protection is not just against competing publications. The target is not simply piracy. Any act of "copying" is presumptively regulated by the statute; any derivative use is within the reach of this regulation. We have gone from a regime where a tiny part of creative content was controlled to a regime where most of the most useful and valuable creative content is controlled for every significant use.

Duration. The first Congress to grant copyright gave authors an initial term of 14 years, which could be renewed for 14 years if the author was living. The current term is the life of the author plus 70 years-which, for an author like Irving Berlin, would mean a protection of 140 years. More disturbingly, we have come to this expanded term through an increasingly familiar practice in Congress of extending the term of copyright both prospectively (to works not yet created) and retrospectively (to works created and still under copyright).

These extensions are relatively new. In the first hundred years, Congress retrospectively extended the term of copyright once. In the next fifty years, it extended the term once again. But in the last forty years, Congress has extended the term of copyright retrospectively eleven times. Each time, it is said, with only a bit of exaggeration, that Mickey Mouse is about to fall into the public domain, the term of copyright for Mickey Mouse is extended.8

You might think that there is something a bit unfair about a regime where Disney can make millions off stories that have fallen into the public domain, but no one else but Disney can make money off Disney's work-apparently forever. You'd be right about that, but we'll consider the fairness (and more important, the constitutionality) in greater detail later on. It is enough for now simply to recognize that even if the scope of controlled content has grown, in principle there is to be a constitutional limitation on this expansion. Some content is to remain in the commons, even if most useful content remains subject to control.

CONTROL, as I have argued, is not necessarily bad. Copyright is a critical part of the process of creativity; a great deal of creativity would not exist without the protections of the law. Without the law, the incentives to produce creative work would be vastly reduced. Large-budget films could not be produced; many books would not get written.9 Copyright is therefore an integral and crucial part of the creative process. And as it has expanded, it has expanded the opportunities for creativity.

But just because some control is good, it doesn't follow that more is better.10 As Judge Richard A. Posner has written, "[T]he absence of copyright protection is, paradoxical as this may seem, a benefit to authors as well as a cost to them."11 It is a benefit because, as we've seen already, creative works are both an input and an output in the creative process; if you raise the cost of the input, you get less of the output.

More important, limited protection has always been the rule. Never has Congress embraced or the Supreme Court permitted a regime that guaranteed perfect control by copyright owners over the use of their copyrighted material. As the Supreme Court has said, "[T]he Copyright Act does not give a copyright holder control over all uses of his copyrighted work."12

Instead, Congress has historically struck a balance between assuring that copyright owners are compensated and assuring that an adequate range of material remains in the public domain for others to draw upon and use. And this is especially true when Congress has confronted new technologies.

Consider the example of piano rolls. In the early 1870s, Henri Fourneaux invented the player piano, which recorded music on a punch tape as a pianist played the music.13 The result was a high-quality copy (relative to the poor quality of phonograph recordings at the time) of music, which could then be copied and played any number of times on other machines. By 1902, there were "about seventy-five thousand player pianos in the United States, and over one million piano rolls were sold."14

Authors of sheet music complained, saying that their content had been stolen. In terms that echo the cries of the recording industry today, copyright holders charged that these commercial entities were making money off their content, in violation of the copyright law.

The Supreme Court disagreed. Though the content the piano player played was taken from sheet music, it was not, the Court held, a "copy" of the music that it, well, copied.15 Piano roll manufacturers (and record companies, too) were therefore free to "steal" the content of the sheet music to make money with their new inventions.

Congress responded quickly to the Court's decision by changing the law. But the change was an interesting compromise. The new law did not give copyright holders perfect control over their copyrighted material. In granting authors a "mechanical reproduction right," Congress gave authors the exclusive right to decide whether and on what terms a recording of their music could be made. But once a recording had been made, others had the right (upon paying two cents per copy) to make subsequent recordings of the same music-whether or not the original author granted permission. This was a "compulsory licensing right," which Congress granted copiers of copyrighted music to assure that the original owners of the copyrighted works would not acquire too much control over subsequent innovation with that work.16

The effect of this compromise, though limiting the rights of original authors, was to expand the creative opportunity of others. New performers had the right to break into the market, by taking music made famous by others and rerecording it, after the payment of a small compulsory fee. Again, the amount of this fee was set by the statute, not by the market power of the author. It therefore was a far less powerful "exclusive right" than the exclusive right granted to other authors.17

This balance is the rule, not the exception, when Congress has confronted a new technology affecting creative rights. It did the same thing with the first real "Napster" in our history-cable television. Cable TV was born by stealing the content of others and reselling that content to consumers. Suppliers of cable services would set up an antenna, capture the commercial broadcasts made by television stations, and then resell those broadcasts to their customers.

The copyright holders did not like this "theft." Twice they asked the Supreme Court to shut cable TV down. Twice the Court said no.18 So it fell to Congress to strike a balance between cable TV and copyright holders. Congress in turn followed the model set by player pianos: cable TV had to pay for the content it broadcast, but the content holders did not have an absolute right to grant or deny the right to broadcast its content. Instead, cable TV got a compulsory licensing system to guarantee that cable operators would be able to get permission to broadcast content at a relatively modest level. Thus content holders, or broadcasters, couldn't leverage their power in the television broadcasting market into power in the cable services market. Innovation in the latter field was protected from power in the former. 19

These are not the only examples of Congress striking a balance between compensation and control. For a time there was a compulsory license for jukeboxes; there is a compulsory license for music and certain pictorial works in noncommercial television and radio broadcasts; there is a compulsory licensing scheme governing satellite television systems, digital audio home recorders, and digital audio transmissions.20

These "compromises" give the copyright holder a guarantee of compensation without giving the copyright holder perfect control over the use of its copyrighted material. In the language of modern law and economics, these rules protect authors through a "liability rule" rather than a "property rule."21 They are perfect instances of the special character of copyright's protection, as they represent the aim to give authors not perfect control of their copyrighted work, but a balanced right that does what the Constitution requires-"promote progress."

Thus, while Congress has expanded the scope of rights protected by the Copyright Clause, as technologies have changed, it has balanced the rights of access against these increases in protection. These balances, however, are not, on balance, even: though limits have been drawn, the net effect is increased control. The unavoidable conclusion about changes in the scope of copyright's protections is that the extent of "free content"-meaning content that is not controlled by an exclusive right-has never been as limited as it is today. More content is controlled by law today than ever in our past. In addition to limited compulsory rights, an author is free to take from work published before 1923; is free to take noncreative work (facts) whenever published; and is free to use, consistent with fair use, a limited degree of others' work. Beyond that, however, the content of our culture is controlled by an ever-expanding scope of copyright.

PHYSICAL

AT THE CONTENT layer, I've argued, the law aims to strike a balance between access and control. Copyrights grant control, but copyrights are constitutionally and statutorily limited to ensure some uncontrolled access. Some parts are controlled; some parts remain free.

No such balance exists at the physical layer, and for the most part, that's a good thing, too. Writing is produced and published on paper; paper is a physical good; in our economy, physical goods are fully controlled by the market. Films require film stock; nondigital film stock is extremely expensive; no right to steal this physical stock exists in our society. Market control is the rule at the physical layer; access is at the pleasure of the property owner.

This control is largely benign, at least where markets are competitive. If the market is not competitive, then power at the physical layer can become harmful. Control at the physical layer can, in at least some contexts, be leveraged into another layer.22 But for this danger, antitrust law is an adequate remedy. As long as the other layers remain relatively free, the control here is not inherently troubling.

The problem, of course, is that these other layers are not relatively free-or at least they weren't free in the dark ages. They were increasingly not free for content; they were especially not free at the layer of code.

CODE

THE CORE constraint on artistic creativity in real space is at the code layer-the constraint on whose work gets produced and distributed where.

The writer becomes an author when his or her work is published. Publication is a process controlled by editors. Editors at The New York Times decide what goes on their pages. Editors at Basic Books decide which books they will print. No one has a right to enter Basic Books and steal access to its printing presses. Nor does anyone have a right to demand that Basic Books transport his texts. The production and distribution of printed material are a wholly privatized activity.

The same is true for music. Rock bands are plenty; many write their own content; most of that content (fortunately, perhaps) never gets heard beyond a neighborhood garage. Whether the work of a musician gets distributed broadly depends upon the decisions of publishers. Record companies choose what gets floated in the market; radio stations (in effect) get paid to play what record companies choose.23

So too with television. You are free to buy commercial time on television, and in some markets you are free to buy program time. But unless you're Ross Perot, these freedoms don't matter much. What gets played on TV is the decision of network owners; what gets broadcast on cable is the choice of cable companies.24

These constraints at the code layer plainly affect the choice of creators to create or not. If the editors of a newspaper are conservative, a liberal columnist is less likely to submit a column to that paper. If newspapers generally are unwilling to be critical of U.S. policy, then authors who would criticize U.S. policy are less likely to waste their time penning the criticism. Communists don't waste much time writing Marxist screenplays. Only the deeply ill informed waste their time translating Adam Smith's work to the silver screen. The author is constrained by the expectation of how the code layer will respond. And the code layer, in those dark ages, at least, was importantly controlled. Though the range of outlets expanded dramatically,25 the concentration in ownership among those outlets increased as well. And the Net is an important constraint on what is made.

Obviously, the code layer interacts with the physical and the content layers. NBC gets to decide what it will broadcast. Because of trespass laws, I can't break into NBC and interrupt the evening news. If I do, I will be arrested for trespass. There is no First Amendment right that I can assert to trespass on NBC's property.

Likewise, NBC's right at the code layer is largely protected against state control by the First Amendment. Congress probably does not have the power to pass a law requiring that NBC give me access to its station. Editorial judgments of television executives are a constitutionally protected right at the code layer.

Commerce

ISSUES OF control matter not just to artists, and the dark ages did more than constrain budding Frank Sinatras. Indeed, among the most significant aspects of the Internet revolution has been the liberation it has given to commerce-not just to commerce in the mode of IBM or GM, but to commerce of the different. The commons of the Net exploded opportunities for commerce that would not otherwise have existed. And this explosion was not, given the architecture of telecommunications before the Net, predicted.

We can see this point quite quickly in two contexts that have been dramatically affected by the Internet-one in the context of coding, the other in the expansion of the market. Both of these contexts were quite different before the Internet, again because of the constraints imposed upon them by the architectures of real space. The opportunities of both have been changed as the technology of the Internet has changed.

CODING

IN 1972, ROBERT FANO, then a researcher at MIT, published a dark and pressing essay titled "On the Social Role of Computer Communications." 26 Fano's fear was that access to computing resources would be increasingly centralized, and that this centralization would do a great damage to democracy. As the power to understand and manipulate data about the world was held by a smaller and smaller number of people, the skew to democracy caused by this concentration would only increase: what was needed, Fano argued, was a different architecture for computer communications, one not centralized within a small number of organizations, but instead made available generally to many.27

Fano had an idea of how to build this different architecture and what this different architecture would look like. To build it would require state intervention to break up the concentrations in computer communications that had emerged. The network thus built would look much like the Internet.

Fano was wrong (though understandably so) about the future. But he wasn't wrong about the past. For computers at the time were expensive devices. Except for universities, programming for them required that you work within an institution that could afford to own one of these devices. If you wanted to work on a large-scale coding project, you needed to be within a company that was producing large-scale code.

For many people, of course, that wasn't a terrible thing. IBM and AT&T were powerful and well-paying companies. Most would consider it a great privilege to work for either.

But if you were not the sort likely to be able to work in these places-if you lived in South Dakota, where there weren't many IBM coding plants, or in China, where not many coding companies were allowed-then this reality was an important constraint. To author code in this world required working within large, typically American, corporations. And for many, this meant they could not author code at all. Just as with research in nuclear science today, the ability to do this research was limited to those who worked for specific organizations.

Again, this barrier is easy to understand. No conspiracy is needed to explain it. Computers were valuable resources; not every Joe could or should have access to play around with them. The economic and processing constraints mean that the system couldn't well leave itself open for others to take. The restrictions here were unfortunate and unintended consequences of economic constraints imposed elsewhere.

Here again we can understand these constraints in terms of Benkler's model. The physical layer of the "computer-communications" architecture was controlled; the very nature of its expense forced users to locate to the machines. Locating the machines in particular places made it easy to control access. The logic of the machine may have been open, but only those with permission were allowed in the "machine room." And finally, while the source code for these machines may not have been controlled (content layer, open), the small number of these machines meant that the value of the open code was limited. Coding, and the creativity realized in coding, was dictated by this architecture that mandated control.

This feature of the dark ages, then, limited the supply of resources to a market of production. Only those in a particular place, only those willing to work within a given structure, could work within coding projects. A wide range of talent was thereby excluded from the practice of coding. The ease with which those resources might be shared with many outside a single organization was limited by the technologies of computer communications that Fano described.

MARKETS

SO TOO are markets constrained. Technology most dramatically affects the extent of the market. The more interconnected markets are, the easier it is for goods from one area to affect the price of goods in another area. Geography is a physical constraint on that interconnection-in real space, greater distance means greater cost. But information supported by broad distributional channels can balance the constraint of geography.

Competition laws and constitutional norms keep this transportation system competitive. Competition laws make it hard for distributors to restrict or control distribution. The Dormant Commerce Clause of the U.S. Constitution makes it hard for states to bias distribution to favor themselves. These legal constraints balance natural tendencies among commercial and political actors. They produce a relatively competitive interstate market for goods and services.

Still, real space constrains. Even if the market were perfectly competitive, the cost of transportation and the high cost of information restrict the market's scope. If you want to sell very weird widgets, and only a hundred thousand people are within range, then you're not likely to be able to sell enough widgets to make it worthwhile. But if you had the world as your market-if the code layer facilitated broad distribution of selective information about widgets, thus lowering the cost of information-then you might have a market large enough to make your weird widget factory work. As Ronald Coase puts it:

People talk about increases in improvements in technology, but just as important are improvements in the way in which people make contracts and deals. If you can lower the costs there, you can have more specialization and greater production By improving the way the market works, you can produce immense benefits, not because it invents new technologies, but because it enables new technologies to be used. 28

The net of these layers of control in real space is relatively simple to map. Creativity may well be inspired by the protection these systems of control establish. But it is also constrained by the limits that these systems of control impose. I can write what I will, but what gets published is a function of what publishers like. I can sing in the shower, but before we sing "Happy Birthday" in a large crowd, we had better call a lawyer.29 My home movies can be shown in my living room, but art students should not expect their films to be shown in theaters. And freedom of speech notwithstanding, no one has the right to fifteen minutes of NBC's airtime. Creativity in the dark ages lives in a world largely without a commons. Permission of others is the necessary condition of one's work being seen elsewhere.

NOW AGAIN, unlike in Lenin's Russia, these systems of control are not the product of conspiracy. The constraints that require control in these different markets for resources are real. Economics is the science of choice in the context of scarcity; it is a positive (if dismal) science that takes the world as it finds it. We can no more will a world where real-space printing presses were free than we can will a spacecraft that could fly as fast as the starship Enterprise.

So by contrasting this economy governed by layers of control with an economy governed by large swaths of the commons, I don't mean to criticize every system of control. Whether control is necessary for a particular good in a particular context depends upon the context-upon the technologies of that context and the character of the resource. Resources held in common in one context (among friends or in a small community) may need to be controlled in another (in a city or between tribes).

In particular, to the extent a resource is physical-to the extent it is rivalrous-then organizing that resource within a system of control makes good sense. This is the nature of real-space economics; it explains our deep intuition that shifting more to the market always makes sense. And following this practice for real-space resources has produced the extraordinary progress that modern economic society has realized.

A part, however, cannot speak for the whole, especially when changes in technology render the assumptions of the old obsolete. Even if the control model makes perfect sense in the world of things, the world of things is not the digital world. We may need fences and perfect control to assure that the world of things runs efficiently. That's what the prosperity of the market, property, and contract teach us.

But perfect control is not necessary in the world of ideas. Nor is it wise. That's the lesson our Framers taught us-in both the limits they placed on the Exclusive Rights Clause and the expanse of protection for free speech they established in the First Amendment. The aim of an economy of ideas is to create incentives to produce and then to move what has been produced to an intellectual commons as soon as can be. The lack of rivalrousness undercuts the justification for governmental regulation. The extreme protections of property are neither needed for ideas nor beneficial.

For here is the key: The digital world is closer to the world of ideas than to the world of things. We, in cyberspace, that is, have built a world that is close to the world of ideas that nature (in Jefferson's words) created: stuff in cyberspace can "freely spread from one to another over the globe, for the moral and mutual instruction of man, and improvement of his condition," because we have (at least originally) built cyberspace such that content is, "like fire, expansible over all space, without lessening [its] density at any point, and like the air in which we breathe, move, and have our physical being, incapable of confinement, or exclusive appropriation."

The digital world is closer to ideas than things, but still it is not quite there. It is not quite true that the stuff in cyberspace is perfectly nonrivalrous in the sense that ideas are. Capacity is a constraint; bandwidth is not unlimited.30 But these are tiny flaws that cannot justify jumping from the largely free to the perfectly controlled. There are problems of coordination and constraints of scarcity. But the solution to these problems is not necessarily systems of control or better techniques of excludability. That cyberspace has flourished as it has largely because of the commons it has built should lead us to ask whether we should tilt more to the free in organizing this space than to the controlled that organizes real space.

Put differently: These imperfections in the capacity of cyberspace-that together may make it more rivalrous than ideas are-should not by themselves force us to treat the resources that cyberspace produces as we would treat real-space resources. If by resisting the model of perfect control we gain something important, then we should do so.

IN THE context of the media, we can be a bit stronger than this. Over the past twenty years, we have seen two changes in the media that seem to pull in different directions. On the one hand, technology has exploded the number of media outlets-increasing the number of television and radio stations as well as newspapers and magazines. On the other hand, concentration in the ownership of these media outlets has also increased. This increase in concentration especially should lead us to ask whether the control enabled in real space should carry over to cyberspace.

The statistics about increased concentration in ownership are undeniable and extraordinary. In 1947, 80 percent of daily newspapers were independently owned; in 1989, only 20 percent were independently owned. Most of the business of the nation's eleven thousand magazines was controlled by twenty companies in 1981; in 1988, that number had fallen to three.31 Books are much the same. The independent publishing market was strong just thirty years ago; with Bertelsmann's purchase of Random House in 1998, the industry is now much more concentrated, dominated by just seven firms.32 The significance of this concentration in books is no doubt less than that in film or other important media. There are still many independent publishers, and the range and diversity of book publishing are quite large. But the inertia is in the direction of concentration. And this inertia may be a source of concern.

Music is even more concentrated.33 The five largest music groups in the United States account for over 84 percent of the U.S. market.34 The same is true of radio. The top three broadcasters control at least 60 percent of the stations in the top one hundred U.S. markets.35 The same is true in film. In 1985, the twelve largest U.S. theater companies controlled 25 percent of the screens; "by 1998, that figure was 61 percent and climbing rapidly." 36 Six firms accounted for over 90 percent of theater revenues in 1997; 132 out of 148 of the "widely distributed" films in 1997 were produced by "companies that had distribution deals with one of the six majors."37 With this concentration, there has been a dramatic drop in foreign films. In the mid-1970s, foreign films accounted for 10 percent of box office receipts. By the late 1990s, the number had fallen to 0.5 percent. 38 Cable and television are no better. In 1999, Robert McChesney could write that "six firms now possess effective monopolistic control over more than 80 percent of the nation, and seven firms control nearly 75 percent of cable channels and programming."39 Those numbers are now much more extreme. 40 Professor Ben Bagdikian summarizes the result as follows: "[D]espite more than 25,000 outlets in the United States, 23 corporations control most of the business in daily newspapers, magazines, television, books, and motion pictures."41 The top firms in this set vastly outbalance the remainder. The top six have more annual media revenue than the next twenty combined.42

The reasons for this increase in concentration are many. I don't mean to argue, as many others have, that we should necessarily consider this increasing concentration inefficient or illegal. There are important efficiencies to be gained by the mergers of large media interests; important gains in coverage have also been realized. And while the conspiracy theories are many and practically unending in scope, we need not believe media conspirators are behind this radical change. The government has loosened its restrictions on concentration, sometimes for good economic reasons; technologies of transmission have changed to the great benefit of all; and the consequence has been an extraordinary concentration in media production.43

But whatever the reason, the results are staggering. And they extend beyond the mere structure of the market. They affect its character as well. The resulting mix of media is strikingly homogenous. The companies that make up the handful of international conglomerates are cookie-cutter variations of one another. Some are slightly larger in music than in film; others are slightly more American in ownership and content. But if you had to characterize the differences in philosophy or attitude among these different media conglomerates, it would be extremely hard (unlike, for example, the situation with newspapers in Britain): there are no clear philosophical or ideological differences among them.44

Many have quite rightly worried that this control by a few who are not very different from each other will have a significant effect on the kind of news that is reported. Andrew Kreig tells a compelling story of the effect of chain management on an American newspaper, driving the respected Hartford Courant to more excessive, sensationalistic reporting.45 The paper he describes is not dissimilar from many others. There are many stories about corporate owners influencing the news within their organizations-steering the news away from stories that reflect negatively upon those corporate owners.46 Congressman Newt Gingrich expressly recommended as much in 1997, when he told the Georgia Chamber of Commerce that business leaders and advertisers "ought to take more direct command of the news-room."47

Even if we ignore this most blatant form of bias, if the media are owned by a handful of companies, each basically holding the very same ideals, how much diversity can we expect in the production of media content? How critical can we believe these media will be? How committed to testing the status quo is this form of organization-itself so dependent upon the status quo-likely to be?48

You don't need to be a radical to be worried about this trend. Even the most committed pro-market ideologues could at least hope for a broader range of competition in ideas and perspective. There is good evidence that competition improves the quality of newspapers.49 And there is a general and broad view that the only justification for the power that the media has is that there is a broad range of views with the same power.50 No more. Never in our history has the concentration of media outlets been greater. Even a believer in the invisible hand might hope that this hand might muck things up a bit.

SOMETHING has mucked things up a bit. Something has entered the field in a way that could make these concentrations change-not the government or a regulation imposed by the government, but the architecture of the Internet we have been describing so far.

For the essence of this power in the handful of media companies that now dominate media internationally is control over distribution and the power it can promise artists.51 Movies run in certain places only; getting films into those places is quite hard. CDs are distributed through predictable channels of distribution-including radio stations, whose choice of what to play or not to play determines which content is popular or not. Breaking into this distribution channel is likewise extremely hard.

The same is true with cable. While many thought that increasing the number of cable channels would mean more valuable competition, in fact, the fragmentation of channels simply induced more commercialization. Fragmentation makes it easier to "slice and dice people demographically" and "maximize advertising revenues."52 Cable has thus not been a source of new innovation (unsurprisingly, as we saw, because the physical, logical, and content layers are all controlled). Instead, as "one cable executive put it in 1998, 'Most entrepreneurs have already gotten the word that the cable field is closed.' "53

But the essence of the Internet that I've described so far is an architecture for distribution that admits of no controllers, architecture that neither needs nor permits the centralization of control that real-space structures demand. And while this lack of control won't on its own mean Hollywood will fail, it will mean that the success of any particular kind of content is more convincingly a function of the desire for that content. Or at least, as we'll see, this is what the traditional media fear.548. Innovation from the Internet

IN BOTH artistic and commercial contexts in real space, there are barriers that keep innovators out. These barriers, for the most part, have been economic and real: the real cost of resources is a real constraint for most who would create. These barriers are obviously not absolute; ours is an extraordinarily creative culture; plainly some overcome the limits I've described. Indeed, if markets were perfectly competitive, one might imagine the optimal number that overcomes the barriers I have described. But markets are not perfect, and costs can be regretted. Hence these barriers are enough to keep innovators away whom we would not otherwise want to exclude. The hassle, the uncertainty, the absolute cost: no doubt these together chill many.

These barriers in real space are a function of its nature or, we could say, its architecture. Not "architecture" in its ordinary sense-buildings and streets-but architecture in a much broader sense: architecture in the sense of the set of physical constraints that one finds, even if these are constraints that man has made. The constraints that are reflected through economics are constraints of architecture in this sense. You can't perfectly and cost-lessly copy a nutritious meal; that takes real resources. You can't costlessly and instantly move your car from one coast to another: that takes time and energy. The constraints of real space are built into the nature of real space, and though technology presses against this nature, it is only so effective. Real constraints remain.

Cyberspace has a different architecture. Its nature is therefore different as well. Digital content can be copied perfectly and practically freely. You can move a great deal of content almost freely and instantly. And you can replicate whatever good there is in one place in many places-almost instantaneously. The barriers of cyberspace in its natural state are radically different from the barriers in real space.

"In its natural state." I spent many pages in Code arguing against just this way of speaking. Cyberspace has no nature. How it is-what barriers there are-is a function of its design, its code. Thus, in this abstract sense, it makes no sense to speak about the nature of this system that is wholly designed by man. Its nature is as man designs it.

But cyberspace at its birth did have a certain character. I've described some of it here and more of it elsewhere.1 The feature of its character at its birth that is most significant for our purposes here is an architecture that disabled the power of any in the middle to control how those at the ends interacted: this is the principle of end-to-end. This design choice of end-to-end assures that those with a new idea get to sell that new idea, the views of the network owner notwithstanding.PART III. DOT.CONTROL

9. Old vs. New

PICTURE the Soviet Union during its final days. Think about the most powerful within that system. These were not inherently evil people, though no doubt, especially during the Stalinist era, there were plenty of psychopaths hanging about. They were quite ordinary in many ways; if you met them at a party, many would strike you as quite liberal and sensible. They were not terribly rich, though some were. And they didn't live in a flourishing society, though they were promised just this as they grew up in the USSR. But by the late 1980s, everyone knew the system had failed. Yet very few were willing to take steps to free that society from state control.

Why? Why wouldn't these "leaders" try to move their society to a better place? Why wouldn't they voluntarily push for a different system of control?

It doesn't take a deep understanding of human psychology to answer this question. Things may have been bad, but how would these leaders know that for them and their families, things would be better under any other system? What incentive was there to release the reins of control, when the resulting system could promise so little that was certain? Like the management of a successful company, they could see the marginal improvements that were possible if they stayed on course. But they could not be confident of improvement if they jumped the other way.

(Here the story of the malevolent giant begins to make more sense. One could well believe the leaders of the Soviet Union expected their society as a whole would be better off under freedom but also believed they would not be able to extract enough of that social gain to make them individually as well off.)

NOW picture the leaders of dominant industries, faced with a disruptive technology. What is their rational response? Is it any different?

The perfect marketeers presume these actors would behave differently from the Soviets. They presume the leaders of dinosaur firms would spin those firms on a dime, to become something radically different.

But why would one believe that? Faced with a disruptive technology that threatens their way of life-their mode of doing business, their vision of the market-why would these leaders voluntarily step down from their place and enter a different market with uncertain returns? Why instead isn't the story that Christensen tells-like all deep truth-obvious, once we see it?

And even more obvious, why wouldn't we expect these leaders of existing dominant industries to use whatever power they have to protect themselves? Rather than yielding to the new technology, wouldn't they take steps to protect the old against the new?

What steps would these be? In the story I have told, there are any number of levers that the old Soviets might use. Most obviously, they could use the force of law to stifle innovation that challenges their power. Or they could use market power to chill the willingness of innovators to challenge their position. Or they could use norms to stigmatize the deviants. Or they could use architecture to hinder the opportunity for innovators to innovate. Any one of these techniques could help strengthen the power of the existing Soviets; any one could be deployed to weaken the opportunity of a challenger.

The balance of this book is a story about how our "old Soviets" are doing precisely this with the Internet. "Soviets" is an unfair term, I know, but the image is precise even if unfair. Changes threaten the power of those now in power; they will work in turn to protect themselves from the changes. In the balance of this book, I want to detail their work to change the Internet, and the legal culture surrounding it, to better protect themselves. Some of these changes are legal; some are technical; and some use the power of the market. But all are driven by the desire to assure that this revolution doesn't muck things up-for them.

There's nothing immoral in this desire. This is not a battle between good and evil. Stockholders demand that management maximize its income; we shouldn't expect management to do anything different.

But even if this is "only business" to them, that does not mean it should be "just business" for us. We need not stand by idly as the Internet is changed. They have their interests; we have ours. And for those who believe that the environment of creativity that the Internet produced was worth something, there is reason to resist the changes that I will describe.10. Controlling the Wires (and Hence the Code Layer)

IN CHAPTER 3, I described an architectural principle that I said helped build an innovation commons: end-to-end. I also described the struggle to assure that in effect that principle would govern on the telephone lines. Keeping those channels open to enable this commons of innovation was an important, if forgotten, part of the history that gave us the Internet.

The lesson from that story was of the power that came from an inability to control: the innovation and creativity that were inspired by a platform that was free.

If there was a time in the past decade when we had learned this lesson, the story of this chapter is that we have now forgotten it. The changes that I will describe in the pages that follow are all examples of the network being rearchitected for control. I have called the inability to discriminate a feature of the original Net's design. But to many-and especially those building out what the network called the Internet will become-this "feature" is a bug. The power to discriminate is increasingly the norm; building a Net to enable it is the aim.

the internet was born on networks linking universities, but it took its first step when it came to the phones. It was when ordinary individuals could dial up an Internet connection that the Internet came alive.

Long before people started dialing into the Internet, however, many were already members of on-line services and on-line communities. Compu-Serve and Prodigy were early market players. America Online came a bit later. These services were born serving content of their own. They were not Internet portals. There was indeed no way to move from their proprietary system to the nonproprietary Internet.

By the mid-1990s, all this changed as the attraction of the Internet grew, and as the competitive threat of ISPs increased. As more and more saw the Internet as an attractive alternative to the edited content of the existing service providers, they pressed their service providers to provide access to the Internet.

As I've suggested, the part of this story that is too often missed is the role that the telephone company played in the birth of the Net or, more accurately, the role the telephone company did not play. For what is striking about the birth of this different mode of communication is how little the telephone companies did in response. As their wires were being used for this new and different purpose, they did not balk. They instead stood by as the Internet was served across their wires.

This was no slight change. When telephones are used for talking, the average usage at a particular house is quite small. Calls are ordinarily short, so the number of circuits needed in a particular region is few.

But when phones began to be used to link to the Internet, this usage changed dramatically. Calls no longer lasted a few minutes on average. People were dialing in and hanging on, and the burden placed on the telephone system was great. The average voice call typically lasts only three to five minutes; the average Internet "call" lasts seventeen to twenty minutes.1

Ordinarily, one imagines that telephone companies would be quick to respond to this change in usage. They would either be quick to increase rates for calls over a certain length or they might restrict usage to certain kinds of telephone numbers (such as those to the ISPs). And we might imagine that telephone companies, if they were creative, would decide to become their own Internet service providers, offering better rates internally than they did to other Internet service providers. In short, there are any number of games telephone companies might play to respond to this demand for Internet services.

Phone companies, however, did not play these games, because they were not allowed to. And they were not allowed to because regulators stopped them.2

As we saw in chapter 3, the telephone company had become a disfavored monopoly. Its power over its wires had first been limited in 1968, in the Carterfone decision,3 and then after growing resistance by Congress and the FCC, most dramatically by the Justice Department in the early 1980s. In 1984, a decree breaking up AT&T was entered, and over the next ten years, Congress and the FCC struggled to find a model under which the Bells created by the breakup would be regulated.

The model finally fixed upon-and ratified by a statute of Congress in 1996-imposed an obligation on the Baby Bells to be neutral about how their lines would be used. The Baby Bells were required to unbundle the services they offered and make it possible for others to compete directly with them. If you wanted to start an ISP, you could connect your service into the telephone company's office. Their wires in a sense became your wires. The important point was preserving and defending neutrality.4

This imposed neutrality had an unintended effect on the Internet and its growth, because while the regulators imagined creating competition in the telephone service, they did not have in their head the idea that this might create a kind of competition with telephone service. They did not imagine the birth of the Internet as a product of their accidental regulation. But that is precisely what their regulation produced. This imposed neutrality about how the wires would be used left the field open for others to use the wires in ways no one ever expected. The Internet was one such way.

THE END-TO-END IN TELEPHONES

AS I DESCRIBED in chapter 3, the end-to-end argument says intelligence in a network should be located at the edge, or ends, of the network, and that the network itself should remain simple. Only those functions that must be placed in the network are placed in the network. Other functions-other intelligence-are left to the applications that run on the network.

The TCP/IP Internet was designed as an end-to-end network. The protocols of TCP/IP simply enable data to be sent across the network. They regulate how data is to be divided and how the resulting packets are shipped. They don't at all care about what is built into the data or how that built-in part works.

Not all networks are end-to-end in this sense. A contrasting network design is, for example, an asynchronous transfer mode (ATM) design. Under the ATM design, the network first establishes a virtual circuit between two endpoints in the network; it then ships data along that circuit. The virtual circuit means the network can control quality of service. But the virtual circuit also means the network is more "intelligent" than another network.5 The circuit could be programmed to be compliant with the end-to-end character of the original network design. But it need not be; it can be much more (intelligent) and hence much less.

These differences make it sound as if there is a fairly technical way to describe whether a network complies with "end-to-end" principles. They suggest that the question of whether a network is end-to-end is simply a question to be answered by technologists.

But let's step back from the technical aspects and look more broadly at the types of control there might be over a network. For if value comes from the absence of control architected into a network, then that value may be compromised by other techniques of control.6

The point should be obvious, but it bears emphasis. In principle, a network could be architected such that each application must "register" itself with the network before that application will run on the network. A program would then send a request to the network-"May I run X?"-and the network would give it a digital token as permission to run. Such a network would not comply with the end-to-end argument. It would be a control-centered network that requires permission before computer resources are used. The permission this network would require is negotiated technically. The machine does the negotiating, and if you don't get a token, your code doesn't run.

Notice, however, that the very same control could be implemented through other means. The network, for example, could have a rule-imposed through contract-that before your computer ran any program, you would have to register that program with the network administrator. This rule would not be enforced through code-you could cheat and sometimes get away with it. But you might expect the network administrator to have code to detect whether you are cheating. And if it finds that you are cheating, it might force you off the network.

Or we might imagine a community network, where there is an understanding about the kinds of applications that would be run on the network and the kinds of applications that would not be run. Roommates might have a network, and to keep it running fast, they might have an understanding not to use the network to download MP3s. Or better yet, the network news protocol that enables USENET to function might include a norm that the system would not be used to distribute commercial advertisements.

Finally, we could imagine a pricing system for controlling how a network is used. The code could charge users based on the bandwidth used or on the amount of time connected. This was the technique, for example, of AOL for many years. It is the technique of many on-line service providers today.

These different techniques-whether architecture, or rules, or norms, or the market-all effect control over what gets used on a network. Control through architecture is just one kind. And since these techniques could overlap, a network could technically be end-to-end, at least from the perspective of the network architect, but because of other rules imposed through these different techniques, it would deviate from the values protected by end-to-end. If rules, norms, or the market vested control in the center, then the values of a decentralized, end-to-end architecture could be lost.

In this sense, the rules governing a network, whether through laws, contract, or norms, can function as a kind of intelligence in the network. This intelligence can be advantageous or not, just as architectural intelligence can be advantageous or not. But whether or not beneficial, my point so far to see is the change they effect over a network where resources are free.

FAT PIPE

INTERNET ACCESS across telephone lines is slow, even though modem technology has improved dramatically. When I first connected with a modem, I was happy to get a speed of 300 baud. My laptop modem now sends and receives data at up to 56,000 bits per second. But still, that speed is far too slow for the kinds of work people do on the network today. One can't surf the Internet quickly at even 56K; nor can one share large files quickly.

This limitation has pushed the market to supply faster and faster ways of getting access to the Net. And the most important new technology for getting fast access to the Net-at least in the immediate term-is "broadband" through cable lines.7

Cable technology was developed in the 1960s as a way of giving remote communities access to television.8 CATV stood for "community access television," and the very first installations simply placed an antenna on a mountain and ran a cable line down to a community in a valley. When it was first built, cable television was essentially an end-to-end system-there was little intelligence in the network; all the power was provided by the broadcaster and the TV; and both could be conceived to be at the ends. It was also essentially one-way, analog content. Television broadcasts were piped to TVs. There was no way for TVs to talk back.

Congress liked cable TV. The idea of spreading television to many was attractive to many politicians. So in the early 1970s, Congress and the FCC began providing incentives for cable networks to be built. And among these incentives was a particularly lucrative asset-monopoly control.

The argument of the cable industry in favor of monopoly was simple: We need, they argued, incentives to risk the investment to build out cable TV. That build-out would be worth it to us only if we could be certain to recover our investment. This certainty would be adequately provided if we had complete control over the programming on our network. If we get to pick and choose the shows we run, and we get protected monopoly status in the local markets we run cable for, then we will have sufficient incentive to build out cable to secure our needs.

Not a bad deal, if you can get it. And even though "every major policy study on how cable should be regulated recommended that cable operators be required to provide at least some degree of non-discriminatory access to unaffiliated program suppliers,"9 Congress and the FCC ignored these recommendations. Cable was given control both over the physical infrastructure that built their network and over the code layer that made their network run.

From our perspective, however, there should be something odd about this decision. Telephones and television were both technologies that depended upon wires. Yet just as the nation was resolving to limit the control that the network owner had over one set of wires-telephone-it was increasing the control the network owner would have over a different set of wires-cable. From our perspective, these different policies for the same thing-wires-deserve an explanation, at least.

But at the time, telephones were as different from television as cars are different from buggies. It was not obvious to legislators (or if it was, they didn't let on) why the rules governing one should also govern the other. And even if it was obvious to some, the commercial pressure for exceptionalism was too great to resist. Just at the time America was coming to second-guess its first great network monopoly (telephones), it embraced and supported the construction of a second with the potential to be just as powerful.

So cable entered its golden years, which were brightened in the late 1970s only by the innovations of Ted Turner. Turner looked at cable and saw a waste of wires. Cable, he felt, could become a competing broadcasting network, not simply the supplicant to television broadcasters. So Turner bought access to a satellite and started broadcasting content across the satellite to cable stations everywhere. Cable thus became a content provider as well as a conduit for other people's content.10

By the early 1990s, cable was the dominant mode of accessing television in America.11 It had gone from the farms to the centers of the largest cities. The number of stations increased dramatically, as the technology enabled hundreds of channels. And the range of channels exploded with the decrease in the number of viewers needed to make any particular channel succeed. When channels multiplied, the opportunity cost for each new channel fell; when opportunity costs fell, then uses of the networks increased.

Cable was about to hit a number of bumps in the road, however. Some were of its own creation-perceived "price gouging" led Congress twice to regulate the prices of cable services. But some it did not control. 12 Satellite TV was the first of these; the Internet was the second.

Satellite TV offered competition to cable in the same way that cable had offered competition to TV. Services like DirecTV provided access to many more channels of television than cable, as well as the possibility to sell TV on a pay-per-view basis. Yet because it used no wires, the costs of providing this service were relatively low-at least when compared to cable. Thus, satellite provided a great challenge to the monopoly that cable was.

To respond to this competitive threat, cable needed to upgrade its systems to make it easier to supply two-way communication. Two-way communication was needed so consumers could make pay-per-view selections for television; fatter pipe would make it possible for cable to provide a wider range of content.

But while upgrading to compete with satellite, cable soon realized that it could also upgrade to provide two-way Internet service. And if it upgraded to provide Internet service, then cable could also be used to provide telephony. Thus the upgrade could secure cable in its primary market, while solidifying cable in these two new and growing markets.

AT&T CABLE

TO UPGRADE, however, would require a great deal of investment and, more significantly, technological development. First, there was no standard for enabling Internet across cable. Second, there was a great deal of poor-quality cable that needed to be upgraded. Some was quite old. And even the cable that was not old would require new technologies to make two-way cable work. So the cable companies formed an independent company-Cable Labs-to develop an open standard for serving cable. This standard-called the DOCSIS standard-would then be usable by modem providers that wanted to build cable modems to serve the growing Internet community.13 And those with low-quality cable lines began replacing their lines with newer technology.

At first, and quite slowly, a number of local cable companies began to experiment with Internet access. An Internet access service provider, @Home, and Road Runner helped the cable systems come on-line. But soon the push for this change in technology came from an entity quite familiar with national communications networks: AT&T.

AT&T was looking for a way to get into the Internet market. In 1995, the Internet had just taken off; AT&T's president, C. Michael Armstrong, decided AT&T had to be a part of the future. So AT&T devised a plan whereby it would purchase an interest in as many cable ventures as possible and slowly combine these cable systems under a single network enabled for broadband content. These networks, in turn, would be supported by selected ISPs-either @Home or Road Runner. Thus the design AT&T envisioned was of an Internet service network that would be supported by a limited number of Internet service providers-namely, those that it would control.

AT&T's reasons for restricting its network to just two ISPs were many, and over time the reasons changed. The essence of its argument was that exclusive dealing with a small number of ISPs was "necessary." At one point, they said it was "technically necessary"-claiming that it would be technically infeasible for AT&T to connect other Internet service providers to the AT&T network. But later, when other cable systems demonstrated how it might be done, AT&T claimed it was "economically necessary"-to give it adequate incentive to develop broadband cable.14

AT&T had eaten a bunch of cable monopolies and was now beginning to prove that you are what you eat: like the cable monopolies in the 1970s (and like AT&T in the 1920s), AT&T claimed a protected network was needed if broadband was to develop.

CABLE WAS not, and is not, the only broadband game in town. Wireless, as I suggested in chapter 5, in principle could become an important competitor. And in many communities, cable has a competitor serving broadband across the telephone lines-DSL.

DSL (digital subscriber line) was developed many years ago.15 It is a way of transmitting data over a telephone line that is also being used for voice. The data is modulated above the frequency where voice service flows, so it doesn't interfere with the telephone conversation. And in tests inside DSL laboratories, there is some hope that it could transmit data at an extraordinarily high rate-52 megabits per second, by some estimates.16

But DSL faces many hurdles, much like the hurdles that handicap cable. For a DSL connection to work, the copper wires in the local loop must be reasonably clean. This requires extensive work by telephone companies to find usable wires at the local loop. And it requires installing new routers no more than two miles from DSL customers. The cost of this upgrade to the copper wire world is huge, though some estimates that I have seen demonstrate that the per customer cost is the same for cable and DSL.

DSL does not have the option, however, of running a closed network. DSL is deployed by telephone companies. Telephone companies (by which I mean local Bells) are regulated to be open.17 That means that telephone companies must give ISPs the right to run their own DSL networks across the telephone companies' wires. And that means that the telephone companies' networks cannot exercise any real power over the kind of Internet service made available across their wires.

It might strike you as odd that the law would require one kind of broadband service-DSL-to remain open to other competitors, while allowing another broadband service-cable-to build the Internet of the future the way cable and telephones were built in the past. Why would the government permit control over the Internet in one case but require open competition in the other?

The answer is that there is no good reason for this inconsistency. It is solely a product of regulatory accident. The regulations governing telephones and all "telecommunications services" are found under Title II of the Communications Act of 1934. The regulations governing cable and all "cable services" are found under Title VI of the Communications Act. Title II requires open access to telecommunications services; Title VI does not. The telephone company is stuck with the position that DSL is a kind of telecommunications service. The cable companies have vigorously argued that broadband cable is not. And so far, though the battles are many, the law is in favor of cable. Cable companies have been allowed to limit the range of ISPs that use their wires, while the telephone company has been required to permit any number of ISPs to have access to its wires.

BUT FORGET what the law is for a moment. Which should it be? Should the lines be kept open, or should cable companies, and phone companies, be allowed to close the lines? Should the government do nothing to protect openness in either case? Or should it consistently demand openness where closed systems reign?

Well, let's first be clear about what's at stake. Recall what end-to-end ensured: that the network would remain simple, and that it would be unable to discriminate against content or applications it didn't like, so that innovations-including those the network didn't like-would be possible on this network. That value is threatened if end-to-end on the Internet is compromised-either technically, by building control into the network (in ways that will become clear later on), or effectively, by layering onto the network rules or requirements that replicate this control. Whatever other closed and proprietary networks there might be, polluting the Internet with these systems of control is a certain way to undermine the innovation it inspires.

This is precisely what is happening on the cable networks right now. While the networks are being architected to be technically consistent with the principle of end-to-end, by requiring that everyone who gets access to cable do so through a small number of controlled ISPs, the cable companies will reserve to themselves the power to control what access they get-in particular, the power to decide whether some content will be favored over other content, whether some sites surf faster, and whether certain kinds of applications are permitted.18

And on the assumption that this control will be allowed, technology firms such as Cisco are developing technologies to enable this control. Rather than a neutral, nondiscriminatory Internet, they are deploying technologies to enable the "walled garden" Internet. The network is built to prefer content and applications within the garden; access to content and applications outside the garden is "disfavored."

"Policy-based routing" replaces the neutral "best efforts" rule. The content favored by the policy becomes the content that flows most easily.19

Already cable has exercised this power to decide which kinds of applications should be permitted and which kinds not. As Jerome Saltzer, one of the coauthors of the "end-to-end" argument, describes, cable networks have already begun to be gatekeepers on the Net. As he writes:

Here are five examples of gatekeeping that have been reported by Internet customers of cable companies:

1. Video limits. Some access providers limit the number of minutes that a customer may use a "streaming video" connection The technical excuse for this restriction is that the provider doesn't have enough capacity for all customers to use streaming video at the same time. But cable companies have a conflict of interest-they are restricting a service that will someday directly compete with cable TV.

2. Server restrictions. While advertising the benefits of being "always on" the Internet, some providers impose an "acceptable use" contract that forbids customers from operating an Internet service, such as a Web site. The technical excuse is that Web sites tend to attract lots of traffic, and the provider doesn't have enough capacity. But again the access provider has a conflict of interest, because it also offers a Web site hosting service (Some providers have adopted a more subtle approach: they refuse to assign a stable Internet address to home computers, thereby making it hard for the customer to offer an Internet service that others can reliably find. And some access providers have placed an artificial bottleneck on outbound data rates to discourage people from running Internet services.)

3. Fixed backbone choice. Access providers choose where they attach to a long distance carrier for the Internet, known as a "backbone provider." The route to the backbone provider and the choice of the backbone provider are important decisions, bundled with the access service

4. Filtering. Data is carried on the Internet in batches called packets, and every Internet packet contains an identifier that gives a rough indication of what this packet is for: e-mail, a Web page, a name lookup, a remote login, or file sharing. Several access providers have begun to examine every packet that they carry, and discard those with certain purposes, particularly those used for file sharing. The technical excuse for this filtering is that many users don't realize that their computer allows sharing of files, and filtering prevents other customers from misusing that feature. But some access providers have imposed filtering on every customer, including those who want to share files And again, there can be a conflict of interest-the access provider has an incentive to find a technical or political excuse to filter out services that compete with the entertainment or Internet services it also offers.

5. No home network. An increasing number of homes have two or more computers interconnected by a home network, and as time goes on we are likely to find that this home network connects television sets, household appliances, and many other things. Some access providers have suggested that they aren't technically prepared to attach home networks, but the technology for doing it was developed in the 1970's. In refusing to attach home networks, providers are actually protecting their ability to assign the network address of the customer. By refusing to carry traffic to Internet addresses they didn't assign, the access provider can prevent the customer from contracting for simultaneous service with any other Internet access provider.20

The most telling of these limits is video. Cable companies make a lot of money streaming video to television sets. The Internet, in the view of some, could become a competitor to cable, by streaming video to computers. Under @Home rules, users were not permitted to stream more than ten minutes of video to their computers.21 And though AT&T offers congestion as a reason for this limitation, at times it is a bit more forthcoming. As AT&T executive Daniel Somers is reported to have said, when asked whether AT&T would permit the streaming of video to computers, "[W]e didn't spend $56 billion on a cable network to have the blood sucked out of our veins."22

Cable's intent to exercise control is clear; it has already exercised control. And if the business model that Cisco sells is as attractive as Cisco sells it to be, then we should expect that cable will continue to exercise control in the future. It will architect and enforce a network where the kinds of uses and content that run on the network are as the network chooses-which is to say, it will build a network just the opposite of the network the Internet originally was.

The evidence of this intent to discriminate was strongest at AT&T's @Home media. As Franois Bar reports, "[T]he @Home 1998 annual report is very clear" on the strategy of discrimination.23 It proposed to steer its customers, unknowingly, toward merchants that partnered with @Home. It would do this through code and marketing-through placement of ads, as well as through "how do I" wizards that would direct customers to selected sites. Their reports "explain how they will provide superior quality performance to partnering merchants."24 In this respect, Bar argues, "@Home is acting very much like Microsoft, using its control of the operating system's architecture to favor some applications over others."25 This closed-access control would allow cable owners to pursue only the exploration and development of new technologies that directly benefit them. "This is not to say that no innovation will take place," Bar argues, "simply that only the technology trajectories that line up with their interest will be pursued."26

ONE CHOICE about these trajectories is particularly important to highlight. Recall from chapter 8 my description of the emerging technologies of peer-to-peer. These technologies presume "peers"-that is, machines that are roughly equal. And though connection speeds on narrowband connections were slow, they were equal upstream and downstream.

Not so with the emerging technologies of broadband. Most of these technologies are faster downstream than they are upstream. Most broadcast more quickly than they receive.

Given the way the Internet is used right now, this imbalance makes good sense. E-mail and Web clicks going up take far less bandwidth than streaming video going down. Hence this structure makes sense of the uses of today-it is optimized, that is, for the uses of today.

But as with any optimization, what's good for today is not necessarily good for tomorrow. More important, how we optimize the network today will affect what good is possible tomorrow. Thus, as we optimize the network for this broadcasting mode, it becomes harder for the peer-to-peer structures to evolve. A world where users are servers doesn't scale well when the connection to the Internet is biased in favor of servers at the center.

Ordinarily, we don't have to worry much about this sort of thing in advance. The original PC market didn't care much about design; Apple Computer then changed that preference. The early PCs didn't have much capacity for sound. Later innovation created an incentive for the early PCs to change. The reason in both cases is the power of the market: as long as the market is free and competitive, these new uses will evolve as consumers want them.

In the context of broadband, however, there may well be a reason to be more skeptical about the market. If the concentrations in ownership continue as they have in the past few years (recently encouraged by an FCC that wants to take a more hands-off approach), then at a certain point there may be a strategic reason for these networks to resist the peer-to-peer way. By architecting networks to enable peer-to-peer, broadband providers will be reducing the power they have to direct users as they wish.

Here's an analogy that might suggest the point: Imagine you're a cable company serving twenty channels in your market. A new technology comes along that would open up two hundred more channels on your cable system. This technology is, let's assume, relatively cheap, and with it you could be certain to increase your communications capability ten times over. But the catch is that the content on these two hundred channels will be provided by your customers. Would you, as the cable company, adopt this technology?

The answer is that it depends. If the cable company could charge its customers differently because they used these different channels-if they could make up for the loss they suffered because fewer were watching the twenty channels by charging something for the use of the two hundred channels-then the cable company in principle should have no problem with the new technology. Prices would be adjusted to assure that the revenues the cable company received were as high as they were before.

But there are three problems with this happy assumption as applied to the Net. First, customers don't notice that they are living within a closed system. If a travel site comes up slowly because it is not a favored site, the user is likely to consider this congestion, not something owing to the network. Second, the business model of some networks is based on "owning" the customer, not on charges for access. The last thing these business models can accept is an architecture that opens more channels. But third, and more important, even if there's a price at which the cable company would be willing to allow this new innovation, that price may be too high to inspire investment in this new form of innovation. The innovation that gets devoted to a free, neutral platform is different from the innovation that gets devoted to a platform where the platform owner can, down the road, simply change its mind.

We've seen this lesson before, and we're at a point where we can state a general claim: Where a disruptive technology emerges, there may be good reason not to extend the power of existing interests into power over that technology. That doesn't mean the new technology should be allowed to defeat the old or, at least, defeat the old for free. For example, no doubt the customers who use the "two hundred new channels" technology on the cable system should have to pay something for this new capacity. But the price they pay should not necessarily be within the control of the dinosaurs. Instead, while compensation is justified, control is not required. Or, better, separating control from compensation may well be a way to induce more innovation.

* * *

MY CLAIM so far can be summarized like this: When the Internet was first born, both norms (among core network facilities) and law (restricting the telephone company) effected an end-to-end environment. This created the initial neutral platform. But as the Internet moves to broadband platforms, neither norms nor the law require network providers to preserve the same innovation environment. The trend instead is toward control-toward layering onto the original code layer of the Internet new technologies that facilitate greater discrimination, and hence control, over the content and applications that can run on the Internet. The Net is thereby moving from the principle of end-to-end that defined its birth to something very different.27

MANY RESIST the view that this control is anything to worry about. Cable is just one of a number of broadband technologies. DSL, as I have noted, is not free to be closed. In many contexts, DSL competes with cable. Hence if consumers value openness, then they can choose DSL over cable. And if they choose DSL because DSL is open, then cable will be pressed to be open as well.28 Thus competitive forces will force the network to open up, even if cable desires to be closed.

I agree with this claim, as far as it goes. To the extent consumers prefer open to closed, they will put pressure on the closed providers to be open. But to move from that claim to the conclusion that therefore there is nothing to worry about is, in my view, premature. There are plenty of reasons to worry that the closed character of cable won't correct itself.

First, there is the issue of numbers, and numbers of two sorts-the number of people on cable broadband, and the number and character of other broadband providers. Cable now has a great lead over DSL in subscribers to the cable system. There were 5 million cable broadband customers and 1.8 million DSL customers in 2000.29 Some predictions suggest that DSL may close in on cable by 2002,30 especially in nonresidential areas where cable does not now exist. But there are just as many who predict that cable will continue to lead.

But second, there is the number of different broadband providers and the character of their business models. For cable is not the only closed system. Wireless-the great hope from chapter 5-is being deployed now in a way that is primarily closed. The architecture for wireless broadband uses the same specifications that cable does-DOCSIS.31 DOCSIS, as you'll remember, doesn't yet provide for simple open access. Thus the future is not cable vs. DSL, but DSL vs. a scad of providers, all of which are closed.

Third, while DSL will be a strong competitor to simple Internet access, cable access provides the opportunity to mix television content with Internet content. And while DSL presses its limits to serve Internet as fast as cable does, cable has a great deal of bandwidth that it can use to supply Internet-related content. Right now cable provides 10 percent of its bandwidth to be used for Internet service. It could easily multiply the number of channels supplying Internet service and become a much more attractive option. Thus, while openness might be on DSL's side, the value of openness to the consumer may be outweighed by the ability to bundle cable more effectively with other video content.

But fourth, and most important, consumers' preferences might not be enough to motivate the market. This is the point we have seen both in chapter 3's discussion of the value of e2e and in chapter 4's discussion of neutral platforms: A closed network creates an externality on innovation generally. It increases the cost of innovation by increasing the range of actors that must license any new innovation. That cost is not borne directly by the consumer. In the long run, of course, if it is a cost, it is borne by the consumer. But in the short run, the consumer doesn't notice the innovation that the closed model chills. Thus the consumer does not completely internalize the costs imposed by a closed system. And hence the pressure the consumer puts on closed systems to open themselves up is not equal to the costs that such closed systems impose on innovation generally.

These are good reasons, I believe, for being skeptical about whether the invisible hand will solve the problem of closed networks. The observation that never in the history of telecommunications has a network voluntarily been opened after being closed is another reason to be skeptical. Finally, the interest of those who own these networks to keep control within the network is huge, and a huge reason to be skeptical about their control.

To see this part of the story, however, we need to shift to a different battle about open architectures-AOL.

AMERICA ONLINE was born far from the Internet. Its birth was as an online service that gave members access to other members. While other services were focused on how to sell product, AOL understood from the very beginning that networks are built by communities.

AOL's community was built by making computers easy and access simple. The company "carpet-bombed" America with AOL disks; it made sign-up simple and access cheap. Quickly AOL built a following that was extraordinary for on-line services.32

The AOL network was not really end-to-end. Lots of intelligence was built into the software with which one connected to AOL. AOL made that intelligence work to assure ease of access as well as control where control was needed. The service held the user's hand, but it required some intelligence to know where the hands were. It was a preprogrammed world, which users took as they found it. No one built additions to AOL or added functionality to AOL without AOL's permission.

That wasn't the case with all on-line communities. MUDs (multi-user domains), for example, were on-line communities where people were free to develop new parts of the on-line, virtual, text-based world.33 If you wanted to add a room to an existing MUD, you simply wrote the code to add the room and submitted it. The space that got built was as the members built it.

In AOL, the only building was that approved by the town planner-AOL. And AOL succeeded in building an extraordinarily popular place.

When the Internet came along, many thought AOL would die. Why pay to get access to preselected content when you could get access much more cheaply to the Internet as a whole? But AOL responded to this challenge by doing what it does best: by building its service to make it easy for users to find their way onto the Internet. The Internet was one place AOL users could go, but then there was also the content on AOL. Both would be available to AOL customers; only the Internet was available to others.

AOL then became another Internet service provider, but with something extra that came from the content it served. It was an ISP plus, because it also had its own content. But many simply used the service to get easy access to the Internet. And AOL then was subject to the fierce competition that every ISP faced. With some five thousand ISPs across America, there was only so much power any one ISP had-even if that ISP had a very large number of customers.

AOL was built on narrowband telephone lines. When broadband came along, AOL faced a critical threat. If broadband service was reserved to just two ISPs, and if it was far superior to the service one could get across the telephone lines, then AOL faced a great challenge from this emerging Internet opportunity. If AOL was barred from broadband, then AOL would be history.

AOL thus joined many who were pushing the FCC as well as local governments to require that broadband cable lines be kept open for competition. This was the "open access" movement; AOL was a key player. In 1999, AOL argued to the city of San Francisco during its open access implementation hearings:

AOL applauds the City for taking this critical step in the implementation of the Board of Supervisors' open access resolution, which wisely supports consumers' freedom to choose their Internet service provider and to access any content they desire-unimpeded by the cable operator.34

AOL had made the same arguments in favor of governmental intervention to the FCC.35 In this campaign, AOL's allies were many. Indeed, before AT&T started buying cable lines, AT&T too was an ally. In Canada, AT&T argued to the Canadian government that access to cable in Canada should be regulated to be open.

AT&T Canada LDS submits that the application of the Commission's forbearance test to the two separate markets for broadband access and information services supports a finding that there is insufficient competition in the market for broadband access services and the market for information services to warrant forbearance at this time from the regulation of services when they are provided by broadcast carriers. As noted above, these carriers have the ability to exercise market power by controlling access to bottleneck facilities required by other service providers. It would appear, therefore, that if these services were deregulated at this time, it would likely impair the development of competition in this market as well as in upstream markets for which such services are essential inputs.36

Vertically integrated cable and telephone facility owners, AT&T had argued, possessed market power and had to be prevented from engaging in anticompetitive practices.37

But when AT&T bought its own cable lines, its story changed. No longer did it believe that cable should be regulated. Instead, AT&T began to argue that the market should regulate cable, and the government should stand aside.

This would become a familiar pattern.

In January 2000, AOL and Time Warner announced to a startled world that they had agreed to merge. Time Warner owned many cable companies; these cable companies would serve AOL content at high speed. AOL had many Internet customers. These customers would be able to get access to Time Warner content. The merger was an ideal opportunity, both companies argued, for synergy in this market. The old and the new would form together one of the most important media companies in the world.

At the same time, AOL announced its policy on open access had changed. It, like AT&T, no longer believed that the government should regulate access. It, like AT&T, believed that the market should regulate itself.38

I'M NOT SURE why people are surprised by flips in corporate policy, any more than we are surprised by flips of politicians. Corporations have a duty to their shareholders. Their job is to make money. If the opportunities present themselves, they will, and should, change their views. They are not institutions of public policy. And they don't deserve the attack that would befall an institution of public policy that so radically, and transparently, switched sides.

But the other side of this obvious point is that we should not treat what corporations say is good public policy as what is good public policy. We should treat them as statements by individuals who are required by law to be self-serving. This is not just "bias"-this is legally mandated bias.

Thus, I discount both AOL's support and AOL's opposition to government regulations to support open access as evidence about whether open access is good policy. The question is not what AOL believes is good for AOL. The question is what is good for the Internet.

And here again we return to the question, What trend should we expect? The opponents of any governmental role here argue that the market will take care of itself. I think that's true-the market will take care of itself. AOL/TW will build itself to maximize its market power. The question is what shape that building will take.

THE DANGER IS what economists would call the problems of vertical integration-where one provider controls the full range of services across the layers I described-content, logical, and physical.39 Outside the Internet, the danger of vertical integration is less. 40 But within a network, the danger grows. Such integration, a report by the National Research Council has concluded, "could, if successful, cause a change in the Internet market, with innovation and creativity becoming more the province of vertically integrated corporations."41 It would, Web founder Tim Berners-Lee worries, be dangerous for innovation generally. "Keeping the medium and the content separate," Berners-Lee writes, "is a good rule in most media. When I turn on the television, I don't expect it to deliberately jump to a particular channel, or to give a better picture when I choose a channel that has the 'right' commercials. I expect my television to be an impartial box. I also expect the same neutrality of software." 42

The danger with the AOL-Time Warner merger is the danger that this vertical integration will induce AOL/TW to engage in discrimination-both discrimination in conduits (favoring their own lines over others) and discrimination in content (favoring their own content over others).

This danger is real. As economists Daniel Rubinfeld and Hal Singer have concluded, given the existing concentration in cable broadband, AOL/TW will have a significant incentive to engage in both forms of discrimination.43 And by mid-2001, AOL Time Warner had begun to prohibit advertisements on their sites for competing Internet access providers.44 Discrimination was threatened; discrimination is being realized.

AS THE CLINTON administration came to an end, one of the last acts of the (by statute, at least, neutral) Federal Trade Commission (FTC) was a sign of some hope. After a long and extensive investigation into the risks of the proposed AOL-Time Warner merger, the FTC, led by its chairman, Robert Pitofsky, conditioned the merger of AOL and Time Warner upon the essential elements of open access. Access to the cable broadband pipes must be kept open, the FTC insisted. Nonaffiliated content must flow without hindrance from AOL or Time Warner. And this unhindered access must include access to Internet-active TV.45

This decision by the FTC was an important breakthrough in the attitude of the government. Until this point, the government's view had been that the market here had to take care of itself. The problem, as the increasing mergers and restrictive access conditions demonstrated, was that the market was taking care of itself. The market was building a protection into the architecture that could change the commons for innovation dramatically.

But this decision is just a first round. (And as I describe in chapter 11, this first round may well be overturned by the courts.) As cable gets built out, as an administration emerges that is more open to allowing the market rather than rules to regulate, as other modes of broadband are built, the constant pressure will be to allow this founding principle of neutrality to itself be neutralized.

And then the question becomes this: If the original Internet architected an innovation space that was free, if it built that space by creating an environment where innovations would not be checked, if it was defined by a code layer that, in Benkler's terms, was open, then as the Internet moves onto fat pipes, will the same principle govern the code layer of the Net? Will broadband respect the principle of end-to-end as narrowband has? And if it doesn't, will the government do anything to resist the change?

WHAT'S AT STAKE here are two models for organizing a communications network, and the choice for us is which model will prevail. On the one hand, there is the model of the perfectly controlled cable provider-owning and controlling the physical, logical, and content layers of its network. On the other hand, there is the model of the Internet-which exerts no control over a physical layer beyond the decision to include equipment or not, and which enables the free exchange of content over a code layer that remains open.

As the Internet moves from the telephone wires to cable, which model should govern? When you buy a book from Amazon.com, you don't expect AOL to demand a cut. When you run a search at Yahoo!, you don't expect your MSN network to slow down anti-Microsoft sites. You don't expect that because the norm of neutrality on the Internet is so strong. Providers provide access to a network that is neutral. That's the essence of what the Internet means.

But the same neutrality does not guide our thinking about cable. If the cable companies prefer some content over others, that's the natural image of a cable provider. If your provider declines to show certain stations, that's the sort of freedom we imagine it should have. Discrimination and choice are at the core of what a cable monopoly does; neutrality here seems silly.

So which model should govern when the Internet moves to cable? Freedom or control?

NOT EVERY increase in control violates the principle of end-to-end. Obviously the ends are free, as far as this principle is concerned, to do what they want with their machines, and while some would resist calling the cable networks an "end," they could well argue that they are just a private network connected to the Internet. To link to the network is not to commit your hard disk to the use of anyone. The physical layer remains controlled, even if the code layer is free.

Here we see the source of the compromise that this chapter is all about. For in an important sense, the cable network is simply asserting the same rights with "its" equipment that I assert over my machine when connected to the Internet. My machine is mine; I'm not required to make it open to the world. To the extent I leave it open, good for the world. But nothing compels me to support it.

Leaving the ends free to choose, then, creates an opportunity for them to choose control where the norm of the Internet has been freedom. And control will be exercised when control is in the interest of the ends. When it benefits the ends to restrict access, when it benefits the ends to discriminate, then the ends will restrict and discriminate regardless of the effect on others.

Here, then, we have the beginnings of a classic "tragedy of the commons."46 For if keeping the network as a commons provides a benefit to all, yet closing individual links in the network provides a benefit to individuals, then by the logic that Garrett Hardin describes in chapter 2 above, we should expect the network "naturally" to slide from dot.commons to dot.control. We should expect these private incentives for control to displace the public benefit of neutrality.47

The closing of the network by the cable companies at the code layer is one example of this slide. If DSL providers were given the choice, they too would do the same. Wireless providers are implementing essentially the same sort of control. AOL Time Warner is insisting that code using its network be code that it controls.

In all these cases, the pressure to exert control is strong; each step makes sense for each company. The effect on innovation is nowhere reckoned. The value of the innovation commons that dot.commons produces is whittled away as the dot.coms rebuild the assumptions of the original Net.

Consider another example of this tragedy in play:

The World Wide Web is crawling with spiders. These spiders capture content and carry it back to a home site. The most common kind of spider is one that indexes the contents of a site. The spider will come to a Web page, index the words on that Web page, and then follow the links on the Web page to other sites. And by following this process as far as the links go, these spiders index the Web.

This index, then, is what you use when you run a search on the Web. There are many Web search engines, each with a slightly different technique. But they all rely upon the ability to spider the Web and gather the data the Web makes available.

These "spiders" are also called "bots." A bot is simply a computer program that runs remotely on another machine. Searching is just one example of the kinds of things computer "bots" do to one another on the Web. Some of those other things are awful: "denial of service attack" is an event where either one or a number of coordinating computers sends repeated requests to a Web page, ultimately overwhelming the server for that page. But in the main, these "things computers do to each other" have been productive and extraordinarily creative.

One example of this creativity comes in the context of auction sites. Auction sites make products available to real-time, wide-scale auctions. eBay is the most famous, but not the only one. eBay opened in 1995 as a place where individuals could offer their stuff in an auction to others. The idea caught on, and competing sites started offering the same service. Amazon.com has its own auction site, as does Yahoo!.

But then customers interested in auctions faced another "metaproblem." If they had things they were watching on many different sites, they had the hassle of traipsing through all those sites to find what they wanted to watch. So where there was a problem, the market quickly provided a response. Bidder's Edge, among others, began to offer a site that did the surfing for you. On one page you could see the status of all your auctions. And Bidder's Edge promised to update this information regularly.

In each case, the innovation is the same. The Web is an open architecture; it begs for people to discover new ways to combine the resources it makes available. In each of these cases, someone did discover a new way of combining resources. And this discovery then produced a new kind of market. Search engines were a defining feature of the original World Wide Web. And the opportunity to quickly compare prices was one of the early promises for competition on the Web.

But in each case, too, there is this undeniable fact: When a search engine spiders the Web, it uses resources of others to build its index. When Best Book Buys enters Amazon.com, it collects the price Amazon offers by using Amazon's servers. In a sense, then, we could say that each of these bots trespasses on the servers of other sites.

To many, this idea of trespassing bots will seem bizarre. But it did not seem too bizarre to the lawyers at eBay. For eBay didn't want bots that created competitors to eBay. And it had imposed a NO-BOT policy on access to its Web site. That is, it indicated in the code of its site that it did not want unlicensed bots to enter its site.

Bidder's Edge ignored that sign. It continued to gather data even though the eBay lawyers told it not to. And eventually it found itself in court, in a lawsuit brought by eBay charging Bidder's Edge with "trespass."

In one sense, of course, the lawsuit was completely right. In the virtual sense in which one "goes" to a Web site, Bidder's Edge's bot was "entering" a computer without the permission of its owner. And "entering" without permission is the classic definition of "trespass."

But in another sense, the claim seemed bizarre. The Web was built on a norm of open access; this was a community that kept its doors unlocked. No one forced eBay to open itself to the World Wide Web. But if it did, it should live by the norm. And if the norm was openness, then it was eBay that committed the offense.

Both sides brought in lawyers to argue their respective points of view. On the side of eBay was an outspoken, and famous, law professor from the University of Chicago-Richard Epstein. Epstein pushed the law-focused answer: Trespass law made perfect sense in the Internet context. Indeed, it made more sense here than in real space. It was simple to establish signs that stated the conditions under which entry was permitted; those signs could be easily read by bots. If a site wanted to restrict access to all save those who pay, then that was perfectly permissible, Epstein argued. The site, after all, "owned" the equipment. Control over the property one owns is perfectly ordinary. 48

On the other side was a lawyer who was a bit more careful with the legal tradition. Law professor Dan Burk argued that the law had been strict only when it came to "land." Other property was protected against unauthorized use. But that protection was not absolute. To support a lawsuit based on trespass to property other than land, the plaintiff would have to demonstrate some sort of harm. But in the Bidder's Edge case, no harm had been pleaded. Thus, under traditional trespass doctrine, eBay should lose.

Both sides had a point, and while my bias is with Burk, I don't mean to deny the plausibility of a different regime. What I do deny, however, is that the answer to this question is obvious. What is most damaging about the submission made by Epstein is its obliviousness to any issue on the other side.

For no doubt we could move to a world where every use of data on the Web had to be licensed. We could generalize from the control everyone has over "his" machine to the power to deny the neutrality of the network generally.

But there are costs to that world. Closing access based on this argument grounded in the physical layer of the Net increases the costs of innovation for the Net generally. If to deploy a technology the innovator must first license its use, this legal requirement then functions as a kind of tax on innovation on the Net.

This is especially true with bot technologies. These devices-the next generation after the spiders that gather data from the Net-would enable agent-driven, fluid marketplaces on the Net. These bots could search out prices, negotiate contracts, and schedule delivery in a way that is far more efficient than any of the existing markets.49

The response to this is that we don't want rules that force people to devote their resources to something they don't want to support. Bidder's Edge didn't pay the servers that it used when it linked to eBay's data. Why should eBay be forced to subsidize a competitor?

But this story could be told both ways around. eBay benefits greatly from a network that is open and where access is free. It is this general feature of the Net that makes the Net so valuable to users and a source of great innovation. And to the extent that individual sites begin to impose their own rules of exclusion, the value of the network as a network declines. If machines must negotiate before entering any individual site, then the costs of using the network climb.

As I said at the start, this closing of sites selectively changes the character of the Net, but not necessarily its compliance with end-to-end. As it increases, however, it does change the commons of the Internet into something different. To the extent that this ability-to select the uses that access to the Net permits-grows, then this permission changes the character of the commons the Internet creates.

THIS DISCRIMINATION is growing in other contexts as well. Sometimes it happens for innocent reasons, sometimes less innocently. An innocent case is the emergence of a technology called "network address technologies" (NATs). NATs are devices for multiplying IP addresses. Every machine on the Internet needs a unique IP address-that's how the Net knows where to send the packets. But NATs make it so many machines can share the same IP address.

NATs were created initially because of an expected shortage of IP addresses. The technology has subsequently grown simply because of the difficulty in coordinating devices in many contexts. Apple, for example, uses NATs to connect machines to its AirPort wireless server. You can plug an AirPort into your cable or DSL modem and then an unspecified number of machines can share the very same IP address.

The problem with NATs is that the techniques used to share IP addresses are not standard. The NAT inserts points of control into the network. Data passing onto a NAT-controlled network must pass through the NAT before the NAT permits it to pass to the end user. If the NAT is unaware of how to process the data from that particular application (either because the NAT was unaware of that application or because it was coded to ignore data of that type), then that application won't function on that NAT-empowered network. Developers of technologies that need to be certain they are talking to a particular machine must therefore survey the world of NATs to make certain their systems will work on all the major brands. This in turn increases the costs of development and, on the margin, may reduce innovation.

No one thinks NAT boxes are part of a conspiracy. This compromise of end-to-end is innocent in the sense that we don't imagine it is implemented for strategic purposes. Nonetheless, it reduces the flexibility of the Internet as a whole.

But there is a solution to the problem that NATs were initially designed to solve-and again, it is to increase capacity. The name space for the Internet (IPv4) is in the process of being upgraded (to IPv6). That will have a practically endless number of addresses,50 thereby eliminating the need for NATs. With endless address space, technologies for "conserving" addresses become unnecessary at best. Thus, rather than imposing this high-coordination cost on technologists developing technology for the Net, increasing the name space would remove the initial reason for the compromise.

Other compromises with end-to-end are less benign. Consider firewalls, for example. A firewall is a technology for controlling interaction between a local network and the Internet. Like the NAT, it is a technology that adds a point of control within the network that could block everything that has not explicitly been admitted by the local network manager. Unforeseen applications thus again pay a heavy price.

Firewall technology, for example, no doubt serves a legitimate purpose in many cases. Sometimes, however, its purpose is expressly to impose a policy on the Net. Many universities, for example, forbid the use of Napster technologies. They enforce this ban by telling their firewalls to block Napster content. This in turn produces something of an arms race, as developers shift their systems to channels that will never be filtered by a firewall. But that shift will only make it harder to use those channels in different applications efficiently. 51

Here too there is a solution that could solve the problem that firewalls answer, but without compromising end-to-end. A technology called IPSec could enable better control over access consistent with end-to-end.52

In each of these cases, then, there are two issues at stake. As technologies for facilitating discrimination increase, one question is where these technologies get located in the Net-on the Net or at the edge. A second question is the effect such discrimination will have, even if it is located at the ends. The end-to-end principle counsels that we locate such discrimination at the ends rather than in the network; but even when it is located at the ends, a widespread pattern of certain types of discrimination could weaken the commons the network now provides.

There are reasons not to worry so much about this kind of discrimination. Where concentration is slight and many different services are available, the risk that any particular concentration will harm innovation is slight as well. Some ends may be Christian Right; as long as they don't interfere with access to the Christian Left, innovation for the Christian Left will not be harmed. The key is to preserve user autonomy; the danger is a technology that might undermine autonomy.

The danger is discrimination engaged in by concentrated actors. Here again we return to the story of concentrating cable. For if we were in a world where there was significant competition in broadband services, with many different suppliers each essentially open-and hence, each not discriminating in the kind of access that is provided-then the danger from closed access in one channel would be greatly reduced. The value of the commons in the highway is not lost simply because some roads become private. But when there isn't a great deal of competition in access, when a small number of companies can set the rules for the whole system, then the dangers in discrimination return. When a few can make decisions about what kinds of innovation will be permitted, the innovation promised by an end-to-end architecture is lost.

The danger of the changes that I have described in this chapter is that just this concentration is occurring. And the dangers in this concentration include the fear that an opportunity for innovation will be lost. We will have used architecture and rules to shift control over how the network can be used, from the many ends that constituted the Internet originally, to a few that own the wires. Control will have been returned to this medium born free.

THERE IS ANOTHER side to the stories I have told. Not every increase in control is driven by a desire to lessen competition; not every increase will have the effect of undermining innovation.

Indeed, some increase in control may well be necessary if investment to build a network is to proceed. Just as cable companies argued initially that control over their cable lines was essential if there were to be a sufficient return from laying cable, so too cable companies today may rightly argue that control is needed if the return is to be enough.

The cable companies may be right. And striking a monopoly deal with a provider is a strategy that governments have employed since the start of governments. My argument cannot begin to resolve the question of whether or not the cable companies are right in their defense. If this infrastructure is to be built without public support, then protected monopoly may well be necessary.

My argument is meant simply to highlight a cost that may well run with a benefit. The Internet is not a community antenna. It is not simply a system for delivering a given kind of content more efficiently. The critical feature of the Internet that sets it apart from every other network before it is that it could be a platform upon which a whole world of activity might be built. The Internet is not a fancy cable television system; the Internet is the highway system, or the system of public roads, carrying bits rather than trucks, but carrying them in ways no one can predict.

When the United States built its highway system, we might have imagined that rather than fund the highways through public resources, the government might have turned to Detroit and said, Build it as you wish, and we will protect your right to build it to benefit you. We might then imagine roads over which only American cars can run efficiently, or exits and entrances that tilt against anything built outside Detroit. Or we could imagine Detroit then auctioning rights to use its network to the highest bidder, or excluding Coke trucks because of an exclusive contract with Pepsi.

This power in Detroit might well have been necessary if Detroit were to have had sufficient incentive to build the highways. But it does not follow that Detroit should be given this power. For however much the state may gain by not having to fund roads on its own, society would lose in the aggregate if the open commons of transportation were lost.

That loss is even more pronounced in the context of the Internet. Roads have many uses, but "many" is still not infinite. Any kind of commerce gets to use the roads: trucks as well as VW bugs; campers as well as pickups. But the physical nature of roads limits the possible "many" uses. Lots are possible, but "the possible" is constrained.

The constraints on the Internet-properly architected-are far fewer. The range of uses is far less constrained. The Internet could be a platform for innovation across the full range of social and political life. Its possible uses are, even this far into its growth, unknowable.

We may gain something by giving network owners power over the network. I don't question that. But we will lose something as well. To the extent we chill innovation that threatens disruption, disruption will be slower in coming. That slowness is a cost that society must account for. We may gain something from the "free" infrastructure monopoly builds. But we lose something with the "controlled" infrastructure that monopoly inevitably wants.

Even more significant, we have no good way to make sure that the gains outweigh the losses. To the extent that the code layer builds an innovation commons, changes at the code layer threaten to exhaust that commons. Changes imposed by broadband providers weaken the value of the neutral platform; changes effected through NATs or firewalls similarly weaken the innovation potential of the Net. All these changes are effected locally, but they also have a global effect. Each may make sense locally, but there's no obvious way to be certain that their effect globally will also make sense.

In this way, changes at the code layer create their own tragedy of the innovation commons. As we might paraphrase Hardin:

Therein is the tragedy. Each [firm] is locked into a system that compels [it] to increase [its control] without limit-in a world that is limited. Ruin is the destination toward which all [firms] rush, each pursuing [its] own best interest in a society that believes in the freedom of the commons. Freedom in a commons brings ruin to all.53

"Ruin" is a strong word, I'll concede. But the dynamic is the same nonetheless: the incentive is for companies to layer control onto the Net; that has been the history of the past five years. But the effect of that incentive is felt by the Net as a whole. Yet its effect on the innovation commons is almost completely ignored.

Against this trend, some rightly argue that government has a role to play to assure that ISPs continue to offer "open IP service." As a recent National Research Council report put it:

[C]oncerns about the vertical integration of the data transport and content businesses and about content control, as seen in recent debates about access to cable broadband Internet systems, could be eased if ISPs committed to providing their customers with open IP service. From this standpoint, the continued delivery of open IP service would be an enlightened move in the long-term interest of the industry.54

Just the sort of wisdom that finds its way into NRC reports and is ignored almost everywhere else.

THE CHANGE that is happening in the context of wires has a particular form. We are in the midst of a radical change in technology; that change threatens existing interests; those interests have an interest in minimizing the threat that this change presents; they can minimize that threat by reestablishing choke points on the system that emerges. They can, in the words of Gerald Faulhaber, use the architecture to regain strategic control.55

This is precisely the change that is happening. As Charles Platt put it in a recent article in Wired, "Everyone knows that the broadband era will breed a new generation of online services, but this is only half the story. Like any innovation, broadband will inflict major changes on its environment. It will destroy, once and for all, the egalitarian vision of the Internet."56

Dinosaurs should die. This lesson we have learned over and over again. And innovators should resist efforts by dinosaurs to keep control. Not because dinosaurs are evil; not because they can't change; but because the greatest innovation will come from those outside these old institutions. Whatever the scientists at Bell Labs understood, AT&T didn't get it. Some may offer a theory to explain why AT&T wouldn't get it. But this is a point most understand without needing to invoke a fancy theory.

Because the Internet is inherently mixed-because it is a commons built upon a layer that is controlled-this tension between the free and the controlled is perpetual. The need for balance is likewise perpetual. But the value of balance is not always seen. This value we need to keep in focus.11. Controlling the Wired (and Hence the Content Layer)

IN THE last chapter, I argued that there is a tension between control at the physical layer and freedom at the code layer, and that this tension affects the incentives for innovation. The original freedom built a commons; more control can undermine that commons; the tragedy is our forgetting the value of the free in our race to perfect control.

The same tension exists at the content layer. Some content the law treats as "owned"-copyright and patents are "intellectual property," owned by individuals and corporations. Other content can't be owned-either content that has fallen into the public domain or content that is outside the scope of Congress's power under the copyright and patent clause of the Constitution. Here, too, balance is important. Yet here, too, the owned chases out the unowned. The pressure to protect the controlled is increasingly undermining the scope for the free.

My aim in this chapter is to describe this dynamic and to suggest how changes that we are seeing right now will affect this dynamic. By the time this book is published, I fear the struggle I am describing will be finished. The courts will have resolved these questions, and the politicians will have no courage to interfere with this resolve. Already the endgame is clear; already property has queered the balance. Hence, already the value of this freedom will have been lost.

THIS CHAPTER is meant to mirror chapter 4, "Commons Among the Wired." Yet it is not directly about the people I spoke of in chapter 4. The "wired" who are affected by the changes I am describing here are not exactly the same "wired" who built the open source and free software movements that I spoke about there.

But in a critical sense, they are the same. Both innovate by building on the content that has gone before. Both therefore reveal how much creativity depends upon the creativity that has gone before. Both show, that is, innovation as adding something to the work of others.

In some cases, the restrictions I describe in this chapter apply directly to the innovators of chapter 4. Patent law, for example, poses one of the most significant threats to the open code movement that there is. But in general, the changes I describe in this chapter are aimed at controlling a new generation of "wired" folks-those who see the platform of the Internet as an opportunity for a different way of producing and distributing content and those who see the content on the Net as a resource for making better and different content. The changes in this chapter are changes that reestablish control over this class of potentially wired souls.

WHEN THE Net emerged into the popular press, there was an anxiety among many about what the Net would make possible. People could do things there that we had discouraged or made illegal here.

Pornography was the most dramatic example of this anxiety. The freedom of the Net meant, the world quickly learned, the freedom of anyone-regardless of age-to read the obscene. The news was filled with instances of kids getting access to material deemed "harmful to minors." The demand of many was that Congress do something to respond.

In 1996, Congress did respond, by passing the Communications Decency Act (CDA).1 Its aim was to protect children from "indecent content" in cyberspace. The act was stupidly drafted, practically impaling itself upon the First Amendment, but its aim was nothing new. Laws have long been used to protect children from material deemed "harmful to minors." Congress was attempting to extend that protection here.

Congress failed. It failed because the CDA was overbroad, regulating speech that could not be regulated constitutionally. And it failed because it had not properly considered the burden this regulation would impose upon activity in cyberspace. The statute required adult IDs before adult content could be made available. But to require sites to keep and run ID machines was to burden Internet speech too severely. Congress would have to guarantee that the burden it was imposing on the Internet generally was no greater than necessary to advance its legitimate state interest-protecting children.

In 1998, Congress tried again. This time it focused on clearly regulable speech-speech that was "harmful to minors." And it was much more forgiving about the technology that would permissibly block kids from "harmful to minors" speech. Still, federal courts struck down the law on the ground that the burden it would impose on the Internet generally was just too great.2

These cases evince a distinctive attitude. Though the state's interest in protecting children is compelling, courts have insisted that this compelling state interest be pursued with care. In effect, a demonstration that the regulation won't harm the Net too broadly is required before this state interest can be promoted. Facts, and patient review, are the rule in this area of the law of cyberspace.

Keep this picture in mind as we work through the examples that follow. For the meaning of Reno v. ACLU is not that porn is okay for kids or that the state's interest in enabling parents to protect their kids from porn is outdated. The Court in Reno was quite explicit: Protecting children from speech harmful to minors is a "compelling" state interest. But this compelling interest must be advanced in ways that are consistent with the other free speech values. The state was free to advance its compelling state interest; but it was required, in so doing, not to kill the rest of the Net.

ABOUT THE same time that parents were panicking about porn on the Net, copyright holders were panicking about copyright on the Net. Just as parents worried that there was no way to keep control over their kids, copyright holders worried that there was no way to keep control over copyrighted content. The same features of the Internet that made it hard to keep kids from porn also made it hard to keep copyrights under control.

Both forms of panicking were premature. While it is true that the Net as it was originally built made it hard to control content (by either keeping it from kids or keeping it from being copied by kids), the Net as it was originally built is not the Net as it must be. Code made the Net as it was; that code could change. And the real issue for policy makers should be whether we can expect code to be developed that would solve this problem of control.

In Code I argued that in the context of copyright, we should certainly expect such code to be developed.3 And if it were developed as its architects described, then the real danger, I argued, is not that copyrighted material would be uncontrolled: the real danger is that copyrighted material would be too perfectly controlled. That the technologies that were possible and that were being deployed would give content owners more control over copyrighted material than the law of copyright ever intended.

This is precisely what we have seen in the past two years, but with a twist that I never expected. Content providers have been eager to deploy code to protect content; that much I and others expected. But now, not only Congress but also the courts have been doubly eager to back up their protections with law.

This part I didn't predict. And indeed, in light of Reno v. ACLU, one would be justified in not predicting it. If parents must go slowly before demanding that the law protect their kids, why would we expect Hollywood to get expedited service?

The answer to that question is best left until after we have surveyed the field. So consider the work of the courts, legislatures, and code writers in their crusade to expand the protections for a kind of "property" called IP.

INCREASING CONTROL

Copyright Bots

IN DORM ROOMS around the country, there are taped copies of old LPs. Taped to the windows, there are posters of rock stars. Books borrowed from friends are on the shelves in some of these rooms. Photocopies of class material, or chapters from assigned texts, are strewn across the floor. In some of these rooms, fans live; they have lyrics to favorite songs scribbled on notepads; they may have pictures of favorite cartoon characters pinned to the wall. Their computer may have icons based on characters from The Simpsons.

The content in these dorm rooms is being used without direct compensation to the original creator. No doubt, no permission was granted for the taping of the LPs. Posters displayed to the public are not displayed with the permission of the poster producers. Books may have been purchased, but there was no contract forbidding passing them to other friends. Photocopying goes on without anyone knowing what gets copied. The lyrics from songs copied down from a recording are not copied with the permission of the original author. Cartoon characters, the exclusive right of their authors, are not copied and posted, on walls or on computer desktops, with the permission of anyone.

All these uses occur without the express permission of the copyright holder. They are unlicensed and uncompensated ways in which copyrighted works get used.

Not all of these uses are impermissible uses. Many are protected by exceptions built into the Copyright Act. When you buy a book, you are free to loan it to someone else. You are free to copy a small section of the book and give it to a friend. Under the Audio Home Recording Act, you are free to copy music from one medium to another. Taped recordings of records are therefore quite legal.

But some of these uses of copyrighted works may well be illegal. To post the poster may be a public display of the poster not authorized by the purchase.4 To use icons on your computer of Simpsons cartoons is said by Fox to violate its rights. And if too much of an assigned text has simply been copied by the student, then that copying may well exceed the scope of "fair use."

The reality of dorm rooms, however-and, for that matter, most private space in real space-is that these violations, if they are violations, don't matter much. Whether or not the law technically gives a student the right to have a Simpsons cartoon on his desktop, there is no practical way for Fox Broadcasting Company to enforce its rights against overeager fans. The friction of real space sets the law of real space. And that friction means that for most of these "violations," there is no meaningful violation at all.

Now imagine all this activity moved to cyberspace. Rather than a dorm room, imagine that a student builds a home page. Rather than taped LPs, imagine he produces MP3 translations of the original records. The Simpsons cartoon is no longer just on his desktop; imagine it is also on his Web server. And likewise with the poster: the rock star, we can imagine, is now scanned into an image file and introduces this student's Web page.

How have things changed?12. Controlling Wireless (and Hence the Physical Layer)

ON THE wall in my office is a multicolored poster. The poster is large (maybe 30 by 42 inches), and it is titled, in beautifully retro typewriter font, "United States Frequency Allocations-The Radio Spectrum." To the left are thirty-three colored boxes, listing the legends for the poster. Thirty list "radio services." Three list "activity codes." Among the activity codes are "government exclusive,"

"government/non-government shared," and "non-governmental exclusive." (Appropriately enough, government exclusive is red, while nongovernmental exclusive is green.)

If you could tilt this poster and give it a bit of a 3D look, it might remind you of the famous New Yorker cartoon maps, where everything close is detailed and significant, while everything far is wide open and unimportant. So it is with spectrum as well. At the highest frequency (30-300 gigahertz), the allocations are a patchwork of tiny colored boxes, sometimes four deep; but as you move down the frequency range, the allocations get wider and less precise. The largest swath is AM radio.

This map, however, doesn't mark out any physical space. It marks the allocation of radio spectrum. The map says what kind of use will be permitted at what range of radio spectrum in any particular part of the territorial United States. It does not say by whom.

As I described in chapter 5, the "by whom" part is determined by a complex set of federal regulations. The FCC makes a decision about who gets to use what spectrum when, and under what conditions. These "licenses" are not really licenses to spectrum. As Thomas Hazlett describes them, they are simply permissions to use certain kinds of equipment at certain times for certain purposes. Their effect is therefore not so much to regulate a resource (spectrum) as it is to determine who has the rights to engage in certain kinds of businesses, where. To say that company X has an FCC license is to say that the government has given company X the right to engage in a certain kind of business (say, radio broadcasting) using certain equipment tuned to certain radio frequencies.1

The manner in which this allocation of rights to use spectrum is made has changed, and it changes still. As Eli Noam describes, in the first era of spectrum use, spectrum was allocated on a first come, first served basis. This was before the federal government entered the field. After 1912, Noam's "second era," it was the government that chose who got what spectrum. This invited predictable biases: existing owners bought the favor of regulators, and regulators in turn protected them. The examples are many, and extraordinary (at least to those who live outside D.C.): Hazlett has cataloged the cases where favored interests have succeeded in using their power over regulators to resist new technologies;2 as Noam writes, "[I]n the early 1950s, only newspaper companies that had editorially endorsed Eisenhower for President had a chance at getting a TV license."3

In the third era (now), the right to use spectrum is increasingly allocated through auctions. The government sells the right to the highest bidder (subject to a scad of typically governmentlike, mainly silly, conditions). That bidder uses the spectrum as the auction specifies or, in a small set of cases, the bidder is then free to reassign the right to others.

Politicians from the Left and the Right just love auctions. For the Left, auctions promise more money for the government to spend; for the Right, auctions sound like markets, and markets are always good.

But as we saw in chapter 5, both auctions and government assignments ignore a fundamentally different way to "allocate" spectrum-namely, not allocating it or, more realistically, not allocating all of it. Rather than assigning rights to use the spectrum resource ex ante, this alternative would allow users to share the resource when the need to use it arose. This sharing would be policed either by a market (in Noam's conception) or by other technological devices designed to deal with congestion.4 Like the Internet, on this latter model, the system would find a technological means to deal with undercapacity. Spectrum could then be shared, and a range of different technologies says how.

How much? How completely? Could shared spectrum govern everywhere?

The optimists in this story say that shared spectrum could service all of our spectrum needs: that we could replace allocated spectrum in one fell swoop and free spectrum for the use of all.5 Others argue that shared spectrum could serve only a small part of our spectrum needs and that we will always need to have some spectrum that is allocated or controlled through a market.6

Whether all or not, however, it is clear that shared spectrum could serve a great deal of our spectrum needs, which means that not all spectrum needs to be allocated for spectrum to be usable. Even under the most conservative estimates of what shared spectrum might offer, shared spectrum could serve a large and important part of our spectrum demand.

Which raises an important question that by now will have become familiar: To the extent spectrum could be shared, what justifies the extent of spectrum allocation that we now see?

TO BUREAUCRATS AND legislators, this kind of question will seem odd. What justifies it? "What justifies it is that we've always done it like this. Any change must be as we permit." Thus, the FCC is moving slowly to open the spectrum that it can, just as fast as it believes is right in the face of any lobbying or opposition we might see.

But to courts, the question of justification is at the core of what they do. And when one asks what justifies a particular system of allocation, "We've always done it like this" is not an answer. The answer is that it is justified only if it is.

So is this system of allocation justified?

When the government allocates a speech resource like spectrum, its decisions are tested according to a well-defined standard. The question is not whether the regulation meets strict scrutiny-as would a regulation that said, for example, that only Republicans may use the spectrum. The question instead is whether the regulation meets intermediate scrutiny-whether "[1] it advances important governmental interests unrelated to the suppression of free speech and [2] does not burden substantially more speech than necessary to further those interests."7

If a court addressed that question now, my sense is that it would clearly decide that the FCC's system for allocating spectrum is just fine. But it would decide that mainly because the alternatives are not yet developed or understood. We don't have many great examples of how spectrum could otherwise be regulated. Most of the mature examples of how spectrum can be used are examples that rely upon this system of allocated spectrum.

But the catch-22 is that the ability of these alternatives to demonstrate their success depends upon the FCC's opening up spectrum for alternative use. The more it opens, and leaves to private market experimentation, the easier it will be to demonstrate that the shared spectrum model works. And likewise with the contrary. The less the FCC leaves open for experimentation, the fewer incentives there will be in the market for innovators to develop new innovation. Thus, the critical need right now is a broad range of spectrum where these alternative uses might demonstrate themselves.

In a sense, this is the telephone network all over again, though this time the wires are ether, and the control is imposed through law alone rather than through the law backing up the control of AT&T's technology. Until innovators are free to use a communications resource (now spectrum, before the wires), innovation will be slowed. Yet in this case, one would expect, the claim to free use is even stronger than with the wires. No investor or corporation built the radio spectrum. This resource was given to us pre-built by Mother Nature. Thus, the claim to free access is simply a claim that the government not get in the way of experimentation by innovators.

AT THE VERY minimum, this possibility suggests a strategy for government regulators (if those regulators were not effectively captured by existing spectrum users). The strategy builds on what we know: that government control over spectrum use has stifled innovation; that it will continue to stifle innovation as long as existing users have a political channel through which they can defend their existing privilege. We therefore should move-as quickly as possible-to a regime where the right to innovate does not depend upon the permission of someone else.

Essentially, two such regimes are possible. As I described in chapter 5, one follows Coase, and the other is a commons. Under the first, spectrum would be propertized and sold on a market. The buyer would be free to manage his spectrum however he saw fit. "Band managers" would control the chunks of spectrum that they own; users wanting to "use" that spectrum would license that right from a wide array of spectrum owners. Assuming the supply of spectrum would be great and the number of competitors large, this system would produce a strong competition in spectrum supply. No single supplier could control innovation any more than a single supplier of paper can control what books get written.

Under the second regime, spectrum is held in a commons and shared in real time by smart technologies for sharing. The rights are not allocated up front; as with the Internet, the demand is managed by protocols as it arises. Here, too, assuming the supply of spectrum is great and the protocols neutral, there would be no one who could stymie innovation. As with the Internet, the system would have no intelligence to discriminate against one form of spectrum use in favor of another.

There are advantages and disadvantages to both regimes. The property regime would produce great competition in spectrum use, and if competition were sufficiently "perfect," then, as I've noted before, the regime would produce the feature of the commons that is most salient here: that strategic action by the resource owner would not be possible. The only costs to the property regime are the burdens of any property regime-the costs imposed on the market by the need to negotiate and secure rights to access.8 And if the ability to "share" spectrum becomes central to efficient spectrum management, then the costs of securing this right to "share" through private contract could become quite prohibitive.

The commons regime, too, would produce great competition in spectrum use. But the danger with the commons is overuse: that the free resource of spectrum would produce more demand than supply; and that important uses of spectrum would be shut down by congestion.

But in this choice between regimes, the mistake is to assume that a single solution is necessary. There is no reason to embrace either the market or the commons completely. Instead, the best strategy for now would be to embrace both solutions vigorously-to mark off significant chunks of spectrum for sale, while leaving significant chunks of spectrum open in a commons. Alongside auctioned space, we should have broad swaths of unowned space. And the government should then assure innovators that the unowned would remain so for a good long time-say, the length of a patent or, better yet, a single copyright (just to get some political energy on the side of reducing the term of copyright).9

This way the market could experiment with technologies and different spectrum uses. Some spectrum uses may need reserved space; let the market provide that. Other spectrum uses may be more flexible-as use of the Internet today is. But the opportunity for broad and creative use of the spectrum would inspire many to develop technologies that they wouldn't otherwise build. As entrepreneur Alex Lightman puts it:

We need to have some slack in the system. We need to have a certain amount of the spectrum not be in the category where it's owned.10

This proposal is relatively neutral. It doesn't take sides in the technology of the future; it instead structures the competitive environment to allow a range of technologies to flourish. It acknowledges that many different solutions are possible; it recognizes the radical change in technologies for using spectrum that we are already seeing; and it simply embraces a strategy for getting the most out of these innovations. But it does say that for the moment, "these things need to be held in common, and they need to be held for the future. Because we don't know what the future will hold."11

SO IS THIS PROPOSAL anything close to what the government is actually doing? The answer is yes in words, but no in reality. At the level of high theory, the government remains committed to developing these alternative uses. But the devil is in the details, and the dungeons of Washington are well detailed.

If there is one thing that is certain about governments and innovation, it is that those who are threatened by new innovation will turn first to the government for help. Spectrum policy is not, and has never been, different. Every new idea is a threat to those who depend upon old ways of doing business. As David Hughes puts it, "[B]ig corporations have always marched to government to lock in their profits."12 So we might well expect that those whose way of doing business depends upon the comfortable life of government-backed monopolies over spectrum will do what they can to make sure that those monopolies are not threatened by this new, free way of using spectrum.

We might expect it, and if we look at what's happening at the FCC, we would also observe it. For as quickly as innovators can develop new ways of using spectrum, incumbents are finding ways to make this innovation harder.

As I've already argued, this is nothing new. The surprise is how blatantly this protectionism continues. Consider, for example, the effort of former FCC chairman William Kennard to license low-power FM radio stations. This was a good move from the standpoint of increasing competition and diversity in speech. Kennard was committed to finding a way to free spectrum resources to enable a broader range of speakers. And there was very good technical evidence that these low-power radio stations-which might support a community action center or a local school-would create no technical interference with existing radio stations.13

The aim of the FCC was to enable local community broadcasts while assuring the broadcasts would not interfere with existing radio stations. The technical staff of the FCC conducted tests to determine how low-power the stations would have to be to assure no interference; the rules the FCC eventually proposed were more conservative than the technical staff recommended. "As a result of the FCC's conservatism, community groups in large urban centers with many incumbent broadcasters would find it difficult, if not impossible, to operate. But it would have enabled over 1,000 community organizations, churches, and schools to create a new medium for local discourse."14

But the existing stations balked. At first they complained to the FCC. When the FCC concluded that their evidence of interference was not substantiated, the broadcasters went to Congress. Congress didn't care much about these low-power stations (not many campaign dollars, after all, come from them). It did care about the broadcasters who were threatened. So Congress passed a law to restrict low-power broadcasters.15 Large FM stations were protected from increased competition; that protection was effected through a law that silenced other speakers. So much for the First Amendment's demand that "Congress shall make no law abridging the freedom of speech."

An example closer to the technology at the core of this chapter is the case of "AirPorts" in "airports." The AirPort is a wireless device sold by Apple Computer. It uses the 802.11b protocol to enable a computer to connect to a network at very fast speeds-11 megabits per second is the maximum for 802.11b, which is about twice the current DSL or cable speed. The device uses spread spectrum technology within one of the three swaths of spectrum that the FCC has allocated for "unlicensed use" for data.

Apple was a pioneer in pushing this form of technology. But the AirPort connects not just Apples. Any computer with an 802.11b wireless card can connect to an AirPort. And other companies, too, are building the equivalent of AirPort modems. Indeed, a whole sector is growing up around this possibility of wireless network access.

Some got the idea of putting AirPorts (or their equivalent) in airports-enabling travelers to connect to their Internet while sitting in an airport lounge. But soon local airport authorities started to complain: wireless modems, they argued, would interfere with air traffic controllers. They would also reduce the usage of pay phones. 16

Now, I don't doubt that interference is possible for some of these new technologies. It is important that we be certain that new technologies don't damage important pieces of the existing infrastructure-at least those parts that we want to keep. But this complaint about air traffic was just silly. There is more interference caused by a hair dryer than by an Apple AirPort modem. And the notion that airport authorities should be able to stop progress to protect their telephone revenue is absurd.17

What's needed in contexts like this is a balanced way to evaluate these claims of interference to resolve whether they are real or just pretext. More generally, what's needed is a commitment to progress in the use of spectrum resources.

Instead the politicians have done just the opposite. Claims of technical interference are not credibly evaluated. Indeed, the FCC has placed the burden on new technologies not to harm existing use at all. 18 As Hazlett puts it, the "system is booby-trapped against new rivals, an irresistible 'at-tractive nuisance' to anticompetitive constituencies."19 Thus, for example, amateur radio operators are allowed to veto new spectrum uses if they interfere at all with existing ham operations. And this pork is not just because of a special favor to amateur operators. Any new use that interferes with any old use must step aside.20

These restrictions on the use of amateur radio spectrum are particularly ironic. The Amateur Radio Service (ARS) defines a range of spectrum that is allocated to amateurs, but the members of the service "share" the spectrum in a commons mode. Any amateur can use any portion of that spectrum at any time and with just about any modulation technique now known. Within the prime "beachfront" spectrum property (from 30 megahertz to 3 gigahertz) there is a great deal allocated to the ARS. But armed with the FCC's veto rule, amateurs are effectively able to veto new and different users of "their" spectrum-despite the obligation imposed by the FCC's regulation to enhance "the value to the public as a voluntary noncommercial communication service." 21

These are technical rules that protect the old from the new. So too are there political rules that achieve the same end. Among these, none is more significant than the commitment and eagerness of the government to sell rights to spectrum, in the way spectrum rights are sold now. 22 This form of auction essentially entrenches the use of spectrum for particular businesses. The license "does not yield the right to deploy spectrum in alternative uses."23 It entrenches a way of speaking of spectrum that is resistant to modern sharing technologies: that the spectrum is "my property." Once that property is established, it will be harder to deploy technologies that "share" other people's "property." (This despite the fact that an applicant for an FCC license must first certify that it will not assert any propertied interest in radio spectrum.)24 As Dave Hughes has asked:

[W]hy should AT&T, who is offering a wireless service, consent to competitors in their own area? [I]t's not just a question of interference now. Now it becomes opening the door by their consent to competition. And the last damn thing big companies want is competition.25

the danger in selling spectrum or, more precisely, in not experimenting broadly with unlicensed spectrum is that existing spectrum users will be able to use purchased spectrum to resist changes in spectrum policy that might threaten their business models.26 By selling spectrum now, before alternative uses can be developed, we create a world where the resources for these new alternatives are held by those with the strongest incentive to stop them. As Eli Noam puts it, it is like "having the old AT&T auction off the right to compete against it. Under such a system, MCI would not have emerged." 27

The concern is not just about spectrum owners; it is also about the nature of the existing spectrum uses. The dominant and fastest-growing spectrum use right now is mobile telephone systems. These systems are architected in just the way the old telephone network was-intelligence is located not at the ends, but instead in the network itself. The cellular phone companies retain control over how the cellular technology develops; if you want a new application for your (increasingly powerful) phone, you will get it only if the telephone company wants you to.

This architecture for a wireless system creates the obvious protectionist risks. And as Charmed Technologies CEO Alex Lightman puts it, we are already seeing these risks mature into protectionist practices.

[T]here is a nice little cozy mnage trois between the companies that are providing infrastructure and the companies that provide the handsets, and the monopoly carriers or the oligopoly carriers.28

By selling the spectrum, the carriers have a strong incentive to assure returns sufficient to recover the investment in spectrum. These returns are best assured (or at least it seems to the companies that are best assured) if the companies husband the market power that is carried over from the non-competitive telephone world (recall: much of the action here is international, where competitive phone systems don't yet exist). Thus, the willingness of the existing players to open up their spectrum to a wildly different form of use-one that would be much more competitive-is unlikely at best. Recall the words of AT&T executive Jack Osterman in 1964: "[W]e'll be damned if we allow the creation of a competitor to ourselves."29

A policy from the FCC that does not create a strong opportunity for an alternative to develop is designed to protect existing interests. To not encourage or permit wide-scale experimentation, to not set aside much broader unlicensed spectrum, to protect existing uses against any interference-these are policies designed to preserve the old against the new. They are just what we would expect from government regulation of spectrum; they are much less than we should demand after the experience of the Internet.

The government's role should be to induce investment where there is a great deal of social value to be created. This is precisely the opportunity with unlicensed spectrum.30

OPPONENTS OF THIS mixed strategy are of two sorts: those who think it is unnecessary-that the market will get us to the right answer without the experiments with open spectrum; and those who think it unwise-that open spectrum is a terrible way to allocate spectrum resources.

The first group thinks that the market has a sufficient incentive to find the optimal use of spectrum. As with any resource, these market mavens argue, privatization will give owners the strongest incentive to commit the resources that they hold to the highest and best use. Thus, if there is an innovative new way to use spectrum, then someone who owns the spectrum has the best incentive to find it and deploy it. Markets deal best with scarcity and choice about innovation. Hence we should be pushing to strengthen the market.

Peter Huber is a good example of this kind of optimist. Huber is a brilliant polymath who, while working full-time as a lawyer, has written some of the most important policy and academic work about government regulation in general and telecommunications policy in particular.31 In his book Law and Disorder in Cyberspace, Huber describes both the market model for allocating spectrum-where spectrum rights are auctioned off up front-and the commons model for allocating spectrum, promoted most strongly by an ally of Huber's, George Gilder. As I described in chapter 5, Gilder argues strongly that we should allocate spectrum as a commons. Auctions, Gilder argues, will simply entrench existing uses; free or common spectrum would create a strong incentive for new uses.

Huber does not reject Gilder's predications. He argues instead that we could get to Gilder's world of spectrum as a commons by first auctioning off all the spectrum and then allowing the market to "reassemble" the rights if that proves efficient.

Markets find ways of reassembling private pieces into public spaces when that is the most profitable thing to do. They may take more time than an omniscient central authority, but finding omniscient central authority takes even longer. For now, the thing to do is to get the spectrum out of government hands, however it can be done, and leave it to the market to re-create the public commons. It will, if the economics are there.32

This is an empirical claim that begs for some evidence. I doubt there is a single example of "private pieces"

"reassembling" into "public spaces." Certainly there are examples of small landowners selling their land to developers; and we might imagine developers selling their land to other developers. But I can't begin to imagine the process by which this buying and selling of rights eventually leads to the spectrum commons that Gilder has described. There are too many sirens of strategic behavior on the way to imagine the private property system working itself pure.

If Huber's model were correct, we should expect that at least in some places the construction of commons has been left solely to a market. But at least within our tradition, however distasteful this is to strong libertarians, the most important commons have been supported by state intervention. Not the intervention of nationalization-these are commons, not state property. Instead, the intervention of a legal system that protects certain resources as open and neutral. Roads are not built through the "reassembling of private pieces" into a national highway grid. They are built by self-conscious commons constructions. 33

Even if one were optimistic in the way Huber is, a more obvious question is this: Why start with the burdens of establishing a property system if there is good reason to believe that the better solution would be a commons? Why not start the other way around-with a commons that might, in certain circumstances, be privatized where needed?

This default of a commons gains support when one considers the kind of spectrum use that is increasingly preferred by researchers. To the extent research shows that a more efficient manner of organizing spectrum is not to narrow the bandwidth allocated while increasing the power of the transmitter, but instead to broaden the bandwidth used with much less power, the model of propertized spectrum makes much less sense.34

It is here that the open spectrum model confronts a different critic-one who argues that a spectrum commons would be a wildly inefficient method of allocating spectrum. And here, Thomas Hazlett is again the strongest voice. In a recent paper addressing this idea of the spectrum commons, Hazlett rightly ties the idea to the Internet. Writes Hazlett:

The spectrum commons idea is motivated by analogy to the Internet. Yet the architecture of the Internet seriously misallocates scarce bandwidth. Because data cannot easily be prioritized, or billed, within the existing Internet protocols, tragedy of the commons appears frequently. High value communications are jammed in congested arteries with massive volumes of data of only marginal significance. Classically, the brain surgeon cannot read the life-or-death CT-scan because the Internet backbone is clogged with junk e-mail.35

But the response to Hazlett's example is not to criticize the Internet. The response is to ask who is this "brain surgeon" reading a CT scan over the Internet? And how does her ability to use the Net determine whether the Net "seriously misallocates" resources? Hazlett offers no data to support the claim that the "tragedy of the commons appears frequently." In fact, capacity has consistently outstripped demand.36

More significant, Hazlett ignores the advantages to innovation that I have identified throughout this book. Let's assume that the Internet "mis-allocates" bandwidth relative to the model Hazlett has. Does Hazlett really believe that the very same innovation (or better) would have been realized had the Internet been architected to "properly allocate" bandwidth from the start?

For there is no conceptual reason why we couldn't have auctioned off the Internet's resources at the start of the Internet. We could easily have imagined protocols to charge and prioritize being implemented from the very start. If we had, would we have produced the same kind of innovation? Across the same range? Had the network been architected to give the network owners control, would we have produced the Internet that we did?

Clearly, in my view, the answer is no. But I'm just a lawyer; I haven't the skill to model this counterfactual. My point is simply the part that Hazlett has not yet accounted for. He must at least show us why the opportunity for innovation that the original Net created was not actually beneficial. And he must likewise show that it would not be beneficial in spectrum, either.

Hazlett has his biases; I have mine. He's enamored of perfect pricing and perfect control; I'm still surprised (lawyer that I am) by the extraordinary innovation that comes from imperfect pricing and leaky control. If we had to take a poll among neutrals-people biased neither as I am nor as Hazlett is-I suspect most would be skeptical about whether a control architecture in the Internet would have produced the same innovation that the commons architecture did. But whatever a poll would indicate, it is here that the debate should occur. Given the creativity and innovation that the original Internet produced, and given how different that innovation is relative to other computer networks and other telecommunications systems, my bet for spectrum would be that an architecture modeled on the Internet would not be so bad.

THE IDEAL MIX in the short term would be a regime that had both a commons and a property component, with the property component subject to an important caveat. There would be broad swaths of spectrum left in the commons; there would be broad swaths that would be sold as Hazlett proposes. But in light of the emerging technologies for sharing, even the spectrum sold as property would be subject to an important qualification: Other users would be free to "share" that spectrum if they followed a "listen first" protocol-the technology would listen to see whether a certain chunk of spectrum were being used at a particular time, and if it weren't, it would be free for the taking.

I recognize that idea is jarring-that "my property" would be free for the taking just because I was not using it. But do you recognize why the idea is jarring? The assumption that fuels the dissonance about property "free for the taking" is that the taken property is exhaustible. I may not be using my car at the moment, but that doesn't mean you should have the right to take it, since your use of my car will, to some degree, deplete the property I have. Cars are exhaustible resources.

Spectrum is not. When I use a bit of spectrum at a particular moment in time, that spectrum is just as good after I'm finished as it was before. My use in no way exhausts the resource. And more important, when spectrum is not used, its value as a resource is not saved. Unused spectrum, like an empty seat on an airplane, is a resource that is lost forever.

Thus, if we adjust our intuitions about what spectrum is -recognizing that even if under some protocols spectrum is rivalrous (in some sense, we both can't use the "same" spectrum at exactly the same time), it is inexhaustible-then a property rule that presumes an opportunity for sharing begins to make sense. The implicit "use it or lose it" requirement, while costly for some sorts of rights, is not costly where the social consequence of not using a particular resource is that that resource is lost forever.

Wouldn't there still be a conflict if I'm "sharing" your spectrum and you then decide you want to use it? In principle, but not in practice. For the "sharing" rule would require that the sharer use the spectrum for an extremely short period of time. Thus, the owner may be delayed, but the delay would not be significant.

This compromise simply recognizes an important limitation in current understanding of how spectrum might optimally be used.37 It may well be that the market model makes most sense, and that in the long run, a market for spectrum will govern all spectrum. But that doesn't mean we need to embrace the market fully now. For as Eli Noam has demonstrated, a market structure could be layered onto open spectrum without embracing the ex ante allocation that auctions envision.38 Like the Ethernet network described in chapter 5, at the time the system needs it, the system would make a request for a reservation. The only addition would be a system for charging for that token of reservation. Noam likens that system to the subway's method of charging for ridership.

The advantage of Noam's solution is that it keeps the cost of spectrum use down. As Noam writes, an auction is simply "a tax on the communications sector and its users."39 Given the size of the bids currently being offered for this resource, it will tend, as Noam argues, to encourage oligopoly. "An auction payment that must be paid in advance is a barrier to entry, unless capital markets are perfect, which they are not."40

We are far from the moment, however, when it would make sense to layer this market onto open spectrum. Just as the National Park Service began charging entrance fees late in its life, so too should we begin without entrance fees and layer them on, neutrally, as needed. This mode of regulating a large chunk of the resource of spectrum would inspire the widest range of spectrum use. And a strong commitment by the government to support open spectrum would convince venture capitalists to invest in this alternative use.

* * *

BUT WHILE theorists favoring the market fight theorists pushing the commons, our government is pursuing neither policy well. Instead, our government pursues policies that are precisely contrary to freeing the spectrum, either through the competition of a well-functioning market or through a commons. It gets pushed by politicians, so it pushes to sell off rights to spectrum without fully embracing the property model. The reason is not ideal spectrum policy. The reason is that it auctions off this form to best protect existing businesses. As Gilder puts it, this push to sell off spectrum is simply a "legal infrastructure and protectionist program for information smokestacks and gas guzzlers." 41 The effect won't be to inspire new ways of using spectrum; instead, the effect will be to entrench the old ways against new uses.

While auctions "seem preferable to the agency's previous policy of simply giving [spectrum] away to 'worthy' applicants," they will not inspire new uses, and they may tend, Gilder argues, "to produce a winner's curse."42 The expected revenue from these sales will exceed $50 billion; that revenue will put pressure on the incumbents to earn supracompetitive returns. The best way to earn supracompetitive returns is to continue the noncompetitive architecture in broadcasting that has earned them profits in the past. Selling spectrum will give the incumbents the means and the motive to make certain that the spectrum does not become (as we as a society should want) a commodity product-constrained by strong and broad competition.

GILDER'S ARGUMENT assumes a kind of irrationality within firms that economists are quick to attack. The money spent on spectrum is a "sunk cost." The rational business would ignore the sunk costs already spent and focus instead on the optimal way to get a return from the assets it has.

But Gilder's fear is not just a result of irrationality; the fear is that powerful actors can work to slow innovation that harms them. The long-run advantages of FM radio didn't stop AM broadcasters from working to kill FM.43 The technical arguments in FM's favor, powerful and unrefuted, were impotent in the face of existing broadcasting interests. And the same danger continues to exist about spectrum management policies. There are too many places for the devil to find details that will effectively kill important new technologies. 44

There is an opportunity here for a crucial layer of the communicative architecture to be opened and made free. Opening it would reduce the pressure on other channels of Internet access. Keeping it free would encourage a wide range of innovation around how that resource is used. But instead of encouraging the use of this resource, instead of expanding it broadly, we are quickly whittling away the opportunity this commons would create. Without justification beyond the knee-jerk bias of our day, we are swallowing the idea that control is better than freedom.

HERE AGAIN, an idea about property is doing all the work-but this time the idea is at its most attenuated. We don't yet have a full property regime for allocating and controlling spectrum. Yet we are still being driven to embrace this single view. We are racing to deny the opportunity for balance, pushed (as we always are) by those who have the least to gain from a world of balance. The possibility of a commons at the physical layer is ignored; even the chance to experiment with the commons is denied. Instead, policy makers on the Right and on the Left race to embrace a system of perfect control.

So strong is this idea of property, so unbalanced is our understanding of its tradition, that we embrace it fully, without limitation, even when it doesn't yet exist, and even when the asset being assigned a property right is not-like the wires of AT&T's cable or the creative genius behind Disney's Mickey Mouse-something anyone has created. We are racing to assign property rights in the air, because we can't imagine that balance could do better.13. What's Happening Here?

IN THE early 1970s, RCA was experimenting with a new technology for distributing film on magnetic tape-what we would come to call video. Researchers were keen not only to find a technology that could reproduce film with high fidelity; they were also keen to find a way to control the use of the technology. Their aim was a technology that could control the use of film distributed on video, so that the owner of the film might maximize its return from the distribution.

The technology eventually chosen was relatively simple. A video would play once, and when finished, the film would lock into place. If a renter of the video wanted to play the video again, he or she would have to return the video to the video store and have the tape unlocked. In this way, the owner of the film could assure that it was being compensated for every use of the copyrighted material.

RCA presented this technology to the Disney Corporation in the early 1970s. In a room with just five of the senior executives from Disney, a young RCA executive, Pat Feely, demonstrated RCA's device. The executives were horrified. They would "never," Feely reports their saying, permit their content to be distributed in this form. For the content, however clever the self-locking tape player was, was still insufficiently controlled. "How could they know," a Disney executive asked Feely, "how many people are going to be sitting there watching" a film? "What's to stop someone else coming in and watching for free?" 1

* * *

AS THE COST OF digital filmmaking falls, educators are experimenting with filmmaking as a way of learning. There's something different about expression in film. As John Seely Brown, chief scientist at Xerox PARC, describes it:

[As] you move into the role of film you get something that most people overlook. [I]n text we can always add the parenthetical comment: Paren, dot, dot, dot, comma, if, sorta And so you string qualifier after qualifier. [But w]hen you do a sketch of an idea there are no qualifiers

[Q]ualifiers don't hold. You've got to decide what the kernel idea is and then you sketch that idea.2

This means learning through filmmaking is different. And in a group of California schools, a number of filmmakers have been experimenting with giving students the tools to make film as a kind of writing. These experiments let the students draw upon a wide range of existing film, which they recombine in new and creative ways and then supplement with new scenes that the students shoot on their own.

This "changes the thinking process," Brown describes. It produces a "completely different experience from writing an essay." And though these educators are just at the start of this experiment, they have already seen the changes it can make in the students it touches. This is a new kind of thinking, enabled by this emerging digital technology.

The product of this creativity, however, can't be displayed publicly. The films these students produce are housed on a private, password-protected network. Lawyers for the university supporting this work have advised the educators that putting the content on the Web would subject the teachers to liability. Thus this creativity is bottled up in a private network, unavailable for other students to view or learn from. The lesson the students learn is that sharing this creativity is not to be allowed.

But why? John Seely Brown asks.

To me, this is where education is going. [This is how] students who grow up digital think and want to learn. [But] we are building a [legal] system that completely suppresses the natural tendencies of today's digital students.

The law says that their creation can't be shared and the technologies that we've seen in chapter 11 will take away the very ability to draw upon film content and mix it with something different.

Here we have an educational crisis in our country and we have this incredible platform where these students are discovering on their own. [But this platform] is progressively going to be closed down instead of opened up.

"Technology makes it possible," Brown concluded, "but the law is going to come in and knock it out."

THE STORY OF this part is easy to summarize. In one sense, each of the changes I describe in these three chapters is very different. Modifications to the broadband environment are different from modifications to patent law; changes in spectrum rules don't quite track the motives of Hollywood.

Yet in another sense, each of these examples is motivated by a common idea or common attitude. In each, an attitude of control, perfected by an idea about property, is in tension with a system that protects a commons. And in each, the idea about property prevails. We race to empower networks to discriminate (after all, they are "their computers"); we race to empower owners of copyright to control new modes of distribution; we race to develop property in the air. Our single, overriding view of the world is that only property matters; our systematic blindness is to the lesson of our tradition-that property flourishes best in an environment of freedom, both freedom from state control and freedom from private control. That a commons can have value greater than the same assets would if enclosed.

The consequence in each of these contexts is a change in the environment within which innovation occurs. That change has been, or threatens to be, a shift from a world where the commons dominates to a world where control has been reclaimed. The shift is away from the open resources that defined the early Internet to a world where a smaller number get to control how resources in this space are deployed.

This change will have consequences. It will entrench the old against the new. It will centralize and commercialize the kinds of creativity that the Net permits. And it will stifle creativity that is outside the picture of the world that those with control prefer.

If there were a reason why this change was necessary-if there were a reason to believe the Net could not advance without it or would be harmed without it-then I would support this shift, however reluctantly.

But no good reason has been given. We are marching backward, undoing the architecture-both the legal and the technical-of the original Net without anyone demonstrating why this change is needed. We are moving resources from the commons into a system of control without an argument about why control will help or why the commons will fail. We are jumping in the name of an ideology without any consideration of the facts of these past ten years.

So why are we making these changes?

In part, there is a dark story to be told. Change threatens existing interests. Channeling change is often the best strategy for preserving that threatened power. Those whose position is threatened by the change the Internet represents have a strong interest in trying to channel that change.

So, too, do they have the means. It is an iron law of politics that the organized beat the unorganized and that the vested have interests that get organized over the unknown. In Washington, decisions are made by representatives conferring with people whose interests are affected by changes in the Net. But who represents the innovations not yet made? Who demands that the platform be kept open for them?

No one. Some of those interests can't afford the negotiation with existing interests; some interests don't even yet exist.

The result is that the pressure in the existing system is biased in favor of the old. Policies get made to favor the old; the interest of the new simply has no voice.

But the larger story here is not about dark forces. It is about a blindness that affects our political culture generally. We have been so captured by the ideals of property and control that we don't even see the benefits from resources not perfectly controlled. Resistance to property is read as an endorsement of the state. The challenge to extreme propertization is read as the endorsement of nationalization.

In the context of intellectual property, the general problem is magnified by another blindness, the error induced by thinking of intellectual property as property. By simplifying the nature of the rights that IP law protects, by speaking of it as property, just like the ordinary property of cars and homes, our thinking is guided in a very particular way. When it is viewed as property, we see endless arguments for strengthening IP and few for resisting that increase.

This is not conspiracy. It is a cultural blindness. We have forgotten what the Framers of the American Constitution knew about the nature of IP, and hence we have lost the balance our Framers had in protecting IP.14. Alt.Commons

THERE ARE changes that could be made. This march backward is neither necessary nor complete. We still have the time to point policy in a different direction. The question is whether we have the will. Are we willing to set principles that will guide the next stage of the Internet's evolution, or will we allow those who have interests inconsistent with those principles to exercise their control?

My aim in this chapter is to outline some of these changes. My list is neither complete nor certain. But it should be a start of a conversation about returning the Net to the conditions that let innovation flourish.

I divide these proposals into the frame of Benkler's three layers. Changes in some might make changes elsewhere unnecessary, but changes in all of them would make the situation much better.

THE PHYSICAL LAYER

THE PHYSICAL layer would seem the least likely for reform, since this layer lives within real space only, and as I've argued, the constraints of real space are, well, real.

But there are a number of places where a change might do some good. And the most important follows from the argument considered in chapter 12.

Free Spectrum

THE PHYSICAL LAYER includes technology upon which, or in which, or over which the network lives. It includes the computers that connect to the Net, the wires they connect to, the routers that feed those wires, and the spectrum that substitutes for the wires.

As I have described, most of these elements are owned-and with one exception, I think properly so. Computers are private property, whether the government's (in the NSA or libraries) or individuals'. The wires, whether copper or fiber, linking these computers to routers to other computers are privately owned. Massive investment laid them; even greater investment has been needed to bring them up to date.

These private investments deserve the reward of private property. With one qualification that I will offer in the next section, the owners of this property should be free to use it as they wish. No one should have the right to sit at my machine. Access to my machine, and the wires of AT&T, should not be free. If it were free, then those who buy the machines and those who lay the wires would lose lots of the reason to buy the machines and lay the wires. If access were free, the incentives to build the Net out would largely be lost.

But the same virtues from control can't be said of spectrum. Or at least they can't be asserted with the same confidence. No one builds spectrum; no investment from AT&T has made it possible. No entitlement justifies special control over spectrum, and the owners of Ryder Truck Rental and Leasing should not have a monopoly right to control the highways just because they purchased expensive equipment to use it. Thus, any monopoly control over spectrum should be allowed only if it can be shown that monopoly control is needed.

My argument is not that exclusive control is not needed. It may well be that Hazlett is right. The congestion in the airwaves may push us to build out a property system. Spectrum auctions-either in advance or in real time-may turn out to be needed to use the spectrum in the best possible way.

But we don't know that yet, and we certainly don't know enough yet to know how spectrum will be used. Thus, rather than architecting the space exclusively for control, we should begin, as much as possible, as we began with the Internet: by building a regime that by design leaves a significant part of these resources in the commons. And once we see how that commons gets used, we can then change how that commons gets controlled.

This argues for a dual strategy. We should be setting aside broad swaths of spectrum as a commons, intermixed with spectrum as property. The market should be assured that both models will survive for the next chunk of time. And regulators should assure that the devices using the commons are smart enough not to spoil the space.1

What would this idea look like in detail? First, spectrum is not all the same. The spectrum the AM radio uses can't do what the spectrum that radio astronomy uses does. Thus, we can't simply carve off a single chunk of spectrum for open, unlicensed use. We must instead set off significant bands at each spectrum level, to assure that innovation for different uses of spectrum would be possible.

Second, we should force the government to give up its obscenely wasteful hoarding of spectrum. When radios were stupid and clear channels were necessary, this hoarding made sense. But the government is not using this spectrum with stupid radios. The most advanced work being done in "software-defined radios"-radios that would, like chameleons, change their character to fit the protocol in the context that works best-is being done by the same group that gave us the Internet-the Defense Advanced Research Projects Agency (DARPA). DARPA is researching software-defined radios that share spectrum smartly. It is, in other words, building the Internet in the air.2

I'm a great fan of DARPA's work, but I don't believe the government should have a monopoly on innovation around networks. A sensible policy would divide spectrum into many swaths, some controlled, some free. Users of the spectrum-meaning the machines that use the spectrum-would then decide which chunks work best for them. Indeed, with software-defined radios, they could make that decision many times over. Out on the highway, a cell phone could connect to a network in the best of several different ways. Some of those ways might use the free space of unlicensed spectrum. Some might log on to controlled spectrum property. But the decision would be a function of which choice made most sense at the moment.

Third, there is no doubt that a sensible spectrum policy for the future will require changes in the way spectrum is used today. Steel mills didn't get a permanent waiver from pollution laws just because their technology was old.3 And pollution is precisely the way we should think about old uses of spectrum: large and stupid towers billow overly powerful broadcasts into the ether, making it impossible for smaller, quieter, more efficient uses of spectrum to flourish. Why should these smokestack technologies get protection, when the steel mills did not? Why not force them to improve their technology-to reduce the pollution they spew forth into the ether-so that others could innovate in yet unimagined ways?

These changes may well force familiar uses of the spectrum off of spectrum completely. Broadcast television, for example, is an extraordinary spectrum guzzler; in most contexts it would be best moved from the air to wires. This is just another instance of what has been called "the Negroponte switch"-that everything that used wireless spectrum would move to wires, and everything using wires would move to the air. Such movement will impose costs on some-just as rebuilding smokestacks imposed costs on Pittsburgh. But these costs in the short run would be easily outweighed by benefits in the long run. By establishing a dual system of free and controlled spectrum, each with an equivalent opportunity to demonstrate its own success, we could ensure future spectrum innovation.

Existing spectrum users will resist these changes. It is unfair, they will argue, to change the rules now. They paid good money, they will insist, for the monopoly control they have purchased. It is wrong for the government to go back on promises it made.

But this argument is absurd. First, technically, this change would not breach any promise. From the very beginning of spectrum allocation, the recipients have been required to affirm that they asserted no property interest in the spectrum rights they got.4 From the very beginning, the express understanding has been that the licenses granted were limited in time and that the risk that spectrum would be allocated differently was a risk these businesses should take into account. For corporations now to claim that, these statements notwithstanding, they expected to have their monopolies in perpetuity is just absurd.

That this argument is being made, however, does signal the importance of the government's acting soon. Rather than racing to auction off more spectrum in the way it has to date, and thereby increasing the sense of entitlement that these spectrum hoarders claim, the government should be marking off much wider areas of spectrum that will be dedicated, for a significant period, to the commons. The government could free existing holders of spectrum to sell their leaseholds to others-thereby facilitating the market that Hazlett wants. But before it puts any more of this resource into a system of control, it should assure that the commons in spectrum is properly secured.

Finally, the FCC should free up greater access to existing unlicensed bands, including the Amateur Radio Service bands. Existing users here too should not have the power to veto something new simply because it is a late-comer.5

If there were a rich and developed physical layer of free spectrum access-permitting many competitors to offer Internet access using this final link of free spectrum-then the need for the government to worry about other modes of access to the Net would be lessened. If a rich and powerful channel is kept open and kept in the commons, then what owners of other channels, not left in the commons, do is less of a concern. The key is balance, but our practice now is at the extreme.

Free Highways

OPENING CHANNELS of spectrum would be one important expansion of the commons. It isn't, however, the only possible change. There are other steps the government might take to open access at the physical layer-not by becoming network owners, but by clearing the way for others to develop and run networks.

The model here is the highway system. While we have grown skeptical about the state's role in many aspects of our life, there remains a strong belief that the state has a place in the provision of basic services like roads. The government has funded the construction of highways and local roads; these highways are then used either "for free" or with the payment of a toll. In either case, the highway functions as a commons.

In some cases there has been a move to privatize highways. Los Angeles, for example, has experimented with private roads, promising lower congestion in exchange. But these experiments exist alongside open, public roads. The two go together, and where only one road is available, it is a public one.

This is a sensible use of public resources. It builds a resource for exchange that all have access to. It balances resources controlled privately; it sets a baseline against which private resources compete.

In my view, the same attitude should guide government policy with respect to the physical infrastructure supporting the Net-in particular, the fiber infrastructure and rights-of-way to deploy alternative access. Just as the state has spent resources on building highways, so too might it need to spend resources on building out the information superhighway.6

Many cities are already picking up on the idea. Chicago, for example, has started funding the deployment of "dark fiber"-meaning simply fiber optic cable that is not immediately connected to any particular service.7 The city does not get into the business of wiring the links or running the routers; the city's job is simply to fund the laying of the cable and let competitors connect to the fiber to sell Internet access.8

The advantage in this way of building the basic physical infrastructure upon which the network is built is that there is no need to strike a bargain with the monopoly devil to finance the deployment of this fiber. But with cable, the government decided to fund the build-out with the grant of monopolies to the cable owners. That strategy proved costly, as the cable owners became invested with eagerness to defend their monopoly. The aim of a monopolist is always to protect his monopoly, and if he's rational, he's willing to spend the net present value of his monopoly to defend it.

But if the state were funding the building of at least some of this basic infrastructure, and if it were kept far from the content that gets played across this infrastructure, then there would be no actor in this system with an incentive to discriminate. Government would lay the pipes; private industry would use the pipes to serve access to local customers.

This point would not stop with dark fiber. Right now the most expensive part of the connection is the "last mile" to the consumer's home. That's expensive in part because the technology for converting from glass to electronics is itself expensive and in part because of the cost of trenches. If every house had to be rewired (or fibered), the cost of running fast Internet to every house would be very high.

One alternative would be to run the fiber to a wireless broadcasting station that then would beam Internet service to many users in the neighborhood. (This would be the inverse of the system that gave us cable TV.) But to do this, providers would have to have access to telephone poles or other places where broadcasting stations might be built.

Here again the state could play a role-either by granting access to state property or by purchasing access to private property. This access would amount to permission for a private company to build a broadcasting service; it would not be an invitation for the state to get into the ISP business. And because the capacity of these fibers is so great, the potential for a bottleneck would be much less.

In both cases, the role the state plays is to assure that bottlenecks not become opportunities for exercising market power. The state builds a competitive environment where there is incentive to behave as a competitor. This commitment will require resources; it would be less pressing if the FCC freed spectrum generally. But it is an appropriately translated role for the state when superhighways carry bits rather than trucks.

THE CODE LAYER

THE CODE LAYER is the heart of the Internet; its particular architecture is what was special. It would make no sense, however, to say that we should fix ourselves to the particular architecture of the network at any one time. The point is not a blind originalism; the point instead is to preserve the values expressed by that original architecture. Some ways to do that are considered here.

Neutral Platforms

THE CRITICAL LAYER to protect if we are to protect innovation on the Net is the code layer-the space where code decides how content and applications flow, and where code could control how innovation develops. It is at this layer that the Internet originally embraced the principle of end-to-end. That principle assured that control was bottom up; that what would succeed would succeed because users demanded it; and that what users demanded would be free to flow to them.

It is the compromise of this principle that threatens the greatest harm to innovation. And the pressure to compromise comes from those who would use their power over architecture to protect a legacy monopoly. The danger exists when control over the platform can translate into the power to protect against new innovation.

We've seen this power in two different contexts. The claims the government made against Microsoft were the clearest example of this danger: Microsoft, the government argued, used its power over the Windows platform to protect the Windows platform from innovation that would threaten it. Likewise in the context of broadband cable: the danger was that by restricting the number of ISPs, the network owner could exercise control over the ISPs to assure that its content, or business model, would not be threatened by certain uses of the Net.

This power, whether exercised over the operating system or over the network within which Internet traffic flows, threatens innovation. The risk of a strategic response by the platform owner reduces the expected benefit from innovating in certain status quo-threatening ways. Thus, if innovation is our goal, our policy should be to minimize the threat of this strategic behavior. A number of strategies to that end are obvious.

First, the government should encourage the development of open code. Open code, as I've argued, risks none of the dangers of strategic behavior that closed code, or controlled networks, do. If open code is used strategically, then the resources to counter that strategic action are always available. Innovators can rely upon the promise of open code in their innovations. They need not worry that what they develop will be swallowed by the platform they develop for.

This encouragement should not be coercive. There's no reason to ban or punish proprietary providers. People should be free to develop code however they wish. But a government has its own interests, and closing its resources to others is not one of them. If the federal government develops a system to handle welfare claims, what reason does it have for hiding the code for that system from the states? Why not let the states take that code and build upon it? And if the states, then so, too, with the universities. In each case, the aim should be to expand the reach of these powerful and valuable resources, not to contract and hoard them when no value to the hoarding exists.

Likewise with the government's choice of operating systems. What reason does the government have for supporting closed code, when open code is as powerful and the externalities from using open code would benefit others? If the PCs that the government owned ran something other than Windows, then the market for these alternative platforms would be wildly expanded. And if the market for alternatives were strong, then the benefits from building for these alternatives would be strong as well.

Again, such a strategy does not flow from animus against proprietary providers. Any environment is richer when built upon a diversity of life. As the various viruses that have plagued the Internet have shown, we are increasingly vulnerable the more concentrated the strain. Opening the code that constitutes cyberspace in as many contexts as possible will enable a flourishing of innovation that need not depend upon a single platform. Microsoft may not like this, but a policy favoring this diversity is not a policy against Microsoft.9

Second, the government should continue to ensure that no major player in the Internet space is able to architect the Internet space to empower its own strategic behavior. If the cable companies want to build a cable television system where they have complete control over the content that flows across their cable, more power to them. Control has been at the core of the cable system from the start. But if cable wants to carry TCP/IP, then the values of the Internet should trump the control of cable. Any major network that wants to piggyback on the Internet's success should piggyback with the values of the Internet kept in mind.

How best can the government carry this role into effect? Historically, the most successful strategy has been banishment. When the government, for example, banished the telephone company from the game of providing computer services, then the telephone company had little interest in playing games among different providers of computer service. If it were simply in the business of selling access to pipe, then those buying that access would have an equal playing field for competing in providing network services.

Since the 1996 Telecommunications Act, the strategy has been different. The government has required the telephone company to compete against ISPs. The telephone company, for example, is permitted to sell you DSL, but it must permit competitor DSL providers to offer you DSL service as well. This is the "open access" requirement. Competition is assured by regulating the Bell companies and by requiring them to unbundle local access services. And this unbundling in turn assures that the telephone company can't play any games.

In this way, the regulation assures that competitive pressures will exist for every mode of broadband access, so that broadband providers won't have the incentive to exercise control in a strategic way. And until enough competitors are on the competitive field, this may well be the best strategy for keeping access to the Internet open.10

Alternatively, we might imagine a simpler regulatory strategy. If the concern at stake is that network providers will leverage control over the network into some control over content-if the concern is that they will have an incentive to compromise the principle of end-to-end-then rather than requiring unbundling of services, the government could adopt a more direct regulatory strategy: if you provide Internet services, then you must provide them consistent with the principle of end-to-end.11

It is hard in the abstract to game that alternative. DSL would compete with cable, fiber, and wireless service in providing Internet traffic. The latter would not be required to facilitate competition on their own facilities, but the competition among facilities might be enough in most places to ensure that prices are kept low. As long as discrimination is not enabled (the consequence of preserving end-to-end), the essential elements of a commons would be preserved as well.

My bias is in favor of the least invasive regulatory response, but it is also a bias in favor of a guarantee of a regulatory response if regulation is needed. The trends have not been toward fast networks that promise uncontrolled access. The trends have been toward control.

One useful point of comparison here is our neighbor to the north, Canada. The Canadians have required open access for broadband providers, and a recent Report of the National Broadband Task Force has endorsed a "bill of rights" for broadband users that assures continued consumer choice for any build-out of a network employing government funds. This choice of policy has apparently not harmed Canadian access. According to a recent OECD report, broadband connections are twice as common in Canada per capita as in the United States.12

Finally, and at a minimum, regulators should begin to evaluate changes to the network in terms of the neutrality of end-to-end. We should begin to think about the trade-offs between control and neutrality explicitly-not because every trade-off would be a sin, but because otherwise, the loss will be invisible.

THE CONTENT LAYER

THE CHANGES at the physical and code layers are significant. They will require a commitment that I am skeptical our politicians are capable of giving. But they are nothing compared with the changes that are required at the content layer. For it is here that we have moved the furthest from sensible policy, and here where there is the strongest political power to resist.

The core idea that we, as a culture, must recapture is that control over content is not to be perfect. Ideas and expression must to some degree be free. That was the aim of copyright law initially-the balance between control and freedom. It was, "even twenty years ago an article of faith [that it] offer only circumscribed, porous protection."13 But these balanced laws now have an ally that threatens to destroy the balance: code.

Technology, tied to law, now promises almost perfect control over content and its distribution. And it is this perfect control that threatens to undermine the potential for innovation that the Internet promises.

To resist this threat, we need specific changes to reestablish a balance between control and creativity. Our aim should be a system of sufficient control to give artists enough incentive to produce, while leaving free as much as we can for others to build upon and create.

In setting this balance, there are a few ideas to keep in mind. First, we live in a world with "free" content, and this freedom is not an imperfection. We listen to the radio without paying for the songs we hear; we hear friends humming tunes that they have not licensed. We refer to plots in movies to tell jokes without the permission of the director. We read books to our children borrowed from a library without any payment for performance rights to the original copyright holder. The fact that content at any particular time is free tells us nothing about whether using that content is "theft." Similarly, an argument for increasing control by content owners needs more than "they didn't pay for this use" to back it up.

Second, and related, the reason perfect control is not our aim is that creation is always the building upon something else. There is no art that doesn't reuse. And there will be less art if every reuse is taxed by the earlier appropriator. Monopoly controls have been the exception in free society; they have been the rule in closed societies.

Finally, while control is needed, and perfectly justifiable, our bias should be clear up front: Monopolies are not justified by theory; they should be permitted only when justified by facts. If there is no solid basis for extending a certain monopoly protection, then we should not extend that protection. This does not mean that every copyright must prove its value up front. That would be a far too cumbersome system of control. But it does mean that every system or category of copyright or patent should prove its value up front. Before the monopoly should be permitted, there should be reason to believe it will do some good- for society, and not just for monopoly holders.

With these ideals in mind, here are some first steps to freeing culture:

Copyright

THE TREND IN copyright law has been to increase copyright's scope and duration, while making the right easier to secure and keep. While the original copyright statutes put a great burden on copyright owners to register their work, make deposits of their work to the government, and renew the copyright after an initial term, now copyright affixes automatically, it extends for the life of the author plus seventy years without any effort by the copyright owner, and the copyright owner need make no effort at all to continue to enjoy this government-granted monopoly.

This shift is bizarre. We have been pushed to this "no effort" monopoly handout by the view that "technical" requirements should not interfere with the right of an author to his or her copyright. That argument sounds good until one considers the other side of the bargain-the public. Copyright owners should not be denied legitimate copyright protection for technicalities, no doubt; but assuring that the reach of state-backed monopolies over speech is not broader than necessary is not a "technicality." If welfare recipients can be denied their benefits because they fail to complete a benefits form properly, then I can't see the unfairness in requiring those who demand state support to defend their monopoly similarly by filling out a registration form.

I would go even further.14

FIVE-YEAR RENEWABLE TERMS

AUTHORS AND CREATORS deserve to receive the benefits of their creation. But when those benefits stop, what they create should fall into the public domain. It does not do so now. Every creative act reduced to a tangible medium is protected for upward of 150 years, whether or not the protection benefits the author. This work thus falls into a copyright black hole, unfree for over a century.

The solution to this black hole of copyright is to force those who benefit from copyright to take steps to protect their state-backed benefit. And in the age of the Internet, those steps could be extremely simple.

Work that an author "publishes" should be protected for a term of five years once registered, and that registration can be renewed fifteen times. If the registration is not renewed, then the work falls into the public domain.

Registration need not be difficult. The U.S. Copyright Office could run a simple Web site where authors could register their work. That Web site could be funded by charges for copyright renewals. When an author wants to renew the copyright, the system could charge the author a renewal fee. That fee might increase over time or depend upon the nature of the work.

This registration site could also take deposits of certain kinds of work. For archive purposes, the site could collect digital copies of all the works copyrighted. And for certain kinds of work-software in particular-those deposits would be required for the protection to be secured. Given how easy the Net has made such transfers, these costs would be relatively small.

"Unpublished works" would be different. If I write an e-mail and send it to a group of my friends, that creativity should be treated differently from the creativity of a published book or recorded song. The e-mail should be protected for privacy reasons, the song and book protected as a quid pro quo for a government-backed monopoly. Thus, for private, unpublished correspondence, I think the current protection is perfectly sensible: the life of the author plus seventy years, automatically created, with no registration or renewal requirements.

One of the strongest reasons that the copyright industry has raised for the elimination of this renewal requirement is the injustice that comes from a family's or author's losing copyright protection merely because of a technicality. If "technicality" means something like the registration was lost in the mail or was delivered two hours late, then the complaint is a good one. There is no reason to punish authors for slips. But the remedy for an overly strict system is a more relaxed system, not no system at all. If a registration is lost or a deadline missed by a short period of time, the U.S. Copyright Office should have the power to forgive.

A change in the copyright term would have no effect on the incentives for authors to produce work today. There is no author who decides whether or not to write a book depending upon whether he or his estate will receive money three-quarters of a century from now. The same with a film producer: Hollywood studios forecast revenues a few years into the future, not ninety-five. The effect on expected income from this change would therefore be tiny.

But the benefit for creativity from more works falling into the commons would be large. If a copyright isn't worth it to an author to renew for a modest fee, then it isn't worth it to society to support-through an array of criminal and civil statutes-the monopoly protected. But the same work that the original author might not value could well be used by other creators in society.

Even more significant, this repository of data about what work is protected would lower the costs of licensing copyrighted works significantly. Because the database would be relatively fresh-with a requirement that contact information be kept current-creators who want to license other creators' work would have a simple tool to do so.

SOFTWARE COPYRIGHT

SOFTWARE IS A special case. The current protection for software is the life of an author plus seventy years or, if a corporation, ninety-five years. This is a parody of the Constitution's requirement that copyright be for "limited times." When Apple Macintosh's operating system falls into the public domain, there will be no machine that could possibly run it. The term of copyright for software is effectively unlimited.

Worse, the copyright system protects software without getting any new knowledge in return. When the system protects Hemingway, we at least get to see how Hemingway writes. We get to learn about his style and the tricks he uses to make his work succeed. We can see this because it is the nature of creative writing that the writing is public. There is no such thing as language that doesn't simultaneously transmit its words.

Software is different. As I've described, software is compiled; the compiled code is essentially unreadable; but to copyright software, the author need not reveal the source code. Thus, while an English Department gets to analyze Virginia Woolf's novels to train writers in better writing, the Computer Science Department doesn't get to examine Microsoft's operating system to train its students in better coding.

The harm from this system of protecting creativity is greater than this loss to computer science education. While the creative works from the sixteenth century can still be accessed and used by others, the data used by software programs from the 1990s are already inaccessible. Once a company producing a certain product goes out of business, there is no simple way to uncover how its products encoded data. The data are thus lost, and the software is unusable. Knowledge has been destroyed.

The reason copyright law doesn't require the production of source code is that it is believed that that would make the software unprotectable. The open code movement might throw that view into doubt, but even if one believes it, the remedy (no source code) is worse than the harm. There are plenty of ways for software to be protected without the protection of law. Copy protection systems, for example, give the copyright holder plenty of control over how and when the software is copied.

If society is to give software producers more protection than they otherwise would get through technology, then we should get something in return. And one thing we could get would be access to the source code after the copyright expires. Thus, I would protect software for a term of five years, renewable once. But that protection would be granted only if the author submitted a copy of the source code to be held in escrow while the work was protected. Once the copyright expired, that escrowed copy would be publicly available from the U.S. Copyright Office server.15

PROTECTING INNOVATION

THE SINGLE MOST striking feature of copyright law is its ability to give owners the power to control innovation in the context of the Net. The power to issue injunctions against technologies for distributing content is an extraordinary power for controlling what new technologies can be created.

This is the power that Hollywood has used to control the development of new distribution technologies. Without any showing of harm, the industry has been able to exercise control over new modes of distribution.

No one industry should have the power to veto the Net's development. No single set of interests should be able to decide what innovations are best. This is especially true when no showing of harm is necessary for this veto to be imposed. And this is precisely the reality with respect to copyright and the Net.

Congress should limit this reactive character of copyright law. While in the ordinary case the copyright holder should not have to prove harm before enforcing a copyright, in a context of significant technological change, a defendant should at least have the opportunity to show that the copyright holder will suffer no harm.16

PROTECTING MUSIC

THE NET HAS created a world where content is free. Napster is the most salient example of this world, but it is not the only one. At any time a user can select the channel of music he or she wants. A song from your childhood? Search on the lyrics and find a recording. Within seconds you can hear any music you want.

This freedom the recording industry calls theft. But they don't call it theft when I hear an old favorite of mine on the radio. They don't call it theft when they are recording takeoffs of prior recorded music. And they don't call it theft when they make a new version of "Jingle Bells." They don't, in other words, call it theft when they are using music for free in ways that have been defined by the copyright system as fair and appropriate uses.

The issue we must confront is whether this free distribution should continue to be free. And the solution to that question is to keep an important distinction in mind: As we've seen, there is a distinction between music being "free" and music being available at zero cost. Artists should be paid, but it doesn't follow that selling music like chewing gum is the only possible way.17

Here, too, a bit of history helps. As I have described, there have been many contexts where Congress had to balance the rights of free access against the rights of control. When the courts said piano rolls were not "copies" of sheet music, Congress balanced the rights of composers against the rights to mechanically reproduce what was composed. It balanced these rights through a compulsory license that enabled payment to artists while assuring free access to the work produced. The same is true in the context of cable TV. As we saw in chapter 7, the Supreme Court twice said that cable TV providers had a right, under existing law, to free TV. Congress finally changed those rights, but again, in a balanced and sensible way. Cable providers got access to television broadcasts, but broadcasters and copyright holders had a right to compensation for that access. This compensation again was set by a compulsory licensing term. Congress protected the author, but not through a property right.

The same solution is possible in the context of music on the Net. 18 But here, rather than balance, the rhetoric is about "theft" and "crime." But was it "theft" when cable TV took television broadcasts?

Congress should empower file sharing by recognizing a similar system of compulsory licenses. These fees should not be set by an industry intent on killing this new mode of distribution. They should be set, as they have always been set, by a policy maker keen on striking a balance. If only such a policy maker were somewhere to be found.

REBUILDING THE CREATIVE COMMONS

THESE CHANGES WOULD affect works produced in the future. They would have no effect on works already produced and protected by these extensive terms. Nor could these changes affect works already produced-the Constitution limits Congress's power to take away property already granted, as well it should.

But there are other ways that Congress might act to create an incentive to build out the creative commons. One way would be to create incentives for holders of copyright to donate their holdings into a public conservancy. I've worked with others to build such a conservancy, but ours is not the only possible one. If Congress gave tax benefits to donors of IP to parallel the tax benefits given to donors of art, then there would be a much greater incentive to donate works to the general weal.19

One context in particular where incentives could do some good is that of orphaned software. Companies often decide that the costs of developing or maintaining software are higher than the benefits. They therefore orphan the software, by neither selling it nor supporting it. They have little reason, however, to make the source code for that software available to others. The code therefore simply disappears, and the products become useless.

If Congress created an incentive for these companies to donate their code to a conservancy, then others could build on the earlier work to produce updated or altered versions. This in turn could improve the software available by avoiding the loss of knowledge that was built into the original code. Orphans could be adopted by others who saw their special benefit.

There are other steps that Congress could take as well. One problem that often plagues creators is the claim that a work is copyrighted when it is not. It is a common practice for publishers, for example, to claim copyright when under the law they plainly do not have such a right. Sheet music publishers, for example, will often put copyright notices on public domain works.

This practice is in violation of existing copyright law. It is a crime to say something is copyrighted when in fact it is not. But the only punishment that the law provides for this crime is an action brought by a U.S. attorney. Not surprisingly, U.S. attorneys have better things to do; no one has ever been prosecuted for violations of this.20

This is a perfect claim for private attorneys general. Professor David Lange suggests that "claims so extravagant in relation to the reality from which they spring ought to be made the subject of a serious counterclaim for punitive damages rooted in some sort of tort to be termed 'unconscionable overreaching.' " 21 Congress could authorize private citizens to bring suits against false copyright claims. If successful, the plaintiffs in these suits should earn a bounty, plus their costs. And if publicized, such suits should change the behavior of publishers.

LIMITS ON CODE

AS I'VE DESCRIBED, the interests that copyright law protects can be protected by technology as well as law. As the DVD case discussed in chapter 11 makes clear, this protection by technology can often reach far beyond the protection of law. Copyright protection systems can limit fair use or extend the term of protection beyond the copyright term. If there is a reason for the balance in copyright law, then there is a reason to be concerned about the imbalance created by this code.

How one thinks about the code on its own is a hard legal question. We don't ordinarily think that there is an affirmative wrong in these private systems of protection. But where the law backs up this code, then there is clear reason to be concerned. Then the protection that the copyright gets is not just the private protection of code; it is also the protection of law. So here, in my view, there is a clear reason to limit the protections of law. Congress should expressly require that any law protecting copyright protection systems not protect those systems beyond the reach of copyright law itself. Only those code protection systems, in other words, that preserve adequate room for fair use would get the protection of Congress's law. (This is a version of what has been called the "Cohen Theorem," named after law professor Julie Cohen: that one has a right to hack copyright protection systems in order to secure fair use.)

LIMITS ON CONTRACT

THE SAME POINT can be made about contract. Often a copyrighted work is sold or licensed subject to a set of terms imposed in a license. Sometimes the terms imposed by the license are inconsistent with the balance that copyright law aims for. If the balance in copyright law is important, then it should not be undermined by a different kind of law-contract law. While not every license is in conflict with copyright law, many licenses are in conflict with the limited protection copyright law is to give.

State laws in particular that give copyright holders greater power than the balance copyright law was to set should be resisted. Among these, the most troubling is a uniform law making its way through the states called the "Uni-form Computer Information Transactions Act"-or UCITA.

UCITA is designed to facilitate transactions in computer information. Its aim is to make on-line contracting easier. But as many have argued (quite convincingly, in my view), the balance that UCITA strikes between the seller and the consumer is not adequate to protect the balance the law intends between copyright and the commons.22

The opposition to UCITA has been strong; the need to adopt a uniform code right now has not been demonstrated. We have very little understanding of what on-line contracts could be. Yet UCITA is marching across the states with the aim to settle a range of issues up front.

This is a mistake. The premise of state uniform laws is that they are to reflect and codify mature and settled understandings of the law. They are not to be leaders; they are simply to clarify and to make uniform where uniformity matters. In this case, the law has been used to expand the rights of one side of a deal relative to another. The process has been captured. The proper response to this capture is to reject the proposal now and wait to see what's needed later.

LIMIT COMMERCIAL EXPLOITATION

PROFESSOR JESSICA LITMAN has taken these recommendations one step further. As she rightly observes, following the work of Professor L. Ray Patterson,23 copyright was originally simply a restriction on commercial entities. It regulated "publishers" and those who "vend"

"maps, charts, and books." Because the law slipped into using the term copy in 1909, it has now extended its reach to every act of duplication, by printing press or computer memory. It now therefore covers actions far beyond the "commercial" exploitation of anything.

Litman therefore argues that we take copyright back to its roots. "[W]hy not," she writes, "start by recasting copyright as an exclusive right of commercial exploitation? Making money (or trying to) from someone else's work without permission would be infringement, as would large scale interference with the copyright holders' opportunities to do so."24 Thus, she argues, we should redraw the border between commercial and noncommercial exploitation, giving authors strong control over the "pirating" of their work by commercial entities, but leaving noncommercial actions outside the reach of the law.

There is a great merit to this idea and, in my view, good reason to explore it extensively. The strongest reason to be skeptical is that the Net itself has now erased any effective distinction between commercial and noncommercial. Napster no doubt is a commercial activity, though the sharing that Napster enables is not. The law might well regulate Napster, but then subsequent p2p technologies would enable the same sort of sharing, just without commercial links. If there were a harm from Napster (and I'm not certain there is), it's not clear what benefit we gain from merely pushing the control underground.

This line-drawing problem reinforces my own view that the better solution is simply to go back to the Framers' notion of limited terms. The great benefit of a technology such as Napster is the ability to get access to music that is no longer made available commercially-songs from the 1930s or 1940s, for example, that still hang in copyright, but that the copyright holders don't make available commercially. If copyright were returned to a meaningfully "limited time," then we wouldn't need to worry so much about drawing commercial vs. noncommercial distinctions. For five or maybe ten years, commercial entities would hold these rights exclusively. Beyond that, the music, like culture generally, would be freely available.

Litman's suggestion does hint at a different limitation on the copyright. We might call this the "use it or lose it" restriction. 25 Once a work is published, if a holder of a copyright does not continue to make it available commercially, then others should have the right to exploit the work. Technically, we could accomplish this balance by giving anyone a right, after a brief period of exclusive control, to license the work under compulsory terms. The terms of such licenses can't be set here. Which would work best depends upon lots of things we can't know in the abstract. But the basic idea is that once a limited monopoly right has been granted, there is no further reason to allow a rights holder to hold up the content. This, like the need to renew, would assure that work was quickly pushed into the public domain.

PATENTS

THE URGENCY in the field of patents is even greater. Here again, patents are not evil per se; they are evil only if they do no social good. They do no social good if they benefit certain companies at the expense of innovation generally. And as many have argued convincingly, that's just what many patents today do.

Our response should be empirical. Congress should demand of the U.S. Patent Office that it perform a regulatory review of its patent regulation and produce from that review a regulatory impact statement. In particular, it should be required to perform an economic study to justify the most controversial extensions of patent right now-business method and software patents. If these forms of innovation regulation can't at least meet the burden of demonstrating that they are more likely to aid innovation than harm it, then Congress should withdraw this form of monopoly protection.

In the meantime, there are smaller changes that Congress might make, all designed to lessen the harm patents generally, and bad patents in particular, might cause.

MORATORIUM

CONGRESS SHOULD enact a moratorium on the offensive use of these questionable patents until this review is complete. While this study occurs, innovators should be free to file for these patents. If someone tries to enforce a patent, then a company should be allowed to defend against that enforcement with patents secured in the ordinary way. But until there is a showing of the good this system does, we should allow the courts to intervene in the regulatory process.

DAMAGES

THE GREATEST HARM that patents create may well be independent of the patents; the greatest harm may well be the harm caused by its enforcement. The real power a patent holder has-whether he or she holds a valid or invalid patent-is the threat of an injunction to stop the use of the patented technology, when damages would be an adequate remedy. Ordinarily, the use of someone else's "property" without negotiation is deemed adequate justification for an injunction; but there is no reason we should have to conceive of patents in this context as "property." Inventors could get an adequate return from the guarantee of reasonable royalties; users of patents could be assured that their innovation would not be blocked by a guarantee of a compulsory license.

This is not an easy idea to sketch briefly. To the extent that technologies are rightfully patented, granting only a compulsory right can, in many contexts, defeat the important incentives that patents produce. Thus, it would be a mistake to give up property protections for patents in all cases.

But you don't have to give up property protections for patents in all cases for it to make sense to adjust patents in the context of the Net. Here, where innovation is sequential and complementary, giving users greater flexibility would reduce the hold-up problem created by patent law.26

REFORM

FINALLY, THERE'S A range of reform that has been pushed on the U.S. Patent Office, much of it extremely valuable.27 Software patents are (relatively speaking) new. At least, they are newer than software itself. For many years, software could not be patented, which means that, for many years, the U.S. Patent Office did not collect data about prior art in patents.

This fact combines with another to make patents in this field particularly uncertain. When a person seeking a patent files the patent application, he or she is to include in the application a list of known "prior art"-earlier inventions that might be related to the invention for which a patent is sought. As the rules are now, however, the applicant need report only what the applicant actually knows. This creates an ostrich incentive: if you're responsible only for what you know, then you have an incentive to do very little research.28

The law deals with problems like this in many contexts. The ordinary response is a "negligence standard": the applicant must file what he or she knows or should have known. This creates a strong incentive for the applicant to discover relevant prior art. And it would help the U.S. Patent Office make a judgment about whether a patent should be granted.

If Congress determines that business method patents are justified, it should also consider the proposals of Jeff Bezos and Tim O'Reilly to grant patent protection for business methods for only a very short period. Bezos proposes five years, but an even shorter period may make sense.29 Network technologies move so quickly that a longer period of protection is never really needed; and whatever distortions this system might produce, they could be minimized by shorting the period of protection.

Congress should also, and most obviously, radically improve funding for the U.S. Patent Office and mandate fundamental improvements in the functioning of that office.30

Finally, Congress should consider the proposals of Congressman Berman to deny patents to business methods that are simply translations from real-space inventions to cyberspace.31 Scholars at first thought these inventions would be denied patent protection because of lack of inventiveness.32 But a clear rule marking the lack of inventiveness has not been shown. 33

THESE CHANGES are just beginnings, but they would be significant beginnings if done. They would together go a great distance in assuring that the space for innovation remains open and that the resources for innovation remain free. They would commit us to an environment that would preserve the innovation we have seen.

  

14

Edward Samuels, The Illustrated Story of Copyright (New York: St. Martin 's Press, 2000), 34.

15

White-Smith Music Publishing Co. v. Apollo Co., 209 U.S. 1, 21 (1908).

16

Congress's initial statute was Act of March 4, 1909, ch. 320(e), 35 Stat. 1075 (1909), superseded by 17 U.S.C. 115 (1988). See generally Fred H. Cate, "Cable Television and the Compulsory Copyright License," Federal Communications Law Journal 42 (1990): 191; C. H. Dobal, "A Proposal to Amend the Cable Compulsory License Provisions of the 1976 Copyright Act," Southern California Law Review 61 (1988): 699; Paul Glist, "Cable Copyright: The Role of the Copyright Office," Emory Law Journal 35 (1986): 621; Stanley M. Besen, Willard G. Manning Jr., and Bridger M. Mitchell, "Copyright Liability for Cable Television: Compulsory Licensing and the Coase Theorem," Journal of Law & Economics 21 (1978): 67.

Robert Merges has argued that compulsory rights do create problems in contexts such as this, and that some property rights will induce the creation of independent institutions that could most cheaply negotiate the rights. See Robert P. Merges, "Contracting into Liability Rules: Intellectual Property Rights and Collective Rights Organizations," California Law Review 84 (1996): 1293; Robert P. Merges, "Institutions for Intellectual Property Transactions: The Case of Patent Pools," in Expanding the Boundaries of Intellectual Property, Rochelle Cooper Dreyfuss and Diane Leenheer Zimmerman, eds. (Oxford: Oxford University Press, 2001), 131. Ian Ayres and Eric Talley reach a very different conclusion. See Ian Ayres and Eric Talley, "Solomonic Bargaining: Dividing a Legal Entitlement to Facilitate Coasean Trade," Yale Law Journal 104 (1995): 1027, 1092-94 (arguing that a liability rule will induce parties to reveal their true valuations and hence is more likely to produce a Coasean trade).

17

See Copyright Act of 1909, ch. 320, (e), 35 Stat. 1075 (1909), superseded by 17 U.S.C. 115 (1982).

18

Fortnightly Corp. v. United Artists Television, Inc., 392 U.S. 390 (1968), and Teleprompter Corp. v. Columbia Broadcasting System, Inc., 415 U.S. 394 (1974).

19

The compulsory right was incorporated in 111 of the 1976 act. As Paul Goldstein has written, "[M]ore explicitly than any other aspect of the [1976 Act], it commits its operation to assumptions about industry structure and regulation."

"Preempted State Doctrines, Involuntary Transfers and Compulsory Licenses: Testing the Limits of Copyright," U.C.L.A. Law Review 24 (1977): 1107, 1127-35. See also Melville B. Nimmer and David Nimmer, "Nimmer on Copyright," 8.18[E], 4.

20

See generally Samuels, 181-82. The satellite TV retransmission right was enacted in 1988. See Satellite Home Viewer Act, Act of November 16, 1988, Pub. L. No. 100-667, 102 Stat. 3935, codified at 17 U.S.C. 119 (supp. 1993). Jukeboxes were covered by the 1976 act, 17 U.S.C. 116. That provision was in tension with the Berne Convention, which forbade compulsory licenses for public performances. In 1989, Congress added 116A, which added negotiated agreements between performance rights associations. In 1993, Congress then repealed the original 116 and renamed 116A to 116. See Scott M. Martin, "The Berne Convention and the U.S. Compulsory License for Jukeboxes: Why the Song Could Not Remain the Same," Journal of the Copyright Society U.S.A. 37 (1990): 262.

21

As Calabresi and Melamed describe, a resource is protected with a liability rule when one using the resources must pay compensation for the use. The resource is protected by a property rule when one using the resource must negotiate for it before it can be taken. Guido Calabresi and Douglas Melamed, "Property Rules, Liability Rules, and Inalienability: One View of the Cathedral," Harvard Law Review 85 (1972): 1089, 1092. When a copyright is protected by a liability rule, those wishing to use the resource can take the resource, as long as they pay the liability price. When it is protected by a property rule, taking the resource without paying for it can make one criminally liable. Robert P. Merges, "Institutions for Intellectual Property Transactions: The Case of Patent Pools," in Expanding the Boundaries of Intellectual Property, R. Dreyfuss, ed. (Ox-ford: Oxford University Press, 2001), 131-32.

22

" Chicago school" analysts argued that a monopolist possesses a fixed amount of market power and therefore can extract only a fixed amount of monopoly profit from consumers, whether from one market or several. On this basis, they concluded that leverage of monopoly power from one market into another is impossible. See, e.g., Robert H. Bork, The Antitrust Paradox: A Policy at War with Itself (New York: Basic Books, 1978); Richard A. Posner, Antitrust Law: An Economic Perspective (Chicago: University of Chicago Press, 1976). More recent economic analyses have demonstrated several mechanisms by which market power in one market can be used to harm competition in another market. As Steven Salop and Craig Romaine put it:

Post-Chicago economic analysis has suggested that there are a number of limiting assumptions required for this single monopoly profit theory to apply. When these assumptions are relaxed, the theory's strong result and the public policy implications no longer hold. There are a number of common market situations in which integration into a second market may raise anticompetitive concerns. These include markets in which the first monopoly is regulated, markets that are characterized by economies of scale and scope and in which the inputs are not used in fixed proportions, and markets with multiple types of buyers. In such markets, it is possible for a monopolist to profitably extend its power into a second market and harm consumers. (footnote omitted)

Steven C. Salop and R. Craig Romaine, "Preserving Monopoly: Economic Analysis, Legal Standards, and Microsoft," George Mason Law Review 7 (1999): 617, 625. See also Michael D. Whinston, "Tying, Foreclosure, and Exclusion," American Economic Review 80 (1990): 837 (demonstrating multiple situations in which foreclosure of the tied market can occur, including a case where the monopolist can precommit to the tie through product design or production processes); Janusz A. Ordover, Garth Saloner, and Steven C. Salop, "Equilibrium Vertical Foreclosure," American Economic Review 80 (1990): 127 (integration across multiple products permits competitor to exclude uninte-grated rival). See also Louis Kaplow, "Extension of Monopoly Power Through Leverage," Columbia Law Review 85 (1985): 515 (expressing early skepticism about the Chicago school analysis). See also Dennis W. Carlton and Michael Waldman, "The Strategic Use of Tying to Preserve and Create Market Power in Evolving Industries" (September 1998 working paper) (arguing that tying deters entry in primary tying markets in addition to providing leverage into tied markets).

23

See Douglas Abell, "Pay-for-Play," Vanderbilt Journal of Entertainment Law & Practice 2 (2000): 52.

24

See 17 U.S.C. 111 (2000). The statute initially set up a Copyright Royalty Tribunal, but that was abolished in favor of private negotiation in 1993. See Robert P. Merges, Peter S. Menell, and Mark A. Lemley, Intellectual Property in the New Technological Age, 2nd ed. (Gaithersburg: Aspen Law & Business, 2000), 481.

Congress initially tried to balance this effective control by establishing rules of access such as the "Fairness Doctrine." See Jerry Kang, Communications Law and Policy: Cases and Materials (Gaithersburg, Md.: Aspen Law & Business, 2001), 85-86. These rules have been drawn into constitutional doubt-see, for example, Miami Herald Publishing Co. v. Tornillo, 418 U.S. 241 (1974), Huddy v. FCC, 236 F.3d 720, 723 (D.C. Cir. 2001)-and are generally viewed as a failure. See L. A. Scot Powe, American Broadcasting and the First Amendment (Berkeley: University of California Press, 1987), 197-209.

25

As FCC chairman Powell has described it:

In 1969, broadcasting consisted of a handful of radio stations in any given market plus two or three television stations affiliated with one of the three major networks. Occasionally, larger markets had an independent station too. Three major networks held more than 90% of the market for video programming. Not so anymore. Not only has the market share of the three largest networks been eroded by cable programming, the last time I looked there were about seven "declared" national television networks Obviously, things have changed a lot In our current technological environment, it can reasonably be argued that there is a bounty, not a scarcity of outlets for expressing one's viewpoint. In the traditional broadcasting arena, the numbers are impressive: There are 1,207 commercial TV stations and 367 noncommercial stations. There are also some 5,000 TV translators and 2,000 low-power TV stations. In addition, there are almost 12,500 radio stations.

Michael K. Powell, "Willful Denial and First Amendment Jurisprudence, Remarks Before the Media Institute," April 22, 1998 (transcript available at http://www.fcc.gov/ Speeches/Powell/spmkp808.html).

26

See Robert M. Fano, "On the Social Role of Computer Communications," Proceedings of the IEEE 60 (September 1972): 1249.

27

Simson L. Garfinkel, Architects of the Information Society: Thirty-Five Years of the Laboratory for Computer Science at MIT (Cambridge, Mass.: MIT Press, 1999), 8-9.

28

Ronald Coase, "Looking for Results," interviewed by Thomas W. Hazlett, Reason (January 1997).

29

See Linda Shrieves, "When It's Your Turn, Here's Why You're Served a Chorus of; The Birthday Song Is Still Copyrighted and Nets Nearly $1 Million a Year in Royalties," Orlando Sentinel, February 27, 2001, E1.

30

Or at least not yet. George Gilder has repeatedly argued that a future infrastructure based on fiber optics would provide "infinite bandwidth." See George Gilder, Telecosm: How Infinite Bandwidth Will Revolutionize Our World (New York: Free Press, 2000); George Gilder, "Rulers of the Rainbow: The New Emperors of the Telecosm Will Use the Infinite Spectrum of Light-Visible and Invisible-to Beef up Bandwidth," Forbes ASAP (October 1998): 104; and George Gilder, "Into the Fibersphere (Fiber Optics)," Forbes (December 1992): 111.

31

Ben H. Bagdikian, The Media Monopoly, 6th ed. (Boston: Beacon Press, 2000), 4.

32

Robert W. McChesney, Rich Media, Poor Democracy: Communication Politics in Dubious Times (Urbana: University of Illinois Press, 1999), 18.

33

The reasons for this increased concentration are hard to track precisely. There are a number of changes that have certainly occurred. The relaxation of rules on ownership of radio stations, for example, has exploded concentration in radio station ownership. This, in turn, has led to an increase in the modern equivalent of "payola." See Douglas Abell, "Pay-for-Play," Vanderbilt Journal of Entertainment Law & Practice 2 (2000): 52. As Boehlert describes it:

There are 10,000 commercial radio stations in the United States; record companies rely on approximately 1,000 of the largest to create hits and sell records. Each of those 1,000 stations adds roughly three new songs to its playlist each week. The [independents] get paid for every one; $1,000 on average for an "add" at a Top 40 or rock station, but as high as $6,000 or $8,000 under certain circumstances.

Eric Boehlert, "Pay for Play," Salon, March 14, 2001, http://www.salon.com/ent/ feature/2001/03/14/payola/print.html, 2.

34

Allyson Lieberman, "Sagging Warner Music out of Tune with AOL TW," New York Post, April 19, 2001, 34. See also Charles Mann, "The Heavenly Jukebox," Atlantic Monthly (September 2000), 39, 53.

35

Boehlert, 2.

36

McChesney, 18.

37

Ibid., 17.

38

Ibid., 33.

39

Ibid., 18.

40

According to the National Cable Television Association's figures, the top seven "multiple system operators," or MSOs, controlled 90 percent of the national cable television market at the end of 2000. http://www.ncta.com/industry_overview/top50mso.cfm. See also Richard Waters, "Appeals Court Overrules Curbs on Cable TV Ownership in U.S. Federal Rules," Financial Times, March 3, 2001, 7. As of March 2001, AOL-Time Warner's cable market share was about 20 percent, while AT&T's share stood at 42 percent (including AT&T's purchase of MediaOne Group and its 25.5 percent stake in Time Warner Entertainment). AT&T's 42 percent market share well exceeds the FCC's cap of 30 percent, leading AT&T to challenge the cap in court as being "arbitrary." See Edmund Sanders and Sallie Hofmeister, "Court Rejects Limits on Cable Ownership; Television: Controversial 30% Cap Is Deemed Unconstitutional, but Consumer Groups Call the Decision 'Devastating,' " Los Angeles Times, March 3, 2001, C1.

41

Bagdikian, 4.

42

Ibid., x.

43

See Mike Hoyt, "With 'Strategic Alliances,' the Map Gets Messy," Columbia Journalism Review (January-February 2000), at http://www.cjr.org/year/00/1/hoyt.asp; Global Media Economics: Commercialization, Concentration and Integration of World Media Markets, Allan B. Albarran and Sylvia M. Chan-Olmsted, eds. (Ames: Iowa State University Press, 1998), 19-31; Dennis W. Mazzocco, Networks of Power: Corporate T.V.'s Threat to Democracy (Boston, Mass.: South End Press, 1994), 1-8. Cf. Benjamin M. Compaine, "Distinguishing Between Concentration and Competition," in Who Owns the Media, 3rd ed., Benjamin M. Compaine and Douglas Gomery, eds. (Mahwah, N.J.: L. Erlbaum Associates, 2000), 537; Douglas Gomery, "Interpreting Media Ownership," in Who Owns the Media?, 3rd ed., Benjamin M. Compaine and Douglas Gomery, eds. (Mahwah, N.J.: L. Erlbaum Associates, 2000), 507.

44

Bagdikian, 7. There are, of course, many who believe there is no necessary link between the mergers and these features of modern media. See, e.g., Steven Rattner, "A Golden Age of Competition," in Media Mergers, Nancy J. Woodhull and Robert W. Snyder, eds. (New Brunswick, N.J.: Transaction Publishers, 1998), 9. See also Bruce M. Owen, Economics and Freedom of Expression: Media Structure and the First Amendment (Cambridge, Mass.: Ballinger Publishing Company, 1975).

45

Andrew Kreig, Spiked: How Chain Management Corrupted America 's Oldest Newspaper (Old Saybrook, Conn.: Peregrine Press, 1987).

46

Bagdikian, 30.

47

McChesney, 245.

48

Compare Judge Posner's comment: "[T]he management of a large publicly held corporation will have difficulty finding issues on which a partisan stand would not alienate large numbers of shareholders." Posner, Economic Analysis of Law, 674.

49

Bagdikian, 129.

50

Ibid., 35.

51

McChesney, 80, 179.

52

Ibid., 250.

53

Ibid., 148.

54

Ibid., 168.

CHAPTER 8

1 See Lawrence Lessig, Code and Other Laws of Cyberspace (New York: Basic Books, 1999).

2

For examples of on-line mapping services, see MapQuest.com at http://www. mapquest.com; Maps On Us at http://www.mapsonus.com; and MapBlast! at http://www. mapblast.com.

3

For examples of on-line translation Web sites, see AltaVista World/Translate at http://world.altavista.com; FreeTranslation.com at http://www.freetranslation.com; and From Language to Language at http://www.langtolang.com.

4

A short list of many examples of on-line dictionaries includes Merriam-Webster OnLine at http://www.m-w.com; Cambridge Dictionaries Online at http://dictionary. cambridge.org; and AllWords.com at http://www.allwords.com. There are also sites that perform aggregate searches through multiple multilingual dictionaries, such as yourDictionary.com at http://www.yourdictionary.com.

5

As we'll see in chapter 11, this is not a slight constraint. Because his site was noncommercial, Eldred could include only work that had fallen into the public domain. When Eldred began, the content constraint meant that works published before 1923 were free, works published after 1923 were only possibly free. But in 1998, Congress changed that by passing the Sonny Bono Copyright Term Extension Act. The Bono Act extended the term of existing copyrights by twenty years, meaning work that was to fall into the public domain in 1999 would now not fall into the public domain until 2019. As we'll see, this turned Eldred into an activist.

6

See http://www.apple.com/hotnews/articles/2001/03/imacdirector/.

7

There are skeptics, however, about whether diversity will increase. Le Duc, for example, argues that the real constraint on diversity in films is not the channels of distribution, but rather the limited attention viewers have for stars. There are only so many stars we can like; they are the true constraint on this mode of production; and as long as that limit remains, the range of film will be restricted as well. Don R. Le Duc, Beyond Broadcasting: Patterns in Policy and Law (New York: Longman, 1987), 128.

8

On the risk of liability, see JaNet Kornblum, "Lyrics Site Takes Steps to Avoid Napster Woes," USA Today, December 12, 2000, available at http://www.usatoday.com/life/ cyber/tech/jk121200.htm.

9

Tom Parsons, "World Wide Web Gives Poets, Poetry Room to Grow," Dallas Morning News, July 30, 2000, 8J.

10

For example, a Web site devoted to Chaucer uses multiple frames to navigate quickly through The Canterbury Tales and define the medieval English terms. See Librarius at http://www.librarius.com/cantales.htm. Another site provides a high-tech multimedia companion to the printed Anthology of Modern American Poetry. See Modern American Poetry at http://www.english.uiuc.edu/maps.

11

See Favorite Poem Project at http://www.favoritepoem.org (featuring readings of famous poems read by individual Americans); Internet Poetry Archive at http://www. ibiblio.org/ipa (offering modern poetry readings by the poets); and e-poets.net at http://www.e-poets.net (featuring contemporary audio poetry).

12

Free in the sense that I have defined the term. The technology is offered under a nondiscriminatory license. The underlying technology is patented. See Brad King, "MP3.com Open to Friends," Wired News, January 19, 2001, at http://www.wired.com/ news/mp3/0,1285,41195,00.html. "Vorbis" is an alternative to MP3 that is royalty free and compresses more than the MP3 format does. Vito Pilieci, "MP3 May Go Way of Eight-Track: Vorbis Audio File Players Would Be Free of Royalty, Patent Fees," National Post, June 29, 2000, C8. The licensing does, however, create problems for open code developers. See Wendy C. Freedman, "Open Source Movement Vies with Classic IP Model, Free Software Is Bound to Have a Significant Effect on Patent, Copyright, Trade Secret Suits," National Law Journal 22 (March 13, 2000): B14.

13

Telephone interview with Michael Robertson, November 16, 2000.

14

Courtney Love, "Courtney Love Does the Math," Salon (June 12, 2000): 5. Love has offered a slightly exaggerated but illustrative description of how the market for music now works:

This story is about a bidding-war band that gets a huge deal with a 20 percent royalty rate and a million-dollar advance. (No bidding-war band ever got a 20 percent royalty, but whatever.) This is my "funny" math based on some reality and I just want to qualify it by saying I'm positive it's better math than what Edgar Bronfman Jr. [the president and CEO of Seagram, which owns Polygram] would provide. What happens to that million dollars? They spend half a million to record their album. That leaves the band with $500,000. They pay $100,000 to their manager for 20 percent commission. They pay $25,000 each to their lawyer and business manager. That leaves $350,000 for the four band members to split. After $170,000 in taxes, there's $180,000 left. That comes out to $45,000 per person. That's $45,000 to live on for a year until the record gets released. The record is a big hit and sells a million copies. (How a bidding-war band sells a million copies of its debut record is another rant entirely, but it's based on any basic civics-class knowledge that any of us have about cartels. Put simply, the antitrust laws in this country are basically a joke, protecting us just enough to not have to rename our park service the Phillip Morris National Park Service.) So, this band releases two singles and makes two videos. The two videos cost a million dollars to make and 50 percent of the video production costs are recouped out of the band's royalties. The band gets $200,000 in tour support, which is 100 percent recoupable. The record company spends $300,000 on independent radio promotion. You have to pay independent promotion to get your song on the radio; independent promotion is a system where the record companies use middlemen so they can pretend not to know that radio stations-the unified broadcast system-are getting paid to play their records. All of those independent promotion costs are charged to the band. Since the original million-dollar advance is also recoupable, the band owes $2 million to the record company. If all of the million records are sold at full price with no discounts or record clubs, the band earns $2 million in royalties, since their 20 percent royalty works out to $2 a record. Two million dollars in royalties minus $2 million in recoupable expenses equals zero! How much does the record company make? They grossed $11 million. It costs $500,000 to manufacture the CDs and they advanced the band $1 million. Plus there were $1 million in video costs, $300,000 in radio promotion and $200,000 in tour support. The company also paid $750,000 in music publishing royalties. They spent $2.2 million on marketing. That's mostly retail advertising, but marketing also pays for those huge posters of Marilyn Manson in Times Square and the street scouts who drive around in vans handing out black Korn T-shirts and backwards baseball caps. Not to mention trips to Scores and cash for tips for all and sundry. Add it up and the record company has spent about $4.4 million. So their profit is $6.6 million; the band may as well be working at a 7-Eleven.

Ibid. Compare Senator Hatch's very different account of music production outside the control of the labels:

I will quote him at length, because his experience is instructive. He said: "As a result of doing it on my own, I get about $7 for every CD that sells in a store. And about $10 per CD sold at concerts. In contrast, I've got a friend who is also a performer/songwriter who opted to sign with a label. He recorded a CD that cost about $18,000 to make, which the label paid for. Now, when one of his CDs sells at a store or at a concert, he makes about $1. The rest of the $7-$10 which I make on my CD sales goes to his label. On top of that, he has to pay back the $18,000 it cost to make the CD out of his $1-per-CD cut. In other words, he won't make a dime until he has sold 18,000 CDs. And then, he still won't own the CD, the label will. They maintain the copyright. It's kind of like paying off your mortgage and then having the bank still own your house.

Orrin G. Hatch, "Address of Senator Orrin G. Hatch Before the Future of Music Coalition," Future of Music Coalition, January 10, 2001, 2. Currently, the average album release is under twenty-five thousand per CD. Jon Healey, "Industry Seeks to Justify Huge Overhead on the Price of Compact Disks," Knight Ridder Tribune Business News, September 3, 2000.

15

Telephone interview with Michael Robertson.

16

Ibid.

17

Ibid.

18

See Testimony of the Future of Music Coalition on "Online Music and Copyright Law," submitted to Senate Judiciary Committee, April 3, 2001, 13 ("The fastest-growing demographic segment using Napster are adults over the age of 24. Research reports have confirmed that one of the major reasons that they are doing so is to access commercial recordings that are no longer commercially available.").

19

See Paul Goldstein, Copyright's Highway: From Gutenberg to the Celestial Jukebox (New York: Hill and Wang, 1994). There are two parts to this conception, of course. One is the "celestial" part-emphasizing universal access. The other is the "jukebox" part-emphasizing payment. Napster emphasized the first.

20

See Scott Kirsner, "Firefly: From the Media Lab to Microsoft," Wired News, April 9, 1998, at http://www.wired.com/news/business/0,1367,11585,00.html; Daniel Lyons, "The Buzz About Firefly," New York Times, June 29, 1997, section 6, 37. See also Andrew L. Shapiro, The Control Revolution: How the Internet Is Putting Individuals in Charge and Changing the World We Know (New York: PublicAffairs, 1999), 84-101.

21

See Laura J. Gurak, Persuasion and Privacy in Cyberspace: The Online Protests Over Lotus MarketPlace and the Clipper Chip (New Haven, Conn., and London: Yale University Press, 1997); Seth Safier, "Between Big Brother and the Bottom Line: Privacy in Cyberspace," Virginia Journal of Law & Technology 5 (2000): 6; Andrew Shapiro, "Privacy for Sale: Peddling Data on the Internet," Nation (June 23, 1997). The FTC has taken an active role in monitoring the monitors. See "United States Federal Trade Commission, Privacy Online: Fair Information Practices in the Electronic Marketplace: A Federal Trade Commission Report to Congress" (Washington, D.C.: Federal Trade Commission, May 2000), available at http://www.ftc.gov/reports/privacy2000/privacy2000. pdf. For a list of current legislation proposed, see current information on the status of pending privacy bills, available at http://www.epic.org/privacy/bill_track.html.

22

This is the argument made by Cass Sunstein, in Republic.com (Princeton, N.J.: Princeton University Press, 2001). As Sunstein argues, how groups are structured-what their composition is, how they deliberate-affects the results that deliberation produces. Cass Sunstein, Republic.com, 65-71.

23

The success rate of advertising is highly controversial. The general consensus is that direct snail mail advertising response rates are generally in the 1-3 percent range. Directed e-mail advertising campaigns may have response rates in the 10-15 percent range, though some estimates run as high as 25 percent. The click-through rate for banner ads on the Web is much lower, estimated at 0.5 percent. Mark Brownlow for Internet Business Forum, available at www.ibizbasics.com/online040301.htm.

24

A "lower cost," of course, does not mean no cost. There is still the cost of publishing a book and at least some cost in an initial promotion.

25

For a summary of peer-to-peer standards in progress, see http://peer-to-peerwg.org. See also http://p2ptracker.com (summarizing current technology); "Business, Bandwidth May Dash Hopes of a Peer-to-Peer Utopia," http://news.cnet.com/news/0-1005-201-3248711-0.html.

26

Clay Shirky, "Clay Shirky's Writings About the Internet: Economics and Culture, Media and Community, Open Source," November 16, 2000, www.openp2p.com/pub/ a/p2p/2000/11/24/shirky1-whatisp2p.html.

27

See, e.g., Nelson Minar and Marc Hedlund, "A Network of Peers: Peer-to-Peer Models Throughout the History of the Internet," in Peer-to-Peer: Harnessing the Benefits of a Disruptive Technology, Andy Oram, ed. (Beijing and Cambridge, Mass: O'Reilly, 2001), 3-15 (describing how the original Internet was "fundamentally designed as a peer-to-peer system" but became increasingly client/server oriented over time owing to Web browser applications, firewalls, and other factors).

28

For background on SETI, see "History of SETI," at http://www.seti-inst.edu/ general/history.html; Eric Korpela et al., "SETI@home: Massively Distributed Computing for SETI," at http://www.computer.org/cise/articles/seti.htm.

29

Howard Rheingold, "You Got the Power," Wired (August 2001), at http://www.wired. com/wired/archive/8.08/comcomp.html?pg=1&topic=&topic_set=.

30

For a useful survey of issues related to P2P, see Peer-to-Peer: Harnessing the Benefits of a Disruptive Technology, Andrew Oram, ed. (Beijing and Cambridge, Mass.: O'Reilly, 2001). See also http://www.oreillynet.com/p2p/ (collecting articles). Xerox PARC has conducted an interesting study of the free-riding problem with p2p technologies. See Eytan Adar and Bernardo A. Huberman, "Free Riding on Gnutella," First Monday, October 2000, 5 at http://www.firstmonday.dk/issues/issue5_10/adar/index.html. For a pessimistic view of the potential for P2P, see Lee Gomes, "P-to-P, B-to-B, RIP?," Wall Street Journal, April 4, 2001, B1.

31

Rheingold, You Got the Power.

32

Andy Oram, "The Value of Gnutella and Freenet," Webreview (May 2000), at http://www.webreview.com/pi/2000/05_12_00.shtml.

33

Again, this is just relative. The claim is not that there is equal access to equally valuable data. Amazon.com is in a better position to market than tinybookseller.com. Likewise, it may well be that scale here makes all the difference. There have been suggestions that these architectures may reduce competition. See Yannis Bakos and Erik Brynjolfsson, "Bundling and Competition on the Internet," Marketing Science, January 2000, available at http://ecommerce.mit.edu/erik/bci-final.pdf (finding that "economies of aggregation" for information goods could adversely affect competition). At this stage, however, I don't think we know enough to say.

For a careful analysis mapping the path to concentration in distribution from the nature of IP rights, see Martin Kretschmer, George Michael Klimis, and Roger Wallis, "The Changing Location of Intellectual Property Rights in Music: A Study of Music Publishers, Collecting Societies and Media Conglomerates," Prometheus 17 (1999): 163.

34

James Boyle, "A Politics of Intellectual Property: Environmentalism for the Net?," Duke Law Journal 47 (1997): 87. See also Carol Rose, "The Several Futures of Property: Of Cyberspace and Folk Tales, Emission Trades and Ecosystems," Minnesota Law Review 83 (1998): 129.

35

See Carl Shapiro, "Will E-Commerce Erode Liberty?," Harvard Business Review (May-June 2000): 189 (book review of Lessig, Code and Other Laws of Cyberspace [New York: Basic Books, 1999]).

CHAPTER 10

1 Peter W. Huber, Michael K. Kellogg, and John Thorne, Federal Telecommunications Law, 2nd ed. (Gaithersburg, Md.: Aspen Law & Business, 1999), 164-65.

2

The regulations that effected this neutrality were many and are described comprehensively in Huber, Kellogg, and Thorne.

3

In Carterfone, the FCC required the Bell system to allow the connection of customer-provided equipment as long as it did not harm the network. The device at issue in the decision permitted communication between a mobile radio and the landline network. The Bell system declined to permit the connection but could not demonstrate the harm interconnection would create. The FCC ordered the carrier to permit the interconnection, and this requirement, in turn, spurred a great deal of innovation for the telephone network. Ibid., 409.

4

My emphasis on neutrality does not deny the effect of other regulatory measures. For example, the fact that ISPs enjoyed business rates for their usage-flat rate pricing, rather than metered pricing-effected an important subsidy for Internet service. See Peter W. Huber et al., Federal Telecommunication Law (Gaithersburg, Md: Aspen Business & Law, 1999), 11.5, 1030.

5

On ATM circuits, see Douglas E. Comer, Computer Networks and Internets, 2nd ed. (Upper Saddle River, N.J.: Prentice-Hall, 1999), 88, 184. On the link to quality of service, see Paul A. David and Raymond Werle, The Evolution of Global Networks: Technical, Organizational and Cultural Dimensions (May 2000), 10.

6

Likewise, if there are vices in networks that are technically not end-to-end compliant, then we might imagine rules that would balance those vices to better achieve the value of end-to-end. For example, other networks that are not end-to-end compliant-such as ATM networks or multiprotocol label switching (MPLS) networks-may have an aim that respects the values that end-to-end promotes. The TCP/IP network, while not "optimized" for any particular use, in effect is optimized for some uses rather than others. It does well, for example, with applications that do not suffer from network latency (e-mail), but not with applications that do suffer from network latency (voice-over IP). These other networks would better enable these other applications, thus rendering the network as a whole more neutral among applications. If the value of end-to-end inheres in the consequences of this neutrality, then a properly implemented mix might achieve end-to-end values without every part of the network being end-to-end. I am grateful to Tim Wu for making this point to me.

7

The Telecommunications Act of 1996 does not define broadband. It refers to broadband as a characteristic of "advanced telecommunications capability," which is defined as "high-speed, switched, broadband telecommunications capability that enables users to originate and receive high-quality voice, data, graphics, and video telecommunications using any technology." Telecommunications Act of 1996, Pub. L. No. 104-104, 706 (c)(1), 110 Stat. 56 (1996). See also 47 U.S.C. 157 note (2001). The FCC filed its Section 706 Report to Congress in 1999 and defined broadband as "the capability of supporting, in both the provider-to-consumer (downstream) and the consumer-to-provider (upstream) directions, a speed (in technical terms, "bandwidth") in excess of 200 kilobits per second (kbps) in the last mile." 14 FCC Rcd. 2398 at 2406 20 (1999). See also http://www.fcc.gov/Bureaus/Cable/Reports/broadbandtoday.pdf.

8

For an excellent and comprehensive history of and background on broadband issues related to cable, see Kim Maxwell, Residential Broadband: An Insider's Guide to the Battle for the Last Mile (New York: John Wiley & Sons, 1999). See also Patrick R. Parsons and Robert M. Frieden, The Cable and Satellite Television Industries (Boston: Allyn and Bacon, 1998).

9

See Mark S. Nadel, "Cablespeech for Whom?," Cardozo Arts & Entertainment Law Journal 4 (1985): 51, 70 and n. 104.

10

See Patrick R. Parsons and Robert M. Frieden, The Cable and Satellite Television Industries (Boston: Allyn and Bacon, 1998), chs. 2, 5; Robert W. Crandall and Harold Furchtgott-Roth, Cable TV: Regulation or Competition? (Washington, D.C.: Brookings Institution, 1996), 1-23; Porter Bibb, It Ain't as Easy as It Looks: Ted Turner's Amazing Story (New York: Crown Publishers, 1993) (Ted Turner biography).

11

By 1987, of the 87.5 million homes with TVs, 44.1 million (50.4 percent) were cable subscribers. Currently about 66 percent of TV households in the country use cable. Parsons and Frieden, 3, 121-22.

12

For a summary, see Jerry Kang, Communications Law and Policy: Cases and Materials (Gaithersburg, Md.: Aspen Law & Business, 2001), 154-61.

13

Mark E. Laubach, David J. Farber, and Stephen D. Dukes, Delivering Internet Connections over Cable: Breaking the Access Barrier (New York: John Wiley, 2001), 11-12. For a primer on DOCSIS and its history, see http://www.cablemodem.com/ DOCSIS.pdf.

14

See, e.g., Federal Communications Commission, "In the Matter of Applications for Consent to the Transfer of Control of Licenses and Section 214 Authorizations from Tele-Communications, Inc., Transferor to AT&T Corp., Transferee, CS Docket No. 98-178," February 18, 1999, 89, available at http://www.fcc.gov/Bureaus/Cable/ Orders/1999/fcc99024.txt ("According to AT&T-TCI, any equal access conditions such as those advocated by opponents to the requested transfers will impose substantial investment costs and expenses on @Home, which will only delay and diminish its deployment of broadband services to residential customers.").

15

See Thomas Starr, John M. Cioffi, and Peter Silverman, Understanding Digital Subscriber Line Technology (Upper Saddle River, N.J.: Prentice-Hall, 1999) (citing 1976).

16

Comer, Computer Networks and Internets, 159.

17

Throughout this section, by "telephone companies" I am referring to the regional Bell operating companies (the RBOCs). Non-RBOCs are not subject to the same obligations of open access under the statute. See AT&T Corp. v. City of Portland, 216 F.3d 871, 879 (9th Cir., 2000).

18

There are more traditional concerns as well. "Over the long term, the cable providers' tying strategy will thus undermine competitive investment in both the broadband transport and portal markets, insulating cable providers from conduit and content competition, and ensuring that the delivery of Internet-based video by competing conduits does not erode cable providers' monopoly power in the market for traditional video programming." Daniel L. Rubinfeld and Hal J. Singer, "Vertical Foreclosure in High Technology Industries: A Case Study of the AOL Time Warner Merger" (Rubinfeld-Singer White Paper), 10. Rubinfeld believes the vertical integration of cable will create an incentive to pursue two foreclosure strategies: (1) conduit discrimination; and (2) content discrimination. Ibid., 29-30.

19

See, for example, Cisco White Paper, "Controlling Your Network-A Must for Cable Operators" (1999), 5, available at http://www.cptech.org/ecom/openaccess/cisco1. html, describing tools to effect discrimination. As one research report summarizes the problem:

The situation is analogous to a customer trying to drive to the bookstore of their choice only to find that roadblocks have been established to channel customers to another bookseller, to the exclusion of all other booksellers. Unlike the roadway analogy, in the online world the end user may not be aware that the roadblocks have been placed and may have their behavior influenced without even knowing that the ISP has limited their ability to choose.

AARP, "Tangled Web: The Internet and Broadband Open Access Policy," January 2001, research.aarp.org, 13.

20

See, e.g., http://web.mit.edu/Saltzer/www/publications/openaccess.html. For Saltzer's model licenses for cable access, see http://web.mit.edu/Saltzer/www/publications/ clauses2.html.

21

See Brock Meeks, "Excite@Home Keeps a Video Collar," ZDNet News, November 1, 1999, at http://www.zdnet.com/zdnn/stories/news/0,4586,2385059,00.html. See also Harold Feld, "Whose Line Is It Anyway?: The First Amendment and Cable Open Access," CommLaw Conspectus 8 (2000): 23, 34; Mark Cooper, "Transforming the Information Superhighway into a Private Toll Road," Colorado Law Review 71 (2000): 1011, 1055. An industry trade journal notes that both Excite@Home and Road Runner limit consumers to ten-minute streaming segments. "PC-TV Convergence Driving Streaming Industry Growth," Warren 's Cable Reg. Monitor, March 1, 1999, 1999 WL 6825624.

22

David Lieberman, "Media Giants' Net Change: Major Companies Establish Strong Foothold Online," USA Today, December 14, 1999, B2. See also telephone interview with David Isenberg, February 14, 2001 ("They couldn't possibly open up their system so that it would have the capability to do TV over IP. That would be killing themselves.").

23

Bar et al., 32.

24

Ibid., 34.

25

Ibid., 35.

26

Ibid., 29.

27

Franois Bar has a similar assessment. See Franois Bar, "The Construction of Marketplace Architecture," Brookings & Internet Policy Institute 15 (forthcoming 2001), (describing the decline of end-to-end).

28

This, of course, is assured only if there is no actual or effective tying between cable products and other services that might effectively protect cable from meaningful competition with DSL.

29

See Joanna Glasner, "DSL Rhymes with Hell," Wired News, January 2001, available at http://www.wired.com/news/business/0,1367,41433,00.html (DSL numbers); Roy Mark, " U.S. Scores First Decline in Internet Subscribers," dc.internet.com, May 2, 2001, available at: http://dc.internet.com/news/article/0,1934,2101_756771,00.html (cable numbers).

30

See Morgan Stanley Dean Witter, The Broadband Report (May 2000). See also David Lake, "Strike Up the Broadband," Industry Standard, February 2, 2001, at http://www.thestandard.com/article/0,1902,21892,00.html. ("In the fourth quarter of 2000, cable-modem providers watched their subscriber base increase 19 percent to 4.2 million users. Still, the adoption of DSL is happening at a faster pace. At the beginning of last year, the number of cable-modem subscribers outnumbered DSL subscribers 5 to 1. Now that ratio hovers at almost 2 to 1, in favor of cable-modem access. Nevertheless, cable modems are expected to retain an edge. In 2003, 51 percent of broadband subscribers will use cable modems, while DSL is expected to account for only 37 percent of the high-speed market, according to Jupiter Research.")

31

Laubach, Farber, and Dukes, 238.

32

The campaign was conceived of by Jan Brandt. "Since Brandt's arrival at AOL in 1993, membership has grown from 250,000 to 8 million [in 1997]." Upside magazine said that "Brandt's carpet-bombing techniques have redefined the use of direct mail in the high-tech industry and pioneered the get-something-for-nothing marketing coups copied by Netscape and other Internet underdogs to achieve brand-name recognition in no time flat." http://media.aoltimewarner.com/media/cb_press_view.cfm? release_num=147.

33

On the character of these virtual worlds, see Julian Dibbell, My Tiny Life: Crime and Passion in a Virtual World (London: Fourth Estate, 1998). See also Lynn Cherny, Conversation and Community: Chat in a Virtual World (Stanford, Calif.: CSLI Publications, 1999).

34

America Online Inc., "Open Access Comments of America Online, Inc." before the Department of Telecommunications and Information Services, San Francisco, October 27, 1999.

35

Comments of America Online, Inc., "In the Matter of Transfer of Control of FCC Licenses of MediaOne Group, Inc. to AT&T Corporation," Federal Communications Commission, CS Docket No. 99-251, August 23, 1999. As it argued:

What this merger does offer, however, is the means for a newly "RBOC-icized" cable industry reinforced by interlocking ownership relationships to (1) prevent Internet-based challenge to cable's core video offerings; (2) leverage its control over essential video facilities into broadband Internet access services; (3) extend its control over cable Internet access services into broadband cable Internet content; (4) seek to establish itself as the "electronic national gateway" for the full and growing range of cable communications services.

To avoid such detrimental results for consumers, the Commission can act to ensure that broadband develops into a communications path that is as accessible and diverse as narrowband. Just as the Commission has often acted to maintain the openness of other last-mile infrastructure, here too it should adopt open cable Internet access as a competitive safeguard-a check against cable's extension of market power over facilities that were first secured through government protection and now, in their broadband form, are being leveraged into cable Internet markets. Affording high-speed Internet subscribers with an effective means to obtain the full range of data, voice and video services available in the marketplace, regardless of the transmission facility used, is a sound and vital policy-both because of the immediate benefit for consumers and because of its longer-range spur to broadband investment and deployment. Here, the Commission need do no more than establish an obligation on the merged entity to provide non-affiliated ISPs connectivity to the cable platform on rates, terms and conditions equal to those accorded to affiliated service providers. (AOL, FCC, 4)

36

Comments of AT&T Wireless Services, Inc., "In the Matter of Inquiry Regarding Software Defined Radios," Federal Communications Commission, ET Docket No. 00-47, July 14, 2000, 15.

37

AT&T Canada Long Distance Services, "Comments of AT&T Canada Long Distance Services Company," before the Canadian Radio-television and Telecommunications Commission, Telecom Public Notice CRTC 96-36: "Regulation of Certain Telecommunications Service Offered by Broadcast Carriers," February 4, 1997. For the best analysis of this change in position, see the submission by Mark Cooper of the Consumer Federation of America, Petition to Deny, in re "Application of America Online and Time Warner for Transfers of Control," before the FCC, CS 00-30 (April 26, 2000):

The dominant and vertically integrated position of cable broadcast carriers requires a number of safeguards to protect against anti-competitive behaviour. These carriers have considerable advantages in the market, particularly with respect to their ability to make use of their underlying network facilities for the delivery of new services. To grant these carriers unconditional forbearance would provide them with the opportunity to leverage their existing networks to the detriment of other potential service providers. In particular, unconditional forbearance of the broadband access services provided by cable broadcast carriers would create both the incentive and opportunity for these carriers to lessen competition and choice in the provision of broadband service that could be made available to the end customer. Safeguards such as rate regulation for broadband access services will be necessary to prevent instances of below cost and/or excessive pricing, at least in the near term.

Telephone companies also have sources of market power that warrant maintaining safeguards against anti-competitive behaviour. For example, telephone companies are still overwhelmingly dominant in the local telephony market, and until this dominance is diminished, it would not be appropriate to forbear unconditionally from rate regulation of broadband access services. (AT&T, p. 15)

In the opinion of AT&T Canada LDS, both the cable companies and the telephone companies have the incentive and opportunity to engage in these types of anticompetitive activities as a result of their vertically integrated structures. For example, cable companies, as the dominant provider of broadband distribution services, would be in a position to engage in above cost pricing in uncontested markets, unless effective constraints are put in place. On the other hand, the telephone company will likely be the new entrant in broadband access services in most areas, and therefore expected to price at or below the level of cable companies. While this provides some assurances that telephone companies are unlikely to engage in excessive pricing, it does not address the incentive and opportunity to price below cost. Accordingly, floor-pricing tests would be appropriate for services of both cable and telephone companies. (AT&T, 16-17)

Furthermore, in the case of both cable and telephone broadcast carriers, safeguards would also need to be established to prevent other forms of discriminatory behaviour and to ensure that broadband access services are unbundled. (AT&T, 17)

38

See, e.g., Lisa Bowman, "Will Merger Shut Lid on Open Access?," ZDNet News, January 11, 2000, available at http://www.zdnet.com/zdnn/stories/news/ 0,4586,2420130,00.html.

39

My point is not that this is the only threat. For example, as Denise Caruso has argued, the merger of Internet backbone providers might lead to a situation where peering on the Internet (exchanging data between peers neutrally) will cease. In "Mergers Threaten Internet's Informal System of Data Exchange," New York Times, February 14, 2000, available at http://www.nytimes.com/library/tech/00/02/biztech/articles/14digi. html, she writes, "In the early days of the Internet, self-interest forced backbone providers into peering [the free sharing of data between service providers] But it is scarcely true today [U]pon completion of the Worldcom-Sprint merger, a single company would control nearly half of the Internet's backbone-making it, literally and figuratively, without peer. Given the furious pace and high stakes of the telecommunications industry today, some fear that it is only a matter of time before one big backbone provider or another refuses to exchange data traffic with one of its peers. What happens then? 'Well, they would have a legitimate excuse,' says Hal Varian, dean of the school of information management at the University of California at Berkeley. 'An ISP could complain, and rightly so, that another ISP was sending them huge amounts of traffic and putting a load on their system That's an excuse to say, 'We can't handle this guy's packets; we aren't going to connect with him.' " Ibid.

40

Timothy F. Bresnahan and Garth Saloner, "Large Firms' Demand for Computer Products and Services: Competing Market Models, Inertia, and Enabling Strategic Change," October 1994 (Research Paper No. 1318 in the Stanford Graduate School of Business Jackson Library).

41

National Research Council, The Internet's Coming of Age (Washington, D.C.: National Academy Press, 2000), chapter 3, 24.

42

Tim Berners-Lee, Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by Its Inventor (San Francisco: HarperSanFrancisco, 1999), 130.

43

See Daniel L. Rubinfeld and Hal J. Singer, "Open Access to Broadband Networks: A Case Study of the AOL/Time Warner Merger," Berkeley Technology Law Journal 16 (2001): 631, 672 ("Our analysis has shown that a policy of conduit discrimination may be profitable post acquisition [and] that content discrimination is likely to be profitable post-acquisition.").

44

See Seth Schiesel, "Cable Giants Block Rival Ads in Battle for Internet Customers," New York Times, June 8, 2001, A1.

Denise Caruso raised concerns about the free speech aspects of this change eighteen months earlier. As she argued, there is an increasing possibility that most of the Internet's content will be controlled by a few-or perhaps even one-large corporations, raising some very troubling issues. "The reasons for urgency are twofold. First is the issue of how to open privately owned broadband Internet access to all comers. In addition, the free-speech issue arises when any single entity, of any size, controls both a transmission medium and the information that flows over it. Open access is a particular concern

Powerful corporations like AT&T and the proposed AOL Time Warner would have the power to balkanize the broadband Internet for their own purposes, with no legal reason to open their networks to competitors."

"Convergence Raises Concerns About Access," New York Times, January 31, 2000, available at http://www.nytimes.com/library/tech/00/ 01/biztech/articles/31digi.html.

45

See Federal Trade Commission, "FTC Approves AOL/Time Warner Merger with Conditions," December 14, 2000, http://www.ftc.gov/opa/2000/12/aol.htm; Federal Trade Commission, Docket No. C-3989. See also John R. Wilke, "AOL and Time-Warner Pledge Cable Access to Ease FTC Fears," Wall Street Journal, December 14, 2000.

46

Or alternatively, a tragedy of an anticommons. The opportunity for any number of players to interfere with open access to the network could be viewed as an anticommons. See Michael A. Heller, "The Tragedy of the Anticommons," Harvard Law Review 111 (1998): 621; James M. Buchanan and Yong J. Yoon, "Symmetric Tragedies: Commons and Anticommons," Journal of Law & Economics 43 (2000): 1.

47

This is not quite the tragedy that Hardin is describing. There is no common physical resource that is being overused. But there is a common virtual resource-the opportunity to innovate freely on the network-that is being misused by restricting the scope of that innovation with respect to one part of the network. That produces an externality, even if it doesn't "overuse" a resource.

48

See Brief of Amici Curiae, Reed Elsevier Inc. et al., 7-9, eBay Inc. v. Bidder's Edge Inc., 100 F. Supp. 2d 1058 (N.D. Cal. 2000) (No. C-99-21200RMW).

49

See Chip Bayers, "The Bot.com Future," Wired (March 2000): 210.

50

The total number of addresses available under Ipv6 is 1038. That is a huge number: "If the address space of IPv4 is compared to 1 millimeter, the address space of IPv6 would be 80 times the diameter of the galactic system." See http://www.wide.ad.jp/wg/ iPv6/index.html.

51

By forcing more traffic over a single port, this arms race interferes with the opportunity to optimize traffic based on ports.

52

The functioning of ports is explained in Craig Hunt, TCP/IP Network Administration, 2nd ed. (Sebastopol, Calif.: O'Reilly, 1998), 42-47. IPSec is described in a series of RFCs, which are available at http://www.ietf.org/html.charters/ipsec-charter.html. I am not claiming IPSec would necessarily be consistent with end-to-end, but simply that it provides a consistent protocol that could be implemented consistent with end-to-end.

53

Garrett Hardin, "The Tragedy of the Commons," Science 162 (1968): 1243, 1244 (emphasis added).

54

National Research Council, 24-25.

55

As Faulhaber put it, "[T]he third view is that, as Adam Smith pointed out, producers are always conspiring about how to fleece the unsuspecting consuming public; it is only the competitive market that keeps their avarice in check. But producers will always search for ways to escape competition, through marketing, customer lock-in, predatory pricing, network effects, etc. AND technological innovation is just a part of this strategic quest for greater profits, perhaps at the consumers' expense."

"Faulhaber Comments at E-2-E Workshop," http://www.law.stanford.edu/e2e/papers.html. Franois Bar has made a similar point: "If the Internet could reduce friction, the same technology can also be deployed to create more of it." Franois Bar, "The Construction of Marketplace Architecture," 5.

56

Charles Platt, "The Future Will Be Fast but Not Free," Wired, May 2001, available at http://www.wired.com/wired/archive/9.05/broadband_pr.html (emphasis added).

CHAPTER 11

1 See 47 U.S.C.223 (Supp. 1996); ACLU v. Reno, 521 U.S. 844 (1997).

2

ACLU v. Reno, 217 F. 3d 162 (3d Cir., 2000).

3

Lawrence Lessig, Code and Other Laws of Cyberspace (New York: Basic Books, 1999), 225.

4

See 17 USCA 106 (2001): "[O]wner of copyright under this title has the exclusive rights (5) in the case of pictorial, graphic, or sculptural works to display the copyrighted work publicly." But the claim would be weak. Except in the most extreme circumstances, the public display of a copyrighted work would be fair use. William Carleton, "Copyright Royalties for Visual Artists: A Display-Based Alternative to the Droit de Suite," Cornell Law Review 76 (1991): 510, 525 ("The general principle that section 109(c) observes, however, according to the House Report, is that 'the lawful owner of a copy of a work should be able to put his copy on public display without the consent of the copyright owner' " [quoting 1976 U.S. Code Cong. & Admin. News 5659, 5693]). See also Ringgold v. Black Entertainment Television, Inc., 126 F. 3d 70 (2nd Cir., 1997).

5

See Kevin V. Johnson, "Show's Fan Sites Fight Off 'Demon;' Fox: Production Company Cites Its Copyrights," USA Today, December 23, 1999, 4D, available at 1999 WL 6862067; and Aaron Barnhart and Kevin V. Johnson, "Twentieth, the Web Slayer: Studio Shifts Its Crusade to 'Buffy' Fans' Web sites," Electronic Media, December 6, 1999, 9, available at 1999 WL 8767348. Fox does not limit itself to the Web. See Twentieth Century Fox Film Corp. v. 316 W. 49th Street Pub. Corp., No. 90 Civ. 6083 (MJL), 1990 WL 165680 (S.D.N.Y., October 23, 1990) (holding that a nightclub could not display _resources/books//The Future of Ideas_ The Fate of the Commo - Lawrence Lessig of The Simpsons on its wall without Fox's permission). Fox says that it "appreciates" fan sites, but not enough to allow them to exist freely. Johnson, 4D. As Warner Bros. Online president Jim Moloshok says, "We decided that we were going to create a better experience for the fans." Ibid.

6

Ibid.

7

Fara Warner et al., "Holes in the Net," Wall Street Journal, August 30, 1999, A1.

8

See http://www.olga.net/about.

9

See, e.g., Siva Vaidhyanathan, Copyrights and Copywrongs: The Rise of Intellectual Property and How It Threatens Creativity (New York: New York University Press, 2001), 355 ("In July 1999, journalist Michael Colton posted an Internet parody of Talk magazine, which is a partnership between Hearst Magazines and Walt Disney-owned Mira-max Films. Miramax lawyers sent a cease-and-desist letter to Earthlink, the Internet company that owned the server on which the parody sat. Earthlink immediately shut down the parody. It only restored the site after Talk editor Tina Brown appealed to the Miramax legal department to let the parody stand. Because of widespread misunderstanding of copyright law, cease-and-desist letters carry inordinate cultural power and can chill if not directly censor expression.").

10

See Digital Millennium Copyright Act, 105 P. L. 304, Sec. 202(c)(1)(iii) (1998).

11

Lessig, 173-75.

12

See Bennett Haselton, "Amnesty Intercepted," December 12, 2000, http://www. peacefire.org/amnesty-intercepted.

13

See Microsystems Software, Inc. v. Scandinavia Online AB, 98 F. Supp. 2d 74 (D. Mass., 2000), aff'd, 226 F. 3d 35 (1st Cir., 2000) (enjoining "all persons in active concert" with Jansson and Skala from "publishing the software source code and binaries known as [CPHack]").

14

See http://www.aclu.org/court/cyberpatrol_motion.html.

15

Sony Computer Entertainment, Inc. v. Connectix Corp, 203 F. 3d 596 (9th Cir., 2000).

16

ProCD, Inc. v. Zeidenberg, 86 F. 3d 1447, 1454 (7th Cir., 1996).

17

See Paul Goldstein, "Copyright and the First Amendment," Columbia Law Review 70 (1970): 983 (describing the ongoing potential of copyright's grant of monopoly over expression as censorship); Pamela Samuelson, "Reviving Zacchini: Analyzing First Amendment Defenses in Right of Publicity and Copyright Cases," Tulane Law Review 57 (1983): 836 (same).

18

For a detailed analysis of the DMCA, see David Nimmer, "A Riff on Fair Use in the Digital Millennium Copyright Act," University of Pennsylvania Law Review 148 (2000): 673 (discussing the formulation, adoption, and practical effect of the DMCA).

19

See Carolyn Andrepont, "Legislative Updates: Digital Millennium Copyright Act: Copyright Protections for the Digital Age," DePaul-LCA Journal of Art & Entertainment Law 9 (1999): 397 (characterizing the DMCA as a necessary tool for protecting on-line copyrighted material); and Michelle A. Ravn, Note, "Navigating Terra Incognita: Why the Digital Millennium Copyright Act Was Needed to Chart the Course of Online Service Provider Liability for Copyright Infringement," Ohio State Law Journal 60 (1999): 755 (arguing that the DMCA was particularly needed to clarify liability for on-line copyright infringements).

20

Or at least not significantly. It is true that if you could play DVD movies on any machine, there would be more machines that might demand DVD content.

21

The MPAA filed suit against four Web site operators in the Southern District of New York and in the District of Connecticut. The DVD Copy Control Association filed suit in California State court against about twenty named defendants and five hundred unnamed ones. For a history of these suits, see Openlaw/DVD: Resources, at http://eon.law.harvard.edu/openlaw/DVD/resources.html (visited April 19, 2001).

22

Universal City Studios, Inc. v. Reimerdes, 82 F. Supp. 2d 211 (S.D.N.Y., 2000) and DVD Copy Control Ass'n, Inc. v. McLaughlin, No. CV 786804, 2000 WL 48512 (Cal. Superior, January 21, 2000).

23

See Michael A. Geist, "iCraveTV and the New Rules of Internet Broadcasting," University of Arkansas at Little Rock Law Review 23 (2000): 223; and John Borland, "On-line TV Service May Spark New Net Battle," CNET.com, at http://news.cnet.com/ news/0-1004-200-1477491.html (last visited April 4, 2001) (describing the launch of the iCraveTV.com Web site). For a scholarly analysis of the case, see Howard P. Knopf, "Copyright and the Internet in Canada and Beyond: Convergence, Vision and Division," European Intellectual Property Review (2000): 262.

24

China Online (visited April 17, 2001), http://www.chinaonline.com (a U.S.-based English-language news site on China). Or alternatively, Human Rights in China (visited April 17, 2001), http://www.hrichina.org (a New York-based Chinese/English-language Web site chronicling Chinese human rights abuses).

25

See Association "Union des tudiants Juifs de France", la "Ligue contre le Racisme et l'Antismitisme" v. Yahoo! Inc. et Yahoo France, T.G.I. Paris, Ordonnance de rfr du 22 mai 2000, available at: http://www.legalis.net/jnet/decisions/responsabilite/ ord_tgi-paris_220500.htm.

26

See Jack L. Goldsmith and Alan O. Sykes, "The Internet and the Dormant Commerce Clause," Yale Law Journal 110 (2001): 785. See also Lawrence Lessig and Paul Resnick, "Zoning Speech on the Internet: A Legal and Technical Model," Michigan Law Review 98 (1999): 395

27

A very similar point is made by Denise Caruso. See "Case Illustrates Entertainment Industry's Copyright Power," New York Times, March 13, 2000, available at http://www. nytimes.com/library/tech/00/03/biztech/articles/13digi.html. As Caruso writes, "The most chilling aspect of the case was [iCraveTV's] response. That is, [it] did not argue the legality of the action against [it], but instead responded by inventing a technology that could stop the discussion dead in its tracks." Caruso continues: "Many people are likely to object strongly to [the site's] balkanized Internet such a system would devolve the Internet into a model very much like the restricted, centralized control of cable television. That is a business model with which the $65 billion media and entertainment industries-the very ones that nearly sued the pants off [of the site]-are quite familiar." Ibid.

28

See Press Release, Recording Industry Association of America, RIAA Statement Concerning MP3.Com's New Services, January 21, 2000, at http://www.riaa.com/ PR_Story.cfm?id=47.

29

The court's ruling in the case determines that the damages are $25,000 per CD. This leads to possible damages of $118 million if the total is determined to be at least 4,700 CDs. UMG Recordings, Inc. v. MP3.Com, Inc., No. 00 CIV. 472(JSR), 2000 WL 1262568, at *6 (S.D.N.Y., 2000).

30

See, e.g., Amy Harmon, "Powerful Music Software Has Industry Worried," New York Times, March 7, 2000, available at http://www.nytimes.com/library/tech/00/03/biztech/ articles/07net.html; Karl Taro Greenfeld, "The Free Juke Box," Time (March 27, 2000), available at http://www.time.com/time/everyone/magazine/sidebar_napster.html; Andy Oram, "Gnutella and Freenet Represent True Technological Innovation," May 12, 2000, at http://www.oreillynet.com/pub/a/network/2000/05/12/magazine/gnutella.html; also Peer-to-Peer: Harnessing the Benefits of a Disruptive Technology, Andy Oram, ed. (Beijing and Cambridge, Mass.: O'Reilly, 2000).

31

Home Recording of Copyrighted Works: Hearing on H.R. 4783, H.R. 4794, H.R. 4808, H.R. 5250, H.R. 5488, and H.R. 5750 Before the Subcomm. on Courts, Civil Liberties, and the Admin. of Justice of the Comm. on the Judiciary, 97th Cong. (2nd session), 8 (1983) (testimony of Jack Valenti, president, Motion Picture Association of America, Inc.). See also Sam Costello, "How VCRs May Help Napster's Legal Fight," Industry Standard, July 25, 2000, available at http://www.thestandard.com/article/ 0,1902,17095,00.html.

32

Universal City Studios, Inc. v. Sony Corp. of Am., 480 F. Supp. 429, 432 (C.D. Cal., 1979) (district court opinion by Judge Ferguson).

33

Ibid., 432.

34

A&M Records, Inc. v. Napster, Inc. 239 F. 3d 1004 (9th Cir., 2001).

35

Harper & Row Publishers, Inc. v. Nation Enterprises, 471 U.S. 539, 588 (1985).

36

Joseph Story, "Commentaries on the Constitution of the United States," R. Rotunda and J. Nowak, eds. (1987), 502, 402.

37

See Testimony of Professor Peter Jaszi, "The Copyright Term Extension Act of 1995: Hearings on S.483 Before the Senate Judiciary Comm.," 104th Cong. (1995), available at 1995 WL 10524355, at *6.

38

Eldred v. Reno, 239 F. 3d 372, 375 (D.C. Cir., 2001).

39

Recording Industry Association of America, 2000 Year-end Statistics (2001), at http://www.riaa.com/pdf/Year_End_2000.pdf.

40

Jeff Leeds, "Album Sales Test the Napster Effect," Los Angeles Times, June 20, 2001, C1.

41

"This case has always been about sending a message to the technology and venture capital communities that consumers, creators and innovators will best flourish when copyright interests are respected." Jim Hu and Evan Hansen, "Record Label Signs Deal with Napster," October 31, 2000, http://news.cnet.com/news/0-1005-200-3345604. html. See also "Online Entertainment: Coming Soon to a Digital Device Near You: Hearing Before the Senate Comm. on the Judiciary," 107th Cong. (2001) (statement of Hilary Rosen, president and CEO, Recording Industry Association of America), available at http://judiciary.senate.gov/te040301hr.htm; Press Release, Record Industry Association of America, Hilary Rosen Press Conference Statement, February 12, 2001, available at http://www.riaa.com/News_Story.cfm?id=371.

42

As Yochai Benkler writes, "[I]ncreases in intellectual property rights are likely to lead, over time, to concentration of a greater portion of the information production function in the hands of large commercial organizations that vertically integrate new production with inventory management." Yochai Benkler, "The Commons as a Neglected Factor of Information Policy," October 3-5, 1998, 74.

Likely, and we might add, have. Compare, as David Isenberg points out, the connection to the history of AT&T: "You can see now in the record industry, for example, that the record companies are unwilling to give up this idea of the control of the physical medium even though they could perhaps do a very good job of artist development." Telephone interview with David Isenberg, February 14, 2001.

43

Telephone interview with Michael Robertson, November 16, 2000.

44

Ibid.

45

See also Richard Watt, Copyright and Economic Theory: Friends or Foes? (Cheltenham, England, and Northampton, Mass.: E. Elgar, 2000), 161-200 (describing role of cooperatives and collectives). For a more comprehensive analysis of compulsory rights, see generally Marshall Leaffer, Understanding Copyright Law, 3rd ed. (New York: M. Bender, 1999), 69-71 (detailing both support for and criticism of compulsory licenses), cited in Michael Freno, note, "Database Protection: Resolving the U.S. Database Dilemma with an Eye Toward International Protection" 34 (2000), Cornell International Law Journal 34: 165, 209 (promoting compulsory licenses in the context of databases); Christopher Scott Harrison, comment, "Protection of Pharmaceuticals as Foreign Policy: The Canada-U.S. Trade Agreement and Bill C-22 Versus the North American Free Trade Agreement and Bill C-91" (2001), North Carolina Journal of International Law and Communications Regulation 26: 457, 525 (advocating generally the free distribution of compulsory licenses); Anthony Reese, "Copyright and Internet Music Transmissions: Existing Law, Major Controversies, Possible Solutions" (2001), University of Miami Law Review 55: 237, 270 (arguing for the extension of compulsory licenses in the area of music on the Internet); Bess-Carolina Dolmo, note, "Examining Global Access to Essential Pharmaceuticals in the Face of Patent Protection Rights: The South African Experience" (2001), Buffalo Human Rights Law Review 7: 137 (explaining the benefits of compulsory licenses for developing countries); Sheldon W. Halpern, "The Digital Threat to the Normative Role of Copyright Law" (2001), Ohio State Law Journal 62: 569, 593 (supporting compulsory licensing for digital _resources/books//The Future of Ideas_ The Fate of the Commo - Lawrence Lessig); Laura N. Gasaway, "Impasse: Distance Learning and Copyright" (2001), Ohio State Law Journal 62: 783 (questioning the practicality of compulsory licenses); Robert P. Merges, "One Hundred Years of Solicitude: Intellectual Property Law, 1900-2000," (2000), California Law Review 88: 2187, 2194, n. 15 (criticizing compulsory licenses in one context); Robert P. Merges, "Contracting into Liability Rules: Intellectual Property Rights and Collective Rights Organizations," California Law Review 84: 1293, (arguing against compulsory licenses for digital content); Ralph Oman, "The Compulsory License Redux: Will It Survive in a Changing Marketplace?" (1986), Cardozo Arts & Entertainment Law Journal 5: 37, 48 (noting that many actors in the area of intellectual property prefer private solutions over compulsory licenses); Scott L. Bach, note, "Music Recording, Publishing, and Compulsory Licenses: Toward a Consistent Copyright Law," Hofstra Law Review 14: 379, 398-401 (arguing that compulsory licenses are unfair to many artists, discourage innovation, and may be unconstitutional under the copyright clause).

46

For a similar argument, see Raymond Shih Ray Ku, "Copyright & Cyberspace: Napster and the New Economics of Digital Technology" (draft on file with author, April 7, 2001) ("[C]yberspace and the economics of digital technology require the un-bundling of the public's interests in the creation and distribution of digital works.").

47

Christopher Stern, "Napster Copyright Fight Goes to Hill," Washington Post, April 4, 2001, E03.

48

Vaidhyanathan, 14.

49

Richard A. Posner, Law and Literature, rev. and enlarged ed. (Cambridge, Mass.: Harvard University Press, 1998), 392.

50

Ibid., 396. See also William M. Landes and Richard A. Posner, "An Economic Analysis of Copyright Law," Journal of Legal Studies 18 (1989): 325.

51

For the history of Judge Kozinski, see Susan Rice's "Profile," in Los Angeles Daily Journal, September 29, 1988, 1. See also History of the Federal Judiciary (Washington, D.C.: Federal Judicial Center, 2001) (bio of Alex Kozinski from Federal Judges Biographical Database), at http://air.fjc.gov/history/judges_frm.html.

52

Vanna White v. Samsung Elecs. Am., Inc.; David Deutsch Assocs., 989 F. 2d 1512, 1514 (1993).

53

Ibid., 34.

54

Ibid., 27.

55

Ibid.

56

Ibid.

57

Ibid., 31.

58

Ibid., 27.

59

Ibid., 30.

60

I do not address the reach of trademark law in this area, but it would only strengthen the argument I am making. Unlike traditional intellectual property, trademarks are perpetual, and their effective power has expanded dramatically. This has become especially significant as the domain name system has had to deal with the conflict between trademarks and domain names. This has tempted the World Intellectual Property Association to build control for trademark interests into the very architecture of the network. See 23-28, "Executive Summary of the Interim Report of the Second WIPO Internet Domain Name Process," available at http://wipo2.wipo.int.

61

The origin of modern economic work here is Kenneth Arrow's "Economic Welfare and the Allocation of Resources for Invention," in National Bureau Committee for Economic Research, The Rate and Direction of Inventive Activity, Economic and Social Factors, Richard Nelson, ed. (Princeton, N.J.: Princeton University Press, 1962), 609. Harold Demsetz responded to this by arguing in favor of a stronger property-based regime. See Harold Demsetz, "Information and Efficiency: Another Viewpoint," Journal of Law & Economics 12 (1969): 1. On the question of optimal protection, see Richard Gilbert and Carl Shapiro, "Optimal Patent Length and Breadth," RAND Journal of Economics 21 (1990): 106.

62

For a careful account of the Framers' view of the patent power, see Edward C. Walterscheid, "Patents and the Jeffersonian Mythology," John Marshall Law Review 29 (1995): 269. Professor Pollack makes a persuasive argument that the conception is limited by Lockean conceptions of the property right. See Malla Pollack, "The Owned Public Domain: The Constitutional Right Not to Be Excluded-or the Supreme Court Chose the Right Breakfast Cereal in Kellogg v. National Biscuit Co.," Hastings Communications & Entertainment Law Journal 22 (2000): 265. For an introduction to the rationale, see Richard A. Posner, Economic Analysis of Law, 4th ed. (Boston: Little, Brown, 1992), 38-41.

63

Steven Shavell and Tanguy van Ypersele, "Rewards Versus Intellectual Property Rights" (NBER Working Paper No. 6956, February 1999), 27. Shavell and Ypersele suggest a reward system to replace a patent system. A similar proposal has been made by Michael Kremer, "Patent Buy-Outs: A Mechanism for Encouraging Innovation" (NBER Working Paper No. 6304, December 1997). Chicago professor Douglas Lichtman has a related proposal to subsidize access to patented drugs. See Douglas Lichtman, "Pricing Prozac: Why the Government Should Subsidize the Purchase of Patented Pharmaceuticals," Harvard Journal of Law & Technology 11 (1997): 123. For criticism of the reward alternative, see F. Scott Kieff, "Property Rights and Property Rules for Commercializing Inventions," Minnesota Law Review 85 (2001): 697, 709-21.

64

Though the economic argument about the effect of patents on innovation remain ambiguous at best. See, e.g., Roberto Mazzoleni and Richard R. Nelson, "Economic Theories About the Benefits and Costs of Patents," Journal of Economic Issues 32 (December 1998): 1031.

65

Adam B. Jaffe, "The U.S. Patent System in Transition: Policy Innovation and the Innovation Process" (NBER Working Paper Series, August 1999), 24-25.

66

Ibid., 26.

67

Ibid.

68

A similar skepticism has been raised about strong property rights where network externalities are strong. See Pamela Samuelson et al., "Manifesto Concerning the Legal Protection of Computer Programs," Columbia Law Review 94 (1994): 2308, 2375, citing Joseph Farrell, "Standardization and Intellectual Property," Jurimetrics Journal 30 (1989): 35, 36-38, 45-46 (discussing network effects).

69

See, e.g., Josh Lerner, "150 Years of Patent Protection" (NBER Working Paper No. 7477, January 2000) (examines 177 policy changes in the strength of protection across sixty countries, over a 150-year period, and concludes that strengthening patent protection had few positive effects on patent applications in the country making the policy change); Mariko Sakakibara and Lee Branstetter, "Do Stronger Patents Induce More Innovation? Evidence from the 1988 Japanese Patent Law Reforms," Rand Journal of Economics 32 (2001): 77 (authors find no evidence of an increase in R&D spending or innovative output). For a long-standing source of skepticism about the effect of strong patents on innovation, see Robert P. Merges and Richard R. Nelson, "On the Complex Economics of Patent Scope," Columbia Law Review 90 (1990): 839 (strong patent assertions in electrical lighting, automobiles, airplanes, radio slowed down innovation).

70

See, e.g., Jean O. Lanjouw, "The Introduction of Pharmaceutical Product Patents in India: 'Heartless Exploitation of the Poor and Suffering'?" (NBER Working Paper No. 6366, 1998); F. Scott Kieff, "Property Rights and Property Rules for Commercializing Inventions," Minnesota Law Review 67 (2001): 727-28. For an excellent and comprehensive account of the actual patenting practice, see John R. Allison and Mark A. Lemley, "Who's Patenting What? An Empirical Exploration of Patent Prosecution," Vanderbilt Law Review 53 (2000): 2099, 2146; John R. Allison and Mark A. Lemley, "How Federal Circuit Judges Vote in Patent Validity Cases," Florida State University Law Review 27 (2000): 745, 765 (concluding no systematic bias in judges' votes).

71

Jaffe, 46.

72

Ibid., 47. Jaffe's argument here is narrower than the point I am making in this section. His concern is the social costs from too much effort being devoted to the pursuit of patented innovation. My concern is the cost of patents on the innovation process generally.

73

"Patently Absurd?" The Economist (June 21, 2001). As the article goes on to report, interviewees from patenting firms indicated that "rather than patenting to win exclusive rights to a valuable new technology, patents were filed more for strategic purposes."

74

Benjamin Franklin, "having no desire of profiting by patents myself," cited "a principle which has ever weighed with me [t]hat, as we enjoy great advantages from the inventions of others, we should be glad of an opportunity to serve others by any invention of ours; and this we should do freely and generously." Benjamin Franklin, The Autobiography of Benjamin Franklin, Frank Woodworth Pine, ed. (Garden City, N.Y.: Garden City Publishing Co., 1916 ed.; originally published 1793), 215-16.

75

George Washington Carver, for example, observed about his inventions that "God gave them to me, how can I sell them to someone else?"

"Inventors," Atlanta Journal & Constitution, February 12, 1999, A17.

76

Robert K. Merton, "A Note on Science and Democracy," Journal of Law & Political Sociology 1 (1942): 115, 123. ("Patents proclaim exclusive rights of use and, often, nonuse. The suppression of invention denies the rationale of scientific production and diffusion.") As Professor Arti Rai describes it, "[T]raditional scientific norms promote a public domain of freely available scientific information, independent choice in the selection of research topics, and (perhaps above all) respect for uninhibited scientific invention." Arti Kaur Rai, "Regulating Scientific Research: Intellectual Property Rights and the Norms of Science," Northwestern University Law Review 94 (1999): 77, 89-90.

More famously, Merton is known for characterizing "science" as "communistic":

"Communism," in the nontechnical and extended sense of common ownership of goods, is a second integral element of the scientific ethos. The substantive findings of science are a product of social collaboration and are assigned to the community. They constitute a common heritage in which the equity of the individual producer is severely limited. An eponymous law or theory does not enter into the exclusive possession of the discoverer and his heirs, nor do the mores bestow upon them special rights of use and disposition. Property rights in science are whittled down to a bare minimum by the rationale of the scientific ethic. The scientist's claim to "his" intellectual "property" is limited to that of recognition and esteem which, if the institution functions with a modicum of efficiency, is roughly commensurate with the significance of the increments brought to the common fund of knowledge. Eponymy-for example, the Copernican system, Boyle's law-is thus at once a mnemonic and a commemorative device.

Merton, 121.

77

Fred Warshofsky, The Patent Wars (New York: Wiley, 1994), 170. Of course, I don't mean to suggest that Microsoft is against software patents. Indeed, in the same memo, Gates goes on to recommend the Microsoft strategy to respond to this new world of patents:

The solution is patent exchanges and patenting as much as we can A future start-up with no patents of its own will be forced to pay whatever price the giants choose to impose. That price might be high: Established companies have an interest in excluding future competitors.

Ibid., at 170-71 (emphasis added).

78

Thomas Jefferson, Letter to James Madison, in Julian P. Boyd, ed., The Papers of Thomas Jefferson 13 (Princeton, N.J.: Princeton University Press, 1956), 440, 442-43.

79

To qualify for patent protection, an invention must be novel, 35 U.S.C. 101, 102 (1982); it must provide utility, ibid.; and it must be nonobvious, ibid., 103. The invention must also be within the list of patentable subject matter, 35 U.S.C. 101 (1982).

80

Jaffe, 9. The changes have also coincided with an increase in the number of patents. On one account, that increase may be because patents are indeed a spur to innovation. Researchers, however, have concluded differently. Samuel Kortum and Josh Lerner, "Stronger Protection or Technological Revolution: What Is Behind the Recent Surge in Patenting?," Carnegie-Rochester Conference Series on Public Policy 48 (1998): 247 (attributing the increase to improvements in the management of research); Bronwyn H. Hall and Rosemarie Ham Ziedonis, "The Patent Paradox Revisited: An Empirical Study of Patenting in the U.S. Semiconductor Industry, 1979-1995," Rand Journal of Economics 32 (2001): 101 (attributing the increase to portfolio races).

81

E-mail from Greg Aharonian, author, Internet Patent News Service, May 28, 2001, on file with author.

Greg Aharonian is perhaps the leading expert on the practice of software and Internet-related patents. While he is a supporter of patents in principle, he has been a strong critic of the U.S. Patent Office. Aharonian estimates the number of software patents in a number of ways. He provided the following data to me:

"TOTAL is the total number of patents issued that year, GREG is my count or estimated count of software patents (using the Greg Aharonian scheme) issued in that year. SOFTWARE is the number of patents in that year that include the word software some-where in the specification, claims, or abstract. 364&395 is the total number of patents issued in these two main computing classes."

YEAR TOTAL GREG SOFTWARE (364&395)

1999 154,534 21,000 (est.) 17,603 6,410 1998 150,961 17,500 (est.) 16,100 10,571 1997 124,181 13,000 (est.) 10,017 8,190 1996 121,799 9,000 9,104 7,922 1995 113,941 6,142 6,951 6,114 1994 113,706 4,569 6,009 5,745 1993 109,876 2,400 4,929 4,862 1992 107,489 1,624 4,068 4,073 1991 106,831 1,500 3,543 3,817 1990 99,210 1,300 3,046 3,606 1989 102,686 1,600 3,090 3,980 1988 84,433 800 2,053 2,708 1987 89,578 800 2,038 2,766 1986 77,039 600 1,483 2,202 1985 77,268 500 1,324 1,978 1984 72,668 400 1,003 1,857 1983 62,005 350 635 1,517 1982 63,291 300 603 1,446 1981 71,105 300 544 1,257 1980 66,206 250 465 1,239 1979 52,480 200 269 986 1978 70,564 150 299 1,272 1977 69,797 100 300 1,320 1976 70,924 100 208 1,113 1975 72,156 100 188 817 1974 62,342 100 188 838 1973 61,019 150 122 871 1972 58,603 200 110 906 1971 50,904 100 68 896

2,537,596 33,635 96,360 91,279

82

Seth Shulman, Owning the Future (Boston: Houghton Mifflin, 1999), 69.

83

Douglas Brotz made this statement at the Public Hearing on Use of the Patent System to Protect Software Related Inventions, January 26, 1994, at the San Jose Convention Center, transcript available at http://lpf.ai.mit.edu/Patents/testimony/statements/ adobe.testimony.html.

84

"Oracle Corporation opposes the patentability of software." Statement available at http://www.base.com/software-patents/statements/oracle.statement.html (visited June 8, 2001).

85

Patent law has long protected process patents. Mark A. Lemley, "Patent Scope and Innovation in the Software Industry," California Law Review 89 (2001): 1, 8.

86

Rochelle Cooper Dreyfuss, " State Street or Easy Street: Is Patenting Business Methods Good for Business?," U.S. Intellectual Property: Law & Policy 6 (2000): 27.

3 2 0 N O T E S

87

State St. Bank & Trust Co. v. Signature Fin. Group, Inc., 149 F. 3d 1368 (Fed. Cir., 1998).

88

Carl Shapiro, "Navigating the Patent Thicket: Cross Licenses, Patent Pools, and Standard-Setting" (working draft paper presented at National Bureau of Economic Research annual conference, May 2000): 2.

89

See Laura Rohde, "BT Flexes Muscles Over U.S. Hyperlink Patent," June 21, 2000, available at http://www.idg.net/idgns/2000/06/21/BTFlexesMusclesOverUSHyperlink. shtml. The Amazon patent has received increasingly skeptical review in the courts. After a district court enjoined Barnes & Noble from using a similar technology, Amazon. com v. Barnesandnoble.com, 73 F. Supp. 2d 1228 (W.D. Wash., 1999), the Court of Appeals for the Federal Circuit reversed the injunction, finding that Barnesandnoble. com had mounted a substantial challenge to the validity of the patent. Amazon.com v. Barnesandnoble.com, 239 F. 3d 1343 (CAFC, 2001).

90

James Bessen and Eric Maskin, "Sequential Innovation, Patents, and Imitation," January 2000, available at http://researchoninnovation.org/patent.pdf (visited on June 10, 2001). See also Patently Absurd: "[T]he rush to acquire patent portfolios could slow down the generation of new ideas."

91

See Edmund W. Kitch, "The Nature and Function of the Patent System," Journal of Law & Economics 20 (1977): 265 (advocating prospect theory). See also Merges and Nelson, 839 (propounding a race-to-invent theory; lethargy in patent development will lead to a problem of "under-fishing"; thus, where innovation is cumulative, narrow patents are better). See also Mark F. Grady and Jay I. Alexander, "Patent Law and Rent Dissipation," Virginia Law Review 78 (1992): 305 (arguing that patents reduce rent dissipation from races to initial invention, races to improvements, and wasteful efforts to keep secrets). For a critique of Grady and Alexander, see Robert P. Merges, "Rent Control in the Patent District: Observations on the Grady-Alexander Thesis," Virginia Law Review 78 (1992): 359.

92

As Professor Julie Cohen describes it:

Kitch based his "prospect" theory on an analogy to nineteenth-century mining claims, which reserved for first-comers all rights to explore the described terrain. Under the prospect theory of patent scope, issued patents would operate as broad reservations of rights in the technical landscape. As a result, patentees could credibly seek to exact royalties for nearly all improvements, whether literally infringing or not. Improvers, meanwhile, would need to think twice before refusing such demands. To a greater degree than ever before, second-comers would need permission to develop and market their innovations.

Julie E. Cohen and Mark A. Lemley, "Patent Scope and Innovation in the Software Industry," California Law Review 89 (2001): 1, 14-15.

93

Ibid., 15. ("It assumes that owners can readily identify, and would readily license, successful improvers; that the gains from coordination would outweigh the costs of any strategic behavior by owners and improvers; and that the initial allocation of stronger property rights to the prospect owner would not adversely affect improvers' incentives [or that an overall increase in productivity would outweigh any such adverse effect].")

94

Cf. Rai, 121 (criticizing patents because of the "losses in creativity and high transaction costs that such grants generate").

95

This point is not specific to the Internet or to software technologies. Professor Arti Rai has made a similar point in the context of basic scientific research:

Bargaining between the patent holder and improver would face a variety of obstacles. First, there would be very substantial difficulties in valuation. It is by no means clear how one goes about valuing an EST of unknown function, let alone uncertain but potentially highly lucrative research using that EST. Disagreement about the value of the patented invention relative to the value of the research project might make it very difficult for the parties to agree on the terms for a license. This disagreement might well be exacerbated by cognitive biases that could lead the EST patent owner to overvalue its asset. In particular, the owner might overestimate the possibility that the EST would lead to the finding of a valuable drug therapy and then negotiate based on this irrational estimation. Negotiation would also be hampered by Arrow's information paradox: it might be impossible for the subsequent researcher to get a license ex ante without disclosing to the patent holder valuable information about its own research plans. But because this research plan would not be protectable as intellectual property, the competitor might fear that the patent holder would appropriate the information for its own use, with no compensating benefit to the competitor. Even if these difficulties did not lead to bargaining breakdown, they would create transaction costs that reduced the cooperative surplus to be gained from a license and would thus deter at least some inventors and improvers from negotiating in the first instance. Transaction costs would be compounded by the likelihood that the would-be follow-on improver would likely have to negotiate licenses not simply with one owner of basic research but with many such owners. For example, in order to develop a commercial treatment for a genetic disease (particularly a polygenic disease), it may be necessary to have access to a large number of ESTs and SNPs, each conceivably patented by a different entity. Similarly, in order to screen potential pharmaceutical products for therapeutic effects and side-effects at the pre-clinical testing stage, it may be very useful to have access to a large number of different receptors, each potentially controlled by a different owner. Some of these difficulties might be addressed if the bargaining occurred ex post, after the follow-on improver had already developed the improvement. In that case, both the initial inventor and the follow-on improver would have greater knowledge of the value of the improvement relative to that of the original invention. Moreover, in the ex post situation, the possibility of "blocking patents" might provide a way around Arrow's information paradox. Blocking patents arise when a follow-on improver takes a patented product and improves it in a nonobvious way. Although the follow-on improver can then secure a patent on that improvement, the improvement may nonetheless infringe the original patent. In the blocking patent situation, neither the initial patent holder nor the follow-on improver can sell the improvement without cross-licensing. But, if the improvement is truly significant, such that the initial inventor would want to sell the improvement itself, it would presumably have a financial incentive to cross-license the improver.

Ibid., 125-26. Rebecca Eisenberg makes a similar point: "Michael Heller and I argue that too many patent rights on 'upstream' discoveries can stifle 'downstream' research and product development by increasing transaction costs and magnifying the risk of bargaining failures." Rebecca S. Eisenberg, "Bargaining over the Transfer of Proprietary Research Tools," 224. See also Robert P. Merges, "Institutions for Intellectual Property Transactions: The Case of Patent Pools," in Expanding the Boundaries of Intellectual Property, Rochelle Cooper Dreyfuss and Diane Leenheer Zimmerman, eds. (Oxford: Oxford University Press, 2001), 127-28.

96

Bessen and Maskin, "Sequential Innovation, Patents, and Imitation."

97

Steven Levy, "The Great Amazon Patent Debate," Newsweek (March 13, 2000): 74

("I asked Bezos if Amazon would have developed 1-Click even if there were no patent system to protect it and anyone could legally rip it off. 'Yes,' he responded without hesitation. 'Very definitely.' "). This point suggests a related reason to be skeptical about these patents. Patents can induce overinvestment in patent protection; the low additional cost to get the protection may induce too much patent duplication. See Posner, Economic Analysis of Law, 39.

98

For skepticism about software patents, see, e.g., Pamela Samuelson, "Benson Revisited: The Case Against Patent Protection for Algorithms and Other Computer Program-Related Inventions," Emory Law Journal 39 (1990): 1025; A. Samuel Oddi, "Beyond Obviousness: Invention Protection in the Twenty-first Century," American University Law Review 38 (1989): 1097 (arguing that patents should be issued only for major innovations); Pamela Samuelson et al., 2308. See also Cohen and Lemley (arguing for narrowing to permit reverse engineering). On business method patents, see, e.g., "One-click Monster," American Lawyer (May 2000): 51; Rochelle Cooper Dreyfuss, "Are Business Method Patents Bad for Business?," Computer & High Technology Law Journal 16 (2000): 263; William Krause, "Sweeping the E-Commerce Patent Minefield: The Need for a Workable Business Method Exception," Seattle University Law Review 24 (2000): 79; Jared Earl Grusd, "Internet Business Methods: What Role Does and Should the Law Play?," Virginia Journal of Law & Technology 4 (1999): 9.

99

"In the United States, despite the long-standing controversy around software patents, there has been virtually no government effort to study the economic effects of expanded patent protection. The one government-commissioned study of which I am personally aware was suspended at the request of a multinational company with a unique position in software patents." Brian Kahin, comments in response to "The Patentability of Computer-Implemented Inventions," available at http://europa.eu.int/comm/ internal_market/en/intprop/indprop/maryland.pdf. The National Academies has launched a study, "Intellectual Property Rights in the Knowledge-Based Economy," http://www. nationalacademies.org/ipr.

100

See "Internet Society Panel on Business Method Patents," http://www/oreillynet. com/lpt/a/434 ("Property, the first of the three debates I argued, I would argue is beyond reproach and the burden of proof is not on those who would need to say property should be but on those who say property should not be because historically societies that did not respect property rights, all rights, ended up in the dust bin of history." [Jay Walker]).

101

Friedrich A. von Hayek, "Free Enterprise and Competitive Order," in Individualism and Economic Order (Chicago: University of Chicago Press), 107, 114. Nobel Prize-winning economist Milton Friedman expressed similar skepticism. "[T]rivial patents are often used as a device for maintaining private collusive arrangements that would otherwise be more difficult or impossible to maintain." Milton Friedman, Capitalism and Freedom (Chicago: University of Chicago Press, 1962), 127.

102

Lest too much of my own bias become apparent, I should note that there are some who argue strongly (and in some ways convincingly) that between a well-functioning copyright and well-functioning patent system, a patent system for software may well be better. As Mark Haynes argues, "[U]nlike copyright, the patent system encourages improvement patents, through which competitors are able to neutralize the patent portfolios of others." Mark A. Haynes, commentary, "Black Holes of Innovation in the Software Arts," Berkeley Technology Law Journal 14 (1999): 567, 574. Likewise, as Mark Lemley and David O'Brien have argued, patents would encourage the deployment of reusable software "components." Mark A. Lemley and David W. O'Brien, "Encouraging Software Reuse," Stanford Law Review 49 (1997): 255. Thus, conceivably, a patent system would encourage innovation. And Professor Scott Kieff has argued quite convincingly that patents spur not just innovation, but also commercialization. See F. Scott Kieff, "Property Rights and Property Rules for Commercializing Inventions," Minnesota Law Review 85 (2001): 697. Commercialization, Kieff argues, depends upon property rules protecting invention, not liability rules. Compare Ian Ayres and Paul Klemperer, "Limiting Patentees' Market Power Without Reducing Innovation Incentives: The Perverse Benefits of Uncertainty and Non-Injunctive Remedies," Michigan Law Review 97 (1999): 985 (arguing for liability, rather than property protection, for patent rights).

I'm skeptical, but not a committed skeptic. These accounts don't (and don't purport to) account for the full costs of a patent system, or the particular costs that would affect software developers. Nor do they deny the burdens imposed by the current patent system. Thus, while such a benefit is possible, that it is possible in the current regime has not, in my view, been shown.

103

Telephone interview with Bob Young, November 14, 2000. See also Kahin, 3-4 ("At the level of individual patents, patents may benefit small firms more than large firms because small-firm options for appropriating returns from innovation are fewer. For example, small firms may be less able to exploit first-mover advantages. At the portfolio level, large firms with large portfolios benefit disproportionately from network effects and economies of scale and scope. This includes their ability to manage transaction costs, which is the subject of the third perspective that I will explore at greater length."); Kahin, 4 ("Small firms [with some exceptions] are generally disadvantaged because they lack in-house patent counsel and their business focus can be easily distracted by litigation or even claims of infringement.").

104

"In the short run, individual patents work to [the benefit of small firms], while in the long run and in the aggregate, patents favor large firms. The more pervasive patenting becomes, the more the long-term, portfolio-level effects dominate." Kahin, 4.

Patents have also been said to be a useful tool for old companies to protect themselves against the new. As Gary Reback, Silicon Valley entrepreneur and attorney, describes the history in the Valley, "[W]e went through a long period like that where I saw companies like IBM going around Silicon Valley leaning on the new generation of companies like Sun to develop a revenue stream out of their patent portfolio." Telephone interview with Gary Reback, November 21, 2000.

105

Tim Berners-Lee, Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by Its Inventor (San Francisco: HarperSanFrancisco, 1999), 196.

106

Richard Stallman, "The GNU Operating System and the Free Software Movement," in Open Sources: Voices from the Open Source Revolution, Chris DiBona, Sam Ockman, and Mark Stone, eds. (Beijing and Sebastopol: O'Reilly, 1999), 53, 67.

107

Robert Young and Wendy Goldman Rohm, Under the Radar: How Red Hat Changed the Software Business-and Took Microsoft by Surprise (Scottsdale, Ariz.: Coriolis Group Books, 1999), 135-36.

108

Peter Wayner, Free for All: How Linux and the Free Software Movement Undercut the High-Tech Titans (New York: HarperBusiness, 2000), 223-24.

109

Kahin, 5.

110

Telephone interview with Bob Young.

111

Ian Mount, "Would You Buy a Patent License from This Man?," eCompany, April 2001, available at http://www.ecompany.com/articles/mag/0,1640,9575,00.html.

112

Carl Shapiro, "Navigating the Patent Thicket: Cross Licenses, Patent Pools, and Standard-Setting," in Innovation Policy and the Economy, vol. 1, Adam Jaffe, Joshua Lerner, and Scott Stern, eds. (Cambridge, Mass.: MIT Press, 2001), 8. Shapiro recommends that antitrust authorities permit packaging licensing for complementary, but not substitute, patents, as a way to reduce the transaction costs associated with the hold-up problem created by patents.

113

C. Shapiro, 7. A related argument is summarized by Denise Caruso. As she argues in "Patent Absurdity," New York Times, February 1, 1999, available at http://www.nytimes. com/library/tech/99/02/biztech/articles/01digi.html, "Ideas are given their literal currency through patent and copyright laws, originally intended to stimulate innovation by protecting inventors from idea snatchers and allowing them to profit more easily from their talents. But some experts worry that an increasing number of individuals and companies are perverting that original purpose with increasingly specious claims to ownership, as well as by stockpiling patents into competitive arsenals." Ibid. Caruso identifies such things as "[a] patent for using a book as a teaching tool" and "a patent for downloading files over the Internet for a fee" as some of the more ridiculous recent developments in intellectual property law and adds that "various technology companies, including IBM, Intel and Hewlett-Packard, use their vast holdings of patents as competitive weaponry in seeking to disable each other with infringement charges Today's relaxed rules for granting patents, and the greater ease with which arsenals can thus be amassed, gives a decided battle advantage to industry heavyweights In today's business environment, in which a company's market value is measured with increasing frequency by the intellectual property it owns, arsenals of patents-specious or not-make an unfortunate kind of sense." Ibid.

114

Warshofsky, The Patent Wars, 170-71 (emphasis added).

115 Michael A. Heller, "The Tragedy of the Anticommons: Property in the Transition from Marx to Markets," Harvard Law Review 111 (1998): 621. The general issue of patents in sequential, or cumulative, innovation is addressed by Suzanne Scotchmer, "Standing on the Shoulders of Giants: Cumulative Research and the Patent Law," Journal of Economic Perspectives 5 (Winter 1991). James Bessen and Eric Maskin argue that this is precisely the kind of innovation that is harmed the most by strong patent protection. See Bessen and Maskin. See also Merges, Institutions for Intellectual Property Transactions, 125.

116

James M. Buchanan and Yong J. Yoon, "Symmetric Tragedies: Commons and Anticommons," Journal of Law & Economics 43 (2000): 1.

117

I've not discussed two other critical aspects of intellectual property law-either trade secret law or trademark law. Both are relevant to the issues of control that I have described, and trademark law in particular has expanded significantly. See, e.g., Alanna C. Rutherford, "Sporty's Farm v. Sportsman's Market: A Case Study In Internet Regulation Gone Awry," Brooklyn Law Review 66 (2000): 421, 437 (describing the proliferation of trademarks on the Net and its undermining of the original justification for trademark law); Matthew A. Kaminer, "The Limitations of Trademark Law in Addressing Trademark Keyword Banners," Santa Clara Computer & High Technology Law Journal 16 (1999): 35, 61-62 ("We should not contain the growth of the Internet, but instead support it") (quote on 62); Glenn A. Gunderson, "Expansion of Trademark Law Yields Trickier Search: Development of Unusual Marks, Dilution Law and the Internet Complicate Clearance Process," National Law Journal, May 31, 1999, C9 (describing the near doubling of trademark applications from 1990 to 1998), cited in Kathleen Donohue, "Trademark Vigilance in the Twenty-first Century: A Pragmatic Approach," Fordham Intellectual Property Media & Entertainment Law Journal 9 (1999): 823, 828, n.18; Claire Ann Koegler, "Here Come the Cybercops 2: Who Should Police Cybermarks?," Nova Law Review 22 (1998): 531, 532-33 (explaining how trademark law has greatly expanded in the context of the Internet).

CHAPTER 12

1 Thomas W. Hazlett, "Spectrum Flash Dance: Eli Noam's Proposal for 'Open Access' to Radio Waves," Journal of Law & Economics 41 (1998): 805.

2

See Hazlett, "The Wireless Craze," 17 ("Allocation and 'technical' rules protected broadcasters from competition as well as from fees or competitive bidding"). The examples include AM radio's protection from FM, ibid., 50-53; the artificial scarcity in VHF-TV licenses, which produced just three national networks, ibid., 53-55; VHF-TV blocking CATV, ibid., 55-62; AM and FM radio's blocking of digital radio, ibid., 62-65; NAB and NPR's blocking of low-power FM radio, ibid., 68-82.

3

Eli Noam, "Beyond Spectrum Auctions: Taking the Next Step to Open Spectrum Access," Telecommunications Policy 21 (1997): 461, 462, n. 5.

4

Ibid. How this market system might work is not yet clear. Presumably, users would bid in real time to get access to available spectrum. Bidding in turn would require that users have some form of identification. The ID need not be a real identity. But every transaction has a name. You generate less overhead validating every request if laying unknown bids elsewhere is allowed. How those requests are handled would determine how efficiently the system could work.

5

See, e.g., Yochai Benkler, "From Consumers to Users: Shifting the Deeper Structures of Regulation Toward Sustainable Commons and User Access," Federal Communications Law Journal 52 (2000): 561; Eli Noam, "Spectrum Auctions: Yesterday's Heresy, Today's Orthodoxy, Tomorrow's Anachronism," Journal of Law & Economics 41 (1998): 765; Yochai Benkler, "Overcoming Agoraphobia: Building the Commons of the Digitally Networked Environment," Harvard Journal of Law & Technology 11 (1997): 287.

6

See Hazlett, "Spectrum Flash Dance"; Gregory L. Rosston and Jeffrey S. Steinberg, "Using Market-Based Spectrum Policy to Promote the Public Interest," Federal Communications Law Journal 50 (1996): 87.

7

Turner Broadcasting System, Inc. v. F.C.C., 520 U.S. 180, 189 (1997). I am assuming (and I would argue) that ordinary First Amendment analysis should apply to the rules allocating spectrum. That assumption, however, is not obvious. You might take the view that spectrum is just like paper (both are used to communicate), and there's not a First Amendment problem with the government's nationalizing paper production. Cf. Ars-berry v. Illinois, 244 F. 3d 558 (7th Cir., 2001) (examining tax as applied to the telephone company).

But in my view, the appropriate analysis begins with the speaker, who would speak using spectrum but for regulations by Congress of that spectrum. That is the same posture the Court has adopted when explaining why regulation of cable television is subject to First Amendment analysis. In both cases, the issue is properly framed: What justifies the state-imposed interference with the speaker's ability to communicate? That question, I expect, will be resolved through the same level of analysis as applied to cable. This is because, as Benkler writes, "enclosure of information affects different organizations engaged in information production differently. This is so because information is not only an output of information production, but also one of its most important inputs." Yochai Benkler, "The Commons as a Neglected Factor of Information Policy," (paper presented at the Telecommunications Policy Research Conference (October 5, 1998): 70. Thus, "the availability of a commons creates incentives that make possible decentralization of content-production." Benkler, "The Commons," 68.

8

"These units are so small as to make the transaction costs involved in negotiating allocation of exclusive property rights to them prohibitive." Benkler, "Overcoming Agoraphobia," 174.

9

See, e.g., Andy Kessler, velcap.com, "Steal This Bandwidth!," e-mail on file with author, June 19, 2001 ("The FCC should set aside some not-for-profit spectrum specifically for wireless access, probably at the same time they auction 3G licenses, and keep encouraging the grassroots to run with new technology.").

10

Telephone interview with Alex Lightman, January 31, 2001.

11

Ibid.

12

Telephone interview with Dave Hughes, November 13, 2000.

13

Federal Communications Commission, "Creation of Low Power Radio Service," January 27, 2000, at http://www.fcc.gov/Bureaus/Mass_Media/Orders/2000/fcc00019.doc.

14

Harry First, "Property, Commons, and the First Amendment: Towards a Core Common Infrastructure" (White Paper of the First Amendment Program at the Brennan Center for Justice at New York University School of Law, 2001), 42-44.

15

Departments of Commerce, Justice, and State, the Judiciary and Related Agencies Appropriations Act, 2001, H.R. 4942, enacting into law H.R. 5548, 106th Congress, Title VI 632 (2000) (enacted).

16

See Bob Brewin, "Airports Ground Use of Wireless: Safety, Loss of Income from Pay Phones Cited," Computerworld (February 19, 2001): 1, http://computerworld.com/ cwi/story/0,1199,NAV47_STO57817_NLTpm,00.html.

17

The real aim of the airports, organized under "wirelessairport.org," appears to be to develop a proprietary, exclusive architecture for wireless technologies within airports, to permit them to extract rents from the technology's use.

18

See, e.g., Hazlett, "The Wireless Craze," 42 ("The burden of proof is on the potential entrant. No incumbent must show that less competition serves the public in order to preserve the status quo, it must only rebut proponents of competition. The default position is that entry does not occur."); Net@EDU, Position Paper on WLAN Radio Frequency Interference Issues ("FCC Part 15 devices like wireless access points must accept all interference that may be caused by the operation of an authorized radio station, by another intentional or unintentional sources [sic]."), 1, available at http://www. educause.edu/asp/doclib/abstract.asp?ID=NE T0014 (visited June 11, 2001); ibid. (discussing ham radio). For a site collecting the FCC's Technical Advisory Committee's work on spectrum, see http://www.jacksons.net/tac/.

19

Hazlett, "The Wireless Craze," 45.

20

Thus, in setting aside 300 MHz within the U-NII band, the FCC stated:

We note that it may also be appropriate to reassess the technical parameters governing U-NII devices in light of second generation MSS systems. For example, second generation MSS systems may be more sensitive and therefore more susceptible to interference from U-NII devices.

Amendment of the Commission's Rules to Provide for Operation of Unlicensed NII Devices in the 5 GHz Frequency Range, 12 F.C.C.R. 1576, 96 (1997). But why should U-NII devices respond to second-generation MSS devices, rather than the other way around? See Yochai Benkler, "Overcoming Agoraphobia," 338, n. 225.

21

FCC Regulations, part 97, subpart A, section 97.1, available at http://www.arrl.org/ FandES/field/regulations/news/part97/.

22

As Hazlett argues, the current regime does not actually sell the right to spectrum. Instead, it sells the right to a certain kind of business, since the license received is a license to use certain equipment at a certain time for certain business purposes. Hazlett, "The Wireless Craze," 102. Thus the criticism of spectrum auctions that I offer here is not a general criticism of auctions; it instead is critical, as Hazlett is, of the particular mode of auctions currently pursued.

23

Ibid., 105.

24

S.J. Res. 125, 69th Cong., 1st Sess. (1926) (Clarence C. Dill).

25

Telephone interview with Dave Hughes.

26

A distinct but related concern is that the relative demand for different uses of spectrum can't be known in advance. As Reed puts it, "[W]hether HDTV or more cellular telephone service would be more useful to society" is not something we can know in advance. Telephone interview with David Reed, February 7, 2001. Yet the existing system of allocation presumes as much.

27

Noam, "Beyond Liberalization II," 473.

28

Telephone interview with Alex Lightman.

29

John Naughton, A Brief History of the Future: The Origins of the Internet (London: Weidenfeld & Nicolson, 1999), 107.

30

Telephone interview with David Reed.

31

As well as other great books. See, for example, Orwell's Revenge: The 1984 Palimpsest (New York: Free Press, 1994).

32

Huber, Law and Disorder in Cyberspace, 75.

33

As Carol Rose argues, however, commons traditionally are not regulated by the state or through law. They instead are regulated through norms. Indeed, their nonstate character is an important element of their status a commons. Rose, "The Comedy of the Commons," 720-21. But today, the most important commons are built by the state and regulated under general law. The highways and parks, for example, are not found by the state anymore. For more on the role of highways in engendering growth because of their commonslike character, see "Toll Roads and Free Roads," 76th Congress, 1st Sess., House Document No. 272, 98 (1939) (Serial Set at 10339) ("The business-generating potentiality of a heavy traffic stream is so great that there is an immediate development of a great variety of roadside establishments all along every new heavily traveled route that is created. Every new highway also, especially in the vicinity of cities, immediately encourages residential development and attracts commercial establishments more interested in the new facility provided by it than in catering to its traffic."); Clay Committee Report, "A 10-Year National Highway Program," 84th Congress, 1st Sess., House Document No. 93, 7 (1955) (Serial Set at 11840) ("An adequate highway system is vital to the continued expansion of the economy. The projected figures for gross national product will not be realized if our highway plant continues to deteriorate. The relationship is, of course, reciprocal; an adequate highway network will facilitate the expansion of the economy which, in turn, will facilitate the raising of revenues to finance the construction of highways.").

34

The origin of this idea is a 1959 paper by J. P. Costas, "Poisson, Shannon, and the Radio Amateur," 47 Proc. IRE 2058 (December 1959). See also Shepard, "A Channel Access Scheme for Large Dense Packet Radio Networks."

35

Hazlett, "The Wireless Craze," 136-37. Thomas W. Hazlett, "An Essay on Airwave Allocation Policy," AEI-Brookings Joint Center for Regulatory Studies (2001).

36

See, e.g., International Bandwidth 2001 (Washington, D.C.: TeleGeography, 2001).

37

As David Reed describes it:

We don't understand how the capacity of communications relates to the number of users. [The historical model assumes] capacity is constant. So if you have ten users, you'll be able to divide it up ten ways and each user will get one-tenth at most [But] there are two plausible arguments from physics that describe constructible systems where, as the number of users of that same spectrum in the same volume of space increases, the amount of available spectrum [increases as well] which means that if you have one hundred times as many stations in that same volume, you can get ten times (or maybe even a hundred times) as much communication capacity.

Telephone interview with David Reed.

38

Noam, "Beyond Spectrum Auctions."

39

Ibid., 465.

40

Ibid., 466.

41

George Gilder, Telecosm: How Infinite Bandwidth Will Revolutionize Our World (New York: Free Press, 2000), 159.

42

Ibid., 159-60. Gilder's point is correct whether or not there is a true "winner's curse." A "winner's curse" exists only when the bid was irrationally high. Richard H. Thaler, "Anomalies: The Winner's Curse," Journal of Economic Perspectives 2 (1988): 191, 192. Whether or not the bid was irrational, it can still create this pressure on the system.

43

The story is told in Lawrence Lessing, Man of High Fidelity: Edwin Howard Armstrong (New York: J. B. Lippincott, 1956).

44

The best example is the slow deployment of ultrawideband (UWB) technologies. A kind of spread spectrum technology, UWB uses extremely low power transmissions that do not rise above the noise floor. The application process at the commission has slowed the approval process of UWB, as incumbents who would be affected by this new competition can slow the process merely by raising questions about the potential interference from this new technology. This shifts the cost of demonstrating the value of UWB to the entrant, which significantly slows (and can kill) the entry process. Thus, "despite the enormous spectrum efficiencies of UWB UWB is going nowhere fast." Hazlett, "The Wireless Craze," 87-88.

CHAPTER 13

1 Telephone interview with Pat Feely (July 18, 2001).

2

Interview with John Seely Brown in Stanford, Calif. (May 2, 2001).

CHAPTER 14

1 George Gilder, Telecosm: How Infinite Bandwidth Will Revolutionize Our World (New York: Free Press, 2000), 163 ("[T]he FCC should not be in the business of licensing spectrum. It should instead issue driver's licenses for radios"); telephone interview with Bill Lane, November 15, 2000 (describing regulatory role); telephone interview with Dave Hughes, November 13, 2000 (describing regulatory role).

2

Software-defined radios were first demonstrated in 1995. Federal Communications Commission, "In the Matter of Inquiry Regarding Software Defined Radios," ET Docket No. 00-47, FCC 00-103 (March 21, 2000), par. 4.

3

See, e.g., 42 U.S.C. 7601 (2001), Title III, Sect. 5(A) (granting a six-year compliance extension only to air polluters who appreciably reduced their emissions before the EPA deadline); Title IV, Sect. 3(b) (allowing factory owners' only "substitution" of SO2 facility reduction requirements to a separate facility under the same ownership).

4

47 U.S.C. 301 (2001).

5

FCC Regulations, part 97, available at http://www.arrl.org/FandES/field/regulations/ news/part97/. Yochai Benkler proposes that the FCC reopen U-NII proceedings to further free spectrum about the 6 GHz layer. Benkler, "Overcoming Agoraphobia," 297. Similarly, the FCC could allow part 15 unlicensed operations in the other ISM bands-such as that allocated under part 18 to 24.05-24.25 GHz. FCC Regulations, part 18, available at http://www.access.gpo.gov/nara/cfr/waisidx_98/47cfr18 _98.html.

6

See, e.g., First, "Property, Commons and the First Amendment," 9 ("Just as municipalities provide sidewalks, roads, and sewers, so too it is important to revive an interest in designing the best possible approach towards public investment in the core infrastructure. This would require careful selection of the right level of public investment. At present, some municipalities are acting in ways that suggests that such public investment could entail deploying conduits and dark fiber [fiber without the electronics attached] cables in municipal streets or sewage systems.").

7

See ibid., at 54-55.

8

Canada has been experimenting extensively with this mode of providing connectivity. Through the development of "customer-owned IP networks," the government is supporting low-cost provision of IP services. For a description, see Timothy Denton, "Customer-Owned IP Networks," March 2001, available at http://www.tmdenton.com.

9

In the first half of 2001, Microsoft launched a campaign against government support for what it called "open source" projects. These projects were "intellectual property-destroyers," since the license under which the code is distributed requires that derivative work also be distributed subject to the same license. This destroys intellectual property, Microsoft argued, because it means that no company can use "open source" products without losing control over its own code. See, e.g., Ben Charny, "Microsoft Raps Open-Source Approach," CNET News, May 3, 2001, available at http://news.cnet. com/news/0-1003-200-5813446.html (Mundie); interview with Bill Gates, Comdex, Fall 2000, available at http://www.key3media.com/comdex/fall2000/daily/keynotes/gates_ interview.html (Gates); Paula Rooney, Balmer: "Linux Is Top Threat to Windows," TechWeb, January 10, 2001, available at http://content.techweb.com/wire/story/ TWB20010110S0006 (Balmer).

If you're this far into this book, then the mistakes in this argument should be patent. The only open source or free software license that has a "viral" component is the GNU GPL. "Open source" software is not software governed by the GPL. Thus Microsoft's argument has nothing to do with "open source" software such as the Apache server or BSD Unix. The only possible target of Microsoft's attack is software licensed under the GPL-namely, GNU/Linux.

But even here, Microsoft's claims are just false. It is not the case that any software that runs with Linux thereby becomes subject to the GPL, any more than any software that runs on the Windows platform becomes subject to the license of Microsoft. The GPL requires that only derivative work be licensed under the GPL-if you take the Linux operating system and modify it in a particular way and then publish it to others, then the product you publish must be under the GPL. That is, no doubt, a restriction on the freedom of developers. They can't simply "take" Linux and do with it as they wish. But neither can anyone simply "take" the Windows operating system and do with it as they wish.

The essence of Microsoft's arguments is simply that the government should not support GPL research. Government funds should not promote a coding project that is not wholly free for anyone to take and do with as they wish. Thus, Microsoft should have no complaint if the government supports coding projects that are dedicated to the public domain (like TCP/IP or the protocols of the Web). Its only complaint should be when the government supports projects that are restrictive of citizens' freedom-in any way.

This is not an argument against open source, it is an argument against GPL. And if it is a strong argument against GPL, then it is also an argument against the government supporting proprietary projects as well. If it is wrong for the government to support Linux because I am not free to do with Linux as I wish, then it is wrong for the government to support Windows because here too I am not free to do with Windows as I wish.

If that is the principle Microsoft is advancing, it is an interesting and valuable point to consider. But only if it is a principle, as opposed to FUD designed to scare the market from a competitor's products. We'll have to see more before we can tell whether this is principle or something less.

10

There may be a constitutional limitation on this form of regulation. At least one court has concluded that requiring open access on cable lines violates the First Amendment. See Comcast Cablevision of Broward County, Inc. v. Broward County, Fla., 124 F. Supp. 2d 685 (S.D. Fla., 2000). For a powerful, if depressing, analysis, see Bran-dan I. Koerner, "AT&T's First Amendment Problem, and Ours," The New Republic (May 14, 2001): 18.

11

This is related to the National Research Council's suggestion that the government push "open IP services"-meaning a policy to preserve the essential features of end-toend IP. See National Research Council, 138-39.

12

Report of the National Broadband Task Force, The New National Dream: Networking the Nation for Broadband Access (2001), 96. The OECD report is described in "Broadband Blues," The Economist (June 21, 2001).

13

Jessica Litman, Digital Copyright (Amherst, N.Y.: Prometheus Books, 2000), 78.

14

Some of the changes I propose here would require changes to or the abrogation of some treaties. Four treaties are especially relevant. The first is the Berne Convention for the Protection of Literary and Artistic Works, 828 U.N.T.S. 221, S. Treaty Doc. No. 99-27, which the United States joined in 1989. The Berne Convention provides for national treatment of foreign authors, meaning that each signatory nation is required to protect foreign authors to the same degree that it protects domestic authors. Additionally, the Berne Convention sets a limit on the minimum level of protection that a signatory nation can offer and requires that materials be automatically protected from the moment of creation, rather than being protected only once they are registered.

The second treaty is the Agreement on Trade Related Aspects of Intellectual Property Rights, Including Trade in Counterfeit Goods, of the General Agreement on Tariffs and Trade, 33 I.L.M. 83 (1994), better known as the TRIPs agreement. The TRIPs agreement was adopted as part of the Uruguay Round of GATT. It sets out minimum standards of protection for intellectual property and is unusual in providing sanctions for copyright violations. Member nations must enact laws giving foreign authors legal remedies for copyright infringement. The agreement not only sets standards for those remedies, but also subjects member nations to trade sanctions if they do not meet the standards. TRIPs also narrows the scope of fair use by requiring that exceptions to copyright protection must occur only in special cases, must not conflict with the normal exploitation of the work, and must not unreasonably prejudice the interests of the holder of the copyright.

The third and fourth treaties were adopted at the UN's World Intellectual Property Organizations (WIPO) Diplomatic Conference in 1996. They are, respectively, the WIPO Copyright Treaty, December 20, 1996, WIPO Doc. CRNR/DC/94 (December 23, 1996), and the WIPO Performances and Phonograms Treaty, December 20, 1996, WIPO Doc. CRNR/DC/95 (December 23, 1996). The WIPO treaties expand the protection afforded to on-line works by requiring countries to extend copyright laws to the Internet. In the United States, this extension took the form of the Digital Millenium Copyright Act of 1998 (DMCA), which prohibits both acts circumventing copy protection and the importation, manufacture, or sale of technologies developed primarily for such circumvention. Willful violation of these provisions is subject to criminal penalties, and both criminal and civil sanctions may be applied to violators even if the underlying use is privileged (even if, for example, the use were to fall within traditional fair use).

15

There is an obvious, and important, problem of security raised by such a system, though there are also obvious and feasible ways to minimize any security risk. For an example of a secure system that was used to effect a settlement between IBM and Fujitsu, see Robert H. Mnookin and Jonathan D. Greenberg, "Lessons of the IBM-Fujitsu Arbitration: How Disputants Can Work Together to Solve Deeper Conflicts," Dispute Resolution Magazine (Spring 1998): 16.

16

Under existing law, "if the intended use is for commercial gain," the likelihood of market harm can be presumed. See Sony Corp. of America v. Universal City Studios, Inc., 464 U.S. 417, 451 (1984). This presumption could be modified in the context of the Internet so that innovators could defend a new use by demonstrating that no substantial likelihood of harm to existing markets exists.

17

Of course, it is not as if artists are really being paid under the existing system-or at least, not most of them. In 1999, eighty-eight recordings accounted for 25 percent of all record sales. Charles C. Mann, "The Heavenly Jukebox," Atlantic Monthly (September 2000).

18

The proposals for this are many. As Jon Potter describes it:

Once the art is disseminated to a single reseller, then other resellers can also have that art for resale for the same terms and conditions, so maybe you can have a standardized nondiscriminatory license provision rather than a compulsory statutory scheme of royalties. So, if Sony needs to sell something through a Sony store, and tries to really control all distribution, then Barnes & Noble gets to do the same deal-whatever the Sony store does with Sony music, or whatever Barnes & Noble gets, Tower gets, but something so that compensation is ensured. And competition is ensured.

Telephone interview with Jon Potter, December 7, 2000.

19

See, for example, "Artists' Contribution to American Heritage Act of 2001," 107th Cong., 1st sess., H.R.1598; Artist-Museum Partnership Act, 107th Cong., 1st sess., S.694; Arts and Collectibles Capital Gains Tax Treatment Parity Act, 107th Cong., 1st sess., S.638; Artists' Contribution to American Heritage Act of 1999, 106th Cong., 1st sess., H.R.3249; 106th Cong., 1st sess., S.217.

20

Stephen Fishman, The Public Domain (Berkeley, Calif.: Nolo, 2000), 2/9 ("Claiming copyright in public domain works is a federal crime, but the maximum penalty for engaging in this sort of criminal conduct is a fine of $2,500 [17 U.S.C. 506(c)]. Moreover, no one has ever been prosecuted for violations [of this provision]").

21

David Lange, "Recognizing the Public Domain," Law & Contemporary Problems 44 (1981): 147, 166.

22

See Gail E. Evans, " Opportunity Costs of Globalizing Information Licenses: Embedding Consumer Rights Within the Legislative Framework for Information Contracts," Fordham Intellectual Property, Media & Entertainment Law Journal 10 (1999):

267. For competing views on UCITA (most of them negative), see "Symposium: Intellectual Property and Contract Law in the Information Age: The Impact of Article 2B of the Uniform Commercial Code on the Future of Transactions in Information and Electronic Commerce," Berkeley Technology Law Review 13 (1998): 809.

23

L. Ray Patterson, "Understanding the Copyright Clause," Journal of the Copyright Society U.S.A. 47 (2000): 365; L. Ray Patterson and Stanley W. Lindberg, The Nature of Copyright: A Law of Users' Rights (Athens, Ga.: University of Georgia Press, 1991); L. Ray Patterson and Judge Stanley F. Birch, Jr., "Copyright and Free Speech Rights," Journal of Intellectual Property Law 4 (1996): 1; L. Ray Patterson, "Copyright and 'The Exclusive Right' of Authors," Journal of Intellectual Property Law 1 (1993): 1, 137; L. Ray Patterson, "Free Speech, Copyright, and Fair Use," Vanderbilt Law Review 40 (1987): 1.

24

Litman, 12-14.

25

The law does not ordinarily have an interest in forcing an owner of a property right to "use it or lose it." For example, I should not have to drive my car all the time to ensure no one else gets to take it. But where property is not exhaustible, and is nonrivalrous, then the balance in favor of the "use it or lose it" rule can shift. Litman's rule is related to a French requirement that if a publisher fails to exploit an assigned right, then, under certain circumstances, the original author can reclaim the right. See Neil Netanel, "Copyright Alienability Restrictions and the Enhancement of Author Autonomy: A Normative Evaluation," Rutgers Law Journal 24 (1993): 347, 390-91.

26

On the role of damage remedies in patent infringement cases, see Mark Schankerman and Suzanne Scotchmer, "Damages and Injunctions in Protecting Intellectual Property," Rand Journal of Economics 32 (2001): 199 (in the context of research tools, damages underdeter infringement, but that nondeterrence may be a good thing for the patent holder, since it prevents a hold-up problem). Carl Shapiro, "Navigating the Patent Thicket: Cross Licenses, Patent Pools, and Standard-Setting," in Innovation Policy and the Economy, vol. 1, Adam Jaffe, Joshua Lerner, and Scott Stern, eds. (Cam-bridge, Mass.: MIT Press, 2001), 8.

27

Professor John Barton, for example, has proposed that Congress (1) raise the standards for patentability (such that an invention is nonobvious only "when the approach seemed quite unlikely to work and still proved successful"); (2) decrease the use of patents to bar research; and (3) improve the U.S. Patent Office to reduce invalid patents. John H. Barton, "Reforming the Patent System," Science 287 (2001): 1933.

28

See Brian Kahin, comments in response to "The Patentability of Computer-Implemented Inventions," 2000, 5 (noting that patent attorneys discourage software professionals from reading patents to avoid "willful infringement"), available at http:// europa.eu.int/comm/internal_market/en/intprop/indprop/maryland.pdf.

29

See, e.g., Daniel R. Harris and Janice N. Chan, "Case Note: Wang Laboratories, Inc. v. America Online, Inc. and Netscape Communications Corp.," Computer & High Technology Law Journal 16 (2000): 449, 457.

30

See, for example, Robert P. Merges, "As Many as Six Impossible Patents Before Breakfast: Property Rights for Business Concepts and Patent System Reform," Berkeley Technology Law Journal 14 (1999): 577 (poor-quality patents, especially business patents, reveal need for PTO reform; PTO jobs and incentives should be restructured; third parties, especially the applicant's competitors, should be consulted early and thoroughly through a European-style opposition system).

31

See A Bill to Amend Title 35, U.S. Code, to Provide for Improvements in the Quality of Patents on Certain Inventions, H.R. 107th Cong., 1st Sess., 1932, 4 (nonobviousness). Congressman Berman's bill also proposes useful changes to facilitate early challenge of business method patents before a patent on these methods is granted. See A Bill to Amend Title 35, U.S. Code, to Provide for Improvements in the Quality of Patents on Certain Inventions, H.R. 107th Cong., 1st Sess., 1332, 3.

32

Pamela Samuelson et al., "Manifesto Concerning the Legal Protection of Computer Programs," Columbia Law Review 94 (1994): 2308, 2331.

33

In March 2000, the U.S. Patent Office launched a Business Method Patent Initiative designed "to ensure high quality patents in this fast-emerging technology." Available at http://www.uspto.gov/web/offices/com/sol/actionplan.html. Fifteen months later, the U.S. Patent Office asked for comment on Prior Art Sources for Business Method Patents. See http://www.uspto.gov/web/offices/com/sol/notices/ab26.html.

CHAPTER 15

1 Orrin G. Hatch, Address of Senator Orrin G. Hatch Before the Future of Music Coalition, Future of Music Coalition, January 10, 2001, available at http://www. senate.gov/~hatch/speech020.htm.

2

Eldred v. Reno, 239 F. 3d 372, 375 (D.C. Cir., 2001).

3

Testimony of Professor Peter Jaszi, The Copyright Term Extension Act of 1995: Hearings on S.483 Before the Senate Judiciary Committee, 104th Cong. (1995), available at 1995 WL 10524355, *6.

4

Oral arguments, United States v. Microsoft, February 26, 2001, available at http:// www.microsoft.com/presspass/trial/transcripts/feb01/02-26.asp.

5

E-mail from John Gilmore, January 19, 2001, to EFF list, on file with author, 6.

6

Telephone interview with Marc Andreessen, December 15, 2000.

7

As Timothy Wu commented to me, "[T]he real successes on the Internet have not been killer apps, but killer platforms. " E-mail from Tim Wu, June 28, 2001. Not, in other words, amazing but proprietary applications that do extraordinary things, but amazing and open platforms upon which others have been free to build. E-mail and the Web are examples, Wu suggests, of killer platforms. Napster and Instant Messaging, while popular, have none of the equivalent open platform characteristics.

8

David Bank's recent book, Breaking Windows, argues that there is more reason to be hopeful about Microsoft. According to Bank, "[I]nteroperability, not lock-in, has become the winning strategy" for Microsoft. David Bank, Breaking Windows: How Bill Gates Fumbled the Future of Microsoft (New York: Free Press, 2001), 237. In June 2000, Microsoft "embraced XML and made it the centerpiece of its new Internet platform." The platform was designed, at least initially, to be a "complot for interoperability." Ibid., 198. Following Christensen's recommendations in Innovator's Dilemma, the strategy was born from a spin-off that Microsoft created with executive Adam Bosworth. Ibid., 196. To "his credit," as Bank writes, "Gates never shut the XML effort down." Ibid., 198.

9

Gordon Cook, "The Meaning of Current Events," The COOK Report, June 20, 2001, available at http://cookreport.com

About the Author

Lawrence Lessig is a professor of law at the Stanford Law School. Previously Berkman Professor of Law at Harvard Law School from 1997 to 2000 and professor at the University of Chicago Law School from 1991 to 1997, he is a graduate of the University of Pennsylvania, Trinity College, Cambridge, and Yale Law School. He clerked for Judge Richard Posner on the Seventh Circuit Court of Appeals and Justice Antonin Scalia on the United States Supreme Court. He is a monthly columnist for The Industry Standard, a board member of the Red Hat Center for Open Source, and the author of Code and Other Laws of Cyberspace.

This file was created with BookDesigner program

bookdesigner@the-ebook.org

1/18/2008

This file was created with  
BookDesigner program  
bookdesigner@the-ebook.org  
1/18/2008

---

Date:

> 1/18/2008

---15. What Orrin Understands

ORRIN HATCH is a conservative. The senior senator from Utah, former chair of the Judiciary Committee, he is a critical force in practically every sphere of the Senate. He was a candidate for president in 2000. And he is admired by most, especially on the Right, as a principled politician and a decent man.

But there's something funny about Hatch. He betrays "policy anomalies"-positions that can't quite be explained on a simple left/right scale. Some of the things that he believes in most are puzzles to many conservatives. And puzzles in a politician are trouble. Unpredictability is not an asset in a political world where results cost lobbyists millions to buy.

Two of Hatch's anomalies are at the core of this book. The first is his concern about the market power and behavior of the Microsoft Corporation. And the second is his affection for emerging technologies like Napster. Hatch was a strong supporter of the Justice Department's investigation into Microsoft's behavior; he is a strong skeptic of the power that music labels have over innovation in the arts.

The pundits think they have an explanation for Hatch's resistance to Microsoft: Corel Corporation, which purchased WordPerfect. WordPerfect had been the dominant word processor. It was a Utah-based company. As with many leading technologies, WordPerfect fumbled the move to GUI interfaces. Microsoft picked up the ball and ran far. Many attribute Hatch's skepticism about Microsoft to these sour grapes.

Hatch's views on Napster are explained in a different way. Hatch is a musician. He has written and recorded many Christian songs. But you don't find the senator's CDs in record stores; the recording labels were not much interested in recruiting the senator from Utah. Thus, Hatch again may have a motive to resent the labels. Therefore, when a new technology comes along that threatens the power of the labels, it is Schadenfreude, not concern, that drives the senator.

It is hard to believe that any politician does what he does for a reason of principle. We live in an era when principled politicians are characters in TV dramas; real politicians are something very different. Thus, the idea that a successful senator would do something that might harm him politically because of ideals strikes us as a fantasy. The stuff of Hollywood, perhaps, but not of Washington, D.C.

But as this book has made clear, there is a principle that would explain Hatch's stand. And while I am no friend of Hatch's, or of many of the policies that he has pushed, I do believe that what pushes Hatch to both positions is a matter of principle. Concentrations of power worry conservatives like Hatch; and in both of these anomalies, concentration of power is at stake.

In the Microsoft case, the fear is that this dominant controller of the platform will be able to use its power to direct evolution. Power over the platform will mean the ability to direct how the platform develops. And the ability to direct how the platform develops is a dangerous power for any single company to hold. It would be awful for the FCC to decide what technologies should look like in the future, then force those technologies on us through the power of law. But likewise, while it wouldn't be as awful, it is still fairly bad that any single company, whether by virtue of the law or because of its control over a platform, could control how technology should develop. Hatch is a believer in the diverse, decentralized market that allows consumers to choose the future. Thus, though he is among the oldest members of the Senate, his spirit is among the closest to what makes the Net run.

The same can be said about the production of culture. Obviously, the government has no legitimate role in controlling how our culture should evolve. What music people listen to and what art they find compelling are matters of private, not public, choice. But even if not as bad as it has been, the world we now have controlling media in our country is worse than the world that Hatch would want. The concentration of power that Hollywood has permits Hollywood a power that Hatch would rather it not have. A better system is less concentrated, less controlled, more diverse and decentralized. As Hatch has written:

[I]f those digital pipes through which the new music will be delivered are significantly narrowed by gatekeepers who limit access to or divert fans to preferred content, a unique opportunity will be lost for both the creators of music and their fans. That is why I think it is crucial that policymakers be vigilant in keeping the pipes wide open.1

As I have argued throughout this book, the architecture that keeps the "pipes wide open" is simply the original architecture of the Net. And a commitment to keeping these pipes open is a commitment to preserving the Net.

In both of these contexts, the senator sees something that ideologues miss: that the greatest lesson of our history is the strength that comes from our economic and cultural diversity. That concentration in either threatens innovation in both-not because concentration alone is necessarily bad, but because concentration gives the concentrated the power to steer evolution as it benefits them.

That power is not within our tradition. It is not what has built the America we admire. And whether you're from the Right or the Left, there is a lesson in what this conservative preaches. We make choices, Hatch shows us, that affect how easily the concentrated can direct the future. We should make choices, Hatch insists, that make it less easy for the future to be directed. Decentralized, diverse, nominated: this is the tradition that Hatch defends; this is the architecture of the original Net.

AS I WRITE THE last pages of this book, the threat to those values grows. A court has just effectively shut down Napster, thereby assuring that the recording industry gets to choose what kind of innovation in the distribution of content will be allowed. Another court has ruled against Eric Eldred's challenge to copyright's bloating, finding that "copyrights are categorically immune from challenges under the First Amendment."2 Though the Constitution speaks of "limited times," Congress is free to give Hollywood "perpetual copyright on the installment plan."3 And streaming across my computer as I write these final paragraphs, judges from the D.C. Circuit Court of Appeals are asking skeptical questions of lawyers for the government defending the judgment against Microsoft. Commenting on the government's defense of Java technologies, one judge has just said, "We are going to replace one monopoly with another right?"4

Though we've seen the new only when it has been freed from the old, that lesson is lost on the Napster court. And though our Framers saw as clearly as we can today that free content fuels innovation, that lesson was forgotten by the court that decided Eric Eldred's case. And though the clearest lesson of the past twenty years is that innovation flourishes best when it flourishes freely on a neutral platform, the judges deciding the Microsoft case cannot even imagine the value of a neutral platform. Is one monopoly really just as good as another?

Alexander Hamilton promised that the judiciary would be "the least dangerous branch." The early history of the Net confirmed Hamilton 's predictions. The Court in Reno v. ACLU spoke of the values in a free Net. It resisted the popular efforts by Congress to regulate it quickly, even if Congress was regulating in the name of important social values.

But the most significant governmental actions affecting the Net in the twenty-first century so far are instances of judges intervening to protect the old against the new. Rather than "wait and see," the law has become the willing tool of those who would protect what they have against the innovation the Net could promise. The law is the instrument through which a technological revolution is undone. And since we barely understand how the technologists built this revolution, we don't even see when the lawyers take it away. As activist and technologist John Gilmore has put it, in a line that captures the puzzle of this book: "[W]e have invented the technology to eliminate scarcity, but we are deliberately throwing it away to benefit those who profit from scarcity I think," Gilmore continues, "we should embrace the era of plenty, and work out how to mutually live in it."5

LATE IN THE afternoon of one of California 's inevitably beautiful days, Marc Andreessen was driving along one of California 's inevitably overcrowded highways. More fitting the traffic than the weather, Andreessen's mood was dark. He was a twenty-nine-year-old computer science graduate who had become one of the most successful entrepreneurs of his generation. Coauthor of an early browser for the World Wide Web (Mosaic), founder of the first company to make the World Wide Web go (Netscape), Andreessen was nonetheless down on the future.

"Innovation," in Andreessen's mind, is what the Web produced. As he told me:

When I came to Silicon Valley, everybody said there's no way in hell that you could ever fund another desktop software company. That's just over. And then in 1995, 1996, 1997, and 1998, all those developers who previously worked on desktop software said, Ah-hah, we're upgrading to a brand-new platform not controlled by anybody-the Internet. [A]ll of a sudden there was an explosion of innovation, a huge number of applications, and [a] huge number [of] companies.6

Innovation "resumed" just at the time when the platform for innovation was neutral and, in the sense that I've described, free: when many different actors were able to bring new ideas to the Net; when they knew that this neutrality meant the old could not control how the new would behave; when the new could behave however the market demanded.

But this innovation, Andreessen said, "is slowing once again Application lock-in [has] actually gotten stronger." The opportunity to innovate outside of the dominant players has again evaporated. We are back to where we were before this revolution began. As control shifts back to the large, the powerful, and the old, and as that control is ratified by the judges in black robes, the opportunity that drew Andreessen from cold but trafficless Illinois disappears. The chance for something different is lost. The innovation age, Andreessen says, "is over." And we are back to a world where innovation proceeds as the dominant players choose.

Andreessen's story is the fear of this book. An "explosion" of innovation grew upon a neutral platform; that explosion is burning out quickly as the platform is increasingly controlled.7 Whether through changes in the physical, or code, or content layers, the change Andreessen worries about is the shift that I have described.

There is little to stop the transformation that worries Andreessen; there is everything to push it along as fast as it can go. This book will be published just as Microsoft's.NET and Hailstorm initiatives hit the network. They promise to integrate an extraordinary range of functionality into the core operating system that Microsoft owns. Emboldened by an expected victory at the court of appeals, Microsoft has expanded the bundling that the government attacked to include a range of services never imagined by government prosecutors. Authentication, instant messaging, e-mail, Web services-all these will be bundled into the core operating system of the next generation of Windows. Anyone who wants to compete in the provision of these services will face as strong a barrier as Netscape faced against a bundled Internet Explorer.

Microsoft is simply responding to another, very different nonneutral platform-the emerging and dominant platform of America Online. After its merger with Time Warner, AOL and its loyal members are another huge and powerful force influencing the future of the Internet. AOL is not an operating system, but for almost a majority of those who use the Internet, it is in effect an operating system. Functionality is served in the AOL suite of software; functionality beyond that is not.

These two companies-AOL Time Warner and Microsoft-will define the next five years of the Internet's life. Neither company has committed itself to a neutral and open platform.8 Hence, the next five years will be radically different from the past ten. Innovation in content and applications will be as these platform owners permit. Additions that benefit either company will be encouraged; additions that don't, won't. We will have re-created the network of old AT&T, but now on the platform of the Internet. Content and access will once again be controlled; the innovation commons will have been carved up and sold.

This is the future of ideas. It could be different, but my sense is that it won't be. If we were more like Hatch, more skeptical of "gatekeepers," whether private or public; if we were less like Jay Walker, eager to view every government-granted privilege as a God-given property right; if we were more like Richard Stallman, committed to a principle of freedom in knowledge and to a practice that assures that the power to control is minimized; if there weren't so few Paul Barans, willing to struggle for many years to force a monopoly to face itself-if all this were so, there would be reason for hope.

But we are not. We are a democracy increasingly ruled by judges. We elect a Congress that is increasingly chained by lobbyists. And we are a culture that deep down believes in this counterrevolution: that strangely thinks that this increase in control makes sense.

As commentator Gordon Cook writes:

The Internet revolution has come and gone. It has created a tremendous burst of innovation[-a] burst that now looks to have been mismanaged [T]he people who did the least to advance the new technologies seem most likely to control them. We are left not with the edge-controlled intelligence of the [end-to-end] network but with the central authoritarian control of the likes of AOL Time Warner.9

The irony astounds. We win the political struggle against state control so as to reentrench control in the name of the market. We fight battles in the name of free speech, only to have those tools turned over to the arsenal of those who would control speech. We defend the ideal of property and then forget its limits, and extend its reach to a space none of our Founders would ever have imagined.

We move through this moment of an architecture of innovation to, once again, embrace an architecture of control-without noticing, without resistance, without so much as a question. Those threatened by this technology of freedom have learned how to turn the technology off. The switch is now being thrown. We are doing nothing about it.Notes

Note: All URLs given as citations are subject to change.

PREFACE

1 Andrew L. Shapiro, The Control Revolution: How the Internet Is Putting Individuals in Charge and Changing the World We Know (New York: PublicAffairs, 1999).

CHAPTER 1

1 Telephone interview with Davis Guggenheim, November 15, 2000. The law on the books in this area (as distinct from what directors and lawyers working for directors think) is "by no means certain." See Melville B. Nimmer and David Nimmer, "Nimmer on Copyright," 13.05[D][3], at 13-222 (2001).

2 For these cases and others, see the extraordinarily helpful site http://www. benedict.com/visual/visual.htm. See also Daniel B. Wood, "Hollywood Loves an Original Idea," Christian Science Monitor, December 15, 1997, http://www.csmonitor.com/ durable/1997/12/15/us/us.4.html; Woods v. Universal City Studios, Inc., 920 F. Supp. 62 (SDNY 1996) (12 Monkeys).

3

See Matthew C. Lucas, "The De Minimis Dilemma: A Bedeviling Problem of Definitions and a New Proposal for a Notice Rule," Journal of Technology Law & Policy 4, no. 3 (2000): 2 (http://journal.law.ufl.edu/~techlaw/4-3/lucas.html).

4

Jessica Litman, Digital Copyright (Amherst, N.Y.: Prometheus Books, 2001), 244-45.

5

Telephone interview with Davis Guggenheim.

6

Niccol Machiavelli, The Prince, 2nd ed. (London and New York: W. W. Norton, 1992), 17.

7

In 1710, the English Parliament passed the Statute of Anne, which, to the horror of its original supporters, was amended to limit the term of copyright to twenty-eight years. In 1774, the House of Lords finally upheld the limit, permitting the works of Shakespeare to fall into the public domain for the first time. Donaldson v. Becket, English Reports 98 (House of Lords, 1774), 251, overturning Millar v. Taylor, Burroughs 4 (1769): 2303, 2308. See Mark Rose, Authors and Owners (Cambridge, Mass.: Harvard University Press, 1993), 97. Had the work of Shakespeare not fallen into the public domain, it would not have been protected in the United States, because foreign copyrights were not protected in the United States until 1891. T. Bender and D. Sampliner, "Poets, Pirates, and the Creation of American Literature," New York University Journal of International Law & Politics 29 (1997): 255. Americans were free to copy English works without the permission of English authors and were free to translate foreign works without the permission of foreign copyright holders.

8

For an introduction, see "The Future of Digital Entertainment" (Special Report), Scientific American 283 (2000): 47.

9

Apple, of course, means something a bit narrower by the term mix. See http://www. apple.com/imac/digitalmusic.html: "Because iTunes is really about freedom. The freedom, first and foremost, to play songs in the order you want, not the order they were first recorded on CD. The freedom to mix and match artists and musical categories as it suits you. The freedom to create your own music CDs. And the freedom to put more than a hundred MP3 songs on a single CD."

10

The relationship to low-cost production is no accident with some modern music. As John Leland describes it, "The digital sampling device has changed not only the sound of pop music, but also the mythology. It has done what punk rock threatened to do: made everybody into a potential musician, bridged the gap between performer and audience." Siva Vaidhyanathan, Copyrights and Copywrongs: The Rise of Intellectual Property and How It Threatens Creativity (New York: New York University Press, 2001), 138. By keeping the cost low, and hence the distance between creator and consumer short, the genre aspires to keep the range of creators as broad as possible. That aspiration for music could, I argue in this book, become an aspiration for creativity more generally.

11

This is the character of Caribbean music as well. "Every new version will slightly modify the original tune," but then, obviously, draw upon and copy it. Ibid., 136.

12

Richard Stallman has been likened to the Moses of what I will call the "open code" movement. The likeness is indeed striking. As I describe in chapter 4, below, Stallman began the movement to build a free operating system. But as with Moses, it was another leader, Linus Torvalds, who finally carried the movement into the promised land by facilitating the development of the final part of the OS puzzle. Like Moses, too, Stallman is both respected and reviled by allies within the movement. He is an unforgiving, and hence for many inspiring, leader of a critically important aspect of modern culture. I have deep respect for the principle and commitment of this extraordinary individual, though I also have great respect for those who are courageous enough to question his thinking and then sustain his wrath.

Stallman insists that those who would advance the values of the free software movement must adopt the language "free" rather than "open." This seems to me an unproductive debate. To the extent Stallman believes that people dilute the insights of the free software movement by minimizing its connection to fundamental values, he is correct. The importance of free and open source software is much more than business, or efficient code. But the remedy to narrowness is not magic words-especially when the magic words tend to confuse rather than clarify. I am partial to the term open-as in open society; I believe it is properly a reference to values as well as the licenses under which code is distributed; and by "open code" I mean to refer to the values across both technical and legal contexts that promote a world where governing structures-code-are fundamentally free.

For an exceptional study of free and open source software, and the incentives that are behind it, see Working Group on Libre Software, "Free Software/Open Source: Information Society Opportunities for Europe?," Version 1.2 (April 2000), http://eu. conecta.it/paper.pdf.

13

In the terms of legal theory, there are two distinct ways in which a resource could be "free" in the sense I mean. Either no one would have any entitlement to the resource, or if someone did have an entitlement, the resource would be protected by a liability rather than a property rule. See Guido Calabresi and Douglas Melamed, "Property Rules, Liability Rules, and Inalienability: One View of the Cathedral," Harvard Law Review 85 (1972): 1089. See also Robert P. Merges, "Institutions for Intellectual Property Transactions: The Case of Patent Pools," in Expanding the Boundaries of Intellectual Property, Rochelle Cooper Dreyfuss and Diane Leenheer Zimmerman, eds. (Oxford: Oxford University Press, 2001), 123, 131 ("The essence of this Framework is this: Calabresi and Melamed assign all legal entitlements to one of two rules, 'property rules' and 'liability rules.' The former are best described as 'absolute permission rules': one cannot take these entitlements without prior permission of the owner. The rightholder, acting individually, thus sets the price. Most real estate fits this description. By contrast, liability rules are best described as 'take now, pay later.' They allow for nonowners to take the entitlement without permission of the owner, so long as they adequately compensate the owner later. In the Calabresi-Melamed Framework, ex post adequate compensation is deemed 'collective valuation.' ").

14

Carol Rose, "The Comedy of the Commons: Custom, Commerce, and Inherently Public Property," University of Chicago Law Review 53 (1986): 711, 712.

15

I don't mean that all such barriers will be properly considered "neutral." Sometimes barriers are merely a tool for raising rivals' costs. Some of those nonneutral barriers will be remedied through antitrust law. But I'm assuming the general case is benign in real space.

CHAPTER 2

1 For useful analyses of "the commons," see generally Anarchy and the Environment: The International Relations of Common Pool Resources, J. Samuel Barkin and George E. Shambaugh, eds. (Albany: State University of New York Press, 1999); Managing the Commons, 2nd ed., John A. Baden and Douglas S. Noonan, eds. (Bloomington, Ind.: Indiana University Press, 1998); Susan J. Buck, The Global Commons: An Introduction (Washington, D.C.: Island Press, 1998); Privatizing Nature: Political Struggles for the Global Commons, Michael Goldman, ed. (London: Pluto Press, in association with Transnational Institute, 1998); Local Commons and Global Interdependence: Heterogeneity and Cooperation in Two Domains, Robert O. Keohane and Elinor Ostrom, eds. (London and Thousand Oaks, Calif.: Sage Publications, 1995); The Political Economy of Customs and Culture: Informal Solutions to the Commons Problem, Terry L. Anderson and Randy T. Simmons, eds. (Savage, Md.: Rowman and Littlefield Publishers, 1993); Making the Commons Work: Theory, Practice and Policy, Daniel W. Bromley, ed. (San Francisco: ICS Press, 1992); Commons Without Tragedy: Protecting the Environment from Overpopulation-A New Approach, Robert V. Anderson, ed. (London: Shepheard-Walwyn; Savage, Md.: Barnes & Noble, 1991); Glenn G. Stevenson, Common Property Economics: A General Theory and Land Use Applications (Cambridge, England, and New York: Cambridge University Press, 1991); Elinor Ostrom, Governing the Commons: The Evolution of Institutions for Collective Action (Cambridge, England, and New York: Cambridge University Press, 1990).

2 7 2 N O T E S

The traditional commons is distinct from what Henry Smith calls a "semicommons." See Henry E. Smith, "Semicommon Property Rights and Scattering in the Open Fields," Journal of Legal Studies 29 (2000): 131 (defining a semicommons as a mix of common and private rights where both are significant and interact).

While the standard view is that a commons induces overuse, some argue that it may inspire underuse. Compare Richard A. Posner, Economic Analysis of Law, 4th ed. (Boston: Little, Brown, 1992), 32, with Frank I. Michelman, "Ethics, Economics and the Law of Property," in Ethics, Economics, and the Law, J. Roland Pennock and John W. Chapman, eds. (New York: New York University Press, Nomos XXIV [series], 1982), 25-27.

2

The OED was compiled from the volunteer efforts of thousands of people sending examples of usage to editors. See Simon Winchester, The Professor and the Madman: A Tale of Murder, Insanity, and the Making of the Oxford English Dictionary (New York: HarperCollins Publishers, 1998).

3

The Oxford English Dictionary, 2nd ed., vol. 3, prepared by J. A. Simpson and E. S. C. Weiner (Oxford: Clarendon Press; New York: Oxford University Press, 1989), 567.

4

The observant will notice that a resource offered in conditions of perfect competition comes close to the conditions that I described obtain with a commons, even though the resource is "owned." Perfect competition constrains the owner in similar ways, though the right not to sell at all distinguishes the two sets of constraints.

5

This is the essence of an entitlement protected by a property rule. See Guido Calabresi and Douglas Melamed, "Property Rules, Liability Rules, and Inalienability: One View of the Cathedral," Harvard Law Review 85 (1972): 1089, 1092. Not all liability rule cases will eliminate this core of discretion. But the system is structured to avoid it.

6

Virgil, Virgil in English Rhythm, 2nd ed., Robert Corbet Singleton, trans. (London: Bell and Daldy [imprint], 1871), iii-iv ("There is a common of language to which both poetry and prose have the freest access.").

7

Garrett Hardin, "The Tragedy of the Commons," Science 162 (1968): 1243. The idea of congestion externalities of course predates Hardin. See Posner, "Economic Analysis of Law," 32-34, citing Frank H. Knight, "Some Fallacies in the Interpretation of Social Cost," Quarterly Journal of Economics 38 (1924): 582.

8

Hardin, 1244 (emphasis added).

9

Ostrom, ch. 3; Robert C. Ellickson, Order Without Law: How Neighbors Settle Disputes (Cambridge, Mass.: Harvard University Press, 1991). See also Making the Commons Work: Theory, Practice and Policy, Daniel W. Bromley, ed. (San Francisco: ICS Press, 1992), part 2 (describing case studies).

10

Elinor Ostrom has been the most effective in demonstrating the "fallacy" of the "tragedy of the commons." As she writes:

What makes these models so dangerous-when they are used metaphorically as the foundation for policy-is that the constraints that are assumed to be fixed for the purpose of analysis are taken on faith as being fixed in empirical settings, unless some external authorities change them.

Ostrom, 6-7. For example, in an article in 1988, The Economist asserts about fisheries that "left to their own devices, fishermen will overexploit stocks" and that "to avoid disaster, managers must have effective hegemony over them." Ibid., 8. But that assumption cannot be made. It depends upon the social system within which the commons exist; often, as Ostrom has demonstrated, control is possible without either state intervention or a system of private property.

11

Obviously, many have noticed the "commonslike" character of the Internet. See, e.g., Douglas S. Noonan, "Internet Decentralization, Feedback, and Self-Organization," 188-89, in Managing the Commons, 2nd ed., John A. Baden and Douglas S. Noonan, eds. (Bloomington, Ind.: Indiana University Press, 1998) ("The Internet is a commons [or several commons]"). See also James Boyle, "A Politics of Intellectual Property: Environmentalism for the Net?," Duke Law Journal 47 (1997): 87. Not all have been as convinced that its commons nature accounts for its innovation. See, e.g., Noonan, "Internet Decentralization," 198 ("In spite of its commons nature, or perhaps because of it").

12

See Yochai Benkler, "From Consumers to Users: Shifting the Deeper Structures of Regulation," Federal Communications Law Journal 52 (2000): 561, 562-63 ("These choices occur at all levels of the information environment: the physical infrastructure layer-wires, cable, radio frequency spectrum-the logical infrastructure layer-software-and the content layer").

13

I'm simplifying vastly from both the OSI seven-layer model of network design, see Douglas E. Comer, Internetworking with TCP/IP, 4th ed. (Upper Saddle River, N.J.: Prentice-Hall, 2000), 181-95; Pete Loshin, TCP/IP Clearly Explained, 2nd ed. (Boston: AP Professional, 1997), 12-18; and Berners-Lee's four-layer description (trans-mission, computer, software, and content) in Tim Berners-Lee, Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by Its Inventor (San Francisco: HarperSanFrancisco, 1999), 129-30.

14

Certain speech has always been regulated on the telephone. For example, in 1883, a local telephone company terminated a subscriber's service for using the word damned with an operator. The company's contract prohibited "profane, indecent or rude language." An Ohio court upheld the company's actions, and the Supreme Court Commission later affirmed the court's decision. For this and other examples, see Peter W. Huber, Michael K. Kellogg, and John Thorne, Federal Telecommunications Law, 2nd ed. (Gaithersburg, Md.: Aspen Law & Business, 1999), 1275-76, 1288-92.

CHAPTER 3

1 Milton Mueller, Universal Service: Competition, Interconnection, and Monopoly in the Making of the American Telephone System (Cambridge, Mass.: MIT Press; Washington, D.C.: AEI Press, 1997), 180. For other useful histories, see A History of Computing Research at Bell Laboratories (1937-1975), Bernard D. Holbrook and W. Stanley Brown, eds. (New York: The Laboratories, 1982); A History of Engineering and Science in the Bell System, M. D. Fagen, ed. (New York: The Laboratories, prepared by members of the technical staff, Bell Telephone Laboratories, 1975-85); Robert W. Garnet, The Telephone Enterprise: The Evolution of the Bell System's Horizontal Structure, 1876-1909 (Baltimore: Johns Hopkins University Press, 1985); Alvin Von Auw, Heritage and Destiny: Reflections on the Bell System in Transition (New York: Praeger, 1983); John Brooks, Telephone: The First Hundred Years (New York: Harper & Row, 1976); George David Smith, The Anatomy of a Business Strategy: Bell, Western Electric, and the Origins of the American Telephone Industry (Baltimore: Johns Hopkins University Press, 1985); Edwin M. Asmann, The Telegraph and the Telephone: Their Development and Role in the Economic History of the United States: The First Century, 1844-1944 (Lake Forest: Lake Forest College, 1980); Amy Friedlander, Natural Monopoly and Universal Service: Telephones and Telegraphs in the U.S. Communications Infrastructure, 1837-1940 (Reston, Va.: Corporation for National Research Initiatives, 1995); John Patrick Phillips, Ma Bell's Millions (New York: Vantage Press, 1970).

2

Mueller, 7.

3

Ibid., 148.

4

Ibid., 188-89.

5

Ibid., 104-13.

6

Ibid., 180-81. See also David F. Weiman and Richard C. Levin, "Preying for Monopoly? The Case of Southern Bell Telephone Company, 1894-1912," Journal of the Political Economy 102 (1994): 103 (proposing a "modified version of the predation hypothesis" that shores up Mueller's work).

7

Mueller, 51, 78-79.

8

The alternative, modern meaning for "universal service" is that every household has access to the network. Mueller argues convincingly this was not the original meaning; ibid., 163. For an argument that the funding obligations that support universal service should be extended to ISPs, see Robert M. Frieden, "Universal Service: When Technologies Converge and Regulatory Models Diverge," Harvard Journal of Law & Technology 13 (2000): 395.

9

Peter W. Huber, Michael K. Kellogg, and John Thorne, Federal Telecommunications Law, 2nd ed. (Gaithersburg, Md.: Aspen Law & Business, 1999), 17.

10

Ibid.

11

Ibid.

12

Ibid.

13

Mueller, 162.

14

Interview with Paul Baran in Stanford, California, November 14, 2000. For other interviews of Baran, see Stewart Brand, "Founding Father," Wired (March 2001), available at http://www.wired.com/wired/archive/9.03/baran_pr.html; interview by David Hochfelder with Paul Baran, electrical engineer, Newark, New Jersey (October 24, 1999), available at http://ieee.org/organizations/history_center/oral_histories/transcripts/ baran.html; interview by J. O'Neill with Paul Baran, Menlo Park, California (March 5, 1990); George Gilder, "Inventing the Internet Again," Forbes (June 2, 1997), 106 (lengthy article about Baran); Katie Hafner and Matthew Lyon, "Casting the Net," The Sciences (September 1, 1996), 32.

15

American Telephone & Telegraphy Co., Telephone Almanac, foreword (1941).

16

Interview with Paul Baran.

17

Ibid.

18

Peter Huber, Orwell's Revenge: The 1984 Palimpsest (New York: Free Press; Toronto: Maxwell Macmillan Canada; New York: Maxwell Macmillan International, 1994), 268-69; Huber, Kellogg, and Thorne, 416.

19

And the decision was reversed by the D.C. circuit. Hush-a-Phone Corp. v. United States, 238 F. 2d 266 (D.C. Cir., 1956).

20

The idea is developed in Kleinrock's dissertation: Leonard Kleinrock, Message Delay in Communication Nets with Storage (1962, unpublished Ph.D. dissertation, Massachusetts Institute of Technology), which was later published in a modified form. See Leonard Kleinrock, Communication Nets: Stochastic Message Flow and Delay (New York: McGraw-Hill, 1964). See also John Naughton, A Brief History of the Future: The Origins of the Internet (London: Weidenfeld & Nicolson, 1999), 92, 118-19 (discussing other earlier contributors to the Internet).

21

Baran attributes to him the discovery of the term. Interview with Paul Baran ("The term 'packet switching' was first used by Donald Davies of the National Physical Laboratory in England, who independently came up with the same general concept in November 1965.").

22

Baran confirmed this history to me in an interview. "So the first level of objections was about technology-that I didn't understand how the telephone system worked, [and] that what I'm proposing could not possibly work." Interview with Paul Baran.

23

Naughton, 107. Authors Katie Hafner and Matthew Lyon recount a similar resistance in Where Wizards Stay Up Late: The Origins of the Internet (New York: Simon & Schuster, 1996), 62-64.

24

Interview with Paul Baran.

25

Ibid.

26

See Steve Bickerstaff, "Shackles on the Giant: How the Federal Government Created Microsoft, Personal Computers, and the Internet," Texas Law Review 78 (1999): 1, 60-61 (explaining that "restrictions on the Bell System helped the personal computer market to develop through diverse competition and innovation"); Peter W. Huber, Law and Disorder in Cyberspace: Abolish the FCC and Let Common Law Rule the Telecosm (Oxford and New York: Oxford University Press, 1997).

27

Naughton, 108-9.

28

Interview with Paul Baran.

29

See J. H. Saltzer et al., "End-to-End Arguments in System Design," available at http://web.mit.edu/Saltzer/www/publications/endtoend/endtoend.pdf; David P. Reed et al., "Active Networking in End-to-End Arguments," available at http://Web.mit.edu/ Saltzer/www/publications/endtoend/ANe2ecomment.html. For treatment of e2e, see Cameron R. Graham, "Cable TV Law 2001: Competition in Video, Internet and Telephony," Practicing Law Institute: PLI Order No. G0-00LY, 642 (2001): 486; Rob Frieden, "Does a Hierarchical Internet Necessitate Multilateral Intervention?," North Carolina Journal of International Law and Commercial Regulation 26 (2001): 361; Robert M. Kossick Jr., "The Internet in Latin America: New Opportunities, Developments and Challenges," Cyberspace Lawyer 6, no. 1 (2001): 11 (noting the use of end-toend principles in Brazil and Mexico); and Lawrence Lessig, foreword, "Cyberspace and Privacy: A New Legal Paradigm?," Stanford Law Review 52 (2000): 987.

30

National Research Council, The Internet's Coming of Age (Washington, D.C.: National Academy Press, 2000), 30.

31

Telephone interview with David P. Reed (February 7, 2001), who contributed to the early design of the Internet protocols-TCP/IP-while a graduate student at MIT.

32

As I describe in this paragraph, the importance of architecture to the character of the Internet was a theme of the early activism of Mitch Kapor. But the author who first focused the importance of architecture on the issues I describe in this book is Kevin Wer-bach. As Werbach wrote in Release 1.0:

Architecture matters. For the most part, today's Net is open, decentralized and competitive. It fosters innovation because it is a standards-based general-purpose platform [But t]he people building the next generation of high-speed access pipes are trying to change this model.

Kevin Werbach, "The Architecture of Internet 2.0," Release 1.0 (February 19, 1999). As Franois Bar puts it, describing the electronic marketplace: "The most fundamental transformation of commercial activities [from cyberspace] is not primarily about efficiency, but has to do with the market and industry structure. It is about architecture." Franois Bar, "The Construction of Marketplace Architecture," Brookings & Internet Policy Institute (forthcoming 2001): 12.

33

Lawrence Lessig, Code and Other Laws of Cyberspace (New York: Basic Books, 1999), 243, note 19 (citing Kapor).

34

To computer scientists, my use of the term architecture is a bit misleading. Computer scientists typically use "architecture" to refer to "the visible characteristics of only the element actually performing instructions, that is, the processor of a computer system"; Harold Lorin, Introduction to Computer Architecture and Organization, 2nd ed. (New York: Wiley, 1989), 10, and analogously for a network. IBM's System/360 is said to be the first computer system to have had an "architecture." According to its architects, "the term architecture [means] the attributes of a system as seen by the programmer, i.e., the conceptual structure and functional behavior, as distinct from the organization of the data flow and controls, the logical design and the physical implementation." Carliss Y. Baldwin and Kim B. Clark, Design Rules, vol. 1 (Cambridge, Mass.: MIT Press, 2000), 215.

I mean the term to be far more general-to refer to "both the Internet's technical protocols (e.g., TCP/IP) and its entrenched structures of governance and social patterns of usage that themselves are not easily changeable, at least not without coordinated action by many parties." Lawrence Lessig and Paul Resnick, "Zoning Internet Speech," Michigan Law Review 98 (1999): 395.

35

Network Working Group, "Request for Comments: 1958, Architectural Principles of the Internet," Brian E. Carpenter, ed. (1996), available at http://www.ietf.org/rfc/ rfc1958.txt.

36

Ibid, 2.1.

37

Ibid.

38

Tim Berners-Lee, Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by Its Inventor (San Francisco: HarperSanFrancisco, 1999), 99.

39

As background, see Peter Cukor and Lee McKnight, "Knowledge Networks, the Internet, and Development," Fletcher Forum of World Affairs (Winter 2001): 43, 46; George Gilder, Telecosm: How Infinite Bandwidth Will Revolutionize Our World (New York: Free Press, 2000), 70-71.

40

Telephone interview with David Isenberg, February 14, 2001.

41

Or at least this is an ideal. See Roger Feldman, "e2e vs. General Edison OnLine," PMA OnLine Magazine, http://www.retailenergy.com/feldman/0007flmn.htm.

42

Telephone interview with David Reed.

43

Berners-Lee, 208.

44

National Research Council, 138.

45

Ibid., 107.

46

Ibid., 36-37.

47

Ibid., 37.

48

Douglas E. Comer, Internetworking with TCP/IP, 4th ed., vol. 1 (Upper Saddle River, N.J.: Prentice-Hall, 2000), 691 (HTTP stands for "hypertext transfer protocol" and is "[t]he protocol used to transfer Web documents from a server to a browser"), 713 (TCP stands for "transmission control protocol"), and 694 (IP stands for "Internet protocol)." Together, TCP and IP allow data delivery between machines on the Internet. "The entire protocol suite is often referred to as TCP/IP because TCP and IP are the two fundamental protocols.").

49

Berners-Lee, 35.

50

See, e.g., Paul E. Ceruzzi, A History of Modern Computing (Cambridge, Mass.: MIT Press, 1998), 301-2 (describing hypertext "inventor" Ted Nelson's debt to Vannevar Bush, quoting Bush: "The human mind operates by association. With one item in its grasp, it snaps instantly to the next that is suggested by the association of thoughts, in accordance with some intricate web of trails carried by the cells of the brain.").

51

See Robert M. Fano, "On the Social Role of Computer Communications," Proceedings of the IEEE 60 (September 1972): 1249.

52

Berners-Lee, 46. See also James Gillies, How the Web Was Born: The Story of the World Wide Web (Oxford and New York: Oxford University Press, 2000); Hafner and Lyon, Internet Dreams: Archetypes, Myths, and Metaphors, Mark J. Stefik and Vinton G. Cerf, eds. (Cambridge, Mass.: MIT Press, 1997).

53

Berners-Lee, 40 (describing Gopher and WAIS growing faster).

54

Ibid. (interconnect).

55

Ibid., 72-73.

56

Ibid.

57

Ibid., 74.

58

Ibid., 99.

59

See Barbara Esbin, "Internet over Cable: Defining the Future in Terms of the Past," Communications Law Conspectus (Winter 1999): 37, 46 ("ARPA began to support the development of communications protocols for transferring data and electronic mail [e-mail] between different types of computer networks."); Needham J. Boddie II et al., "A Review of Copyright and the Internet," Campbell Law Review 20 (1998): 193, 196 ("Funding for the Internet comes from five federal agencies, various universities and states, and private companies such as IBM and MCI.").

60

Naughton, 83-85.

61

Ibid., 84.

62

See, e.g., http://www.asiapoint.net/insight/asia/countries/myanmar/my_spedev.htm.

63

Among other restrictions, AT&T was not permitted to get into the computer business. This fact becomes quite important in explaining the birth of Unix. For a comprehensive and balanced account of the effect of these limitations, see Bickerstaff, 14-17.

64

Bickerstaff, 25.

65

For more on the history of the FCC's Computer I and Computer II decrees, see ibid., 1-37; Huber, Kellogg, and Thorne, 5.4.

66

National Research Council, 130-31, n. 18 (describing best efforts as consequences of uniformity).

67

"Humans can tolerate about 250 msec of latency before it has a noticeable effect," http://www.dialogic.com/solution/Internet/4070Web.htm.

68

They are described in Mark Gaynor et al., "Theory of Service Architecture: How to Encourage Innovation of Services in Networks" (working paper, 2000, on file with author), 14.

69

Indeed, George Gilder believes the increasing capacity of optical fiber will render moot this debate about QoS. "Today on the Internet, the consensus claims that QoS will be indispensable for voice and video. But with true bandwidth abundance, QoS complexities are irrelevant-an ATM tax imposed on the vast bandwidth of fiber with its 10 to the minus 15 error rates, far better than the reliability of telephone circuits." Gilder, Telecosm, 80.

This point is made expressly with respect to quality of service and end-to-end in an important paper by Barbara van Schewick, "The End-to-End Principle in Network Design, Its Impact on Innovation and Competition in the Internet, and Its Future in the Network Architecture of the Next Generation Internet" (working paper, September 2000, on file with author). Van Schewick distinguishes between two forms of QoS-integrated services architecture and differentiated services architecture (6, 7)-and argues that the former puts more pressure on the end-to-end principle than the latter. She argues that a more general solution to the QoS problem is overprovisioning (7), and motivates that argument with strong evidence of the incentives of network providers to differentiate between different applications. This, she argues, reduces "the incentives for a potential innovator to develop new applications." Ibid., 7.

Van Schewick is the first researcher I know of to link QoS, overprovisioning, and endto-end, and I have drawn heavily upon her understanding in this context and throughout this book.

Others are skeptical of the overprovisioning solution. For overprovisioning to work, they argue, overprovisioning must exist at every point of the network. But that requires coordinated action that is impossible in the context of the Internet. Thus, the solution is an ideal without a mechanism to achieve it.

This skepticism is appropriate, but it does suggest another response. If the market "naturally" would not choose overprovisioning, but overprovisioning would nonetheless be a productive response, then we may well have identified a role for government. I describe this a bit more in chapter 12.

70

For examples of QoS technologies, and a discussion of QoS generally, see Joe Lardieri, "Quality of Service: Which Flavor Is Right for You?," Telecommunications (August 1999): 53; Dave Kosiur, "Directing Traffic with a Touch of Class," PC Week (March 23, 1998): 91; David B. Miller, "Quality of Service: Directing Data Through the Network," ENT (October 6, 1999): 22.

71

See Gilder, Telecosm, 158-64.

72

The critical change necessary for this "fibered" network to achieve this level of capacity is the emergence of optical routing technologies. The emerging bottleneck in the network is the relatively slow speed of electronic switches. For an introduction, see "The Ultimate Optical Network" (Special Report), Scientific American 284 (January 2001): 80.

73

See, e.g., Bill Frezza, "Telecosmic Punditry: The World Through Gilder-Colored Glasses," Internet Week (December 4, 2000): 47; Rob Walker, "The Gildercosm," Slate magazine (September 11, 2000), available at http://slate.msn.com/code/MoneyBox/ MoneyBox.asp?Show=9/11/2000&idMessage=6030 (Gilder runs "against the current wisdom that sees bandwidth shortage as a problem"); Julian Dibbell, "From Here to Infinity," Village Voice, September 5, 2000, 65 ("It takes either profound sloth or transcendent faith to persist in voicing such breathless sentiments."). For a more favorable review, see, e.g., Blair Levin, "Review, TELECOSM: How Infinite Bandwidth Will Revolutionize Our World," Washington Monthly (September 1, 2000): 54.

74

E-mail from Timothy Wu to Lawrence Lessig, February 16, 2001.

75

This claim depends upon the assumption that the value of the network activity was higher than the telephone activity it displaced. That assumption about social value therefore has little relation to the actual costs.

CHAPTER 4

1 Technically, some of these functions could be provided if the program simply provided perfectly transparent representations of the "application program interfaces," or APIs. In some contexts this distinction will be important, but I ignore it here. A sufficiently skilled programmer could also learn something about compiled code through reverse engineering. The success of this would depend upon how well structured the original code was.

2

Ceruzzi, A History of Modern Computing, 108.

3

For a brief history of Unix, see William Shattuck, "The Meaning of UNIX," in The Unix System Encyclopedia, 2d ed.,Yates Ventures, eds. (Palo Alto, Calif.: Yates Ventures, 1985), 89, 93-94; Peter H. Salus, A Quarter Century of UNIX (Reading, Mass.: Addison-Wesley Publishing Company, 1994), 5-61; Ronda Hauben, "The History of UNIX," available at http://www.dei.isep.ipp.pt/docs/unix.html (last visited June 12, 2001).

4

Robert Young and Wendy Goldman Rohm, Under the Radar: How Red Hat Changed the Software Business-and Took Microsoft by Surprise (Scottsdale, Ariz.: Coriolis Group Books, 1999), 21; Donald K. Rosenberg, Open Source: The Unauthorized White Papers (Foster City, Calif.: Hungry Minds, 2000), 9.

5

Ceruzzi, 94-95.

6

Ibid., 92.

7

Ibid., 283 (The result was that "UNIX was a godsend for university computer science departments").

8

Young and Rohm, 20.

9

See Richard Stallman, "The GNU Operating System and the Free Software Movement," in Open Sources: Voices from the Open Source Revolution, Chris DiBona, Sam Ockman, and Mark Stone, eds. (Beijing and Sebastopol: O'Reilly, 1999), 53-54.

10

This commons would be built, in Stallman's ideal world, with nonproprietary code. Stallman is not opposed, however, to commercial software. As he explains, "Commercial software and proprietary software are totally different concepts. 'Commercial' refers to the financial arrangement of the software. 'Proprietary' refers to what the users are permitted to do. Free software must have the freedom to copy, to modify, and to have the source code. So proprietary software is mutually exclusive with free software, but there can be commercial software that [is] free software." Telephone interview by Hiroo Yamagata with Richard M. Stallman, August 8, 1997.

11

Peter Wayner, Free for All: How Linux and the Free Software Movement Undercut the High-Tech Titans (New York: HarperBusiness, 2000), 36.

12

For a discussion of Stallman and the history of GNU/Linux, see ibid., 9, 34-36, 67-68; Stallman, 53-66; Mark Leon, "Richard Stallman, GNU/Linux," InfoWorld (October 9, 2000): 62.

13

See, e.g., Linus Torvalds and David Diamond, Just for Fun: The Story of an Accidental Revolutionary (New York: HarperBusiness, 2001); Pekka Himanen, Manuel Castells (epilogue), and Linus Torvalds (prologue), The Hacker Ethic and the Spirit of the Information Age (New York: Random House, 2001); Paula Rooney, "No. 11: The Dark Horse," Computer Reseller News, November 15, 1999.

14

Stallman: "Around 1992, combining Linux with the not-quite-complete GNU system resulted in a complete free operating system. (Combining them was a substantial job in itself, of course.) It is due to Linux that we can actually run a version of the GNU system today." Stallman, 65.

What to call this resulting OS-"Linux" or "GNU/Linux"-is a hotly contested issue. Indeed, I've received more heat about this issue than about any other I've discussed in this book. Supporters of the free software movement insist the product is GNU/Linux and object that calling it "Linux" underplays the importance of Stallman and the Free Software Foundation. Others emphasize the other great work that has spread this free OS and insist that "Linux" is enough of a moniker. As Bob Young, chairman of Red Hat, wrote me, "While Linus gets more credit than he deserves for the whole 800 MB OSs that are known by their 16 MB Linux kernel, Richard gets less credit than he deserves. But where Linus gets far more credit than he demands, Richard demands more credit than he deserves (not that he does not deserve a great deal)." E-mail, June 8, 2001.

It is an unfortunate feature of the current debate around open source and free software projects that so much energy is devoted to what things are called-with great fury directed at those who fail to speak properly (using "open" where "free" is to be used; using "closed" where "proprietary" should be used). I'm guilty of some of those language sins in this book, and only sometimes self-consciously. I will refer generally to the Linux OS as "Linux," though sometimes as "GNU/Linux" when the reminder helps. The important point for our purposes is not who gets credit for what, but that the kernel of the Linux OS is licensed under the Free Software Foundation's General Public License (GPL). That is, I believe, a critically important feature of this debate, and one that must be kept clear.

15

For a recent and compelling history of the birth of Linux, see Glyn Moody, Rebel Code: Linux and the Open Source Revolution (Cambridge, Mass.: Perseus Press, 2001).

16

Linus Torvalds, "The Linux Edge," in Open Sources: Voices from the Open Source Revolution, Chris DiBona, Sam Ockman and Mark Stone, eds. (Beijing and Sebastopol: O'Reilly, 1999), 101.

17

Torvalds, "The Linux Edge," 101.

18

Moody, Rebel Code, 130. See also Dorte Toft, "Open Source Group Forms Nonprofit," Industry Standard (July 1, 1999), at http://www.thestandard.com/article/ 0,1902,5377,00.html (visited on May 27, 2001).

19

See http://www.isc.org/products/BIND/bind-history.html.

20

Tim O'Reilly, "Hardware, Software, and Infoware," in Open Sources: Voices from the Open Source Revolution, Chris DiBona, Sam Ockman, and Mark Stone, eds. (Beijing and Sebastopol: O'Reilly, 1999) 189, 191.

21

Wayner, 172.

22

See Carolyn Duffy Marsan, "Sendmail Adds Admin Console, IPv6 Support; New Unix-Based Tool Can Manage Sendmail-Based E-mail Systems Across a Widespread Organization," Network World (February 7, 2000).

23

Alan Cox, "This Is How Free Software Works," wideopennews 5, 8.6, http:// www2.usermagnet.com/cox/index.html.

24

There is a resistance to calling these "exclusive rights"

"monopolies." Frank Easter-brook, "Intellectual Property Is Still Property," Harvard Journal of Law & Public Policy 13 (1991): 108-09. But technically, of course, they are. That, however, does not entail that a given exclusive right has any monopoly power. There is a purpose, however, in calling these rights "monopolies"-to shift the burden of defending the rights to those who claim they do some good. This was the strategy of McCauley in the mid-nineteenth century as well. See Siva Vaidhyanathan, Copyrights and Copywrongs: The Rise of Intellectual Property and How It Threatens Creativity (New York: New York University Press, 2001), 142.

25

Letter from Thomas Jefferson to Isaac McPherson (August 13, 1813), in The Writings of Thomas Jefferson, vol. 6 (H. A. Washington, ed., 1861), 175, 180.

26

Though GPL is often referred to as a "copyleft" license, copyleft depends fundamentally upon copyright. Without a copyright, there would be no way to ensure that users of a bit of software code agreed to further obligations imposed upon the code by the GPL. Software dedicated to the public domain, for example, would not have the capacity to ensure that users were bound by GPL. But with copyrighted code offered subject to the terms of GPL, the user either complies with the license or is an infringing user of the code.

As I describe more below, GPL is not the only, or perhaps even the most important, open code license. Other open code licenses differ from GPL by not requiring that subsequent derivative works open up their own code base. For an evaluation of GPL's viability under German law, see Axel Metzger and Till Jaeger, "Open Source Software and German Copyright Law," International Review of Industrial Property & Copyright Law 32 (2001):52.

27

See http://www.fsf.org/copyleft/gpl.html.

28

Stallman, 56.

29

See Moody, 168 (describing open source definition).

30

For a fuller discussion of various open code licenses and their implications, see David McGowan, "Legal Implications of Open Source Software," University of Illinois Law Review (2001): 101, 113-19; Patrick K. Bobko, "Can Copyright Keep 'Open Source' Software Free?," AIPLA Quarterly Journal 28 (2000): 81; Ira V. Heffan, "Note: Copyleft: Licensing Collaborative Works in the Digital Age," Stanford Law Review 49 (1997): 1487; Daniel B. Ravicher, "Facilitating Collaborative Software Development: The Enforceability of Mass-Market Public Software Licenses," Virginia Journal of Law & Technology 5 (2000): 11.

31

Wayner, 260.

32

The latest operating system from Apple, OS X, includes a portion called Darwin that is licensed under an open source license. See http://www.apple.com/macosx/tour/ darwin.html.

33

For detailed analyses of this case, see Ken Auletta, World War 3.0: Microsoft and Its Enemies (New York: Random House, 2001); David Bank, Breaking Windows: How Bill Gates Fumbled the Future of Microsoft (New York: Free Press, 2001); John Heilemann, Pride Before the Fall: The Trials of Bill Gates and the End of the Microsoft Era (New York: HarperCollins, 2001).

34

The government isn't the only one to make this allegation. As Red Hat chairman Bob Young describes it, "Microsoft appeared to have gone so far as to dictate how fast other companies could bring out new products-or if they could bring them out at all, for that matter." Young and Rohm, 7.

35

Some attempts were made at graphical user interface desktop environments before Windows hit pay dirt, including VisiOn (by VisiCalc), Top View (by IBM), GEM (by Digital Research), and Interface Manager (Microsoft's precursor to Windows); none fared well with consumers, however. Ceruzzi, 276.

36

In a well-known story, the company didn't actually write DOS but simply licensed it from Tim Paterson of Seattle Computer Products. Microsoft paid "about $15,000 for the rights to use Seattle Computer Products's work." Microsoft later paid more for the complete rights. Ibid., 270.

37

Stan Miastkowski, "A Cure for What Ails DOS," BYTE (August 1990): 107.

38

See Caldera v. Microsoft, 72 F. Supp. 2d 1295, 1299 (D. Utah, 1999).

39

Jim Carlton, Apple: The Intrigue, Egomania, and Business Blunders That Toppled an American Icon (New York: Times Books/Random House, 1997), 39-47.

40

Ibid., 28.

41

"Strategic behavior arises when two or more individuals interact and each individual's decision turns on what that individual expects the others to do." Douglas G. Baird, Robert H. Gertner, and Randal C. Picker, Game Theory and the Law (Cam-bridge, Mass.: Harvard University Press, 1994), 1. As I use the term, I mean to restrict it to cases where the resulting behavior raises questions under antitrust law.

As David Bank writes, the notion of a competitive strategy to protect Microsoft's base was at the core of Microsoft's product design. This was referred to within Microsoft as the "strategy tax." As Bank writes:

Gates invariably pressed the same point: Be more "strategic." He demanded hooks to lock in customers. The goal was to drive interdependencies between various Microsoft products and thus make it more painful for customers to switch to a competitor for any individual feature.

Bank, 134. Gates placed the need for "strategic advantage" above concern for "customers." As Gates wrote in an e-mail titled "HTML 'Openness' ":

People were suggesting that Office had to work equally well with all browsers and that we shouldn't force Office users to use our browser This is wrong and I wanted to correct this.

Ibid., 73-74. Instead, as he went on to instruct, the product should be designed to be "Office/Windows/Microsoft specific." Ibid.

This "strategy tax" was "inherently demoralizing," Bank writes. Ibid., 134. Software teams wanted to "win on the merits," ibid., not based on tricks built into the code. More generally, Bank argues, many within the firm believed the strategy tax would eventually harm Microsoft competitively. As Ben Slivka said to Gates, "I think your focus on 'common code' as a goal in itself has served to stifle innovation." Ibid., 99. Slivka pushed "un-fettered innovation," free from "the constraints of the company's historical franchises." Ibid. Another senior executive, Brad Silverberg, wrote, "I simply do not want to spend my life in meetings struggling with the internal issues, getting pissy mail from Billg, [or] hearing from people who want me to do unnatural and losing things to 'protect' Windows." Ibid., 175.

The idea that a company might succeed in this market without proprietary control-without, in other words, a "strategy tax"-might seem counterintuitive. Companies such as Cisco, however, demonstrate that success can come from something other than playing games with platforms. Cisco has flourished despite the fact that the platform it builds upon-the protocols of TCP/IP-are neutral and unowned. No doubt it faces fierce competition, but it has succeeded nonetheless.

As I describe in chapter 14, Bank believes the strategy within Microsoft has now changed to embrace neutral, rather than "taxing," competition.

42

Elizabeth Corcoran, "Microsoft Deal Came Down to a Phone Call," Washington Post, July 18, 1994, A1.

43

Ibid.

44

As David Bank writes:

At 3 am on a rainy Monday in April 1995, Gates declared in an email to his top lieutenants that the "Internet is destroying our position as the setter of standards [It] is taking away our power every day and will have eroded it irretrievably by the time broadband is pervasive on the course we are on right now.["]

Bank, 26. The core of the threat, Gates believed, was Netscape. "As Netscape's standards became more important than Windows'," Gates wrote, Netscape's browser would "commoditize the underlying operating system." Ibid., 27.

Gates was equally concerned about Microsoft's embrace of Java. As he wrote after reading an analysis that pushed neutral development of Java, "This scares the hell out of me It's still very unclear to me what our [OSes] will offer to Java that will make them unique enough to preserve our market position." Ibid.

45

The government relied upon extensive evidence from Mr. Gates's own e-mail and statements. See United States v. Microsoft, 84 F. Supp. 2d 9, 59-60 (D.D.C. 1999).

46

While it is a mistake to read too much into the loose use of the term leverage, it is clear that Microsoft executives believed "leveraging" IE through the use of Windows was the most effective way to gain market share. Bank, 70 (quoting Allchin). "We should think first about an integrated solution-that is our strength." Ibid., 69.

For a careful analysis of current economic thinking about tying and exclusive contracts (at the core of the Microsoft case), see Michael D. Whinston, "Exclusivity and Tying in United States v. Microsoft: What We Know, and Don't Know," Journal of Economic Perspectives 15 (2001): 63.

47

Ibid., 11, 13.

48

Ibid., 212.

49

Ibid., 228.

And obviously, too, it can control more than the pace of innovation. My concern in Code and Other Laws of Cyberspace was precisely the regulation that gets effected through code-"Code is Law"-and the concern that the more code controls our life, the more we should care about who the coders are. Denise Caruso has written powerfully on the same idea. As she puts it, "[I]t is still true today that software-written by a team of sleep-deprived programmers in some fusty cubicle-is the code that lays down the absolute law by which we live our digital lives. We are not free to change that code; our choice is to love it or leave it. In the Microsoft trial the battle is really over whose law is to be sovereign, software's or the government's."

"The Legacy of Microsoft's Trial," New York Times, December 6, 1999, available at http://www.nytimes.com/library/tech/ 99/12/biztech/articles/06digi.html. She adds, "[C]yberspace is already regulated by software code, which by its nature decides whether and under what circumstances we will have privacy or anonymity and who can or cannot have access and how. Though this fact was not so terrifying when the software running the global Internet was built on open standards, the situation has become significantly more ominous as the code has become commercialized and thus private and not open to public scrutiny [W]hen commercial code begins to determine the Internet's architecture, it creates a kind of privatized law that must be regulated if the public interest and public values are to be democratically represented." Ibid.

50

One of the most important Unix clones was a project called BSD-the Berkeley Software Distribution Project. The BSD project was born when Unix was still free. After AT&T reclaimed control of Unix, it demanded that BSD cease its distribution. BSD refused, and lawsuits ensued. After protracted litigation, the right of BSD (with certain code rewritten) to distribute its version of Unix free of AT&T was sustained through a settlement. For more on the BSD lawsuit and BSD generally, see Wayner, 38, 49-54 (discussing AT&T's lawsuit against BSD), and 92-95 (discussing the development of BSD); Cheryl Garber, "USL v. Berkeley," UNIX Review 10 (1992): 32.

BSD gave its code away under a very weak license. Anyone could do anything with the code, as long as he kept the copyright notices clear. But as groups developed to support this free code, divisions among the troops developed as well: from this common base of BSD, the project forked to include FreeBSD, NetBSD, and OpenBSD, among others. Wayner, 98-99.

51

There is an interesting but unresolved debate about whether GPL (the strictest of the open code licenses) does more to stem the risk of forking than more permissive open code licenses. Compare Young and Rohm, 180 (arguing that GPL minimizes forking), with Wayner, 221 (arguing the contrary).

52

Ibid., 210.

53

Eric S. Raymond, The Cathedral and the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary (Beijing and Sebastopol, Calif.: O'Reilly, 1999), 151.

54

Wayner, 289.

55

"IBM Extends Software Leadership on Linux," IBM Press Release, December 8, 2000.

56

Economists have only begun to examine the incentives that might affect open code projects. Josh Lerner and Jean Tirole have summarized some of the benefits, relative to closed code projects, as follows: (1) lower costs due to (a) familiarity of the code from, e.g., university training, and (b) customization/bug-fixing advantages; and (2) higher benefits, especially due to signaling from (a) better performance measurement (easier to demonstrate skill), (b) "full initiative" (because no supervisory involvement), and (c) greater labor market fluidity. See Josh Lerner and Jean Tirole, "The Simple Economics of Open Source" (NBER Working Paper No. 7600, December 2000).

James Bessen has offered a far more ambitious model of the benefits from open code projects. In a paper not yet published, Bessen argues that open code provision is superior to closed source provision when the public good (i.e., software) is a complex public good. The intuition is that the costs of debugging complex software projects become prohibitively high within a closed code environment. As Bessen writes, "[I]f a product had 100 independent features and each combination took only one second to test, then the job could not be finished before the sun is predicted to swallow the earth even if every human currently alive spent every second until then testing." James Bessen, "Open Source Software: Free Provision of Complex Public Goods" (working version, April 2001), http://researchoninnovation.org. Bessen concludes that "strong property rights may actually limit provision of complex [public goods]," and hence that open code projects have a competitive advantage over closed.

57

This argument is related to a point made by Carl Shapiro and Hal R. Varian in Information Rules: A Strategic Guide to the Network Economy (Boston, Mass.: Harvard Business School Press, 1999), 256-59. As they argue there, in a market heavily dependent upon standards, there is often an incentive for competitors to contribute to public standards rather than to make the standard proprietary. The battle over the standard HTML is an example of this. For a while, both Netscape and Microsoft tried to develop extensions to HTML that were peculiar to their own servers and Web development software. Their aim was to split the common standard base to induce the market to follow their own design. Later, however, they both apparently decided that this effort to divide the standard would not be profitable; better if there were more on a common platform than fewer on a proprietary platform. Varian and Shapiro demonstrate the conditions under which that behavior could be rational. See also Varian, "Buying, Sharing, and Renting Information Goods," Journal of Industrial Economics 48 (2000): 473. On Microsoft's attitude toward a neutral HTML, see Bank, 74 ("If Microsoft doesn't get to do anything 'proprietary' with HTML in the browser," Gates wrote, then "we have to stop viewing HTML as central to our strategy and get onto another strategy."). See also ibid., 73 (regarding Office). See also Moody, 200. Moody believes that because the Apache server was a dominant server for HTTP, and uncontrolled by any commercial venture, it did not fracture in the same way. See ibid., 149.

58

For articles identifying scenarios in which this proposition fails, see Douglas Lichtman et al., "Shared Information Goods," Journal of Law & Economics 42 (1999): 117; Hal R. Varian, "Buying, Sharing, and Renting Information Goods," Journal of Industrial Economics 48 (2000): 473; Douglas Lichtman, "Property Rights in Emerging Platform Technologies," Journal of Legal Studies 29 (2000): 615.

59

James Boyle, Shamans, Software, and Spleens: Law and the Construction of the Information Society (Cambridge, Mass.: Harvard University Press, 1996), ch. 4.

60

This is especially the case with embedded systems, which have exploded recently because of the ability to incorporate essential operating system code freely. See "Em-bedded Linux Basics," http://lw.itworld.com/linuxworld/lw-2000-05/lw-05-embedded_p. html; "Companies Bet on Embedded Linux," http://www.forbes.com/2000/04/07/mu4. html.

CHAPTER 5

1 See Susan J. Douglas, Inventing American Broadcasting 1899-1922, reprint ed. (Baltimore: Johns Hopkins University Press, 1997), 227-28; Eszter Hargittai, "Radio's Lessons for the Internet," Communications ACM 43 (2000): 51, 54.

2

See National Broadcasting Co. v. U.S., 319 U.S. 190, 210 (1943). Radio "broadcasting," however, did not begin until KDKA began its service in Pittsburgh in 1920. See Thomas W. Hazlett, "The Wireless Craze, the Unlimited Bandwidth Myth, the Spectrum Auction Faux Pas, and the Punchline to Ronald Coase's 'Big Joke': An Essay on Air-wave Allocation Policy" (Working Paper 01-01, AEI-Brookings Joint Center for Regulatory Studies, January 2001), 95 (Harvard Journal of Law and Technology, spring 2001).

3

See Red Lion Broadcasting v. FCC, 395 U.S. 367, fn4 (1969). The statute was the Radio Act of 1927, ch. 169, 44 Stat. 1162 (1927).

4

For an excellent account of the emergence of broadcasting as we would recognize it, see Douglas, 292-322.

5

Robert W. McChesney, Rich Media, Poor Democracy: Communication Politics in Dubious Times (Urbana: University of Illinois Press, 1999), 192.

6

Ibid., 194.

7

Hazlett, "The Wireless Craze," 48.

8

Edward S. Herman and Robert W. McChesney, The Global Media: The New Missionaries of Corporate Capitalism (London and Washington, D.C.: Cassell, 1997), 138.

9

Erik Barnouw, A History of Broadcasting in the United States, vol. 1 (New York: Oxford University Press, 1966-70), 96, 177-78, 243, 262-66.

10

See Peter W. Huber, Michael K. Kellogg, and John Thorne, Federal Telecommunications Law, 2nd ed. (Gaithersburg, Md.: Aspen Law & Business, 1999), 220-21, 865-66.

11

NBC v. U.S., 213.

12

Ibid., 228 (Murphy, J., dissenting).

13

R. H. Coase, "The Federal Communications Commission," Journal of Law and Economics 2 (1959): 1.

14

The idea originated in an article by a University of Chicago law student. See Leo Herzel, " 'Public Interest' and the Market in Color Television Regulation," University of Chicago Law Review 18 (1951): 802, 811-12 ("The FCC could lease channels for a stated period to the highest bidder without making any other judgment of the economic or engineering adequacy of the standards to be used by the applicant."). See also Richard A. Posner, Economic Analysis of Law, 4th ed. (Boston: Little, Brown, 1992), 673 (saying the Supreme Court's rationale for different spectrum regulation rules "is economic nonsense").

Ronald Coase, however, made the idea famous. See Yochai Benkler, "Overcoming Agoraphobia: Building the Commons of the Digitally Networked Environment," Harvard Journal of Law & Technology 11 (1998): 287, 316-17.

15

As Coase wrote: "But it is a commonplace of economics that almost all resources used in the economic system (and not simply radio and television frequencies) are limited in amount and scarce, in that people would like to use more than exists. Land, labor and capital are all scarce, but this, of itself, does not call for government regulation. It is true that some mechanism has to be employed to decide who, out of the many claimants, should be allowed to use the scarce resource. But the way this is usually done in the American economic system is to employ the price mechanism, and this allocates resources to users without the need for government regulation." R. H. Coase, "The Federal Communications Commission," Journal of Law & Economics 2 (1959): 1, 14.

16

Hazlett has been prolific in advancing this argument. See, e.g., Hazlett and Sosa, "Chilling the Internet? Lessons from FCC Regulation of Radio Broadcasting," Michigan Telecommunications & Technology Law Review 4 (1997-98): 35; Hazlett, "Physical Scarcity, Rent Seeking, and the First Amendment," Columbia Law Review 97 (1997): 905; Hazlett, "Assigning Property Rights to Radio Spectrum Users: Why Did FCC License Auctions Take 67 Years?," Journal of Law & Economics 41 (1998): 529; Hazlett, "Spectrum Flash Dance: Eli Noam's Proposal for 'Open Access' to Radio Waves," Journal of Law & Economics 41 (1998): 805; Hazlett and Sosa, "Was the Fairness Doctrine a 'Chilling Effect'? Evidence from the Postderegulation Radio Market," Journal of Legal Studies 26 (1997): 279; Hazlett, "The Rationality of U.S. Regulation of the Broadcast Spectrum," Journal of Law & Economics 23 (1990): 133.

17

See, e.g., Hazlett, "Spectrum Flash Dance," 816. ("Profit maximization will force competitive band managers to devise better technical means to increase wireless communications traffic.")

18

The source of the skepticism is traced, as Yochai Benkler describes, to Claude E. Shannon, "Communication in the Presence of Noise," Proceedings of the IRE 37 (1949): 10, and Claude E. Shannon, "A Mathematical Theory of Communication," Bell System Technology Journal 27 (1948): 379 and 623 (two-part publication). "These articles lay out the theoretical underpinnings of direct sequencing spread spectrum." Benkler, Overcoming Agoraphobia, 323, note 171.

19

In order to be called "spread spectrum," two conditions must be met: (1) the transmitted signal bandwidth is greater than the minimal information bandwidth needed to successfully transmit the signal; and (2) some function other than the information itself is being employed to determine the resultant transmitted bandwidth. Robert C. Dixon, "Why Spread Spectrum?," IEEE Communication Society Magazine (July 1975): 21-25. See also Yochai Benkler, "From Consumers to Users: Shifting the Deeper Structures of Regulation Toward Sustainable Commons and User Access," Federal Communications Law Journal 52 (2000): 561, 576-77. For an excellent study that links technological and policy questions, see Stuart Buck et al., "Spread Spectrum: Regulation in Light of Changing Technologies" (1998), http://cyber.law.harvard.edu/ltac98/studentpapers.html. See also "The Unwired World" (Special Report), Scientific American 278 (1998): 69.

20

E-mail from David P. Reed to Lawrence Lessig, March 25, 2001.

21

For an introduction, see http://www.freesoft.org/CIE/Topics/60.htm. (This scheme, one of many that can regulate access to a hardware medium, is referred to as CSMA/ CD, an acronym for "carrier sense, multiple access/collision detect.") See also Douglas E. Comer, Internetworking with TCP/IP, 4th ed., vol. 1 (Upper Saddle River, N.J.: Prentice-Hall, 2000), 28 (explaining how the Ethernet handles access and collision detection through CSMA/CD).

22

Hazlett, "Physical Scarcity," 928-29; Hazlett, "The Wireless Craze," 10.

23

For discussions of the technology, see Benkler, From Consumers to Users, 576-77; Benkler, Overcoming Agoraphobia, 290 ("Technological developments in digital information processing and wireless communications have made possible an alternative regulatory approach. It is now possible to regulate wireless communications as we do the Internet-with minimal standard protocols-or the highway system-with limited governmentally imposed rules of the road."); ibid., part IV, 322-30 (describing examples of current business models utilizing spread spectrum technologies where unlicensed operations are permitted: proprietary infrastructure cellular network-Metricom's Ricochet wireless network, ad hoc network of equipment owned by users-rooftop networks, and publicly owned infrastructure of unlicensed devices-the NSF field tests).

24

See Steve Stroh, " Hollywood Star Was a Wireless Pioneer," CLEC Magazine (March-April 2000): 20. See also http://www.ncafe.com/chris/pat2/index.html (describing Lamarr's contribution).

25

Anna Couey, "The Birth of Spread Spectrum: How 'The Bad Boy of Music' and 'The Most Beautiful Girl in the World' Catalyzed a Wireless Revolution-in 1941," http://www.sirius.be/lamarr.htm. Lamarr's work inspired what is known as the "frequency-hopping" form of spread spectrum. "Direct sequence," or CDMA, was developed later.

26

For more on the technology, see Roger L. Peterson, Rodger E. Ziemer, and David E. Borth, Introduction to Spread-Spectrum Communications (Englewood Cliffs, N.J.: Prentice-Hall, 1995); Amer A. Hassan, John E. Hershey, and Gary J. Saulnier, Perspectives in Spread Spectrum (Boston and London: Kluwer Academic, 1998).

27

Telephone interview with Dave Hughes, November 13, 2000.

28

Ibid.

29

See http://www.techweek.com/articles/8-23-99/wireless.htm.

30

See, e.g., U.S. Congress, Office of Technology Assessment, Telecommunications Technology and Native Americans: Opportunities and Challenges, OTA-ITC-621 (Washington, D.C.: U.S. Government Printing Office, August 1995). See also statement "FCC Takes Steps to Promote Access to Telecommunications on Tribal Lands" (FCC news release, June 8, 2000).

31

The FCC has set aside three data bands as "unlicensed," meaning users can deploy devices that rely upon these bands without permission from the FCC or a spectrum owner. The regulations governing this use are found in part 15 of the FCC's rules, and the bands are 915 MHz (902-928 MHz), 2.4 GHz (2,400-2,483.5 MHz), and 5.7 GHz (5,725-5,850 MHz). Part 15 devices may not cause any harmful interference to authorized services and must accept any interference that may be received. C.F.R., 47, 15.5. Operation under these rules was limited to frequency-hopping and direct sequence spread spectrum systems. But the FCC announced its intention to broaden the technologies permitted. See "In the Matter of Amendment of Part 15 of the Commission's Rules Regarding Spread Spectrum Devices" (DA 00-2317) (May 11, 2001); "Operation of Radio Frequency Devices Without an Individual License," 54 Fed. Reg. 17,710 (1989), codified at C.F.R., 47; pt. 2 and pt. 15 (1996). In addition, the FCC has six unlicensed bands for non-spread spectrum use. They are the Citizens Band, the Radio Control Service Band, the Low Power Radio Service Band, the Wireless Telemetry Service Band, the Medical Implant Communications Band, and the Family Radio Service Band.

32

802.11b refers to a standard developed by the IEEE. See http://www.manta.ieee. org/groups/802/11/. The standard enables wireless protocols in the unlicensed spectrum in the 2.5 GHz range at 11 Mbps.

33

See also James Gleick, "The Wireless Age: Theories of Connectivity" New York Times Magazine, April 22, 2001 (discussing other wireless technologies).

34

No doubt there are many who are competing to develop technologies for these un-licensed options. For a description of the range of companies, see Amara D. Angelica, "Wireless Internet Access," TechWeek, http://www.techweek.com/articles/8-23-99/wireless. htm.

35

This does not mean the regulation would necessarily be harmless. Bad regulation at the device level could well inhibit innovation just as bad regulation at the spectrum level has inhibited innovation.

36

These "free spectrum" or "open spectrum" advocates are all very different, though they share a belief that spectrum should be managed differently. The differences among them boil down to their attitude about scarcity. At one extreme is a group we might call utopians. These people, such as Dave Hughes, Paul Baran, David Reed, and George Gilder, believe that it is more likely than not that spectrum, properly used, would in essence be unlimited. See, e.g., Paul Baran, "Is the UHF Frequency Shortage a Self-Made Problem?," (paper given at the Marconi Centennial Symposium, Bologna, Italy, June 23, 1995, on file with the Columbia Law Review), 3; Gilder, Telecosm, 158-64; David Hughes and Kambiz Hooshmand, "ABR Stretches ATM Network Resources," Data Communications 24, no. 5 (April 1995): 123.

At the other extreme is Eli Noam, who believes that spectrum is scarce and therefore needs a system for rationing. See Eli Noam, "The Future of Telecommunications Regulation," NRRI Quarterly Bulletin 20 (1999): 17; Eli Noam, "Spectrum Auctions: Yesterday's Heresy, Today's Orthodoxy, Tomorrow's Anachronism: Taking the Next Step to Open Spectrum Access," Journal of Law & Economics 41 (1998): 765; Eli Noam, "Beyond Auctions: Open Spectrum Access," in Regulators' Revenge: The Future of Telecommunications Deregulation, Tom W. Bell and Solveig Singleton, eds. (Cato Institute, 1998), 1: Eli Noam, "Will Universal Service and Common Carriage Survive the Telecommunications Act of 1996?," Columbia Law Review 97 (1997): 955; Eli Noam, "Spectrum and Universal Service," Telecommunications Policy 21 (1997); Eli Noam, "Taking the Next Step Beyond Spectrum Auctions: Open Spectrum Access," IEEE Communications Magazine 33 (1995): 66; Eli Noam, "The Federal-State Friction Built into the 1934 Act and Options for Reform," in American Regulatory Federalism & Telecommunications Infrastructure, Paul Teske, ed. (Hillsdale, N.J.: Lawrence Erlbaum Associates, 1995), 113-14; Eli Noam, "Beyond Liberalization II: The Impending Doom of Common Carriage," Telecommunications Policy 18 (1994): 435; Eli Noam, "A Public and Private-Choice Model of Broadcasting," Public Choice 55 (1987): 163. His system has the flavor of a market, but it is more the kind of market that controls access to subways than the ownership of taxis: just as a subway rider purchases a token at the moment he or she needs to ride the subway, the user of spectrum, in real time, would purchase a token that would assure his or her right to use spectrum. Unlike subway tokens, however, these tokens would fluctuate in price as the demand for spectrum changes. During times of great congestion, the price would go up; during times of low usage, the price would fall. This market would be competitive, so the prices would be neutral, and hence the proposal still fits my definition of a commons. But it is different from Gilder's and Reed's in that it requires money to get access.

In the middle is Yochai Benkler, who is agnostic about the technology but clear about the constitutional norm. See Yochai Benkler, "Siren Songs and Amish Children: Autonomy, Information, and Law," New York University Law Review 76 (2000): 23, 81, where he talks about constitutionality. If spectrum is effectively unlimited, or if it could be organized to be effectively unlimited, then it should be structured to permit free access. But if congestion is a problem, then we should allocate access to the spectrum to minimize that congestion and possibly, if necessary, adopt a structure like Noam's. See also Benkler, "From Consumers to Users," 561; Benkler, "Viacom-CBS Merger: From Consumers to Users: Shifting the Deeper Structures of Regulation Toward Sustainable Commons and User Access," Federal Communications Law Journal 52 (2000): 561; Benkler, "Free as the Air to Common Use: First Amendment Constraints on Enclosure of the Public Domain," New York University Law Review 74 (1999): 354; Benkler, "Communications Infrastructure Regulation and the Distribution of Control over Content," Telecommunications Policy 22 (1998): 183; Benkler, "Overcoming Agoraphobia," 287.

These are important differences, though in the end they matter in only one sense. No one believes it has been proven that spectrum as it is presently used is unlimited. David Reed points to the research of Tim Shepard and others demonstrating that a wireless network could be structured so that an increase in the number of users actually increases total capacity. Tim Shepard, Decentralized Channel Management in Scalable Multihop Spread-Spectrum Packet Radio Networks, MIT, EECS thesis, 1995. See also Timothy J. Shepard, "A Channel Access Scheme for Large Dense Packet Radio Networks," at http://www.acm.org/pubs/articles/proceedings/comm/248156/p219-shepard/ p219-shepard.pdf. Reed argues that the " 'capacity' of a free space radio network is not fixed, but instead is an increasing function of the density of user 'terminals' in that space." Telephone interview with David Reed, February 7, 2001. This means that as more people enter the shared spectrum space (as the number of terminals, that is, increases), the available spectrum increases, not decreases. The more nodes there are on the network, the closer these nodes are; the closer they are, the weaker the signal connecting these nodes must be; the weaker the signal, the more signals there can be. A network of wireless nodes could expand spectrum capacity as the number of nodes increases.

But even without this increasing capacity, Baran and Hughes both argue that given existing capacity, properly deployed, we could fulfill all the need we have for delivering data across the ether without any constraint at all.

The differences between those who see bandwidth as essentially unlimited and those who see it as scarce obscure a more fundamental agreement: All would agree that spectrum use could undergo a radical shift-a paradigm change, in Eli Noam's terms. All would agree that this change would fundamentally alter the nature of our use of spectrum and would lead to an explosion of innovation in the use of spectrum that would not otherwise, under either the government or the market model, exist. Everyone now concedes that state-licensed spectrum has stifled innovation-the most glaring example was the government's stalling, because of the pressure of industry, the development of FM radio. See Edwin R. Armstrong: A Man and His Invention (Eli Noam, ed., forthcoming). All agree that the alternative of allowing spectrum to be sold is like "having the old AT&T auction off the right to compete against itself." Noam, "Beyond Spectrum Auctions," 473.

Noam and Benkler have an even stronger (from a legal perspective) argument against the sale of spectrum. In a world where the control of spectrum was not "necessary," as the Supreme Court said it was in the NBC case, why was control of spectrum constitutionally permitted? Spectrum is speech, and the regulation of spectrum is the regulation of speech. The constitutional status of spectrum auctions is rendered problematic by the emergence of this alternative technology. Control is not "necessary" anymore, any more than control of newspapers is necessary. It would certainly be unconstitutional to force newspapers to buy a license to print (the way taxi drivers have to buy a medallion to drive a taxi), so why is it any less unconstitutional to force a newspaper to buy the right to broadcast?

The key to this use of spectrum would be a robust software-defined radio (SDR) technology, meaning a radio whose protocols get set by software, enabling easy switching among different frequencies, carriers, and networks. Just as a modem, for example, can switch among a number of protocols for modulating data transmissions, so too could a SDR switch among protocols for communicating between radios. The switching in SDRs is much greater than in modems, however, as it would include not only protocols, but also power levels and modes of transmitting. But in principle, the system could allow radios to sniff the environment and determine which kind of communication for that environment would work best.

The government is doing some of the most important work in this area. DARPA, which was responsible in part for the birth of the Internet, is experimenting with SDRs that could communicate in battlefield contexts. To make this succeed, however, the government needs "a common architecture so that you're not tied to one particular technology as an implementation of your next-generation radio." Telephone interview with Bill Lane, FCC, November 15, 2000. It is also important, as Intel has argued in comments to the FCC, that the rules unbundle radio control from user application software, so as to facilitate the broadest range of innovation at the software level. As Intel argues, "[A] programmable platform would lower entry costs for the independent third party developers who played a key role in delivering innovative solutions elsewhere." Intel, "Comments of Intel Corporation, In the Matter of Authorization and Use of Software Defined Radios," ET Docket No. 00-47 (Washington), 4.

FCC chairman Michael Powell has a similar intuition about the potential: "[A]dvanced technologies such as spread spectrum have ushered in all sorts of innovative and efficient services. Indeed, rather than being a uniquely scarce resource[], spectrum has the potential to be a bottomless resource, unlike coal, oil, or timber[,] which are more susceptible to depletion. Perhaps it is uniquely abundant rather than uniquely scarce." Michael K. Powell, "Willful Denial and First Amendment Jurisprudence," Remarks Before the Media Institute, April 22, 1998 (transcript available at http://www. fcc.gov/Speeches/Powell/spmkp808.html), 5 (emphasis added).

37

George Gilder, Telecosm: How Infinite Bandwidth Will Revolutionize Our World (New York: Free Press, 2000), 160.

CHAPTER 6

1 Carol Rose, "The Comedy of the Commons: Custom, Commerce, and Inherently Public Property," University of Chicago Law Review 53 (1986): 711, 713.

2

Ibid., 712.

3

Ibid., 744.

4

See ibid., 752 (describing how "antiholdout" rules historically protected public usage of roads from private actors who might "siphon off its public value"). The development of the highway system has had strong direct (employment) and indirect (productivity) effects on the United States economy. See M. Ishaq Nadiri and Theofanis P. Mamuneas, "Contribution of Highway Capital to Output and Productivity Growth in the U.S. Economy and Industries," Department of Transportation Federal Highway Administration, August 1998, available at http://www.fhwa.dot.gov/policy/gro98cvr.htm; T. A. Heppenheimer, "The Rise of the Interstates," American Herald of Inventions & Technology 7 (1991): 8; Mark H. Rose, Interstate 15-67 (Knoxville: University of Tennessee Press, 1990). See also The New America That's Coming (1956) (special report by editors of Automotive Industries) (describing effect of "superhighways program on industry, commerce and vehicle design").

To the extent transportation has been privately owned, the dominant regulatory rule has been common carriage. Railroads, for example, were privately owned; but they were also subject to common carrier obligations. The effect of these common carrier regulations was to render the common carrier a commons as well. See, e.g., Andrew A. Nimelman, "Of Common Carriage and Cable Access: Deregulation of Cable Television by the Supreme Court," Federal Communications Law Journal 34 (1982): 167, 173; Robert Means and Deborah Cohn, "Common Carriage of Natural Gas," Tulane Law Review 59 (1985): 529. As Yochai Benkler writes, however, "after the internal combustion engine was invented, it was not a better system for awarding railroad franchises that was needed, but a well-regulated commons like our national highway system." Yochai Benkler, "The Commons as a Neglected Factor of Information Policy" (paper presented at Telecommunications Policy Research Center conference, October 3-5, 1998), 68. And indeed, the revenue from rail transportation was $28,348,895,000 in 1992, compared with $135,437,000,000 for local and long-haul trucking services. See section 21, "Transportation-Land," of 1997 U.S. Economic Census conducted by U.S. Census Bureau, available at http://www.census.gov/prod/2/gen/96statab/transind.pdf.

5

Rose, "The Comedy of the Commons," 759, citing President of Cincinnati v. Lessee of White, 31 U.S. (6 Pet.) 429 (1832) (recognizing an implied dedication of a square for traditional public use). See also Hanoch Dagan and Michael A. Heller, "The Liberal Commons," Yale Law Journal 110 (2001): 549; Michael A. Heller, "The Tragedy of the Anticommons: Property in the Transition from Marx to Markets," Harvard Law Review 111 (1998): 621, 622-26; Alison Rieser, "Prescriptions for the Commons: Environmental Scholarship and the Fishing Quotas Debate," Harvard Environmental Law Review 23 (1999): 393; Elinor Ostrom, Governing the Commons: The Evolution of Institutions for Collective Action (Cambridge, England, and New York: Cambridge University Press, 1990), 2-23.

Henry Smith has identified a similar strategic cost in semicommons contexts. See Henry E. Smith, "Semicommon Property Rights and Scattering in the Open Fields," Journal of Legal Studies 29 (2000): 131, 161-62.

6

Rose, "The Comedy of the Commons," 769.

7

Robert Merges offers a complementing argument, focusing on interoperability and the value brought by individuals' investment in, for example, learning the commands in a program. Robert Merges, "Who Owns the Charles River Bridge? Intellectual Property and Competition in the Software Industry" (working paper, 1999).

8

The classic text supporting a broad range of open or free resources, building on the work of Joseph Schumpeter, is Richard R. Nelson and Sidney G. Winter, An Evolutionary Theory of Economic Change (Cambridge, Mass.: Belknap Press of Harvard University Press, 1982). As they write: "[I]nnovation in the economic system-and indeed the creation of any sort of novelty in art, science, or practical life-consists to a substantial extent of a recombination of conceptual and physical materials that were previously in existence. The vast momentum of scientific, technological, and economic progress in the modern world derives largely from the fact that each new achievement is not merely the answer to a particular problem, but also a new item in the vast storehouse of components that are available for use, in 'new combinations,' in the solution of other problems in the future." Ibid., 130. For a recent work advancing an analytic framework for evaluating "innovation policy," see Brett Frischmann, "Innovation and Institutions: Rethinking the Economics of U.S. Science and Technology Policy," Vermont Law Review 24 (2000): 347.

9

See David P. Reed, Jerome H. Saltzer, and David D. Clark, "Comment on Active Networking and End-to-End Arguments," IEEE Network 12, no. 3 (May-June 1998): 69-71.

10

Ostrom too has found that commons problems are best solved as commons where environments are "uncertain and complex." Ostrom, 88-89.

11

The argument is related to a point made by a number of scholars about patents. Where the use of an invention is unknown, the transaction costs of licensing the invention are high. This can mean that patents over such inventions inhibit, rather than induce, innovation. See Arti Kaur Rai, "Regulating Scientific Research: Intellectual Property Rights and the Norms of Science," Northwestern University Law Review 94 (1999): 77, 136-37; James Bessen and Eric Maskin, "Sequential Innovation, Patents, and Imitation" (Cambridge University, Department of Economics, Working Paper 00-01, January 2000).

12

As David Reed puts the same point:

In an application of technologies and structure you don't understand, there's a great value in creating the opportunity and framework in which innovators can experiment and build on a platform that allows them [to] do what they want to do.

Telephone interview with David Reed, February 7, 2001.

13

Mark Gaynor et al., "Theory of Service Architecture," 42. Baldwin and Clark use a similar notion to explain the value of modular design. See Carliss Y. Baldwin and Kim B. Clark, Design Rules, vol. 1 (Cambridge, Mass.: MIT Press, 2000), 234-37 ("Modularity creates design options and in so doing can radically change the market value of a given set of designs.").

14

Telephone interview with David Reed.

15

Though, as he acknowledges, the idea was first suggested by Dean Kim Clark. See Kim B. Clark, "The Interaction of Design Hierarchies and Market Concepts in Technological Evolution," Research Policy 14 (1985): 235-51. For a more recent study adding support to Christensen's thesis, see Richard Foster and Sarah Kaplan, Creative Destruction: Why Companies That Are Built to Last Underperform the Market-and How to Successfully Transform Them (New York: Currency/Doubleday, 2001). There is also a link to the work of William Abernathy, who has argued that "as designers worked on the agenda of problems established by the dominant design, the locus of competition between firms would shift from product improvement to cost reduction. A 'productivity dilemma' would then emerge, as the search for lower cost drove out, first, large innovations, and later, all but the most minor innovations." See Baldwin and Clark, 57.

The intuition behind Christensen's work-that an existing firm holds psychological commitments that make it hard for the firm to follow new leads-has support in a wide range of work in innovation economics. See, e.g., Paul Romer, "Thinking and Feeling," American Economic Review 90 (2000): 439.

16

E-mail from David S. Isenberg to Lawrence Lessig, January 29, 2001.

17

Clayton M. Christensen, The Innovator's Dilemma: The Revolutionary National Bestseller That Changed the Way We Do Business (Cambridge, Mass.: Harvard Business School Press, 1997).

18

Telephone interview with David Reed.

19

See Jim Carlton, Apple: The Inside Story of Intrigue, Egomania, and Business Blunders (New York: Times Books/Random House, 1997), 38-61 (describing Apple's internal licensing debate).

20

Baldwin and Clark, 11. In Baldwin and Clark's terms, modularity describes an "interdependence within and independence across modules." Carliss Y. Baldwin and Kim B. Clark, Design Rules, vol. 1 (Cambridge, Mass.: MIT Press, 2000), 63. Modules are units "whose structural elements are powerfully connected among themselves and relatively weakly connected to elements in other units." They thus depend upon "a framework-and architecture-that allows for both independence of structure and integration of function." Ibid.

"Architecture" is thus fundamental to all modular systems. As Baldwin and Clark describe, "[T]he word is both an evocative term with a rich set of associations, and a technical term, meant to have a precise meaning in a particular context." Ibid., 215. A proper architecture enables parts to be broken into smaller units, with clean and clear interfaces among the smaller modules.

Baldwin and Clark thus advance four "principles of modularity": (1) Create nested, regular, hierarchical structures in a complex system; (2) define independent components within an integrated architecture; (3) establish and maintain rigorous partitions of design information into hidden and visible subsets; (4) invest in clear interfaces and "good" module tests. Ibid., 413. These together "made it possible for human beings of individually limited capacity to design and produce ever more complex machines and programs. [T]he resulting modular designs could change in unpredicted, yet coordinated ways. They could be improved via a decentralized, value-seeking process, which we have called design evolution." Ibid.

21

See Michael Walzer, Spheres of Justice: A Defense of Pluralism and Equality (New York: Basic Books, 1984).

22

See, e.g., Margaret Jane Radin, Contested Commodities (Cambridge, Mass.: Harvard University Press, 1996), 132-36.

23

Letter from Thomas Jefferson to Isaac McPherson (August 13, 1813), in The Writings of Thomas Jefferson 6 (H. A. Washington, ed., 1861), 175, 180.

24

Rose, "The Comedy of the Commons," 742.

25

Some researchers have suggested that there is a way of organizing access to spectrum that would increase the spectrum as the number of users increases. See Timothy J. Shepard, "A Channel Access Scheme for Large Dense Packet Radio Networks," at http://www.acm.org/pubs/articles/proceedings/comm/248156/p219-shepard/ p219-shepard.pdf. See also Rick Boyd-Merritt, "Engineer's Wireless Internet in a Box Draws Interest," EE Times.com, http://www.eetimes.com/story/OEG19981007S0014.

26

The term is of recent origin. Professor Fisher traces its origins to the late nineteenth century; William W. Fisher III, "The Growth of Intellectual Property: A History of the Ownership of Ideas in the United States " (1999) 2, 8, available at http://cyber.law. harvard.edu/ipcoop/97fish.1.html. The term appears to have been used twice before 1900; Mitchell v. Tilghman, 86 U.S. 287 (1873) and Davoll v. Brown, 7 Fed. Cas. 197 (Cir. Ct., D. Mass. 1845). These early usages were essentially translations of European documents, except for Davoll, where the court uses the term in exactly its modern sense:

Only thus can ingenuity and perseverance be encouraged to exert themselves in this way usefully to the community; and only in this way can we protect intellectual property, the labors of the mind, productions and interests as much a man's own, and as much the fruit of his honest industry, as the wheat he cultivates, or the flocks he rears.

Its appearance in federal cases since the turn of the twentieth century has grown dramatically:

I am grateful to Professor Hank Greely for these data. Of course, in some sense intellectual property is certainly property; see Frank Easterbrook, "Intellectual Property Is Still Property," Harvard Journal of Law & Public Policy 13 (1991): 108. But to the untrained, the connotations of the term property will not suggest the limited nature this "property" is to have. As Vaidhyanathan argues:

It is essential to understand that copyright in the American tradition was not meant to be a "property right" as the public generally understands property. It was originally a narrow federal policy that granted a limited trade monopoly in exchange for universal use and access.

Siva Vaidhyanathan, Copyrights and Copywrongs: The Rise of Intellectual Property and How It Threatens Creativity (New York: New York University Press, 2001), 21.

27

Jessica Litman, Digital Copyright (Amherst, N.Y.: Prometheus Books, 2000), 15.

28

See, e.g., Pamela Samuelson et al., "A Manifesto Concerning the Legal Protection of Computer Programs," Columbia Law Review 94 (1994): 2308, 2427; Raymond Shih Ray Ku, "Copyright & Cyberspace: Napster and the New Economics of Digital Technology" (draft on file with author, April 7, 2001), 9 ("[W]hile digital technology facilitates the copyright and distribution of digital information, it also permits greater control over the use and distribution of information.").

CHAPTER 7

1 As Judge Posner writes, distinguishing the rules for land from the rules for copyrighted material:

One reason [the two are different] is that it is more inefficient to have unowned land lying around (say, as the result of the expiration of a time-limited property right) than to have unowned intellectual property. Ideally, all land should be owned by someone, to prevent the congestion externalities that we discussed in connection with the natural pasture from arising. But there is no parallel problem concerning information and expression. A's use of some piece of information will not make it more costly for B to use the same information.

Richard A. Posner, Economic Analysis of Law, 4th ed. (Boston: Little, Brown, 1992), 41.

Number of Per 100k Decade References cases

1900-1919 1 1920-1929 0 1930-1939 4 1.163 1940-1949 9 3.399 1950-1959 15 5.038 1960-1969 11 2.827 1970-1979 56 8.213 1980-1989 341 26.42 1990-1999 1721 86.37 2000-2001 466

2

"Works are not crafted out of thin air." James Boyle, Shamans, Software, and Spleens: Law and the Construction of the Information Society (Cambridge, Mass.: Harvard University Press, 1996), 57.

3

Feist Publications, Inc. v. Rural Telephone Service Co., Inc., 499 U.S. 340, 345-47 (1991).

4

Siva Vaidhyanathan, Copyrights and Copywrongs: The Rise of Intellectual Property and How It Threatens Creativity (New York: New York University Press, 2001), 203.

5

The United States finally extended copyright protection to foreign publishers through the International Copyright Act of 1891, ch. 565, 13, 26 Stat. 1110 (1891). Before 1891, "the United States was notorious for its singular and, in many regards, cavalier attitude toward the intellectual property of foreigners." William P. Alford, "Making the World Safe for What? Intellectual Property Rights, Human Rights and Foreign Economic Policy in the Post-European Cold War World," New York University Journal of International Law & Politics 29 (1997): 135, 146. See also Jessica Litman, Digital Copyright (Amherst, N.Y.: Prometheus Books, 2000), 15.

6

Richard A. Posner, Law and Literature, rev. and enlarged ed. (Cambridge, Mass.: Harvard University Press, 1998), 389. The first United States case to decide the question comes in 1853, when a circuit court held that the copyright to Uncle Tom's Cabin did not reach a German translation of the same work. Vaidhyanathan, 92-93.

7

John Tebbel, A History of Book Publishing in the United States: The Creation of an Industry 1630-1865 (New York: R. R. Bowker, 1972), 141.

8

These laws were Pub. L. No. 87-668, 76 Stat. 555 (1962); Pub. L. No. 89-142, 79 Stat. 581 (1965); Pub. L. No. 90-141, 81 Stat. 464 (1967); Pub. L. No. 90-416, 82 Stat. 397 (1968); Pub. L. No. 91-147, 83 Stat. 360 (1969); Pub. L. No. 91-555, 84 Stat. 1441 (1970); Pub. L. No. 92-170, 85 Stat. 490 (1971); Pub. L. No. 92-566, 86 Stat. 1181 (1972); Pub. L. No. 93-573, title I, 104, 88 Stat. 1873 (1974).

9

On books, see Stephen Breyer, "The Uneasy Case for Copyright: A Study of Copyright in Books, Photocopies, and Computer Programs," Harvard Law Review 84 (1970): 281 (acknowledging the economic rationale for copyright protection of books and films, but not software). The MPAA estimates the average cost of a feature film (including studio overhead and capitalized interest) was $51.5 million in 1999. See MPAA, "MPAA Average Negative Costs," slide 14 of 44 (visited June 21, 2001), http://www.mpaa.org/ useconomicreview/2000Economic/slide.asp?ref=14.

10

As Yochai Benkler writes, "[M]ainstream economics very clearly negates the superstition that if some property rights in information are good, then more rights in information are even better." Yochai Benkler, "A Political Economy of the Public Domain: Markets in Information Goods Versus the Marketplace of Ideas," in Expanding the Boundaries of Intellectual Property: Innovation Policy for the Knowledge Society, Rochelle Cooper Dreyfuss and Diane Leenheer Zimmerman, eds. (Oxford: Oxford University Press, 2001), 267, 271. As Vaidhyanathan argues:

Through a series of case studies in different media through the 20th Century, it argues for 'thin' copyright protection: just strong enough to encourage and reward aspiring artists, writers, musicians, and entrepreneurs, yet porous enough to allow full and rich democratic speech and the free flow of information.

Vaidhyanathan, 8.

The skepticism among economists about perfect or extremely strong copyright protection is well known. For an expansive economic account, see Richard Watt, Copyright and Economic Theory: Friends or Foes? (Cheltenham, England, and Northampton, Mass.: E. Elgar, 2000). For a rich philosophical survey of justifications for copyright, see Peter Drahos, A Philosophy of Intellectual Property (Aldershot, England, and Brookfield, Vt.: Dartmouth Publishing Company, 1996).

11

Posner, Law and Literature, 391. A related point is made by Watt, who describes conditions under which piracy of copyrighted work is in fact favorable to the copyright owner. See Richard Watt, Copyright and Economic Theory: Friends or Foes? (Cheltenham, England, and Northampton, Mass.: E. Elgar, 2000), 58-67, 201 ("some copyright 'piracy' is highly likely to be socially efficient"). As Watt concludes, economic theory then is guilty of pointing out that there exist cases in which legal copyright protection hampers rather than helps society in general. Perhaps more surprisingly, economists can show that legal copyright protection can also hamper copyright holders and producers of originals themselves. Hence, "economic theory can perhaps best be thought of as throwing out a warning to copyright advocates, that they should take care not to lobby for policy that ends up damaging the interests of copyright holders, or those of the society in general." See ibid., 200.

12

Twentieth Century Music Corp. et al. v. Aiken, 422 U.S. 151, 154-55 (1975).

13

While Fourneaux is credited with inventing the first player piano, in 1902, Melville Clark was the first to create one with the full eighty-eight-key range of the standard piano. Clark was also one of the first to produce the player and the piano combined in a self-contained unit. See Harvey Roehl, Player Piano Treasury: The Scrapbook History of the Mechanical Piano in America as Told in Story, Pictures, Trade Journal Articles and Advertising (Vestal, N.Y.: Vestal Press, 1961); Arthur W. J. G. Ord-Hume, Pianola: The History of the Self-Playing Piano (London: George Allen & Unwin, 1984).